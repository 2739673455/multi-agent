{"instance_id": "solar_panel_1", "selected_database": "solar_panel", "query": "How likely is the 'solar plant west davidport' (matching the name regardless of case) to be down when we need it? Give me its system unavailability score, just the number, to four decimal points.", "normal_query": "For the solar plant labeled 'solar plant west davidport' (case-insensitive match), calculate its system unavailability. Display the result as a scalar value, rounded to 4 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT ROUND(CAST(om.\"mttrh\" / (om.\"mtbfh\" + om.\"mttrh\") AS numeric), 4)\nFROM operational_metrics om\nJOIN plant_record pr ON om.\"snapops\" = pr.\"snapkey\"\nJOIN plants p ON pr.\"sitetie\" = p.\"sitekey\"\nWHERE LOWER(p.\"sitelabel\") = 'solar plant west davidport'\nLIMIT 1;"], "external_knowledge": [4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 4, "distinct": false, "order": false}}
{"instance_id": "solar_panel_2", "selected_database": "solar_panel", "query": "I need to know the financial hit from plants with recurring warranty issues—the ones whose warranty status is 'claimed' and have had three or more claims logged against them. Can you figure out the total lifetime revenue loss for them, but only count ones where we know their go-live date and degradation? Just assume they all have 15 years left, produce 500,000 kwh a year, and we sell the power at 12 cents. Give me the grand total.", "normal_query": "Calculate the total projected lifetime revenue loss for all plants that are flagged for Warranty Claim Risk. For this calculation, only include plants where the commissioning date and cumulative degradation are known. For the projection, assume a remaining lifetime of 15 years, an average annual energy production of 500,000 kwh, and an energy price of $0.12/kwh. Present the total loss as a single value.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH warranty_risk_plants AS (\n    SELECT \n        p.\"sitekey\",\n        p.\"goliveon\"\n    FROM plants p\n    WHERE LOWER(p.\"warrstate\") = 'claimed' AND p.\"warrclaims\" > 2\n),\nadr_calcs AS (\n    SELECT\n        wrp.\"sitekey\",\n        (ep.elec_perf_snapshot -> 'efficiency' ->> 'cumulative_deg_pct')::real / NULLIF(EXTRACT(YEAR FROM AGE(NOW(), wrp.\"goliveon\")), 0) AS adr\n    FROM warranty_risk_plants wrp\n    JOIN plant_record pr ON wrp.\"sitekey\" = pr.\"sitetie\"\n    JOIN electrical_performance ep ON pr.\"snapkey\" = ep.\"snaplink\"\n    WHERE (ep.elec_perf_snapshot -> 'efficiency' ->> 'cumulative_deg_pct') IS NOT NULL AND wrp.\"goliveon\" IS NOT NULL\n)\nSELECT \n    SUM(ac.adr * 500000 * 0.12 * 15) AS total_lifetime_revenue_loss\nFROM adr_calcs ac;"], "external_knowledge": [11, 28, 7], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_3", "selected_database": "solar_panel", "query": "If we could magically cool the panels for snapshot pv945724 down to 25 degrees celsius, what would its power output be? Give me the temperature-corrected performance in watts, with two decimal points.", "normal_query": "For the snapshot 'pv945724', calculate the temperature-corrected performance. Use a reference temperature of 25°c. Display the result in watts, rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH thermal_loss AS (\n    SELECT\n        (((env_snapshot -> 'temperatures' ->> 'cell_temp_c')::real - 25) * ABS(pm.tempcoeff)) AS loss_factor,\n        (elec_perf_snapshot -> 'power' ->> 'power_now_w')::real AS actual_power\n    FROM electrical_performance ep\n    JOIN plant_record pr ON ep.snaplink = pr.snapkey\n    JOIN plants p ON pr.sitetie = p.sitekey\n    JOIN panel_models pm ON p.modhook = pm.modkey\n    JOIN environmental_conditions ec ON pr.snapkey = ec.snapref\n    WHERE LOWER(ep.snaplink) = 'pv945724'\n)\nSELECT ROUND(CAST(actual_power / (1 - loss_factor) AS numeric), 2)\nFROM thermal_loss;"], "external_knowledge": [10, 3], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "solar_panel_4", "selected_database": "solar_panel", "query": "For the maintenance event pv937101, did the repair cost more than the revenue we lost during the downtime? To figure that out, you'll have to clean up the revenue loss text by stripping out any '$' or ',' characters. Tell me the maintenance cost to revenue impact ratio, just the number, rounded to two decimals.", "normal_query": "What is the maintenance cost to revenue impact ratio for the snapshot 'pv937101'? The calculation requires cleaning the revenue loss text by removing dollar signs and commas to convert it to a numeric value. Calculate it and return a single numeric value rounded to 2 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT ROUND(CAST(maintcost / REPLACE(REPLACE(revloss, '$', ''), ',', '')::numeric AS numeric), 2)\nFROM operational_metrics\nWHERE snapops = 'PV937101';"], "external_knowledge": [18], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "solar_panel_5", "selected_database": "solar_panel", "query": "How many of our plants are real lemons, both losing more than a quarter of their potential power and being offline for more than one day out of every twenty? Make sure you only use records that have all the numbers needed for the math. Just give me the total count.", "normal_query": "What is the total count of plants that are classified as both an underperforming asset, meaning its performance ratio is less than three-quarters, and a chronic downtime asset, meaning its availability is below nineteen-twentieths? Only include snapshots where all data necessary for the calculations is available and valid. Return a single integer.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH asset_kpis AS (\n    SELECT \n        p.sitekey,\n        ((ep.elec_perf_snapshot -> 'energy_yield' ->> 'specific_yield_kwh_kw')::real / ((ec.env_snapshot -> 'irradiance' ->> 'poa_irr_w_m2')::real / 1000)) AS performance_ratio,\n        (1 - (om.mttrh / (om.mtbfh + om.mttrh))) AS system_availability\n    FROM plants p\n    JOIN plant_record pr ON p.sitekey = pr.sitetie\n    JOIN electrical_performance ep ON pr.snapkey = ep.snaplink\n    JOIN environmental_conditions ec ON pr.snapkey = ec.snapref\n    JOIN operational_metrics om ON pr.snapkey = om.snapops\n    WHERE \n        (ep.elec_perf_snapshot -> 'energy_yield' ->> 'specific_yield_kwh_kw') IS NOT NULL\n        AND (ec.env_snapshot -> 'irradiance' ->> 'poa_irr_w_m2') IS NOT NULL\n        AND (ec.env_snapshot -> 'irradiance' ->> 'poa_irr_w_m2')::real > 0\n        AND om.mttrh IS NOT NULL AND om.mtbfh IS NOT NULL\n)\nSELECT COUNT(DISTINCT sitekey)\nFROM asset_kpis\nWHERE performance_ratio < 0.75 AND system_availability < 0.95;"], "external_knowledge": [32, 9, 0, 33, 8, 4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_6", "selected_database": "solar_panel", "query": "Using the latest data for each plant, find the one that costs the most to run for its size, and tell me how much power it loses internally. I need the system power loss ratio for whichever plant has the biggest operational expenditure index. Give me the number to 4 decimal places, and only consider plants and snapshots with all the necessary and valid data to make the calculation crash-proof.", "normal_query": "For the plant with the highest operational expenditure index based on its most recent snapshot, what is its system power loss ratio, presented to 4 decimal places? Only plants with a known, non-zero power capacity and snapshots with known power values should be considered, and the logic must prevent division-by-zero errors.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH latest_snapshots AS (\n    SELECT sitetie, MAX(snapts) as max_ts\n    FROM plant_record\n    GROUP BY sitetie\n),\nplant_kpis AS (\n    SELECT\n        p.sitekey,\n        (om.maintcost + om.cleancost + om.replcost) / NULLIF(p.cap_mw * 1000, 0) AS oei,\n        (ep.elec_perf_snapshot -> 'power' ->> 'power_loss_w')::numeric / NULLIF(((ep.elec_perf_snapshot -> 'power' ->> 'power_now_w')::numeric + (ep.elec_perf_snapshot -> 'power' ->> 'power_loss_w')::numeric), 0) AS system_power_loss_ratio\n    FROM plants p\n    JOIN plant_record pr ON p.sitekey = pr.sitetie\n    JOIN latest_snapshots ls ON pr.sitetie = ls.sitetie AND pr.snapts = ls.max_ts\n    JOIN operational_metrics om ON pr.snapkey = om.snapops\n    JOIN electrical_performance ep ON pr.snapkey = ep.snaplink\n    WHERE p.cap_mw IS NOT NULL AND p.cap_mw > 0\n      AND (ep.elec_perf_snapshot -> 'power' ->> 'power_now_w') IS NOT NULL\n      AND (ep.elec_perf_snapshot -> 'power' ->> 'power_loss_w') IS NOT NULL\n)\nSELECT ROUND(CAST(system_power_loss_ratio AS numeric), 4)\nFROM plant_kpis\nWHERE oei IS NOT NULL\nORDER BY oei DESC\nLIMIT 1;"], "external_knowledge": [34, 6, 13], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "solar_panel_7", "selected_database": "solar_panel", "query": "When our panel busbars are as corroded as they can get, how much does the quality drop? Calculate the average fill factor degradation for all panels in the worst category for corrosion (regardless of case), but only use data where we have both a before and after fill factor. Give me the result to 3 decimal places.", "normal_query": "What is the average fill factor degradation for panels where the busbar corrosion has reached the highest level of severity (case-insensitive)? Only include snapshots where both initial and current fill factors are known. Display the result to 3 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    ROUND(CAST(AVG((ep.elec_perf_snapshot -> 'iv_curve' ->> 'fill_factor_initial')::real - (ep.elec_perf_snapshot -> 'iv_curve' ->> 'fill_factor_now')::real) AS numeric), 3)\nFROM electrical_performance ep\nJOIN mechanical_condition mc ON ep.snaplink = mc.snapmk\nWHERE LOWER(mc.mech_health_snapshot -> 'module_surface' ->> 'busbar_corrosion') = 'severe'\n  AND (ep.elec_perf_snapshot -> 'iv_curve' ->> 'fill_factor_initial') IS NOT NULL \n  AND (ep.elec_perf_snapshot -> 'iv_curve' ->> 'fill_factor_now') IS NOT NULL;"], "external_knowledge": [5, 46], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": false}}
{"instance_id": "solar_panel_8", "selected_database": "solar_panel", "query": "When a plant with hjt panels breaks, what's the average cost to fix it? Calculate the mean repair cost for those plants (matching 'hjt' regardless of case), assuming they've been running for two years straight and have a valid, positive mtbf record. Give me the final number, rounded to a whole dollar.", "normal_query": "Determine the mean repair cost for plants using the 'hjt' panel type (case-insensitive), assuming a total operational time of 2 years (17520 hours). Only include snapshots with a known and positive mtbf for the calculation. Provide the result rounded to the nearest dollar.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    ROUND(CAST(AVG(om.maintcost / (17520 / om.mtbfh)) AS numeric))\nFROM operational_metrics om\nJOIN plant_record pr ON om.snapops = pr.snapkey\nJOIN plants p ON pr.sitetie = p.sitekey\nJOIN panel_models pm ON p.modhook = pm.modkey\nWHERE LOWER(pm.pnlkind) = 'hjt' \n  AND om.mtbfh IS NOT NULL \n  AND om.mtbfh > 0;"], "external_knowledge": [19], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 0, "distinct": false, "order": false}}
{"instance_id": "solar_panel_9", "selected_database": "solar_panel", "query": "When our electrical systems fail, how much money do we lose? Add up all the revenue loss from every incident with an 'electrical integrity failure', making sure to strip the dollar signs and commas from the text to get the total.", "normal_query": "What is the total revenue loss for snapshots where there is an electrical integrity failure? To perform the sum, the revenue loss text must be cleaned by removing dollar signs and commas. Sum up the cleaned revenue loss for these records.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT SUM(REPLACE(REPLACE(om.revloss, '$', ''), ',', '')::numeric)\nFROM operational_metrics om\nJOIN mechanical_condition mc ON om.snapops = mc.snapmk\nWHERE LOWER(mc.mech_health_snapshot -> 'electrical_integrity' ->> 'grounding_status') = 'failed'\nOR LOWER(mc.mech_health_snapshot->'electrical_integrity'->>'bypass_diode_status') != 'normal';"], "external_knowledge": [30], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_10", "selected_database": "solar_panel", "query": "After accounting for all the internal power drains, what's the actual juice each plant is sending to the grid right now? Only using snapshots where we know both the power loss and current output, and their combined total isn't zero, give me a list of plant names and their latest effective power output, rounded to two decimal places, with the most powerful plant at the top.", "normal_query": "For each site, calculate the effective power output using the most recent snapshot. Only include snapshots where both power loss and current power output are known, and their sum is not zero to prevent calculation errors. Display the site label and the calculated power in a table, sorted by the effective power in descending order. Show the result to 2 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH latest_snapshots AS (\n    SELECT sitetie, MAX(snapts) as max_ts\n    FROM plant_record\n    GROUP BY sitetie\n),\nloss_ratio AS (\n    SELECT \n        pr.sitetie,\n        (ep.elec_perf_snapshot -> 'power' ->> 'power_loss_w')::numeric / \n            NULLIF(((ep.elec_perf_snapshot -> 'power' ->> 'power_now_w')::numeric + \n            (ep.elec_perf_snapshot -> 'power' ->> 'power_loss_w')::numeric), 0) AS splr,\n        (ep.elec_perf_snapshot -> 'power' ->> 'power_now_w')::numeric + (ep.elec_perf_snapshot -> 'power' ->> 'power_loss_w')::numeric AS gross_power\n    FROM electrical_performance ep\n    JOIN plant_record pr ON ep.snaplink = pr.snapkey\n    JOIN latest_snapshots ls ON pr.sitetie = ls.sitetie AND pr.snapts = ls.max_ts\n    WHERE (ep.elec_perf_snapshot -> 'power' ->> 'power_loss_w') IS NOT NULL \n      AND (ep.elec_perf_snapshot -> 'power' ->> 'power_now_w') IS NOT NULL\n)\nSELECT \n    p.sitelabel,\n    ROUND(CAST(lr.gross_power * (1 - lr.splr) AS numeric), 2) AS effective_power\nFROM plants p\nJOIN loss_ratio lr ON p.sitekey = lr.sitetie\nORDER BY effective_power DESC;"], "external_knowledge": [16, 13], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "solar_panel_11", "selected_database": "solar_panel", "query": "For the plants that are aging terribly—meaning their performance drops by more than 0.5% a year—how long does it typically take to fix them? I need the average mean-time-to-repair for these 'accelerated aging assets'. The age calculation needs to be safe for new plants. Give me the answer in hours, rounded to two decimal places.", "normal_query": "Find the average mean time to repair for all plants classified as accelerated aging assets, defined as those with an Annual Degradation Rate greater than 0.5%. The calculation for the degradation rate must handle cases where the plant's age is zero. Round to 2 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH accelerated_aging AS (\n    SELECT DISTINCT p.sitekey\n    FROM plants p\n    JOIN plant_record pr ON p.sitekey = pr.sitetie\n    JOIN electrical_performance ep ON pr.snapkey = ep.snaplink\n    JOIN mechanical_condition mc ON pr.snapkey = mc.snapmk\n    WHERE \n        -- Major Module Degradation criteria\n        LOWER(mc.mech_health_snapshot -> 'module_surface' ->> 'busbar_corrosion') = 'severe' AND\n        LOWER(mc.mech_health_snapshot -> 'module_surface' ->> 'delamination_status') = 'major' AND\n        (mc.mech_health_snapshot -> 'module_surface' ->> 'microcrack_count')::integer > 5 AND\n        -- High Annual Degradation Rate criteria\n        p.goliveon IS NOT NULL AND\n        ((ep.elec_perf_snapshot -> 'efficiency' ->> 'cumulative_deg_pct')::real / NULLIF(EXTRACT(YEAR FROM AGE(NOW(), p.goliveon)), 0)) > 0.5\n)\nSELECT ROUND(CAST(AVG(om.mttrh) AS numeric), 2)\nFROM operational_metrics om\nJOIN plant_record pr ON om.snapops = pr.snapkey\nWHERE pr.sitetie IN (SELECT sitekey FROM accelerated_aging);"], "external_knowledge": [36, 7, 31], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "solar_panel_12", "selected_database": "solar_panel", "query": "How many times have our panels gotten so dirty that they're losing more than three-twentieths of their potential energy? Just give me the total count.", "normal_query": "Count the number of snapshots where the power loss from soiling means that for every 200 watts of potential power, more than 30 watts are lost. Return a single integer value.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT COUNT(*)\nFROM electrical_performance\nWHERE (elec_perf_snapshot -> 'efficiency' ->> 'soil_loss_pct')::real > 15;"], "external_knowledge": [26, 2], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_13", "selected_database": "solar_panel", "query": "Which of our plants are a recurring headache for warranty claims, with more than just a couple of filings? I need a list of sites whose status is 'claimed' (regardless of case). Show their names and how many claims they've had, from most to least.", "normal_query": "List all plants where the number of warranty claims exceeds the typical initial one or two filings, and their warranty status is 'claimed' (case-insensitive). Show the site label and the number of warranty claims. Sort by the number of claims in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    p.\"sitelabel\",\n    p.\"warrclaims\"\nFROM plants p\nWHERE LOWER(p.\"warrstate\") = 'claimed' AND p.\"warrclaims\" > 2\nORDER BY p.\"warrclaims\" DESC;"], "external_knowledge": [28], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "solar_panel_14", "selected_database": "solar_panel", "query": "Among our plants in the toughest, highest-risk locations, what's the worst we've seen dirt and grime impact performance? I need the highest soiling loss index from any site that's in that top risk category. Give me the percentage.", "normal_query": "What is the highest soiling loss index recorded for a plant that is located in one of our designated top-tier environmental risk zones (case-insensitive)? Return the value as a percentage.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    MAX(CAST(ep.elec_perf_snapshot->'efficiency'->>'soil_loss_pct' AS REAL))\nFROM electrical_performance ep\nJOIN plant_record pr ON ep.\"snaplink\" = pr.\"snapkey\"\nJOIN plants p ON pr.\"sitetie\" = p.\"sitekey\"\nWHERE LOWER(p.\"envtag\") = 'high';"], "external_knowledge": [2, 27], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_15", "selected_database": "solar_panel", "query": "Let's get a financial forecast for our worst panels, the ones that degrade so fast they'll lose over 14% of their power in 20 years. What's the total projected revenue loss over their remaining 15-year lifespan? Base the calculation on a standard 400,000 mwh annual output and a sale price of $50 per mwh.", "normal_query": "What is the total lifetime revenue loss projection for all plants using panel models that are projected to lose more than 14% of their output over a 20-year lifespan? Assume an average annual energy production of 400,000 mwh, an energy price of $50/mwh, and a remaining lifetime of 15 years for all plants.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH high_degradation_models AS (\n    SELECT \"modkey\", \"degyrrate\" FROM panel_models WHERE \"degyrrate\" > 0.7\n), plant_adr AS (\n    SELECT \n        p.\"sitekey\", \n        hdm.degyrrate / 100 AS adr\n    FROM plants p\n    JOIN high_degradation_models hdm ON p.\"modhook\" = hdm.\"modkey\"\n)\nSELECT \n    SUM(pa.adr * 400000 * 50 * 15)\nFROM plant_adr pa;"], "external_knowledge": [7, 11, 20], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_16", "selected_database": "solar_panel", "query": "How much are the different types of panels losing their voltage punch over time? I need you to group by the panel technology, making sure to ignore case, and then figure out the average voltage degradation factor for each. But hey, only use data where we actually have a valid 'before' and 'after' voltage to compare, and make sure the starting voltage isn't zero. List the panel types and their average voltage loss, with the worst ones first.", "normal_query": "For each distinct panel model type, calculate the average voltage degradation factor. This calculation should only use snapshots that contain all the necessary voltage data and where the initial voltage reading is a positive number. The panel type should be converted to lowercase before grouping. Display the panel kind and the average degradation factor, sorted by the factor in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    LOWER(pm.\"pnlkind\") as pnlkind,\n    AVG((CAST(ep.elec_perf_snapshot->'iv_curve'->>'vmp_initial_v' AS REAL) - CAST(ep.elec_perf_snapshot->'iv_curve'->>'vmp_now_v' AS REAL)) / CAST(ep.elec_perf_snapshot->'iv_curve'->>'vmp_initial_v' AS REAL))\nFROM electrical_performance ep\nJOIN plant_record pr ON ep.\"snaplink\" = pr.\"snapkey\"\nJOIN plants p ON pr.\"sitetie\" = p.\"sitekey\"\nJOIN panel_models pm ON p.\"modhook\" = pm.\"modkey\"\nWHERE \n    ep.elec_perf_snapshot->'iv_curve'->>'vmp_initial_v' IS NOT NULL \n    AND ep.elec_perf_snapshot->'iv_curve'->>'vmp_now_v' IS NOT NULL\n    AND CAST(ep.elec_perf_snapshot->'iv_curve'->>'vmp_initial_v' AS REAL) > 0\nGROUP BY LOWER(pm.\"pnlkind\")\nORDER BY AVG((CAST(ep.elec_perf_snapshot->'iv_curve'->>'vmp_initial_v' AS REAL) - CAST(ep.elec_perf_snapshot->'iv_curve'->>'vmp_now_v' AS REAL)) / CAST(ep.elec_perf_snapshot->'iv_curve'->>'vmp_initial_v' AS REAL)) DESC;"], "external_knowledge": [14], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": true, "order": true}}
{"instance_id": "solar_panel_17", "selected_database": "solar_panel", "query": "For the machines that are down more than one day in a 20-day period, what's the average price tag on a single repair? To calculate the mean repair cost, you'll need to figure out how long each machine has been running. Only use data where the mtbf and service time are positive.", "normal_query": "What is the average mean repair cost for assets that are offline more than 5% of the time? The calculation requires the total time in service, which must be derived from the snapshot and go-live dates, and only include snapshots where mtbf and total hours are positive.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH chronic_downtime_assets AS (\n    SELECT \n        snapops\n    FROM operational_metrics\n    WHERE (1 - (mttrh / (mtbfh + mttrh))) < 0.95 AND mttrh IS NOT NULL AND mtbfh IS NOT NULL AND (mtbfh + mttrh) > 0\n), time_in_service AS (\n    SELECT\n        pr.snapkey,\n        EXTRACT(EPOCH FROM (pr.snapts - p.goliveon))/3600 AS total_hours\n    FROM plant_record pr\n    JOIN plants p on pr.sitetie = p.sitekey\n)\nSELECT \n    AVG(om.maintcost / (tis.total_hours / om.mtbfh))\nFROM operational_metrics om\nJOIN time_in_service tis ON om.snapops = tis.snapkey\nWHERE om.snapops IN (SELECT snapops FROM chronic_downtime_assets) AND tis.total_hours > 0 AND om.mtbfh > 0;"], "external_knowledge": [4, 8, 19, 33], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_18", "selected_database": "solar_panel", "query": "How many of our plants have a major electrical issue right now? I'm talking about situations where the grounding is shot or the bypass diodes are not running in their normal state. Just give me a count of the unique plants with these problems, and don't worry about the case of the status text.", "normal_query": "Count the number of distinct plants where the electrical integrity is compromised, indicated by either a complete failure of the grounding system or a bypass diode status that is anything other than nominal (checks performed case-insensitively).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT COUNT(DISTINCT pr.sitetie)\nFROM mechanical_condition mc\nJOIN plant_record pr ON mc.snapmk = pr.snapkey\nWHERE \n    LOWER(mc.mech_health_snapshot->'electrical_integrity'->>'grounding_status') = 'failed'\n    OR LOWER(mc.mech_health_snapshot->'electrical_integrity'->>'bypass_diode_status') != 'normal';"], "external_knowledge": [30], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": true, "order": false}}
{"instance_id": "solar_panel_19", "selected_database": "solar_panel", "query": "After accounting for all the power being lost inside the system, what was the actual usable power output for snapshot 'pv945724'? Give me the final number in watts.", "normal_query": "What is the effective power output for snapshot 'pv945724'? Calculate it and return the value in watts.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH loss_ratio AS (\n    SELECT \n        CAST(elec_perf_snapshot->'power'->>'power_loss_w' AS REAL) / \n        (CAST(elec_perf_snapshot->'power'->>'power_now_w' AS REAL) + CAST(elec_perf_snapshot->'power'->>'power_loss_w' AS REAL)) as r_loss,\n        CAST(elec_perf_snapshot->'power'->>'power_now_w' AS REAL) + CAST(elec_perf_snapshot->'power'->>'power_loss_w' AS REAL) as p_gross\n    FROM electrical_performance\n    WHERE snaplink = 'PV945724'\n)\nSELECT \n    lr.p_gross * (1 - lr.r_loss)\nFROM loss_ratio lr;"], "external_knowledge": [13, 16], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_20", "selected_database": "solar_panel", "query": "For the panels specifically made by longi (regardless of case), how much has their current output dropped on average? To get a good average, please only use records where you have a valid, positive starting current to compare against. Calculate the mean current degradation factor across all of them.", "normal_query": "What is the average current degradation factor for all panel models from the manufacturer 'longi' (case-insensitive)? For an accurate average, include only snapshots that have a valid, positive initial current reading to compare against the current reading.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    AVG((CAST(ep.elec_perf_snapshot->'iv_curve'->>'imp_initial_a' AS REAL) - CAST(ep.elec_perf_snapshot->'iv_curve'->>'imp_now_a' AS REAL)) / CAST(ep.elec_perf_snapshot->'iv_curve'->>'imp_initial_a' AS REAL))\nFROM electrical_performance ep\nJOIN plant_record pr ON ep.snaplink = pr.snapkey\nJOIN plants p ON pr.sitetie = p.sitekey\nJOIN panel_models pm ON p.modhook = pm.modkey\nWHERE \n    LOWER(pm.makertag) = 'longi'\n    AND ep.elec_perf_snapshot->'iv_curve'->>'imp_initial_a' IS NOT NULL\n    AND ep.elec_perf_snapshot->'iv_curve'->>'imp_now_a' IS NOT NULL\n    AND CAST(ep.elec_perf_snapshot->'iv_curve'->>'imp_initial_a' AS REAL) > 0;"], "external_knowledge": [15], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_M_1", "selected_database": "solar_panel", "query": "Let's make a special table for problems that need immediate attention, call it `high_risk_alerts`. It needs to store the snapshot id, the alert status, both maintenance and replacement priorities, and when it happened. After creating it, fill it with any alert that's so serious we'd need to send our top people out or order a new part right away. Make sure to find these alerts regardless of case. Also, make sure the snapshot id links back to the main plant record table.", "normal_query": "Create a new table `high_risk_alerts` with columns for the snapshot key, alert state, maintenance priority, replacement priority, and the timestamp of the snapshot. Then, populate it by inserting records for any issue that would require either dispatching a senior engineer or ordering a replacement part before the end of the day (checks must be case-insensitive). Add a foreign key constraint on the snapshot key referencing `plant_record`.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE \"high_risk_alerts\" (\n    \"snap_key\" TEXT NOT NULL,\n    \"alert_state\" TEXT,\n    \"maint_prio\" TEXT,\n    \"repl_prio\" TEXT,\n    \"snap_ts\" TIMESTAMP WITHOUT TIME ZONE,\n    FOREIGN KEY (\"snap_key\") REFERENCES plant_record(\"snapkey\")\n);\n-- Note: The prompt asks to create a table AND populate it. This requires an INSERT statement after creation.\nINSERT INTO high_risk_alerts (snap_key, alert_state, maint_prio, repl_prio, snap_ts)\nSELECT \n    a.\"snapalrt\",\n    a.\"alrtstate\",\n    a.\"maintprio\",\n    a.\"replprio\",\n    pr.\"snapts\"\nFROM alert a\nJOIN plant_record pr ON a.\"snapalrt\" = pr.\"snapkey\"\nWHERE LOWER(a.\"maintprio\") = 'high' OR LOWER(a.\"replprio\") = 'high';"], "external_knowledge": [24, 25], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Cleanup\n    execute_queries('DROP TABLE IF EXISTS high_risk_alerts;', db_name, conn)\n    # Run creation and insertion\n    execute_queries(pred_sqls, db_name, conn)\n    # Check if table was created\n    table_check = execute_queries(\"SELECT to_regclass('high_risk_alerts');\", db_name, conn)[0]\n    assert table_check[0][0] is not None, 'Table high_risk_alerts was not created.'\n    # Check data integrity\n    data_check = execute_queries(\"SELECT COUNT(*) FROM high_risk_alerts WHERE LOWER(maint_prio) = 'high' OR LOWER(repl_prio) = 'immediate';\", db_name, conn)[0][0][0]\n    total_high_alerts = execute_queries(\"SELECT COUNT(*) FROM alert WHERE LOWER(maintprio) = 'high' OR LOWER(replprio) = 'immediate';\", db_name, conn)[0][0][0]\n    assert data_check == total_high_alerts, 'The number of high priority alerts does not match.'\n    # Final cleanup\n    execute_queries('DROP TABLE IF EXISTS high_risk_alerts;', db_name, conn)"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_M_2", "selected_database": "solar_panel", "query": "I need a handy summary of how our plants are doing right now. Can you create a view called `v_plant_performance_overview`? It should show the plant's name, when the data was taken, how much power it was making, how much sunlight was hitting it, and the cell temperature. Make sure it only shows the very latest data we have for each plant.", "normal_query": "Create a view named `v_plant_performance_overview`. This view should join data from the `plants`, `electrical_performance`, and `environmental_conditions` tables. It must display the site label, snapshot timestamp, power output, plane-of-array irradiance, and cell temperature for the most recent snapshot of each plant.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE VIEW v_plant_performance_overview AS\nWITH latest_snapshots AS (\n    SELECT \n        pr.sitetie,\n        max(pr.snapts) AS max_snapts\n    FROM plant_record pr\n    GROUP BY pr.sitetie\n)\nSELECT \n    p.sitelabel,\n    pr.snapts,\n    ep.elec_perf_snapshot->'power'->>'power_now_w' AS power_output_w,\n    ec.env_snapshot->'irradiance'->>'poa_irr_w_m2' AS poa_irradiance_w_m2,\n    ec.env_snapshot->'temperatures'->>'cell_temp_c' AS cell_temperature_c\nFROM plants p\nJOIN latest_snapshots ls ON p.sitekey = ls.sitetie\nJOIN plant_record pr ON ls.sitetie = pr.sitetie AND ls.max_snapts = pr.snapts\nJOIN electrical_performance ep ON pr.snapkey = ep.snaplink\nJOIN environmental_conditions ec ON pr.snapkey = ec.snapref;"], "external_knowledge": [], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Cleanup\n    execute_queries('DROP VIEW IF EXISTS v_plant_performance_overview;', db_name, conn)\n    # Run creation\n    execute_queries(pred_sqls, db_name, conn)\n    # Check if view was created\n    view_check = execute_queries(\"SELECT to_regclass('v_plant_performance_overview');\", db_name, conn)[0]\n    assert view_check[0][0] is not None, 'View v_plant_performance_overview was not created.'\n    # Check row count - should be equal to distinct number of plants\n    view_count = execute_queries('SELECT COUNT(*) FROM v_plant_performance_overview;', db_name, conn)[0][0][0]\n    plant_count = execute_queries('SELECT COUNT(DISTINCT sitetie) FROM plant_record;', db_name, conn)[0][0][0]\n    assert view_count == plant_count, 'View should have one row per plant.'\n    # Final cleanup\n    execute_queries('DROP VIEW IF EXISTS v_plant_performance_overview;', db_name, conn)"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_M_3", "selected_database": "solar_panel", "query": "I need a faster way to see yearly energy production. Create a materialized view called `mv_yearly_plant_yield`. It should calculate the total kilowatt-hours produced by each plant for each year and store it, but only use records that actually have a yield value. The view should have the plant's name, the year, and the total yield.", "normal_query": "Create a materialized view named `mv_yearly_plant_yield` which summarizes the total energy yield for each plant for each year. It should include the site label, the year, and the total energy yield in kwh, only including records where the energy yield is not null.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE MATERIALIZED VIEW mv_yearly_plant_yield AS\nSELECT \n    p.sitelabel,\n    EXTRACT(YEAR FROM pr.snapts) AS yield_year,\n    SUM(CAST(ep.elec_perf_snapshot->'energy_yield'->>'energy_yield_kwh' AS REAL)) as total_yield_kwh\nFROM plants p\nJOIN plant_record pr ON p.sitekey = pr.sitetie\nJOIN electrical_performance ep ON pr.snapkey = ep.snaplink\nWHERE ep.elec_perf_snapshot->'energy_yield'->>'energy_yield_kwh' IS NOT NULL\nGROUP BY p.sitelabel, EXTRACT(YEAR FROM pr.snapts);"], "external_knowledge": [114], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    execute_queries('DROP MATERIALIZED VIEW IF EXISTS mv_yearly_plant_yield;', db_name, conn)\n    execute_queries(pred_sqls, db_name, conn)\n    # Check view creation\n    view_check = execute_queries(\"SELECT to_regclass('mv_yearly_plant_yield');\", db_name, conn)[0]\n    assert view_check[0][0] is not None, 'Materialized view mv_yearly_plant_yield was not created.'\n    # Check some data\n    data_check = execute_queries(\"SELECT COUNT(*) FROM mv_yearly_plant_yield;\", db_name, conn)[0][0][0]\n    assert data_check > 0, 'Materialized view is empty.'\n    execute_queries('DROP MATERIALIZED VIEW IF EXISTS mv_yearly_plant_yield;', db_name, conn)"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_M_4", "selected_database": "solar_panel", "query": "Let's build a cleaning schedule table. Call it `panel_cleaning_schedule`. It needs a unique ID for each entry, the plant's ID, the date it was last cleaned, and the date it's due next. Then, fill it up for all our plants using the latest cleaning info from their mechanical health reports to calculate the next due date.", "normal_query": "Create a new table `panel_cleaning_schedule` with columns `schedule_id` (Primary Key, Serial), `site_key` (Foreign Key to plants), `last_cleaned_date` (Date), and `next_cleaning_due` (Date). Populate it for all plants, setting `last_cleaned_date` to the most recent `last_clean_date` from `mechanical_condition` and `next_cleaning_due` by adding the `cleaning_cycle_days` to that date.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE panel_cleaning_schedule (\n    schedule_id SERIAL PRIMARY KEY,\n    site_key TEXT REFERENCES plants(sitekey),\n    last_cleaned_date DATE,\n    next_cleaning_due DATE\n);\n\nINSERT INTO panel_cleaning_schedule (site_key, last_cleaned_date, next_cleaning_due)\nWITH last_clean_data AS (\n    SELECT \n        pr.sitetie, \n        (mc.mech_health_snapshot->'mount_cleaning'->>'last_clean_date')::date as last_clean_date,\n        (mc.mech_health_snapshot->'mount_cleaning'->>'cleaning_cycle_days')::integer as cycle_days,\n        ROW_NUMBER() OVER(PARTITION BY pr.sitetie ORDER BY (mc.mech_health_snapshot->'mount_cleaning'->>'last_clean_date')::date DESC) as rn\n    FROM mechanical_condition mc\n    JOIN plant_record pr ON mc.snapmk = pr.snapkey\n    WHERE mc.mech_health_snapshot->'mount_cleaning'->>'last_clean_date' IS NOT NULL\n)\nSELECT \n    lcd.sitetie,\n    lcd.last_clean_date,\n    lcd.last_clean_date + lcd.cycle_days\nFROM last_clean_data lcd\nWHERE lcd.rn = 1;"], "external_knowledge": [154, 155], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    execute_queries('DROP TABLE IF EXISTS panel_cleaning_schedule;', db_name, conn)\n    execute_queries(pred_sqls, db_name, conn)\n    # Check table creation\n    table_check = execute_queries(\"SELECT to_regclass('panel_cleaning_schedule');\", db_name, conn)[0]\n    assert table_check[0][0] is not None, 'Table panel_cleaning_schedule was not created.'\n    # Check data population\n    count_check = execute_queries('SELECT COUNT(*) FROM panel_cleaning_schedule;', db_name, conn)[0][0][0]\n    assert count_check > 0, 'Table was not populated.'\n    # Check calculation for a specific plant\n    sample_check = execute_queries(\"SELECT next_cleaning_due - last_cleaned_date FROM panel_cleaning_schedule WHERE site_key = 'SP9227';\", db_name, conn)[0][0][0]\n    assert sample_check == 38, 'Date calculation is incorrect for SP9227.'\n    execute_queries('DROP TABLE IF EXISTS panel_cleaning_schedule;', db_name, conn)"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_M_5", "selected_database": "solar_panel", "query": "I want a tool to quickly tell me how old a plant is. Can you create a function called `get_plant_age`? You give it a plant's ID, and it should spit out its current age in years.", "normal_query": "Create a function `get_plant_age` that takes a site key as input and returns the age of the plant in years (as a real number) based on its go-live date and the current date.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION get_plant_age(p_sitekey TEXT)\nRETURNS real AS $$\nDECLARE\n    plant_age real;\nBEGIN\n    SELECT EXTRACT(YEAR FROM AGE(CURRENT_DATE, goliveon))\n    INTO plant_age\n    FROM plants\n    WHERE sitekey = p_sitekey;\n    RETURN plant_age;\nEND;\n$$ LANGUAGE plpgsql;"], "external_knowledge": [], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Cleanup\n    execute_queries('DROP FUNCTION IF EXISTS get_plant_age(text);', db_name, conn)\n    # Create the function\n    execute_queries(pred_sqls, db_name, conn)\n    # Test the function\n    # Note: This requires knowing the goliveon date for SP9227 from the data (2018-02-28)\n    # and the current date of the test environment.\n    # For this static test, we assume current date is around mid-2025\n    age_check = execute_queries(\"SELECT get_plant_age('SP9227');\", db_name, conn)[0][0][0]\n    assert 6.0 < age_check < 8.0, f'Function returned unexpected age: {age_check}'\n    # Cleanup\n    execute_queries('DROP FUNCTION IF EXISTS get_plant_age(text);', db_name, conn)"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_M_6", "selected_database": "solar_panel", "query": "I want a 'hall of fame' for extreme weather events at our plants. Can you make a view called `v_environmental_extremes`? It should find the highest ambient temperature, strongest wind speed, and most intense uv index ever recorded across all sites. For each of these records, show which plant it happened at, what the record-breaking value was, and when it happened.", "normal_query": "Create a view `v_environmental_extremes` which, for each environmental variable, shows the plant site label, the value, and the timestamp for the all-time maximum recorded value. Include ambient temperature, wind speed, and uv index.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE VIEW v_environmental_extremes AS\nWITH all_data AS (\n    SELECT \n        pr.sitetie,\n        p.sitelabel,\n        pr.snapts,\n        CAST(ec.env_snapshot->'temperatures'->>'ambient_temp_c' AS REAL) AS ambient_temp,\n        CAST(ec.env_snapshot->'wind_rain_snow'->>'wind_speed_m_s' AS REAL) AS wind_speed,\n        CAST(ec.env_snapshot->'atmospheric'->>'uv_index' AS REAL) AS uv_index\n    FROM environmental_conditions ec\n    JOIN plant_record pr ON ec.snapref = pr.snapkey\n    JOIN plants p ON pr.sitetie = p.sitekey\n), max_temp AS (\n    SELECT sitelabel, snapts, ambient_temp as value, 'Max Temp (C)' as metric FROM all_data WHERE ambient_temp = (SELECT MAX(ambient_temp) FROM all_data)\n), max_wind AS (\n    SELECT sitelabel, snapts, wind_speed as value, 'Max Wind (m/s)' as metric FROM all_data WHERE wind_speed = (SELECT MAX(wind_speed) FROM all_data)\n), max_uv AS (\n    SELECT sitelabel, snapts, uv_index as value, 'Max UV Index' as metric FROM all_data WHERE uv_index = (SELECT MAX(uv_index) FROM all_data)\n)\nSELECT * FROM max_temp\nUNION ALL\nSELECT * FROM max_wind\nUNION ALL\nSELECT * FROM max_uv;"], "external_knowledge": [], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    execute_queries('DROP VIEW IF EXISTS v_environmental_extremes;', db_name, conn)\n    execute_queries(pred_sqls, db_name, conn)\n\n    view_check = execute_queries(\"SELECT to_regclass('v_environmental_extremes');\", db_name, conn)[0]\n    assert view_check[0][0] is not None, 'View v_environmental_extremes was not created.'\n\n    data_check = execute_queries('SELECT COUNT(*) FROM v_environmental_extremes;', db_name, conn)[0][0][0]\n    assert data_check >= 3, 'View should contain at least 3 rows for the 3 metrics.'\n\n    execute_queries('DROP VIEW IF EXISTS v_environmental_extremes;', db_name, conn)"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_M_7", "selected_database": "solar_panel", "query": "Let's make a log of all our plants that aren't up to code. Create a table called `compliance_issues` with an id, the plant's id, a space for a description, and the date it was logged. After you create it, go through the main plants list and add an entry for every single one that's failed its compliance checks (ignoring case). You can just put 'Initial non-compliance record' for the description.", "normal_query": "Create a new table `compliance_issues` with columns for `issue_id`, `plant_sitekey`, `issue_description`, and `date_logged`. Then, insert a record for every plant that has failed to meet its regulatory standards, based on a case-insensitive check of its compliance flag, using the specific description 'Initial non-compliance record'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE compliance_issues (\n    issue_id SERIAL PRIMARY KEY,\n    plant_sitekey TEXT,\n    issue_description TEXT,\n    date_logged DATE DEFAULT CURRENT_DATE\n);\n\nINSERT INTO compliance_issues (plant_sitekey, issue_description)\nSELECT sitekey, 'Initial non-compliance record'\nFROM plants\nWHERE LOWER(complyflag) = 'non-compliant';"], "external_knowledge": [], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    execute_queries('DROP TABLE IF EXISTS compliance_issues;', db_name, conn)\n    execute_queries(pred_sqls, db_name, conn)\n\n    table_check = execute_queries(\"SELECT to_regclass('compliance_issues');\", db_name, conn)[0]\n    assert table_check[0][0] is not None, 'Table compliance_issues was not created.'\n\n    data_check = execute_queries('SELECT COUNT(*) FROM compliance_issues;', db_name, conn)[0][0][0]\n    non_compliant_count = execute_queries(\"SELECT COUNT(*) FROM plants WHERE complyflag = 'Non-Compliant';\", db_name, conn)[0][0][0]\n    assert data_check == non_compliant_count, 'Number of inserted records does not match non-compliant plants.'\n\n    execute_queries('DROP TABLE IF EXISTS compliance_issues;', db_name, conn)"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_M_8", "selected_database": "solar_panel", "query": "I need a new place to keep track of our plant's health stats. Can you create a table called `plant_kpi_summary`? It should have columns for the site's id, its age in years, its annual performance drop, and its uptime percentage.", "normal_query": "Create a new table named `plant_kpi_summary` to store key performance indicators. The table should include a key for the site (text, primary key), the plant's age in years (real), its annual degradation rate (real), and its system availability (real).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE plant_kpi_summary (\n    site_key TEXT PRIMARY KEY,\n    plant_age_years REAL,\n    annual_degradation_rate REAL,\n    system_availability REAL,\n    FOREIGN KEY (site_key) REFERENCES plants(sitekey)\n);"], "external_knowledge": [7, 8], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Clean up any previous runs\n    drop_table = \"DROP TABLE IF EXISTS plant_kpi_summary;\"\n    execute_queries(drop_table, db_name, conn)\n    # Execute the creation SQL\n    execute_queries(pred_sqls, db_name, conn)\n    # Check if the table was created\n    check_table_sql = \"SELECT to_regclass('plant_kpi_summary');\"\n    table_check_result = execute_queries(check_table_sql, db_name, conn)[0]\n    assert table_check_result[0][0] is not None, \"The table 'plant_kpi_summary' was not created.\"\n    # Check for columns\n    check_columns_sql = \"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'plant_kpi_summary' ORDER BY ordinal_position;\"\n    columns_result = execute_queries(check_columns_sql, db_name, conn)[0]\n    expected_columns = [('site_key', 'text'), ('plant_age_years', 'real'), ('annual_degradation_rate', 'real'), ('system_availability', 'real')]\n    assert columns_result == expected_columns, f\"Columns in 'plant_kpi_summary' are incorrect. Expected {expected_columns}, got {columns_result}.\"\n    # Clean up after test\n    execute_queries(drop_table, db_name, conn)"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_M_9", "selected_database": "solar_panel", "query": "Let's make a quick-look list of the absolute worst problems. Create a view, call it `v_critical_alerts_details`, for every alert that's got the highest possible priority for both a maintenance dispatch and a part replacement. Make sure you find them regardless of case. Show me the plant name, when it happened, and the event count.", "normal_query": "Create a view named `v_critical_alerts_details` that lists the site label, the snapshot timestamp, and the alert count for all snapshots where the issue is so severe it has been assigned the maximum priority level for both maintenance and replacement (checks performed case-insensitively).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE VIEW v_critical_alerts_details AS\nSELECT \n    p.\"sitelabel\",\n    pr.\"snapts\",\n    a.\"alrtcnt\"\nFROM alert a\nJOIN plant_record pr ON a.\"snapalrt\" = pr.\"snapkey\"\nJOIN plants p ON pr.\"sitetie\" = p.\"sitekey\"\nWHERE LOWER(a.\"maintprio\") = 'high' AND LOWER(a.\"replprio\") = 'high';"], "external_knowledge": [29, 24, 25], "test_cases": ["from datetime import date\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # DROP the view for cleanup\n    drop_view = \"DROP VIEW IF EXISTS v_critical_alerts_details;\"\n    execute_queries(drop_view, db_name, conn)\n    # pred sql result\n    execute_queries(pred_sqls, db_name, conn)\n    # check if the view was created\n    check_view_sql = \"SELECT to_regclass('v_critical_alerts_details');\"\n    view_check = execute_queries(check_view_sql, db_name, conn)[0]\n    assert view_check[0][0] is not None, \"The view 'v_critical_alerts_details' was not created.\"\n    # query the view for content\n    view_query = \"SELECT sitelabel FROM v_critical_alerts_details WHERE LOWER(sitelabel) = 'solar plant west davidport';\"\n    pred_view_data = execute_queries(view_query, db_name, conn)[0]\n    # Based on sample data for PV937101, it is a critical alert\n    assert len(pred_view_data) > 0, \"Data in the view is incorrect or missing.\"\n    assert pred_view_data[0][0] == 'Solar Plant West Davidport', 'Incorrect sitelabel found in view.'\n    # Clean up\n    execute_queries(drop_view, db_name, conn)"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "solar_panel_M_10", "selected_database": "solar_panel", "query": "I want to start logging all our repair jobs. Can you set up a new table for me called `maintenance_log`? It needs a unique id for each entry, a reference to the snapshot it's related to, the date of the repair, a description of what was done, and how much it cost. Make sure the snapshot reference actually links to a real record.", "normal_query": "Create a new table `maintenance_log` with columns `log_id` (serial primary key), `snap_reference` (text), `log_date` (date), `action_taken` (text), and `cost` (numeric(10, 2)). Add a foreign key on `snap_reference` to the `plant_record` table.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE maintenance_log (\n    log_id SERIAL PRIMARY KEY,\n    snap_reference TEXT,\n    log_date DATE,\n    action_taken TEXT,\n    cost NUMERIC(10, 2),\n    FOREIGN KEY (snap_reference) REFERENCES plant_record(snapkey)\n);"], "external_knowledge": [], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Clean up any previous runs\n    drop_table = \"DROP TABLE IF EXISTS maintenance_log;\"\n    execute_queries(drop_table, db_name, conn)\n    # Execute the creation SQL\n    execute_queries(pred_sqls, db_name, conn)\n    # Check if the table was created\n    check_table_sql = \"SELECT to_regclass('maintenance_log');\"\n    table_check_result = execute_queries(check_table_sql, db_name, conn)[0]\n    assert table_check_result[0][0] is not None, \"The table 'maintenance_log' was not created.\"\n    # Insert a test row and check it\n    insert_sql = \"INSERT INTO maintenance_log(snap_reference, action_taken, cost) VALUES ('PV937101', 'Replaced inverter', 5500.75);\"\n    execute_queries(insert_sql, db_name, conn)\n    select_sql = \"SELECT action_taken, cost FROM maintenance_log WHERE snap_reference = 'PV937101';\"\n    select_result = execute_queries(select_sql, db_name, conn)[0]\n    assert select_result[0][0] == 'Replaced inverter', 'Data insertion or retrieval failed for action_taken.'\n    assert float(select_result[0][1]) == 5500.75, 'Data insertion or retrieval failed for cost.'\n    # Clean up after test\n    execute_queries(drop_table, db_name, conn)"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "hulushows_1", "selected_database": "hulushows", "query": "Let’s check which shows have tons of content across different releases but no written description. Add up their standard content (episodes, clips, etc.) across all tiers, keep only the ones with over 500 total, and no annotations. Show each show’s ID, name, and total volume—sorted by volume, highest first.", "normal_query": "I want to identify all Incomplete High-Engagement Titles. Compute the total content volume for each title by summing up standard content quantities across all distribution records. Then check whether the title has any descriptive annotation. Can you only include titles with a high total volume (greater than 500) and no annotations? List each title's ID, name, and total content volume, sorted by volume in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH parsed_rollups AS (\n  SELECT\n    sr.srkeys AS content_key,\n    (\n      /* Ep_Vol */\n      CASE\n        WHEN (sr.contentvols->'standard_content'->>'Ep_Vol') IS NULL THEN 0\n        WHEN (sr.contentvols->'standard_content'->>'Ep_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[kK]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Ep_Vol','[^0-9\\.]','','g'))::numeric * 1000\n        WHEN (sr.contentvols->'standard_content'->>'Ep_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[mM]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Ep_Vol','[^0-9\\.]','','g'))::numeric * 1000000\n        WHEN (sr.contentvols->'standard_content'->>'Ep_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[bB]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Ep_Vol','[^0-9\\.]','','g'))::numeric * 1000000000\n        WHEN (sr.contentvols->'standard_content'->>'Ep_Vol') ~* '^[0-9]+(\\.[0-9]+)?$'\n          THEN (sr.contentvols->'standard_content'->>'Ep_Vol')::numeric\n        ELSE COALESCE(NULLIF(regexp_replace(sr.contentvols->'standard_content'->>'Ep_Vol','[^0-9]','','g'),''),'0')::numeric\n      END\n      +\n      /* Clip_Vol */\n      CASE\n        WHEN (sr.contentvols->'standard_content'->>'Clip_Vol') IS NULL THEN 0\n        WHEN (sr.contentvols->'standard_content'->>'Clip_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[kK]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Clip_Vol','[^0-9\\.]','','g'))::numeric * 1000\n        WHEN (sr.contentvols->'standard_content'->>'Clip_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[mM]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Clip_Vol','[^0-9\\.]','','g'))::numeric * 1000000\n        WHEN (sr.contentvols->'standard_content'->>'Clip_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[bB]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Clip_Vol','[^0-9\\.]','','g'))::numeric * 1000000000\n        WHEN (sr.contentvols->'standard_content'->>'Clip_Vol') ~* '^[0-9]+(\\.[0-9]+)?$'\n          THEN (sr.contentvols->'standard_content'->>'Clip_Vol')::numeric\n        ELSE COALESCE(NULLIF(regexp_replace(sr.contentvols->'standard_content'->>'Clip_Vol','[^0-9]','','g'),''),'0')::numeric\n      END\n      +\n      /* Game_Vol */\n      CASE\n        WHEN (sr.contentvols->'standard_content'->>'Game_Vol') IS NULL THEN 0\n        WHEN (sr.contentvols->'standard_content'->>'Game_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[kK]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Game_Vol','[^0-9\\.]','','g'))::numeric * 1000\n        WHEN (sr.contentvols->'standard_content'->>'Game_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[mM]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Game_Vol','[^0-9\\.]','','g'))::numeric * 1000000\n        WHEN (sr.contentvols->'standard_content'->>'Game_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[bB]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Game_Vol','[^0-9\\.]','','g'))::numeric * 1000000000\n        WHEN (sr.contentvols->'standard_content'->>'Game_Vol') ~* '^[0-9]+(\\.[0-9]+)?$'\n          THEN (sr.contentvols->'standard_content'->>'Game_Vol')::numeric\n        ELSE COALESCE(NULLIF(regexp_replace(sr.contentvols->'standard_content'->>'Game_Vol','[^0-9]','','g'),''),'0')::numeric\n      END\n      +\n      /* Season_Vol */\n      CASE\n        WHEN (sr.contentvols->'standard_content'->>'Season_Vol') IS NULL THEN 0\n        WHEN (sr.contentvols->'standard_content'->>'Season_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[kK]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Season_Vol','[^0-9\\.]','','g'))::numeric * 1000\n        WHEN (sr.contentvols->'standard_content'->>'Season_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[mM]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Season_Vol','[^0-9\\.]','','g'))::numeric * 1000000\n        WHEN (sr.contentvols->'standard_content'->>'Season_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[bB]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Season_Vol','[^0-9\\.]','','g'))::numeric * 1000000000\n        WHEN (sr.contentvols->'standard_content'->>'Season_Vol') ~* '^[0-9]+(\\.[0-9]+)?$'\n          THEN (sr.contentvols->'standard_content'->>'Season_Vol')::numeric\n        ELSE COALESCE(NULLIF(regexp_replace(sr.contentvols->'standard_content'->>'Season_Vol','[^0-9]','','g'),''),'0')::numeric\n      END\n      +\n      /* Feature_Vol */\n      CASE\n        WHEN (sr.contentvols->'standard_content'->>'Feature_Vol') IS NULL THEN 0\n        WHEN (sr.contentvols->'standard_content'->>'Feature_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[kK]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Feature_Vol','[^0-9\\.]','','g'))::numeric * 1000\n        WHEN (sr.contentvols->'standard_content'->>'Feature_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[mM]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Feature_Vol','[^0-9\\.]','','g'))::numeric * 1000000\n        WHEN (sr.contentvols->'standard_content'->>'Feature_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[bB]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Feature_Vol','[^0-9\\.]','','g'))::numeric * 1000000000\n        WHEN (sr.contentvols->'standard_content'->>'Feature_Vol') ~* '^[0-9]+(\\.[0-9]+)?$'\n          THEN (sr.contentvols->'standard_content'->>'Feature_Vol')::numeric\n        ELSE COALESCE(NULLIF(regexp_replace(sr.contentvols->'standard_content'->>'Feature_Vol','[^0-9]','','g'),''),'0')::numeric\n      END\n      +\n      /* Trailer_Vol */\n      CASE\n        WHEN (sr.contentvols->'standard_content'->>'Trailer_Vol') IS NULL THEN 0\n        WHEN (sr.contentvols->'standard_content'->>'Trailer_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[kK]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Trailer_Vol','[^0-9\\.]','','g'))::numeric * 1000\n        WHEN (sr.contentvols->'standard_content'->>'Trailer_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[mM]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Trailer_Vol','[^0-9\\.]','','g'))::numeric * 1000000\n        WHEN (sr.contentvols->'standard_content'->>'Trailer_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[bB]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'Trailer_Vol','[^0-9\\.]','','g'))::numeric * 1000000000\n        WHEN (sr.contentvols->'standard_content'->>'Trailer_Vol') ~* '^[0-9]+(\\.[0-9]+)?$'\n          THEN (sr.contentvols->'standard_content'->>'Trailer_Vol')::numeric\n        ELSE COALESCE(NULLIF(regexp_replace(sr.contentvols->'standard_content'->>'Trailer_Vol','[^0-9]','','g'),''),'0')::numeric\n      END\n      +\n      /* FilmClip_Vol */\n      CASE\n        WHEN (sr.contentvols->'standard_content'->>'FilmClip_Vol') IS NULL THEN 0\n        WHEN (sr.contentvols->'standard_content'->>'FilmClip_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[kK]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'FilmClip_Vol','[^0-9\\.]','','g'))::numeric * 1000\n        WHEN (sr.contentvols->'standard_content'->>'FilmClip_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[mM]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'FilmClip_Vol','[^0-9\\.]','','g'))::numeric * 1000000\n        WHEN (sr.contentvols->'standard_content'->>'FilmClip_Vol') ~* '^[0-9]+(\\.[0-9]+)?\\s*[bB]$'\n          THEN (regexp_replace(sr.contentvols->'standard_content'->>'FilmClip_Vol','[^0-9\\.]','','g'))::numeric * 1000000000\n        WHEN (sr.contentvols->'standard_content'->>'FilmClip_Vol') ~* '^[0-9]+(\\.[0-9]+)?$'\n          THEN (sr.contentvols->'standard_content'->>'FilmClip_Vol')::numeric\n        ELSE COALESCE(NULLIF(regexp_replace(sr.contentvols->'standard_content'->>'FilmClip_Vol','[^0-9]','','g'),''),'0')::numeric\n      END\n    )::numeric AS std_content_sum\n  FROM show_rollups sr\n),\nagg_per_title AS (\n  SELECT\n    c.content_key,\n    c.content_title,\n    SUM(p.std_content_sum) AS total_content_volume\n  FROM parsed_rollups p\n  JOIN core c ON c.content_key = p.content_key\n  GROUP BY c.content_key, c.content_title\n),\nno_annotation AS (\n  SELECT content_key\n  FROM core\n  WHERE TRIM(COALESCE(annotations, '')) = ''\n)\nSELECT\n  a.content_key,\n  a.content_title,\n  a.total_content_volume\nFROM agg_per_title a\nJOIN no_annotation n USING (content_key)\nWHERE a.total_content_volume > 500\nORDER BY a.total_content_volume DESC;"], "external_knowledge": [63, 4, 38, 0], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_2", "selected_database": "hulushows", "query": "I want to find shows that show up in three or more different subscription tiers. For each show, can you count how many unique tiers it’s available in? First, keep the ones that are in at least three tiers, and then sort the results from the most widely distributed to the last.", "normal_query": "I want to know all Multitier Syndicated Shows. For each show with at least three tiers, show its unique identifier and the number of tiers it appears in. Sort the results by tier count in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH tier_distribution AS ( SELECT srkeys AS content_key, COUNT(DISTINCT srlinks) AS tier_count FROM show_rollups GROUP BY srkeys ) SELECT content_key, tier_count FROM tier_distribution WHERE tier_count >= 3 ORDER BY tier_count DESC;"], "external_knowledge": [41, 3], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_3", "selected_database": "hulushows", "query": "Let’s find out which titles are getting strong user scores even though they don’t have any trailers or clips. I want to look across all content and find the highest user rating among those that don’t offer any visual previews but still include a valid score. Just return that one number, rounded to 2 decimals—it tells us how well these visually sparse titles are performing.", "normal_query": "My goal is to identify the Highly Rated but Visually Empty titles in the catalog. Specifically, I want to calculate the highest user rating among all titles that have no available trailers or clips but still include valid user score data.Give me the maximum user score across these titles, rounded to 2 decimals", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH visually_empty_titles AS ( SELECT content_key, content_title, REGEXP_REPLACE(genreclass->>'User_Score', '[^0-9.]', '', 'g')::numeric AS user_score FROM core JOIN content_info USING (content_key) WHERE (mediacounts->'content_volumes'->>'Film_Clips')::int = 0 AND (mediacounts->'content_volumes'->>'Clips_Total')::int = 0 AND genreclass->>'User_Score' IS NOT NULL ) SELECT ROUND(MAX(user_score), 2) AS max_rating FROM visually_empty_titles;"], "external_knowledge": [70, 4, 30, 1], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "hulushows_4", "selected_database": "hulushows", "query": "I want to find out how long it's been since each show got any new updates. For each show, check the most recent update date. But if there's no update info, just use the launch date instead. Then, I’d like to see how many days it's been since that date, and treat that as the staleness score. If a show is available in multiple tiers, take the smallest one. Can you show the show ID and the number of days it's been stale? Finally, sort the list so the stalest shows—that is, the ones that haven't been updated in the longest time—come first.", "normal_query": "For each show, I need to measure the Temporal Staleness Index (TSI). Please determine how many days have passed since the show last had any updates. If no update timestamp is available, use the launch date as a fallback. I’d like to see the show ID along with its staleness index, and the minimum value of this index across all its distribution tiers. Sort the results so that the shows with the highest staleness appear first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH content_dates AS (\n  SELECT \n    srkeys AS content_key,\n    COALESCE(\n      CASE \n        WHEN latestadd ~ '^\\d{4}-\\d{2}-\\d{2}' THEN TO_DATE(SUBSTRING(latestadd FROM 1 FOR 10), 'YYYY-MM-DD')\n        WHEN latestadd ~ '^\\d{4}/\\d{1,2}/\\d{1,2}$' THEN TO_DATE(latestadd, 'YYYY/MM/DD')\n        WHEN latestadd ~ '^\\d{2}/\\d{2}/\\d{2}$' THEN TO_DATE(latestadd, 'MM/DD/YY')\n        WHEN latestadd ~ '^[A-Za-z]{3,9} \\d{1,2}, \\d{4}$' THEN TO_DATE(latestadd, 'Mon DD, YYYY')\n        ELSE NULL\n      END,\n      CASE \n        WHEN launchmoment ~ '^\\d{4}-\\d{2}-\\d{2}' THEN TO_DATE(SUBSTRING(launchmoment FROM 1 FOR 10), 'YYYY-MM-DD')\n        WHEN launchmoment ~ '^\\d{4}/\\d{1,2}/\\d{1,2}$' THEN TO_DATE(launchmoment, 'YYYY/MM/DD')\n        WHEN launchmoment ~ '^\\d{2}/\\d{2}/\\d{2}$' THEN TO_DATE(launchmoment, 'MM/DD/YY')\n        WHEN launchmoment ~ '^[A-Za-z]{3,9} \\d{1,2}, \\d{4}$' THEN TO_DATE(launchmoment, 'Mon DD, YYYY')\n        ELSE NULL\n      END\n    ) AS effective_date\n  FROM show_rollups\n)\nSELECT \n  content_key,\n  EXTRACT(DAY FROM AGE(CURRENT_DATE, MIN(effective_date))) AS temporal_staleness_index\nFROM content_dates\nWHERE effective_date IS NOT NULL\nGROUP BY content_key\nORDER BY temporal_staleness_index DESC;"], "external_knowledge": [57, 14], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_5", "selected_database": "hulushows", "query": "How many titles are spread across over six nested genre tags and lean more on short clips, including both general clips and film-related clips, than full-length features?", "normal_query": "Count how many shows meet the Over-Fragmented Offering classification in the catalog.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH per_show AS (  SELECT     c.content_key,     regexp_split_to_array(       COALESCE(c.genreclass->>'Hierarchical_Genres', ''),       '[~|]+'     ) AS genres,     COALESCE((NULLIF(ci.mediacounts->'content_volumes'->>'Clips_Total',''))::int, 0)     + COALESCE((NULLIF(ci.mediacounts->'content_volumes'->>'Film_Clips',''))::int, 0)       AS short_form_count,     COALESCE((NULLIF(ci.mediacounts->'content_volumes'->>'Feature_Films',''))::int, 0)       AS feature_count   FROM core c   JOIN content_info ci USING (content_key) ), fragmented AS (   SELECT content_key   FROM per_show   WHERE cardinality(genres) > 6     AND short_form_count > feature_count ) SELECT COUNT(*) AS fragmented_offering_count FROM fragmented;"], "external_knowledge": [71], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "hulushows_6", "selected_database": "hulushows", "query": "Let’s all find groups of shows that belong to the same franchise. Can you only include franchises that have at least two shows? For each group, can you show me the franchise ID, how many shows it has, and list the show titles? Also, I need to sort the list so that the biggest franchises with the most shows come first.", "normal_query": "Please find all franchise groups. For each group with at least two shows, list the franchise ID, total show count, and the list of show titles. Sort the results by show count in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH franchise_titles AS ( SELECT series_id, COUNT(*) AS show_count, STRING_AGG(content_title, ', ') AS titles_in_franchise FROM core WHERE series_id IS NOT NULL GROUP BY series_id HAVING COUNT(*) >= 2 ) SELECT * FROM franchise_titles ORDER BY show_count DESC;"], "external_knowledge": [13], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_7", "selected_database": "hulushows", "query": "I want to find out how many episodes there are on average in each season for every show. Can you look at shows where we know both the total number of episodes and how many seasons they have. For each one, give me the show ID, how many episodes it has, how many seasons, and the average episodes per season. Please skip anything where the season count is missing or zero. Finally, show the ones with the highest average first.", "normal_query": "Please calculate the average number of episodes per season for each show. Can you only include shows with both episode and season counts? For each, list the show ID, total episodes, total seasons, and the episode-to-season ratio. Importantly, exclude entries with missing or zero seasons. Sort results by the ratio in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH episode_ratio_calc AS ( SELECT content_key, (mediacounts->'content_volumes'->>'Episode_Total')::int AS episode_count, (mediacounts->'content_volumes'->>'Seasons_Total')::int AS season_count, CASE WHEN (mediacounts->'content_volumes'->>'Seasons_Total')::int > 0 THEN ROUND(((mediacounts->'content_volumes'->>'Episode_Total')::float / (mediacounts->'content_volumes'->>'Seasons_Total')::float)::numeric, 2) ELSE NULL END AS season_to_episode_ratio FROM content_info WHERE TRIM(mediacounts->'content_volumes'->>'Episode_Total') ~ '^[0-9]+$' AND TRIM(mediacounts->'content_volumes'->>'Seasons_Total') ~ '^[0-9]+$') SELECT content_key, episode_count, season_count, season_to_episode_ratio FROM episode_ratio_calc WHERE season_to_episode_ratio IS NOT NULL ORDER BY season_to_episode_ratio DESC;"], "external_knowledge": [23], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "hulushows_8", "selected_database": "hulushows", "query": "Let’s figure out what the most frequent top-end maturity rating is across all the shows. Basically, I want to scan all the records, grab the maturity info, and tell me which of those high-end ratings pops up the most. Just return the one that shows up the most often.", "normal_query": "To support catalog analysis, compute the Most Common Peak TV Rating across All Distribution Records. It should consider all available distributiondata, extract their rating information, and determine the single most frequently assigned rating value. Give me a single text result representing the most common rating.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH raw AS (SELECT TRIM(ratinginfo->>'Peak_Rating') AS rating FROM show_rollups WHERE TRIM(ratinginfo->>'Peak_Rating') IS NOT NULL AND TRIM(ratinginfo->>'Peak_Rating') <> ''), norm AS (SELECT rating, UPPER(REGEXP_REPLACE(rating, '[^A-Za-z0-9]+', '', 'g')) AS base FROM raw), cleaned AS (SELECT CASE WHEN base ~ '^(TV)?Y7FV$' THEN 'TV-Y7-FV' WHEN base ~ '^(TV)?Y7$' THEN 'TV-Y7' WHEN base ~ '^(TV)?Y$' THEN 'TV-Y' WHEN base ~ '^(TV)?G$' THEN 'TV-G' WHEN base ~ '^(TV)?PG$' OR base='PARENTALGUIDANCE' THEN 'TV-PG' WHEN base ~ '^(TV)?14$' OR base='T14' OR base='FOURTEEN' THEN 'TV-14' WHEN base ~ '^(TV)?MA$' OR base='MATURE' THEN 'TV-MA' WHEN base IN ('NR','UNRATED','UR') THEN 'NR' ELSE UPPER(TRIM(rating)) END AS normalized_rating FROM norm), counts AS (SELECT normalized_rating, COUNT(*) AS freq FROM cleaned GROUP BY normalized_rating) SELECT normalized_rating AS most_common_peak_rating FROM counts ORDER BY freq DESC LIMIT 1;"], "external_knowledge": [81], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_9", "selected_database": "hulushows", "query": "Which franchises are producing the most content? Group shows in the same franchise and add up their episodes. Some episode counts may be text or invalid — after trimming whitespace, parse only digit strings (digits 0–9 only) and treat the rest as zero. Show only franchises with more than 100 total episodes, listing the identifier, number of shows, and total episodes from largest to smallest.", "normal_query": "Generate a Franchise Engagement Summary by grouping shows that belong to the same franchise. The episode count field may be stored as text and can include non-numeric values; after trimming whitespace, parse only digit strings (digits 0–9 only) and treat everything else as zero. Only include franchises whose total number of episodes exceeds 100. For each franchise, provide its identifier, the number of shows it contains, and the combined episode count, sorted by total episodes in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH franchise_summary AS ( SELECT c.series_id, COUNT(*) AS total_shows, SUM(CASE WHEN TRIM(ci.mediacounts->'content_volumes'->>'Episode_Total') ~ '^[0-9]+$' THEN (TRIM(ci.mediacounts->'content_volumes'->>'Episode_Total'))::int ELSE 0 END) AS total_episodes FROM core c JOIN content_info ci ON c.content_key = ci.content_key WHERE c.series_id IS NOT NULL GROUP BY c.series_id ) SELECT series_id, total_shows, total_episodes FROM franchise_summary WHERE total_episodes > 100 ORDER BY total_episodes DESC, series_id;"], "external_knowledge": [72], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_10", "selected_database": "hulushows", "query": "Let’s see how our shows are spread out across the different subscription plans. For each plan, I want to know how many titles it has and what chunk of the full catalog that is. Just give me the plan name, the total count of media in it, and what percentage of the catalog that represents. Start with the plans that have the biggest share of content.", "normal_query": "Determine the Tier Distribution Ratio to understand how media content is shared across different access levels. First, sum up the total media volume available under each tier. Then compute the overall media total across all tiers. For each tier, calculate its share of the total by dividing the tier’s media volume by the grand total. List the tier ID, tier type, media total, and its Tier Distribution Ratio. Sort the results by the ratio in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH base AS (\n  SELECT DISTINCT sr.srlinks AS tier_key, sr.srkeys AS content_key\n  FROM show_rollups sr\n),\nper_content_media AS (\n  SELECT\n    b.tier_key,\n    b.content_key,\n    CASE\n      WHEN TRIM(ci.mediacounts->'content_volumes'->>'Videos_Total') ~ '^\\d+$'\n        THEN (ci.mediacounts->'content_volumes'->>'Videos_Total')::bigint\n      ELSE 0\n    END AS videos_total\n  FROM base b\n  JOIN content_info ci ON ci.content_key = b.content_key\n),\ntier_media_totals AS (\n  SELECT tier_key, SUM(videos_total) AS media_total\n  FROM per_content_media\n  GROUP BY tier_key\n),\ntotal_sum AS (\n  SELECT SUM(media_total) AS grand_total FROM tier_media_totals\n)\nSELECT\n  t.tier_key,\n  r.tiertype,\n  t.media_total,\n  ROUND(t.media_total * 1.0 / NULLIF(ts.grand_total, 0), 4) AS tier_distribution_ratio\nFROM tier_media_totals t\nJOIN rollups r ON r.tierkey = t.tier_key\nCROSS JOIN total_sum ts\nORDER BY tier_distribution_ratio DESC;"], "external_knowledge": [36, 1], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "hulushows_11", "selected_database": "hulushows", "query": "Let’s see which franchises are really making waves across different subscription levels. We’re looking for those that have at least 3 shows, and those shows appear across 3 or more tiers. For each of these franchise powerhouses, show me the franchise ID, how many shows they’ve got, and how many tiers they show up in. Sort the list by number of shows to spotlight the most widely spread ones first.", "normal_query": "To evaluate Syndicated Franchise Engagement, we need to check which franchise groups have both a strong show count and wide distribution. For each franchise, count how many shows belong to it and how many unique distribution tiers those shows appear in. These shows should include franchises with at least 3 shows and presence in 3 or more tiers. List the franchise ID, number of shows, and number of tiers, ordered by show count in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH franchise_shows AS (\n  SELECT\n    c.series_id,\n    COUNT(DISTINCT c.content_key) AS show_count\n  FROM core c\n  WHERE c.series_id IS NOT NULL\n  GROUP BY c.series_id\n),\nfranchise_tiers AS (\n  SELECT\n    c.series_id,\n    COUNT(DISTINCT sr.srlinks) AS tier_count\n  FROM core c\n  JOIN show_rollups sr\n    ON sr.srkeys = c.content_key\n  WHERE c.series_id IS NOT NULL\n  GROUP BY c.series_id\n)\nSELECT\n  fs.series_id AS franchise_id,\n  fs.show_count,\n  ft.tier_count\nFROM franchise_shows fs\nJOIN franchise_tiers ft\n  ON ft.series_id = fs.series_id\nWHERE fs.show_count >= 3\n  AND ft.tier_count >= 3\nORDER BY fs.show_count DESC;"], "external_knowledge": [3], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_12", "selected_database": "hulushows", "query": "Let’s dive into the main genre types that keep popping up in our show catalog. I’m only interested in shows labeled as Drama, Comedy, or Animation and Cartoons. For each of those, can you pull together a quick list that includes the show’s ID, its title, and what genre it’s tagged under? Sort the list by title.", "normal_query": "We want to analyze Primary Genre Classification across our show catalog. For this, filter and retrieve all titles that fall under the Drama, Comedy, or Animation and Cartoons categories. For each matching title, show its unique ID, name, and its primary genre type. Sort the results alphabetically by title.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT c.content_key, c.content_title, c.genreclass->>'Primary_Genre' AS primary_genre FROM core c WHERE c.genreclass->>'Primary_Genre' IN ('Drama', 'Comedy', 'Animation and Cartoons') ORDER BY c.content_title;"], "external_knowledge": [74], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_13", "selected_database": "hulushows", "query": "I want to look at how packed each show’s video library is. Can you pull up a list that shows the total number of video items for each show and group them into three levels? Label them High if they’ve got over 500 videos, Medium if they’re between 200 and 500, and Low if they’re under 200. Let’s sort the list so the shows with the most content show up first, and include the show ID, total count, and the volume level tag.", "normal_query": "For each show, compute its total number of video items and classify it using the Content Volume Level Classification. Return the show ID, total volume, and the resulting volume category, ordered by total volume from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH content_volume_classification AS ( SELECT ci.content_key, (ci.mediacounts->'content_volumes'->>'Videos_Total')::int AS total_video_volume, CASE WHEN (ci.mediacounts->'content_volumes'->>'Videos_Total')::int > 500 THEN 'High' WHEN (ci.mediacounts->'content_volumes'->>'Videos_Total')::int BETWEEN 200 AND 500 THEN 'Medium' ELSE 'Low' END AS volume_level FROM content_info ci WHERE (ci.mediacounts->'content_volumes'->>'Videos_Total') IS NOT NULL ) SELECT content_key, total_video_volume, volume_level FROM content_volume_classification ORDER BY total_video_volume DESC;"], "external_knowledge": [75], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_14", "selected_database": "hulushows", "query": "Which show feels the most crammed with promotional stuff? Just give me the one with the heaviest promo presence overall.", "normal_query": "Find the Maximum Promo Saturation Ratio across all shows in the catalog.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT MAX(((CASE WHEN NULLIF(BTRIM(tiernotices->'free_tier'->>'Alert_Note'), '') IS NOT NULL THEN 1 ELSE 0 END) + (CASE WHEN NULLIF(BTRIM(tiernotices->'free_tier'->>'Avail_Note'), '') IS NOT NULL THEN 1 ELSE 0 END) + (CASE WHEN NULLIF(BTRIM(tiernotices->'free_tier'->>'Promo_Note'), '') IS NOT NULL THEN 1 ELSE 0 END) + (CASE WHEN NULLIF(BTRIM(tiernotices->'free_tier'->>'Expire_Note'), '') IS NOT NULL THEN 1 ELSE 0 END) + (CASE WHEN NULLIF(BTRIM(tiernotices->'member_tier'->>'Alert_Note'), '') IS NOT NULL THEN 1 ELSE 0 END) + (CASE WHEN NULLIF(BTRIM(tiernotices->'member_tier'->>'Avail_Note'), '') IS NOT NULL THEN 1 ELSE 0 END) + (CASE WHEN NULLIF(BTRIM(tiernotices->'member_tier'->>'Promo_Note'), '') IS NOT NULL THEN 1 ELSE 0 END) + (CASE WHEN NULLIF(BTRIM(tiernotices->'member_tier'->>'Expire_Note'), '') IS NOT NULL THEN 1 ELSE 0 END)) / 8.0) AS max_promo_saturation_ratio FROM promo_info;"], "external_knowledge": [82], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "hulushows_15", "selected_database": "hulushows", "query": "How many shows land in our usual user-score buckets—Low, Medium, or High? Just give me the total.", "normal_query": "Report the total number of shows whose user scores fall into the standard Low, Medium, or High buckets.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH standard_score_ranges AS (  SELECT 'Low' AS tier, 0.0 AS min_score, 2.0 AS max_exclusive, false AS include_max UNION ALL  SELECT 'Medium',        2.0,             4.0,               false UNION ALL  SELECT 'High',          4.0,             5.0,               true ), scores AS (  SELECT     c.content_key,     NULLIF(       substring(COALESCE(c.genreclass->>'User_Score',                          c.genreclass->>'userscore',                          c.genreclass->>'user_score','')                 FROM '([0-9]+(?:\\.[0-9]+)?)'),       ''     )::float AS raw_score   FROM core c ), bounded AS (  SELECT content_key,          CASE WHEN raw_score BETWEEN 0 AND 5 THEN raw_score END AS score   FROM scores ) SELECT COUNT(DISTINCT b.content_key) AS shows_with_matched_score_range FROM bounded b JOIN standard_score_ranges r   ON b.score >= r.min_score  AND (b.score < r.max_exclusive OR (r.include_max AND b.score <= r.max_exclusive));"], "external_knowledge": [83], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "hulushows_16", "selected_database": "hulushows", "query": "I want to find shows that show up in three or more different subscription tiers. For each show, can you count how many unique tiers it’s available in? First, keep the ones that are in at least three tiers, and then sort the results from the most widely distributed to the last.", "normal_query": "I want to know all Multitier Syndicated Shows. For each show with at least three tiers, show its unique identifier and the number of tiers it appears in. Sort the results by tier count in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH tier_counts AS (     SELECT         srkeys AS content_key,         COUNT(DISTINCT r.tiertype) AS tier_count     FROM show_rollups s     JOIN rollups r       ON s.srlinks = r.tierkey     GROUP BY srkeys ) SELECT     c.content_key,     t.tier_count FROM tier_counts t JOIN core c   ON t.content_key = c.content_key WHERE t.tier_count >= 3 ORDER BY t.tier_count DESC;"], "external_knowledge": [76], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_17", "selected_database": "hulushows", "query": "Let’s grab the shows where the bigger of their trailer or feature count is over 100. Show the ID, title, and that number, sorted from highest to lowest.", "normal_query": "Find shows whose Peak Media Load is greater than 100. Give me the show ID, title, and the peak value, sorted from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH show_rollups AS (   SELECT * FROM (     VALUES       ('SH001', '{\"standard_content\": {\"Feature_Vol\": \"120\", \"Trailer_Vol\": \"95\"}}'::jsonb),       ('SH002', '{\"standard_content\": {\"Feature_Vol\": \"60\",  \"Trailer_Vol\": \"130\"}}'::jsonb),       ('SH003', '{\"standard_content\": {\"Feature_Vol\": \"99\",  \"Trailer_Vol\": \"98\"}}'::jsonb),       ('SH004', '{\"standard_content\": {\"Feature_Vol\": \"\",    \"Trailer_Vol\": \"200\"}}'::jsonb),       ('SH005', '{\"standard_content\": {\"Feature_Vol\": \"150\", \"Trailer_Vol\": \"\"}}'::jsonb)   ) AS t(srkeys, contentvols) ), core AS (   SELECT * FROM (     VALUES       ('SH001', 'Galaxy Showdown', 'COMP01'),       ('SH002', 'Ocean Deep',      'COMP02'),       ('SH003', 'Lost in Code',    'COMP03'),       ('SH004', 'Shadow War',      'COMP04'),       ('SH005', 'Quiet Forest',    'COMP05')   ) AS t(content_key, content_title, studiolink) ), media_volumes AS (   SELECT     sr.srkeys AS content_key,     c.content_title,     COALESCE(NULLIF(sr.contentvols->'standard_content'->>'Feature_Vol','')::int, 0) AS feature_vol,     COALESCE(NULLIF(sr.contentvols->'standard_content'->>'Trailer_Vol','')::int, 0) AS trailer_vol,     GREATEST(       COALESCE(NULLIF(sr.contentvols->'standard_content'->>'Feature_Vol','')::int, 0),       COALESCE(NULLIF(sr.contentvols->'standard_content'->>'Trailer_Vol','')::int, 0)     ) AS peak_media_volume   FROM show_rollups sr   JOIN core c ON sr.srkeys = c.content_key ) SELECT   mv.content_key,   mv.content_title,   mv.peak_media_volume FROM media_volumes mv WHERE mv.peak_media_volume > 100 ORDER BY mv.peak_media_volume DESC;"], "external_knowledge": [77, 67], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_18", "selected_database": "hulushows", "query": "I want to see how shows rank based on what viewers think. Just group them by how well they’re rated, ignore anything without a proper score, and tell me the show ID, name, how it scored, and which group it ended up in—start from the highest-rated and go down.", "normal_query": "Analyze show-level user ratings to assign each show to its corresponding Episode Rating Band. Only include shows with valid numeric scores. For each show, return its ID, title, user score, and band, sorted from highest to lowest score.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH rating_data AS (   SELECT     c.content_key,     c.content_title,     CAST(       NULLIF(         REGEXP_REPLACE(           COALESCE(             c.genreclass ->> 'User_Score',             c.genreclass ->> 'userscore',             c.genreclass ->> 'user_score',             ''           ),           '[^0-9\\.]',           '',           'g'         ),         ''       ) AS FLOAT     ) AS userscore   FROM core c ), banded AS (   SELECT     content_key,     content_title,     userscore,     CASE       WHEN userscore < 3.5 THEN 'Low'       WHEN userscore <= 4.2 THEN 'Medium'       ELSE 'High'     END AS rating_band   FROM rating_data   WHERE userscore IS NOT NULL ) SELECT content_key, content_title, userscore, rating_band FROM banded ORDER BY userscore DESC;"], "external_knowledge": [78, 69, 30, 4, 1], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 8, "distinct": false, "order": true}}
{"instance_id": "hulushows_19", "selected_database": "hulushows", "query": "Which shows actually have film clips? List the ones with the most film-related clips first. For each show, show the title, how many film clips it has, and a quick flag for Has Clips or No Clips.", "normal_query": "I want to check film-clip availability for each show. For every show, return its ID, title, the number of film-related clips, and a flag saying Has Clips if that count is greater than 0, otherwise No Clips. Sort from highest to lowest film-clip count.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH clip_check AS (\n  SELECT\n    sr.srkeys AS content_key,\n    c.content_title,\n    CAST(NULLIF(BTRIM(sr.contentvols->'standard_content'->>'FilmClip_Vol'), '') AS INT) AS film_clip_vol\n  FROM show_rollups sr\n  JOIN core c ON sr.srkeys = c.content_key\n),\nclip_per_show AS (\n  -- If a show appears in multiple tiers, take the maximum film-clip count once per show\n  SELECT content_key, MAX(film_clip_vol) AS film_clip_vol\n  FROM clip_check\n  GROUP BY content_key\n),\nflagged AS (\n  SELECT\n    cps.content_key,\n    c.content_title,\n    cps.film_clip_vol,\n    CASE WHEN COALESCE(cps.film_clip_vol, 0) > 0 THEN 'Has Clips' ELSE 'No Clips' END AS clip_flag\n  FROM clip_per_show cps\n  JOIN core c ON cps.content_key = c.content_key\n)\nSELECT content_key, content_title, film_clip_vol, clip_flag\nFROM flagged\nORDER BY film_clip_vol DESC NULLS LAST;"], "external_knowledge": [72, 4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_20", "selected_database": "hulushows", "query": "Let’s see which shows are loading up on promo messages. For each one, count availability updates, promo messages, alerts, and expiration notices across the free and member tiers. Only include shows with at least one note, and list them starting with the most.", "normal_query": "Show the Promotional Intensity Summary for each show with at least one note. Include the show ID and the total count, sorted descending.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH notes AS (   SELECT     p.content_key,     (       (CASE WHEN NULLIF(BTRIM(p.tiernotices->'free_tier'->>'Avail_Note'),  '') IS NOT NULL THEN 1 ELSE 0 END) +       (CASE WHEN NULLIF(BTRIM(p.tiernotices->'free_tier'->>'Promo_Note'),  '') IS NOT NULL THEN 1 ELSE 0 END) +       (CASE WHEN NULLIF(BTRIM(p.tiernotices->'free_tier'->>'Alert_Note'),  '') IS NOT NULL THEN 1 ELSE 0 END) +       (CASE WHEN NULLIF(BTRIM(p.tiernotices->'free_tier'->>'Expire_Note'), '') IS NOT NULL THEN 1 ELSE 0 END) +       (CASE WHEN NULLIF(BTRIM(p.tiernotices->'member_tier'->>'Avail_Note'),  '') IS NOT NULL THEN 1 ELSE 0 END) +       (CASE WHEN NULLIF(BTRIM(p.tiernotices->'member_tier'->>'Promo_Note'),  '') IS NOT NULL THEN 1 ELSE 0 END) +       (CASE WHEN NULLIF(BTRIM(p.tiernotices->'member_tier'->>'Alert_Note'),  '') IS NOT NULL THEN 1 ELSE 0 END) +       (CASE WHEN NULLIF(BTRIM(p.tiernotices->'member_tier'->>'Expire_Note'), '') IS NOT NULL THEN 1 ELSE 0 END)     ) AS total_promo_notes   FROM promo_info p ), final AS (   SELECT content_key, total_promo_notes   FROM notes   WHERE total_promo_notes > 0 ) SELECT content_key, total_promo_notes FROM final ORDER BY total_promo_notes DESC, content_key;"], "external_knowledge": [80], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_M_1", "selected_database": "hulushows", "query": "Let’s drop in a new show using these exact values: make the ID 900001, set the official name to new-show-canonical, call it New Show Title, link it to series 99999999, tag it to studio 8, and add the note ‘This is a newly added show for fall season release.’ For genres, store a JSON with score 4.25, type show, main genre Science Fiction, and breakdown Science Fiction~Space|Adventure. Once that’s saved, return what you added.", "normal_query": "Add a brand-new show with these exact details: ID 900001, official name new-show-canonical, title New Show Title, series 99999999, studio 8, and the note This is a newly added show for fall season release. For its genre info, save a JSON that has a score 4.25, type show, main genre Science Fiction, and a breakdown Science Fiction~Space|Adventure. After saving, show me the inserted record.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH new_core_entry AS (\n  SELECT\n    900001 AS content_key,\n    'new-show-canonical' AS canonical_name,\n    'New Show Title' AS content_title,\n    99999999 AS series_id,\n    8 AS studiolink,\n    'This is a newly added show for fall season release.' AS annotations,\n    '{\"User_Score\":\"4.25\",\"Content_Type\":\"show\",\"Primary_Genre\":\"Science Fiction\",\"Hierarchical_Genres\":\"Science Fiction~Space|Adventure\"}'::jsonb AS genreclass\n)\nINSERT INTO core (content_key, canonical_name, content_title, series_id, studiolink, annotations, genreclass)\nSELECT content_key, canonical_name, content_title, series_id, studiolink, annotations, genreclass\nFROM new_core_entry\nRETURNING *;\n"], "external_knowledge": [0, 1, 6], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn): cleanup_sql = \"DELETE FROM core WHERE content_key = 900001;\"; execute_queries(cleanup_sql, db_name, conn); execute_queries(pred_sqls, db_name, conn); check_sql = \"SELECT content_key, canonical_name, content_title, series_id, studiolink FROM core WHERE content_key = 900001;\"; result = execute_queries(check_sql, db_name, conn)[0]; assert len(result) == 1, \"Expected one row to be inserted.\"; assert result[0][0] == 900001, \"Inserted content_key does not match.\"; assert result[0][1] == \"new-show-canonical\", \"Inserted canonical_name does not match.\"; assert result[0][2] == \"New Show Title\", \"Inserted content_title does not match.\"; assert result[0][3] == 99999999, \"Inserted series_id does not match.\"; assert result[0][4] == 8, \"Inserted studiolink does not match.\"; execute_queries(cleanup_sql, db_name, conn);"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "hulushows_M_2", "selected_database": "hulushows", "query": "So, which studios are really cranking out the content? Let’s create a function called calculate_studio_activity_index that tells us how many entries a studio has in the system. Just pass in the studio’s ID, and it’ll return the total number of catalog records linked to that studio—even if some titles repeat. Simple enough, right? Oh, and while we’re at it—find the show with ID 54 and update its official name to ‘updated-family-guy’.", "normal_query": "Create a PostgreSQL function called calculate_studio_activity_index that computes the Studio Activity Index and returns the calculated value. The function takes one parameter: the unique identifier of a studio. It calculates the total number of content records that are associated with the given studio in the catalog, counting all entries regardless of whether the titles repeat. The result is an integer representing the count of all such records. Additionally, update the canonical name of a specific show in the catalog. Locate the show using its unique content key, which is 54, and set its canonical name to 'updated-family-guy'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION calculate_studio_activity_index(p_studio_id BIGINT)\nRETURNS INTEGER\nLANGUAGE sql\nAS $$\n  SELECT COUNT(*)::INTEGER\n  FROM core\n  WHERE studiolink = p_studio_id\n$$;\n\nUPDATE core\nSET canonical_name = 'updated-family-guy'\nWHERE content_key = 54;"], "external_knowledge": [84], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn): execute_queries(\"INSERT INTO companies VALUES (9999, 100, 'Test Studio', 'Test', 'test-studio'); INSERT INTO core VALUES (900001, 'a', 'b', 1, 9999, '', NULL), (900002, 'c', 'd', 1, 9999, '', NULL);\", db_name, conn); result = execute_queries([\"SELECT calculate_studio_activity_index(9999);\"] , db_name, conn); assert result and result[0][0][0] == 2; execute_queries(\"DELETE FROM core WHERE content_key IN (900001, 900002); DELETE FROM companies WHERE entity_key = 9999;\", db_name, conn); return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "hulushows_M_3", "selected_database": "hulushows", "query": "Let’s check how much content each subscription gets. Just give me the plan name—like “free” or “subscriber”—and I’ll count all the shows linked to it. Don’t worry about casing or spaces; it should match even if someone types it differently.", "normal_query": "Create a function that returns the number of unique shows available under a given subscription plan like \"free\" or \"subscriber\". Match the plan name in a case-insensitive and trimmed way to ensure accurate mapping. Return the total number of linked shows.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION count_titles_by_tier(p_tiertype TEXT) RETURNS INTEGER AS $$\nDECLARE\n  v_count INTEGER;\nBEGIN\n  SELECT COUNT(DISTINCT sr.srkeys) INTO v_count\n  FROM rollups r\n  JOIN show_rollups sr ON r.tierkey = sr.srlinks\n  WHERE LOWER(TRIM(r.tiertype::TEXT)) = LOWER(TRIM(p_tiertype));\n  RETURN v_count;\nEND;\n$$ LANGUAGE plpgsql;\n"], "external_knowledge": [3], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn): execute_queries(pred_sqls, db_name, conn); test_result = execute_queries([\"SELECT count_titles_by_tier('free');\"], db_name, conn); print(test_result); assert test_result and isinstance(test_result[0][0][0], int); assert test_result[0][0][0] >= 0; return 1"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_M_4", "selected_database": "hulushows", "query": "Let’s check how many titles belong to a given series. Just pass in a series ID, and we’ll return the total number of titles linked to that series.", "normal_query": "We need to calculate the number of distinct titles that belong to a specific series to support the Series Entry Count metric. Given a series identifier as input, the system should return a single integer representing how many entries are part of that series.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION count_titles_by_series(p_series_id BIGINT) RETURNS INTEGER AS $$ DECLARE v_count INTEGER; BEGIN SELECT COUNT(*) INTO v_count FROM core WHERE series_id = p_series_id; RETURN v_count; END; $$ LANGUAGE plpgsql;"], "external_knowledge": [3], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn): pred_query_result = execute_queries(pred_sqls, db_name, conn); test_result = execute_queries([\"SELECT count_titles_by_series(11730);\"], db_name, conn); print(test_result); return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_M_5", "selected_database": "hulushows", "query": "Let’s see how our shows break down by age-appropriateness—like “TV-Y”, “TV-PG”, etc. Just group them and count how many land in each level, making sure different casing or extra spaces are treated the same.", "normal_query": "Could you help me get a quick overview of how shows are distributed across different TV Rating types? For each rating, return how many shows fall under it, normalizing the rating values by lowercasing and trimming to avoid mismatches.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE VIEW tv_rating_distribution AS\nWITH base AS (\n  SELECT NULLIF(LOWER(TRIM(sr.ratinginfo->>'TV_Rating')), '') AS rating_norm\n  FROM show_rollups sr\n)\nSELECT\n  rating_norm AS rating,\n  COUNT(*) AS title_count\nFROM base\nWHERE rating_norm IS NOT NULL\nGROUP BY rating_norm\nORDER BY rating;"], "external_knowledge": [2], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    execute_queries([\n        \"DELETE FROM companies WHERE entity_key = 99998;\",\n        \"DELETE FROM core WHERE content_key = 99901;\",\n        \"DELETE FROM show_rollups WHERE srkeys = 99901;\",\n        \"INSERT INTO companies (entity_key, company_name) VALUES (99998, 'Studio A');\",\n        \"INSERT INTO core (content_key, canonical_name, content_title, series_id, studiolink) VALUES (99901, 'test-canon', 'test-title', 1001, 99998);\",\n        \"INSERT INTO show_rollups (srkeys, srlinks, ratinginfo) VALUES (99901, 1, '{\\\"TV_Rating\\\": \\\"TV-Y7\\\"}');\"\n    ], db_name, conn)\n    execute_queries([\"DROP VIEW IF EXISTS tv_rating_distribution;\"] , db_name, conn)\n    execute_queries(sol_sqls, db_name, conn)\n    result = execute_queries([\"SELECT * FROM tv_rating_distribution WHERE rating = 'tv-y7';\"], db_name, conn)\n    assert result and result[0][0][0] == 'tv-y7' and result[0][0][1] >= 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_M_6", "selected_database": "hulushows", "query": "I want to know if all shows in a series share the same name? Just use check_series_title_uniformity with the series ID. it returns true if the titles match across the board, false if they don’t.", "normal_query": "A function named check_series_title_uniformity is required. This function determines the Series Title Uniformity Flag for a given series. It checks whether all shows linked to the same series share an identical canonical title. The output is a boolean value—true if all titles match, and false otherwise.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION check_series_title_uniformity(p_series_id BIGINT)\nRETURNS BOOLEAN AS $$\nDECLARE\n  name_count INTEGER;\nBEGIN\n  SELECT COUNT(DISTINCT LOWER(TRIM(canonical_name))) INTO name_count\n  FROM core\n  WHERE series_id = p_series_id;\n  RETURN name_count = 1;\nEND;\n$$ LANGUAGE plpgsql;\n"], "external_knowledge": [87], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn): execute_queries([\"INSERT INTO core (content_key, canonical_name, content_title, series_id, studiolink) VALUES (900001, 'test-name', 'test-title-1', 20001, 8), (900002, 'test-name', 'test-title-2', 20001, 8);\"] , db_name, conn); pred_query_result = execute_queries(pred_sqls, db_name, conn); test_result = execute_queries([\"SELECT check_series_title_uniformity(20001);\"], db_name, conn); print(test_result); assert test_result and test_result[0][0][0] is True; return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_M_7", "selected_database": "hulushows", "query": "Let’s figure out which studios have been the busiest. For each one, can you show me how many titles they’ve worked on? Just include the studios that are actually linked to content, and sort the list so the most active ones show up first. I need this saved as a permanent table called studio_catalog_size.", "normal_query": "We need to create a persistent table of all Studio Catalog Size data for our content analysis. Please set up a table called studio_catalog_size that includes each studio’s unique identifier and the total number of titles linked to that studio. The count should be grouped by studio and sorted from the most prolific to the least. Please note only include entries that are explicitly associated with a studio.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE studio_catalog_size AS SELECT studiolink AS studio_id, COUNT(*) AS num_titles FROM core WHERE studiolink IS NOT NULL GROUP BY studiolink ORDER BY num_titles DESC;"], "external_knowledge": [88], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn): execute_queries([\"DELETE FROM core;\", \"DELETE FROM companies;\", \"INSERT INTO companies (entity_key, company_name) VALUES (100, 'Test Studio');\", \"INSERT INTO core (content_key, canonical_name, content_title, series_id, studiolink) VALUES (1, 'A', 'Title A', NULL, 100);\", \"INSERT INTO core (content_key, canonical_name, content_title, series_id, studiolink) VALUES (2, 'B', 'Title B', NULL, 100);\", \"DROP TABLE IF EXISTS studio_catalog_size;\"], db_name, conn); execute_queries(pred_sqls, db_name, conn); test_result = execute_queries([\"SELECT * FROM studio_catalog_size;\"], db_name, conn); assert test_result and test_result[0][0][0] == 100; assert test_result[0][0][1] == 2; return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_M_8", "selected_database": "hulushows", "query": "Let’s figure out which studios have been the busiest in the catalog and save it in a table called title_count_per_studio. For each one, can you show me their ID, name, and how many shows they’ve worked on? Only count the ones that are actually linked to a studio. We’ll need to pull the studio info by joining the show records with the studio list. Then, sort the results so the studios with the most titles show up first.", "normal_query": "Let’s build a persistent table called title_count_per_studio to analyze Title Count per Studio for catalog assessment. This table should include each studio’s unique ID, its canonical name, and the number of titles linked to it. Only include entries where a valid studio association exists. The result must be grouped by studio and sorted so the most prolific studios appear first. Join is required between the show catalog and the studio registry. The output will be a structured table listing studio ID, studio name, and how many titles are attributed to each.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["DROP TABLE IF EXISTS title_count_per_studio;", "CREATE TABLE title_count_per_studio AS\nSELECT\n  c.studiolink AS studio_id,\n  LOWER(TRIM(comp.canonical_name)) AS studio_name,\n  COUNT(*) AS num_titles\nFROM core c\nJOIN companies comp ON c.studiolink = comp.entity_key\nWHERE c.studiolink IS NOT NULL\nGROUP BY c.studiolink, LOWER(TRIM(comp.canonical_name))\nORDER BY num_titles DESC;"], "external_knowledge": [89], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn): execute_queries([\"DELETE FROM core;\", \"DELETE FROM companies;\", \"INSERT INTO companies (entity_key, company_name, canonical_name) VALUES (88, 'StudioTest Inc', ' StudioTest  ');\", \"INSERT INTO core (content_key, canonical_name, content_title, studiolink) VALUES (88801, 'A', 'Alpha Show', 88), (88802, 'B', 'Beta Show', 88);\"], db_name, conn); execute_queries(pred_sqls, db_name, conn); test_result = execute_queries([\"SELECT * FROM title_count_per_studio WHERE studio_id = 88;\"], db_name, conn); assert test_result and test_result[0][0][0] == 88; assert test_result[0][0][1] == 'studiotest'; assert test_result[0][0][2] == 2; return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_M_9", "selected_database": "hulushows", "query": "Set up a permanent table called avg_title_length_per_studio so we can track how long each studio’s show titles usually are. It should include which studio it is and the average number of characters in the titles of its shows. We’re only defining the structure for avg_title_length_per_studio right now—no data yet.", "normal_query": "Please create a permanent table named avg_title_length_per_studio to track the average length of show titles per production studio. The table must have two columns: (1) the studio’s unique ID and (2) the average number of characters in titles of shows linked to that studio. This step only defines the schema for avg_title_length_per_studio—do not insert any data.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE avg_title_length_per_studio (studio_id BIGINT PRIMARY KEY, studio_title_avg_length FLOAT NOT NULL);"], "external_knowledge": [89], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn): execute_queries([\"DROP TABLE IF EXISTS avg_title_length_per_studio;\", \"CREATE TABLE avg_title_length_per_studio (studio_id BIGINT PRIMARY KEY, studio_title_avg_length FLOAT NOT NULL);\", \"INSERT INTO avg_title_length_per_studio (studio_id, studio_title_avg_length) VALUES (101, 12.5);\"], db_name, conn); test_result = execute_queries([\"SELECT * FROM avg_title_length_per_studio WHERE studio_id = 101;\"], db_name, conn); print(test_result); assert test_result and test_result[0] and test_result[0][0][0] == 101; assert abs(test_result[0][0][1] - 12.5) < 1e-6; return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "hulushows_M_10", "selected_database": "hulushows", "query": "Let’s check how busy our release schedule was in a particular year. I need a function that takes in a year and tells me how many shows were launched during that time. It should go through the catalog and count only the shows whose launch dates fall in that year, but only for test titles with srkeys 900001 and 900002. Please don’t include the rest of the system’s data. The result should just be a number showing how many of those selected titles came out in that year.", "normal_query": "Create a function named get_launch_count_by_year that computes the Launch Year Distribution for a specific year. This function analyzes the release history by counting how many titles were launched in the specified year. It operates over the catalog of shows, using each show's recorded launch timestamp, and filters to only include test data with srkeys in (900001, 900002). The output is a single integer indicating the number of titles launched in that year.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION get_launch_count_by_year(p_year TEXT)\nRETURNS INTEGER AS $$\nDECLARE\n  year_count INTEGER;\nBEGIN\n  SELECT COUNT(*) INTO year_count\n  FROM show_rollups\n  WHERE\n    (\n      CASE \n        WHEN launchmoment ~ '^\\d{4}-\\d{2}-\\d{2}$' THEN EXTRACT(YEAR FROM TO_DATE(launchmoment, 'YYYY-MM-DD'))::TEXT\n        WHEN launchmoment ~ '^\\d{2}/\\d{2}/\\d{2}$' THEN EXTRACT(YEAR FROM TO_DATE(launchmoment, 'MM/DD/YY'))::TEXT\n        WHEN launchmoment ~ '^[A-Za-z]{3,9} \\d{1,2}, \\d{4}$' THEN EXTRACT(YEAR FROM TO_DATE(launchmoment, 'Mon DD, YYYY'))::TEXT\n        ELSE NULL\n      END\n    ) = p_year\n    AND srkeys IN (900001, 900002);\n  RETURN year_count;\nEND;\n$$ LANGUAGE plpgsql;"], "external_knowledge": [91], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn): execute_queries([\"DELETE FROM show_rollups WHERE srkeys IN (900001,900002,900003);\",\"DELETE FROM core WHERE content_key IN (900001,900002,900003);\",\"INSERT INTO core (content_key, content_title) VALUES (900001, 'Test Show A'), (900002, 'Test Show B'), (900003, 'Test Show C');\",\"INSERT INTO show_rollups (srkeys, srlinks, launchmoment) VALUES (900001,1,'2022-01-15'),(900002,2,'2022-06-30'),(900003,3,'2022-12-05'),(900001,4,'2021-03-10');\"], db_name, conn); pred_query_result = execute_queries(pred_sqls, db_name, conn); test_result = execute_queries([\"SELECT get_launch_count_by_year('2022');\"], db_name, conn); print(test_result); assert test_result and test_result[0][0][0] == 2; return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_1", "selected_database": "cybermarket_pattern", "query": "Give me all platforms sorted by its risk score, most dangerous on top and show 4 digits.", "normal_query": "List each marketplace with its Marketplace Risk Score (MRS), rounded to 4 decimal places, highest first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT m.\"PlatCode\",\n       ROUND(\n         (0.4*(m.platform_compliance->>'vuln_inst_count')::numeric +\n          0.3*(m.platform_compliance->>'sec_event_count')::numeric +\n          0.3*COALESCE(NULLIF(REGEXP_REPLACE(m.\"RepScore\",'[^0-9.]','','g'), '')::numeric,0))/100\n       ,4) AS mrs\nFROM markets m\nORDER BY mrs DESC;"], "external_knowledge": [0], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "cybermarket_pattern_M_1", "selected_database": "cybermarket_pattern", "query": "Mark every seller who's currently being investigated or getting a lot of attention from authorities as “High” on the compliance scale, leave the already-High ones alone, and give me the IDs that changed.", "normal_query": "Set the compliance category to “High” for all sellers with an active investigation or high attention from authorities, skipping those already at “High”. Return the IDs of the sellers that were updated.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE vendors SET \"ComplianceRisk\"='Critical' WHERE (\"InvestigationFlag\"='Active' OR \"LE_Interest\"='High') AND \"ComplianceRisk\" IS DISTINCT FROM 'Critical' RETURNING \"SellerKey\";"], "external_knowledge": [10], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    cursor = conn.cursor()\n    cursor.execute('BEGIN;')\n    cursor.execute(\"\"\"\n        SELECT COUNT(*)\n        FROM vendors\n        WHERE (\\\"InvestigationFlag\\\"='Active' OR \\\"LE_Interest\\\"='High')\n          AND \\\"ComplianceRisk\\\" IS DISTINCT FROM 'Critical'\n    \"\"\")\n    expect = cursor.fetchone()[0]\n    cursor.execute(pred_sqls[0])\n    assert cursor.rowcount == expect\n    cursor.execute(\"\"\"\n        SELECT COUNT(*)\n        FROM vendors\n        WHERE (\\\"InvestigationFlag\\\"='Active' OR \\\"LE_Interest\\\"='High')\n          AND \\\"ComplianceRisk\\\" IS DISTINCT FROM 'Critical'\n    \"\"\")\n    still = cursor.fetchone()[0]\n    assert still == 0\n    cursor.execute('ROLLBACK;')\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_M_2", "selected_database": "cybermarket_pattern", "query": "Add a daily review entry for each sale the model rates over 70% fraud risk and doesn't already have one.", "normal_query": "Create a daily review entry for every transaction with model-assessed fraud probability above 70% that currently has no review entry.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["INSERT INTO alerts(\"EventTag\",\"ReviewFreq\",\"alert_case_management\") SELECT t.\"EventCode\",'Daily',jsonb_build_object('needs_followup_stat',true,'severity_level_stat','High') FROM transactions t JOIN risk_analytics ra ON ra.\"TxnLink\"=t.\"EventCode\" LEFT JOIN alerts a ON a.\"EventTag\"=t.\"EventCode\" WHERE ra.\"FraudProb\">70 AND a.\"EventTag\" IS NULL;"], "external_knowledge": [18], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cur = conn.cursor()\n    cur.execute('BEGIN;')\n    cur.execute(\"SELECT COUNT(*) FROM transactions t JOIN risk_analytics ra ON ra.\\\"TxnLink\\\" = t.\\\"EventCode\\\" WHERE ra.\\\"FraudProb\\\" > 70 AND NOT EXISTS (SELECT 1 FROM alerts a WHERE a.\\\"EventTag\\\" = t.\\\"EventCode\\\")\")\n    missing = cur.fetchone()[0]\n    cur.execute(pred_sqls[0])\n    assert cur.rowcount == missing\n    cur.execute(\"SELECT COUNT(*) FROM transactions t JOIN risk_analytics ra ON ra.\\\"TxnLink\\\" = t.\\\"EventCode\\\" WHERE ra.\\\"FraudProb\\\" > 70 AND NOT EXISTS (SELECT 1 FROM alerts a WHERE a.\\\"EventTag\\\" = t.\\\"EventCode\\\")\")\n    after_missing = cur.fetchone()[0]\n    assert after_missing == 0\n    cur.execute('ROLLBACK;')"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_M_3", "selected_database": "cybermarket_pattern", "query": "Purge the top-priority alert cases that are resolved and whose next review date is over 180 days old.", "normal_query": "Delete alert cases at the highest escalation level that are resolved and have a next review date more than 180 days ago.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["DELETE FROM alerts a WHERE a.alert_case_management->>'escalation_tier_stat'='Level3' AND a.alert_case_management->>'case_state_stat'='Resolved' AND a.\"NextReviewDt\"<CURRENT_DATE-INTERVAL '180 days';"], "external_knowledge": [19], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cur = conn.cursor()\n    cur.execute('BEGIN;')\n    cur.execute(\"SELECT COUNT(*) FROM alerts WHERE alert_case_management->>'escalation_tier_stat'='Level3' AND alert_case_management->>'case_state_stat'='Closed' AND \\\"NextReviewDt\\\" < CURRENT_DATE - INTERVAL '180 days'\")\n    old_closed = cur.fetchone()[0]\n    cur.execute(pred_sqls[0])\n    assert cur.rowcount == old_closed\n    cur.execute(\"SELECT COUNT(*) FROM alerts WHERE alert_case_management->>'escalation_tier_stat'='Level3' AND alert_case_management->>'case_state_stat'='Closed' AND \\\"NextReviewDt\\\" < CURRENT_DATE - INTERVAL '180 days'\")\n    remain = cur.fetchone()[0]\n    assert remain == 0\n    cur.execute('ROLLBACK;')"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_M_4", "selected_database": "cybermarket_pattern", "query": "Save the current list of sites that meet the security rule, along with their computed rating, into a fresh archive—replace any prior archive.", "normal_query": "Archive the current list of Secure Platforms together with their Marketplace Risk Score, replacing any existing archive if present.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["DROP TABLE IF EXISTS secure_platform_archive;CREATE TABLE secure_platform_archive AS SELECT m.\"PlatCode\",(0.4*(m.platform_compliance->>'vuln_inst_count')::numeric+0.3*(m.platform_compliance->>'sec_event_count')::numeric+0.3*COALESCE(NULLIF(REGEXP_REPLACE(m.\"RepScore\",'[^0-9.]','','g'),'')::numeric,0))/100 AS mrs FROM markets m WHERE(m.platform_compliance->>'vuln_inst_count')::numeric<10 AND(m.platform_compliance->>'protection_meas_count')::numeric>15;"], "external_knowledge": [17, 0], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn): cur = conn.cursor(); cur.execute('BEGIN;'); cur.execute(\"DROP TABLE IF EXISTS secure_platform_archive;\"); cur.execute(\"CREATE TABLE secure_platform_archive AS SELECT m.\\\"PlatCode\\\", (0.4 * (m.platform_compliance->>'vuln_inst_count')::numeric + 0.3 * (m.platform_compliance->>'sec_event_count')::numeric + 0.3 * COALESCE(NULLIF(REGEXP_REPLACE(m.\\\"RepScore\\\", E'[^0-9.]', '', 'g'), '')::numeric, 0)) / 100 AS mrs FROM markets m WHERE (m.platform_compliance->>'vuln_inst_count')::numeric < 10 AND (m.platform_compliance->>'protection_meas_count')::numeric > 15;\"); cur.execute(pred_sqls[0]); cur.execute(\"SELECT indexname FROM pg_indexes WHERE tablename = 'secure_platform_archive' AND indexname = 'secure_platform_archive_plat_idx'\"); idx = cur.fetchone(); idx = idx[0] if idx else 'NULL'; print('Index found:', idx); cur.execute('ROLLBACK;')"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_2", "selected_database": "cybermarket_pattern", "query": "Split shoppers into three risk-per-dollar groups; for each group, show how many shoppers there are, what fraction of their orders go across countries, and how often their sessions look highly/medium/low hidden.", "normal_query": "Group buyers into three buckets based on Buyer Risk Dollar Ratio; for each bucket, return the buyer count, the share of their transactions that are cross-border, and the distribution of session anonymity (High/Medium/Low).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH brdr AS (SELECT b.\"AcqCode\" AS buyer_id,NULLIF(REGEXP_REPLACE(b.\"buyer_risk_profile\"->>'risk_dollar_ratio','[^0-9.]','','g'),'')::numeric AS brdr FROM buyers b),th AS (SELECT percentile_disc(0.3333) WITHIN GROUP(ORDER BY brdr) AS p1,percentile_disc(0.6667) WITHIN GROUP(ORDER BY brdr) AS p2 FROM brdr WHERE brdr IS NOT NULL),bucketed_buyers AS (SELECT br.buyer_id,CASE WHEN br.brdr IS NULL THEN NULL WHEN br.brdr<=th.p1 THEN 'Low' WHEN br.brdr<=th.p2 THEN 'Medium' ELSE 'High' END AS risk_bucket FROM brdr br CROSS JOIN th),buyer_set AS (SELECT buyer_id,risk_bucket FROM bucketed_buyers WHERE risk_bucket IS NOT NULL),tx AS (SELECT bs.risk_bucket,t.\"EventCode\" AS event_code,LOWER(COALESCE(t.\"CrossBorder\"::text,'')) IN('true','t','1','yes','y') AS is_cross_border FROM buyer_set bs JOIN transactions t ON t.\"AcqLink\"=bs.buyer_id),sess AS (SELECT tx.risk_bucket,tx.event_code,LOWER(cs.\"AnonLevel\") AS anon_level FROM tx LEFT JOIN connection_security cs ON cs.\"TxnPointer\"=tx.event_code),buyers_agg AS (SELECT risk_bucket,COUNT(DISTINCT buyer_id) AS buyer_count FROM buyer_set GROUP BY risk_bucket),tx_agg AS (SELECT risk_bucket,COUNT(DISTINCT event_code) AS txn_count,COUNT(DISTINCT CASE WHEN is_cross_border THEN event_code END) AS cross_border_txn_count FROM tx GROUP BY risk_bucket),sess_agg AS (SELECT risk_bucket,COUNT(DISTINCT event_code) AS sess_count,COUNT(DISTINCT CASE WHEN anon_level='high' THEN event_code END) AS anon_high_sessions,COUNT(DISTINCT CASE WHEN anon_level='medium' THEN event_code END) AS anon_medium_sessions,COUNT(DISTINCT CASE WHEN anon_level='low' THEN event_code END) AS anon_low_sessions FROM sess GROUP BY risk_bucket) SELECT b.risk_bucket,b.buyer_count,COALESCE(t.txn_count,0) AS txn_count,ROUND(100.0*COALESCE(t.cross_border_txn_count,0)/NULLIF(COALESCE(t.txn_count,0),0),2) AS cross_border_txn_pct,ROUND(100.0*COALESCE(s.anon_high_sessions,0)/NULLIF(COALESCE(s.sess_count,0),0),2) AS anon_high_pct,ROUND(100.0*COALESCE(s.anon_medium_sessions,0)/NULLIF(COALESCE(s.sess_count,0),0),2) AS anon_medium_pct,ROUND(100.0*COALESCE(s.anon_low_sessions,0)/NULLIF(COALESCE(s.sess_count,0),0),2) AS anon_low_pct FROM buyers_agg b LEFT JOIN tx_agg t ON t.risk_bucket=b.risk_bucket LEFT JOIN sess_agg s ON s.risk_bucket=b.risk_bucket ORDER BY CASE b.risk_bucket WHEN 'Low' THEN 1 WHEN 'Medium' THEN 2 ELSE 3 END;"], "external_knowledge": [5, 11, 25], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cybermarket_pattern_3", "selected_database": "cybermarket_pattern", "query": "Give me a list of sellers with their transaction flow scores, plus details about how complicated their shipping networks are.", "normal_query": "List vendors along with their Platform Liquidity Rate (PLR), including metrics related to Shipping Route Complexity.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH v_plr AS (SELECT v.\"SellerKey\",NULLIF(REGEXP_REPLACE(v.\"vendor_compliance_ratings\"->>'liq_rate','[^0-9.]','','g'),'')::numeric AS plr FROM vendors v) SELECT p.\"SellerKey\",p.plr,COUNT(t.\"EventCode\") AS txn_count,COALESCE(SUM(CASE WHEN t.\"RouteComplex\"='Simple' THEN 1 ELSE 0 END),0) AS simple_txn,COALESCE(SUM(CASE WHEN t.\"RouteComplex\"='Medium' THEN 1 ELSE 0 END),0) AS medium_txn,COALESCE(SUM(CASE WHEN t.\"RouteComplex\"='Complex' THEN 1 ELSE 0 END),0) AS complex_txn,ROUND(100.0*COALESCE(SUM(CASE WHEN t.\"RouteComplex\"='Simple' THEN 1 ELSE 0 END),0)/NULLIF(COUNT(t.\"EventCode\"),0),2) AS simple_pct,ROUND(100.0*COALESCE(SUM(CASE WHEN t.\"RouteComplex\"='Medium' THEN 1 ELSE 0 END),0)/NULLIF(COUNT(t.\"EventCode\"),0),2) AS medium_pct,ROUND(100.0*COALESCE(SUM(CASE WHEN t.\"RouteComplex\"='Complex' THEN 1 ELSE 0 END),0)/NULLIF(COUNT(t.\"EventCode\"),0),2) AS complex_pct FROM v_plr p LEFT JOIN transactions t ON t.\"VendorLink\"=p.\"SellerKey\" GROUP BY p.\"SellerKey\",p.plr ORDER BY p.plr DESC NULLS LAST;"], "external_knowledge": [6, 24], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cybermarket_pattern_4", "selected_database": "cybermarket_pattern", "query": "Give me how fast each session processed threats, and the levels of login verification for buyers.", "normal_query": "Provide Threat Handling Rate (THR) for each security session, ordered from highest to lowest. Additionally, include metrics related to Buyer Authentication Levels.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT c.\"TxnPointer\" AS session_id, SPLIT_PART(c.\"Threat_handle_rate\", ' ', 1)::numeric AS thr, COALESCE(b.\"AuthLevel\", 'Unknown') AS buyer_auth_level FROM connection_security c LEFT JOIN transactions t ON t.\"EventCode\" = c.\"TxnPointer\" LEFT JOIN buyers b ON b.\"AcqCode\" = t.\"AcqLink\" ORDER BY thr DESC NULLS LAST;"], "external_knowledge": [7, 22], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cybermarket_pattern_5", "selected_database": "cybermarket_pattern", "query": "I want to know the keyword-hitting values for all customer and internal chats to identify high-risk patterns. Round to 3 decimal places and show in descending order", "normal_query": "Calculate Suspicion Signal Density (SSD) for every communication thread, rounded to 3 decimal places and shown in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT cm.\"EventLink\",\n       ROUND(\n         (cm.communication_details->>'keyword_match_count')::numeric /\n         GREATEST((cm.communication_details->>'msg_count_total')::numeric,1)\n       ,3) AS ssd\nFROM communications cm\nORDER BY ssd DESC;"], "external_knowledge": [8, 20], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "cybermarket_pattern_M_5", "selected_database": "cybermarket_pattern", "query": "Update table statistics and query plans for the vendors table, focusing on improving efficiency-related query performance.", "normal_query": "Analyze the vendors table to refresh statistics for Compliance Efficiency Index (CEI) queries.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["ANALYZE vendors;"], "external_knowledge": [2], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cur = conn.cursor()\n    cur.execute('BEGIN;')\n    cur.execute(pred_sqls[0])\n    # If no exception is raised, the ANALYZE succeeded\n    cur.execute('ROLLBACK;')"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_6", "selected_database": "cybermarket_pattern", "query": "Show me all protected platforms, whether they're up or down, how many serious escalation cases they have, and how bad their current alerts are.", "normal_query": "List all Secure Platforms and their current operational status. Also include metrics related to Tier-3 Escalation Case and Alert Severity Levels.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH secure_platforms AS (SELECT m.\"PlatCode\", m.\"OperStatus\" FROM markets m WHERE m.platform_compliance->>'sec_audit_stat' = 'Pass') SELECT s.\"PlatCode\", s.\"OperStatus\", COUNT(*) FILTER (WHERE a.alert_case_management->>'escalation_tier_stat' = 'Level3') AS tier3_case_count, COUNT(*) FILTER (WHERE a.alert_case_management->>'severity_level_stat' = 'Critical') AS critical_alerts, COUNT(*) FILTER (WHERE a.alert_case_management->>'severity_level_stat' = 'High') AS high_alerts, COUNT(*) FILTER (WHERE a.alert_case_management->>'severity_level_stat' = 'Medium') AS medium_alerts, COUNT(*) FILTER (WHERE a.alert_case_management->>'severity_level_stat' = 'Low') AS low_alerts FROM secure_platforms s LEFT JOIN transactions t ON t.\"PlatformKey\" = s.\"PlatCode\" LEFT JOIN alerts a ON a.\"EventTag\" = t.\"EventCode\" GROUP BY s.\"PlatCode\", s.\"OperStatus\" ORDER BY s.\"PlatCode\";"], "external_knowledge": [10, 19, 26], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_7", "selected_database": "cybermarket_pattern", "query": "Tell me how many live listings we have in each category, along with which ones have weird descriptions and how many sketchy buyers are interacting with them.", "normal_query": "Count active listings for each Product Category, shown in descending order. Besides, show metrics related to Language Patterns, Suspicious Buyer.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH active_listings_by_cat AS (SELECT p.\"ProdCat\" AS product_category, COUNT(*) AS active_listing_count FROM products p WHERE COALESCE((p.product_availability->>'qty_avail')::bigint, 0) > 0 GROUP BY p.\"ProdCat\"), lang_metrics_by_cat AS (SELECT tp.\"ProdCat\" AS product_category, COALESCE(SUM((c.communication_details->>'msg_count_total')::bigint) FILTER (WHERE c.communication_details->>'lang_pattern_type' = 'Suspicious'), 0) AS suspicious_msg_count, COALESCE(SUM((c.communication_details->>'msg_count_total')::bigint) FILTER (WHERE c.communication_details->>'lang_pattern_type' = 'Variable'), 0) AS variable_msg_count, COALESCE(SUM((c.communication_details->>'msg_count_total')::bigint) FILTER (WHERE c.communication_details->>'lang_pattern_type' = 'Consistent'), 0) AS consistent_msg_count FROM transaction_products tp JOIN communications c ON c.\"EventLink\" = tp.\"EventLink\" GROUP BY tp.\"ProdCat\"), buyer_metrics AS (SELECT b.\"AcqCode\", (b.buyer_risk_profile->>'behavior_consistency_scr')::numeric AS bcs, NULLIF(REGEXP_REPLACE(b.buyer_risk_profile->>'risk_dollar_ratio', '[^0-9.]', '', 'g'), '')::numeric AS brdr FROM buyers b), medians AS (SELECT percentile_cont(0.5) WITHIN GROUP (ORDER BY bcs) AS median_bcs, percentile_cont(0.5) WITHIN GROUP (ORDER BY brdr) AS median_brdr FROM buyer_metrics WHERE bcs IS NOT NULL AND brdr IS NOT NULL), suspicious_buyers AS (SELECT bm.\"AcqCode\" FROM buyer_metrics bm CROSS JOIN medians m WHERE bm.bcs IS NOT NULL AND bm.brdr IS NOT NULL AND bm.bcs < m.median_bcs AND bm.brdr > m.median_brdr), sus_buyer_by_cat AS (SELECT tp.\"ProdCat\" AS product_category, COUNT(DISTINCT t.\"AcqLink\") FILTER (WHERE sb.\"AcqCode\" IS NOT NULL) AS suspicious_buyer_count, COUNT(DISTINCT t.\"AcqLink\") AS total_buyer_count FROM transaction_products tp JOIN transactions t ON t.\"EventCode\" = tp.\"EventLink\" LEFT JOIN suspicious_buyers sb ON sb.\"AcqCode\" = t.\"AcqLink\" GROUP BY tp.\"ProdCat\") SELECT a.product_category, a.active_listing_count, COALESCE(l.suspicious_msg_count, 0) AS suspicious_msg_count, COALESCE(l.variable_msg_count, 0) AS variable_msg_count, COALESCE(l.consistent_msg_count, 0) AS consistent_msg_count, COALESCE(s.suspicious_buyer_count, 0) AS suspicious_buyer_count, COALESCE(s.total_buyer_count, 0) AS total_buyer_count, CASE WHEN COALESCE(s.total_buyer_count, 0) = 0 THEN 0 ELSE ROUND((COALESCE(s.suspicious_buyer_count, 0)::numeric / s.total_buyer_count) * 100, 2) END AS suspicious_buyer_pct FROM active_listings_by_cat a LEFT JOIN lang_metrics_by_cat l USING (product_category) LEFT JOIN sus_buyer_by_cat s USING (product_category) ORDER BY a.active_listing_count DESC, a.product_category;"], "external_knowledge": [11, 28, 16], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cybermarket_pattern_8", "selected_database": "cybermarket_pattern", "query": "Break down transactions by how complicated their shipping routes were, then show me the counts with the trickiest routes at the top.", "normal_query": "Show the number of transactions per Shipping Route Complexity label, highest first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT t.\"RouteComplex\" AS route_complexity,\n       COUNT(*) AS txn_count\nFROM transactions t\nGROUP BY route_complexity\nORDER BY txn_count DESC;"], "external_knowledge": [12, 13], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cybermarket_pattern_9", "selected_database": "cybermarket_pattern", "query": " Tell me how the average security score stacks up across sessions with different privacy levels, rounded to 2 decimal places, from totally open to fully masked connections.", "normal_query": "List average OpSec score for each Session Anonymity Level, rounded to 2 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT c.\"AnonLevel\" AS anonymity_level,\n       ROUND(AVG(c.\"OpSecMetric\")::numeric,2) AS avg_opsec\nFROM connection_security c\nGROUP BY anonymity_level\nORDER BY avg_opsec DESC;"], "external_knowledge": [13, 18, 15], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "cybermarket_pattern_M_6", "selected_database": "cybermarket_pattern", "query": "I need to optimize the database for cross-border transaction lookups - could you create a dedicated index for those searches?", "normal_query": "Create an index to speed up searches for Cross-Border Transactions.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE INDEX IF NOT EXISTS idx_transactions_crossborder ON transactions(\"CrossBorder\") WHERE \"CrossBorder\" = 1;"], "external_knowledge": [11], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cur = conn.cursor()\n    cur.execute('BEGIN;')\n    cur.execute('DROP INDEX IF EXISTS idx_transactions_crossborder;')\n    cur.execute(pred_sqls[0])\n    cur.execute(\"SELECT 1 FROM pg_indexes WHERE indexname='idx_transactions_crossborder'\")\n    idx = cur.fetchone()\n    assert idx is not None\n    cur.execute('ROLLBACK;')"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_10", "selected_database": "cybermarket_pattern", "query": "I want to know the average keyword-hitting values for all customer and internal chats to identify high-risk patterns. Round to 3 decimal places.", "normal_query": "Return the average Suspicion Signal Density (SSD) across all communications, rounded to 3 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ssd AS (SELECT (cm.communication_details->>'keyword_match_count')::numeric/NULLIF((cm.communication_details->>'msg_count_total')::numeric, 0) AS ssd FROM communications cm) SELECT ROUND(AVG(ssd), 3) AS avg_ssd FROM ssd;"], "external_knowledge": [15, 8], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_M_7", "selected_database": "cybermarket_pattern", "query": "Make a table called 'suspicious_buyers_cap' that lists all the shady buyers, but only include ones that hit at least $10 in suspicious activity.", "normal_query": "Create table suspicious_buyers_cap listing Suspicious Buyers with a $10 cap.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE IF NOT EXISTS suspicious_buyers_cap AS WITH parsed AS (SELECT b.\"AcqCode\" AS acq_code, CAST(NULLIF(REGEXP_REPLACE(b.buyer_risk_profile->>'risk_metric_scr', '[^0-9.+-]', '', 'g'), '') AS numeric) AS risk_metric_scr, CAST(NULLIF(REGEXP_REPLACE(b.buyer_risk_profile->>'behavior_consistency_scr', '[^0-9.+-]', '', 'g'), '') AS numeric) AS behavior_consistency_scr, CAST(NULLIF(REGEXP_REPLACE(b.buyer_risk_profile->>'risk_dollar_ratio', '[^0-9.+-]', '', 'g'), '') AS numeric) AS risk_dollar_ratio FROM buyers b), thresholds AS (SELECT PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY behavior_consistency_scr) AS bc_p25, PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY risk_dollar_ratio) AS rdr_p75 FROM parsed), scored AS (SELECT p.*, CASE WHEN p.risk_dollar_ratio IS NULL OR p.risk_dollar_ratio = 0 THEN NULL ELSE p.risk_metric_scr / p.risk_dollar_ratio END AS suspicious_amount_usd FROM parsed p) SELECT s.acq_code AS \"AcqCode\", s.risk_metric_scr, s.behavior_consistency_scr, s.risk_dollar_ratio, s.suspicious_amount_usd, t.bc_p25 AS bc_threshold_p25, t.rdr_p75 AS rdr_threshold_p75, 'P25 for behavior_consistency' AS bc_rule, 'P75 for risk_dollar_ratio' AS rdr_rule, 500::numeric AS suspicious_amount_cutoff_usd FROM scored s CROSS JOIN thresholds t WHERE s.behavior_consistency_scr IS NOT NULL AND s.risk_dollar_ratio IS NOT NULL AND s.suspicious_amount_usd IS NOT NULL AND s.behavior_consistency_scr < t.bc_p25 AND s.risk_dollar_ratio > t.rdr_p75 AND s.suspicious_amount_usd >= 10;"], "external_knowledge": [16], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cur = conn.cursor()\n    cur.execute('BEGIN;')\n    cur.execute('DROP TABLE IF EXISTS suspicious_buyers_cap;')\n    cur.execute(pred_sqls[0])\n    cur.execute(\"SELECT COUNT(*) FROM suspicious_buyers_cap\")\n    rows = cur.fetchone()[0]\n    assert rows > 0\n    cur.execute('ROLLBACK;')"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_M_8", "selected_database": "cybermarket_pattern", "query": "I need to mandate sessions secured by two factor across the board. Please configure the system to upgrade any active sessions still relying on basic authentication.", "normal_query": "Force Premium Authentication by setting auth_protocol_type to \"2FA\" for every session that is currently using \"Basic\".", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE connection_security SET connection_security_metrics = jsonb_set(connection_security_metrics, '{auth_protocol_type}', '\"2FA\"', false) WHERE connection_security_metrics->>'auth_protocol_type' = 'Basic';"], "external_knowledge": [14], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cur = conn.cursor()\n    cur.execute('BEGIN;')\n    cur.execute(\"SELECT COUNT(*) FROM connection_security WHERE connection_security_metrics->>'auth_protocol_type'='Basic'\")\n    basic_before = cur.fetchone()[0]\n    cur.execute(pred_sqls[0])\n    assert cur.rowcount == basic_before\n    cur.execute(\"SELECT COUNT(*) FROM connection_security WHERE connection_security_metrics->>'auth_protocol_type'='Basic'\")\n    basic_after = cur.fetchone()[0]\n    assert basic_after == 0\n    cur.execute('ROLLBACK;')"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_11", "selected_database": "cybermarket_pattern", "query": "I need the total number of transactions that were both marked as fraud and involved cross-border payments.", "normal_query": "Count Fraud-Flagged Transactions that are Cross-Border.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT COUNT(*) AS cross_border_fraud\nFROM transactions t\nJOIN risk_analytics ra ON ra.\"TxnLink\" = t.\"EventCode\"\nWHERE ra.\"FraudProb\" > 70\n  AND t.\"CrossBorder\" = 1;"], "external_knowledge": [18, 11], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_12", "selected_database": "cybermarket_pattern", "query": "Calculate how many hours we typically take to close Tier-3 escalations. Show the average value, rounded to hundredths.", "normal_query": "Return the average resolve time in hours for Tier-3 Escalation Cases, rounded to 2 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT ROUND(AVG( (a.alert_case_management->>'resolve_hours')::numeric ),2) AS avg_resolve_hrs_t3\nFROM alerts a\nWHERE a.alert_case_management->>'escalation_tier_stat' = 'Level3';"], "external_knowledge": [19, 27, 29, 9], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_13", "selected_database": "cybermarket_pattern", "query": "How many platforms show as 'active' right now?", "normal_query": "Count platforms currently marked as Active.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT COUNT(*) AS active_platforms\nFROM markets m\nWHERE m.\"OperStatus\" = 'Active';"], "external_knowledge": [20], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_M_9", "selected_database": "cybermarket_pattern", "query": "Show me where our response is slowest—give me a quick breakdown by key groups, a percentile snapshot, and the 50 slowest sessions.", "normal_query": "Analyze connection_security to optimize Threat Handling Rate reports.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["ANALYZE connection_security;"], "external_knowledge": [7], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cur = conn.cursor()\n    cur.execute('BEGIN;')\n    cur.execute(pred_sqls[0])  # should run without error\n    cur.execute('ROLLBACK;')"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_14", "selected_database": "cybermarket_pattern", "query": "How many shoppers are using advanced authentication?", "normal_query": "Count buyers who have Advanced authentication.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT COUNT(*) AS adv_auth_buyers\nFROM buyers b\nWHERE b.\"AuthLevel\" = 'Advanced';"], "external_knowledge": [22], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_15", "selected_database": "cybermarket_pattern", "query": "What's the overall revenue from digital goods? Round the result to 2 decimal places.", "normal_query": "Sum total sales value for Digital product listings, rounded to 2 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT ROUND(SUM((tp.\"PriceAmt\")*tp.\"QtySold\")::numeric,2) AS total_digital_sales_value\nFROM transaction_products tp\nWHERE tp.\"ProdCat\" = 'Digital';"], "external_knowledge": [23], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_16", "selected_database": "cybermarket_pattern", "query": "What's the average distance traveled for shipments with complex routes? Round the result to 2 decimal places.", "normal_query": "Compute the average geographical distance for shipments on complex routes and round the result to two decimals.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH parsed AS (SELECT NULLIF(regexp_replace(t.\"GeoDistScore\", '[^0-9\\\\.]', '', 'g'), '')::numeric AS geo_km FROM transactions t WHERE t.\"RouteComplex\" = 'Complex') SELECT ROUND(AVG(geo_km), 2) AS avg_geo_dist_complex FROM parsed WHERE geo_km IS NOT NULL;"], "external_knowledge": [24], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_M_10", "selected_database": "cybermarket_pattern", "query": "Set up the secure-platform snapshot—only create it if it isn't there yet.", "normal_query": "Create the secure-platform summary materialized view if it does not already exist.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE MATERIALIZED VIEW IF NOT EXISTS secure_platform_summary AS\nSELECT m.\"PlatCode\",\n(\n0.4 * (m.platform_compliance->>'vuln_inst_count')::numeric +\n 0.3 * (m.platform_compliance->>'sec_event_count')::numeric +\n 0.3 * COALESCE(NULLIF(REGEXP_REPLACE(m.\"RepScore\", '[^0-9.]', '', 'g'), '')::numeric, 0)\n) / 100 AS mrs\nFROM markets m\nWHERE (m.platform_compliance->>'vuln_inst_count')::numeric < 10\n  AND (m.platform_compliance->>'protection_meas_count')::numeric > 15;"], "external_knowledge": [17, 0], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn): cur = conn.cursor(); cur.execute('BEGIN;'); cur.execute('DROP MATERIALIZED VIEW IF EXISTS secure_platform_summary;'); cur.execute(pred_sqls[0]); cur.execute('SELECT COUNT(*) FROM secure_platform_summary'); row = cur.fetchone()[0]; assert row >= 0; cur.execute('ROLLBACK;')"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_17", "selected_database": "cybermarket_pattern", "query": "How many critical alerts do we have?", "normal_query": "Count alerts with Critical severity level.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT COUNT(*) AS critical_alerts\nFROM alerts a\nWHERE a.alert_case_management->>'severity_level_stat' = 'Critical';"], "external_knowledge": [26], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_18", "selected_database": "cybermarket_pattern", "query": "What's the ratio of sales went through escrow? Round to 2 decimal places.", "normal_query": "Calculate the ratio of transactions that used escrow, rounded to 2 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT ROUND(\n  SUM( CASE WHEN (t.transaction_financials->>'escrow_used_stat') = 'Yes' THEN 1 ELSE 0 END)::numeric /\n  NULLIF(COUNT(*),0),2) AS escrow_use_ratio\nFROM transactions t;"], "external_knowledge": [2, 6, 27], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_19", "selected_database": "cybermarket_pattern", "query": "How many message threads contain irregular phrasing, sudden language switches, or machine translated text that indicate possible deception?", "normal_query": "Count communication threads with Suspicious language patterns.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT COUNT(*) AS suspicious_language_threads\nFROM communications c\nWHERE c.communication_details->>'lang_pattern_type' = 'Suspicious';"], "external_knowledge": [28], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cybermarket_pattern_20", "selected_database": "cybermarket_pattern", "query": "How many buyers have unpredictable spending trends?", "normal_query": "Count buyers with Variable spend pattern.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT COUNT(*) AS variable_spend_buyers\nFROM buyers b\nWHERE b.buyer_risk_profile->>'spend_pattern' = 'Variable';"], "external_knowledge": [29, 4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "archeology_scan_1", "selected_database": "archeology_scan", "query": "I'd like to see which of our dig sites have the best scan quality ratings. Could you show me each site's ID and name along with their average quality score, sorted best to worst?", "normal_query": "I'd like to see a quality assessment of scans across our archaeological sites. Show site code, site name, average Scan Quality Score for each site and rank them from highest to lowest quality.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT s.zoneref AS site_code, st.zonelabel AS site_name, ROUND(AVG(POWER(10.0 / NULLIF((log10(NULLIF((pc.cloud_metrics->>'Scan_Resol_Mm')::real, 0) * 1000.0) / NULLIF(log10(NULLIF((pc.cloud_metrics->>'Point_Dense')::bigint, 0)), 0) * 5.0), 0), 1.5) * ((pc.cloud_metrics->>'Cover_Pct')::real / 100.0 * (1 + ((pc.cloud_metrics->>'Lap_Pct')::real / 100.0) * (1 - (pc.cloud_metrics->>'Cover_Pct')::real / 100.0))) * POWER(GREATEST(1.0 - (pc.cloud_metrics->>'Noise_Db')::real / 30.0, 0), 2))::numeric, 2) AS avg_sqs FROM scans s JOIN pointcloud pc ON s.arcref = pc.arcref AND s.crewref = pc.crewref JOIN sites st ON s.zoneref = st.zoneregistry GROUP BY s.zoneref, st.zonelabel ORDER BY avg_sqs DESC;"], "external_knowledge": [3], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "archeology_scan_2", "selected_database": "archeology_scan", "query": "Which sites need urgent conservation work? Please show me each location's ID, name, structural condition, preservation status, and whether they're in a high-risk category.", "normal_query": "Could you help me find archaeological sites that might need urgent conservation attention? I'm particularly interested in identifying sites that fall into Degradation Risk Zones. For each site, I'd like to see their code, name, structural state, and preservation status, along with their Risk Zone Category. This information would help our conservation team prioritize their efforts.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n    st.zoneregistry AS site_code,\n    st.zonelabel AS site_name,\n    c.structstate AS structural_state,\n    (st.site_status->>'Pres_Stat') AS preservation_status,\n    CASE\n        WHEN (st.site_status->>'Pres_Stat') IN ('Poor', 'Critical') AND c.structstate <> 'Stable' THEN 'Degradation Risk Zone'\n        ELSE 'Not in Risk Zone'\n    END AS risk_zone\nFROM sites st\nLEFT JOIN conservation c ON st.zoneregistry = c.zoneref;"], "external_knowledge": [14, 26, 52], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "archeology_scan_3", "selected_database": "archeology_scan", "query": "Where are the best places to do scanning based on weather conditions? Show me each site's ID and name with their average environmental condition score indicating suitability for scanning operations.", "normal_query": "I'm planning our upcoming archaeological scanning sessions and want to understand which sites have the most favorable scanning environments. Could you show me a report with each site's code, name, and its average Environmental Suitability Index? This would help us prioritize locations where we'll get the best scan quality.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT e.zoneref AS site_code, st.zonelabel AS site_name, ROUND(AVG(100.0 - 2.5 * ABS(COALESCE((e.ambient_cond->>'Ambic_Temp')::real, 20.0) - 20.0) - POWER(ABS((COALESCE((e.ambient_cond->>'Hume_Pct')::real, 50.0) - 50.0)/2.0), 1.5) - 600.0 / (COALESCE((e.ambient_cond->>'Illume_Lux')::bigint, 1000) + 100.0))::numeric, 2) AS avg_esi FROM environment e JOIN sites st ON e.zoneref = st.zoneregistry WHERE e.ambient_cond IS NOT NULL AND e.ambient_cond->>'Ambic_Temp' IS NOT NULL AND e.ambient_cond->>'Hume_Pct' IS NOT NULL AND e.ambient_cond->>'Illume_Lux' IS NOT NULL GROUP BY e.zoneref, st.zonelabel ORDER BY avg_esi DESC;"], "external_knowledge": [7], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "archeology_scan_4", "selected_database": "archeology_scan", "query": "How reliable are our scan alignments? For each alignment record, could you show me the registration accuracy relative to scan resolution and the registration confidence category. I need to see its registration ID, project ID, accuracy measurements, error values, calculated ratio, and the confidence category.", "normal_query": "I'm evaluating the quality of our scan registrations and would like to understand which ones are most reliable for spatial analysis. Could you show me the Registration Accuracy Ratio and Registration Confidence Level for each registration? I'd need to see the registration ID, project ID, accuracy measurements, error values, calculated RAR (rounded to 2 decimal places), and what confidence level that translates to.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT r.logregistry AS registration_id, r.arcref AS project_id, COALESCE((r.reg_accuracy->>'Log_Accu_Mm')::real, 1.0) AS registration_accuracy_mm, COALESCE((r.reg_accuracy->>'Err_Val_Mm')::real, 1.0) AS error_value_mm, ROUND((COALESCE((pc.cloud_metrics->>'Scan_Resol_Mm')::real, 1.0) / (COALESCE((r.reg_accuracy->>'Log_Accu_Mm')::real, 1.0) * SQRT(1.0 + COALESCE((r.reg_accuracy->>'Err_Val_Mm')::real, 1.0) / COALESCE((r.reg_accuracy->>'Log_Accu_Mm')::real, 1.0))))::numeric, 2) AS rar, CASE WHEN (COALESCE((pc.cloud_metrics->>'Scan_Resol_Mm')::real, 1.0) / (COALESCE((r.reg_accuracy->>'Log_Accu_Mm')::real, 1.0) * SQRT(1.0 + COALESCE((r.reg_accuracy->>'Err_Val_Mm')::real, 1.0) / COALESCE((r.reg_accuracy->>'Log_Accu_Mm')::real, 1.0)))) > 1.5 AND COALESCE(r.reg_accuracy->>'Log_Method', '') ILIKE '%Target%' THEN 'High Confidence' WHEN (COALESCE((pc.cloud_metrics->>'Scan_Resol_Mm')::real, 1.0) / (COALESCE((r.reg_accuracy->>'Log_Accu_Mm')::real, 1.0) * SQRT(1.0 + COALESCE((r.reg_accuracy->>'Err_Val_Mm')::real, 1.0) / COALESCE((r.reg_accuracy->>'Log_Accu_Mm')::real, 1.0)))) BETWEEN 1.0 AND 1.5 THEN 'Medium Confidence' ELSE 'Low Confidence' END AS confidence_level FROM registration r JOIN pointcloud pc ON r.arcref = pc.arcref AND r.crewref = pc.crewref WHERE r.reg_accuracy IS NOT NULL AND pc.cloud_metrics IS NOT NULL ORDER BY rar DESC;"], "external_knowledge": [33, 44], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "archeology_scan_5", "selected_database": "archeology_scan", "query": "Which archaeologicalsites have the best digital preservation? Rank our locations showing their ID, designation, and a comprehensive metric for evaluating digital preservation quality, with the best first.", "normal_query": "For our archaeological site evaluation, I need to quantify the Digital Preservation Quality metrics across our collection. Please compute a comprehensive DPQ index for each archaeological location. Present the results in descending order of DPQ values, displaying only the site identification code, site designation, and calculated DPQ value (rounded to two decimal places) to facilitate prioritization of our digital preservation resources.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH adc_data AS (\n    SELECT\n        s.zoneref,\n        AVG(\n            -- SQS calculation: (10/SRI)^1.5 * (SCE/100) * (1 - noise/30)^2 * 0.4\n            (POWER(10.0 / (\n                ln(COALESCE((pc.cloud_metrics->>'Scan_Resol_Mm')::real, 1.0) * 1000.0) / \n                ln(COALESCE((pc.cloud_metrics->>'Point_Dense')::bigint, 1000)::real) * 5.0\n            ), 1.5) *\n            (COALESCE((pc.cloud_metrics->>'Cover_Pct')::real, 0.0) * \n             (1 + COALESCE((pc.cloud_metrics->>'Lap_Pct')::real, 0.0) / 100.0 * \n              (1 - COALESCE((pc.cloud_metrics->>'Cover_Pct')::real, 0.0) / 100.0)) / 100.0) *\n            POWER(GREATEST(0.0, 1.0 - COALESCE((pc.cloud_metrics->>'Noise_Db')::real, 0.0) / 30.0), 2) * 0.4)\n            + \n            -- MFS calculation: MCR * (TDI/10) * (1 + exp(-geom_accuracy))\n            (COALESCE((m.mesh_specs->>'Facet_Faces')::bigint, 0)::real / \n             GREATEST(1.0, COALESCE((m.mesh_specs->>'Facet_Verts')::bigint, 1)::real * \n                     POWER(COALESCE((m.mesh_specs->>'Facet_Res_Mm')::real, 1.0), 2)) * 1000.0 *\n             (COALESCE((m.mesh_specs->>'Tex_Pix')::bigint, 1024)::real / \n              GREATEST(1.0, sqrt(COALESCE((m.mesh_specs->>'Facet_Faces')::bigint, 1)::real) * \n                      COALESCE((m.mesh_specs->>'Facet_Res_Mm')::real, 1.0)) * 0.01 *0.1) *\n             (1 + exp(-COALESCE((m.mesh_specs->>'Geom_Delta_Mm')::real, 1.0))) * 0.4)\n            + \n            -- SCE calculation: Coverage * (1 + overlap/100 * (1 - coverage/100)) * 0.2\n            (COALESCE((pc.cloud_metrics->>'Cover_Pct')::real, 0.0) * \n             (1 + COALESCE((pc.cloud_metrics->>'Lap_Pct')::real, 0.0) / 100.0 * \n              (1 - COALESCE((pc.cloud_metrics->>'Cover_Pct')::real, 0.0) / 100.0)) * 0.2)\n            - \n            -- Noise penalty: 5 * sqrt(noise/10)\n            (5.0 * sqrt(GREATEST(0.0, COALESCE((pc.cloud_metrics->>'Noise_Db')::real, 0.0) / 10.0)))\n        ) AS adc\n    FROM scans s\n    JOIN pointcloud pc ON s.arcref = pc.arcref\n    LEFT JOIN mesh m ON s.zoneref = m.zoneref\n    WHERE pc.cloud_metrics IS NOT NULL\n    GROUP BY s.zoneref\n),\nmfs_data AS (\n    SELECT\n        m.zoneref,\n        AVG(\n            -- MCR * (TDI/10) * (1 + exp(-geometric_accuracy))\n            COALESCE((m.mesh_specs->>'Facet_Faces')::bigint, 0)::real / \n            GREATEST(1.0, COALESCE((m.mesh_specs->>'Facet_Verts')::bigint, 1)::real * \n                    POWER(COALESCE((m.mesh_specs->>'Facet_Res_Mm')::real, 1.0), 2)) * 1000.0 *\n            (COALESCE((m.mesh_specs->>'Tex_Pix')::bigint, 1024)::real / \n             GREATEST(1.0, sqrt(COALESCE((m.mesh_specs->>'Facet_Faces')::bigint, 1)::real) * \n                     COALESCE((m.mesh_specs->>'Facet_Res_Mm')::real, 1.0)) * 0.01 * 0.1) *\n            (1 + exp(-COALESCE((m.mesh_specs->>'Geom_Delta_Mm')::real, 1.0)))\n        ) AS mfs\n    FROM mesh m\n    WHERE m.mesh_specs IS NOT NULL\n    GROUP BY m.zoneref\n),\nrar_data AS (\n    SELECT\n        s.zoneref,\n        AVG(\n            -- RAR = scan_resolution / (registration_accuracy * sqrt(1 + error_val/registration_accuracy))\n            COALESCE((pc.cloud_metrics->>'Scan_Resol_Mm')::real, 1.0) / \n            GREATEST(0.001, COALESCE((r.reg_accuracy->>'Log_Accu_Mm')::real, 1.0) * \n                    sqrt(1.0 + GREATEST(0.0, COALESCE((r.reg_accuracy->>'Err_Val_Mm')::real, 0.0) / \n                                        GREATEST(0.001, COALESCE((r.reg_accuracy->>'Log_Accu_Mm')::real, 1.0)))))\n        ) AS rar\n    FROM registration r\n    JOIN scans s ON r.arcref = s.arcref\n    JOIN pointcloud pc ON r.arcref = pc.arcref\n    WHERE r.reg_accuracy IS NOT NULL \n      AND pc.cloud_metrics IS NOT NULL\n    GROUP BY s.zoneref\n),\nsce_data AS (\n    SELECT\n        s.zoneref,\n        AVG(\n            -- SCE = coverage * (1 + overlap/100 * (1 - coverage/100))\n            COALESCE((pc.cloud_metrics->>'Cover_Pct')::real, 0.0) * \n            (1 + COALESCE((pc.cloud_metrics->>'Lap_Pct')::real, 0.0) / 100.0 * \n             (1 - COALESCE((pc.cloud_metrics->>'Cover_Pct')::real, 0.0) / 100.0))\n        ) AS sce\n    FROM scans s\n    JOIN pointcloud pc ON s.arcref = pc.arcref\n    WHERE pc.cloud_metrics IS NOT NULL\n    GROUP BY s.zoneref\n),\nerr_data AS (\n    SELECT\n        s.zoneref,\n        AVG(\n            -- Error ratio for penalty calculation\n            COALESCE((r.reg_accuracy->>'Err_Val_Mm')::real, 0.0) / \n            GREATEST(0.001, COALESCE((pc.cloud_metrics->>'Scan_Resol_Mm')::real, 1.0))\n        ) AS error_ratio\n    FROM registration r\n    JOIN scans s ON r.arcref = s.arcref\n    JOIN pointcloud pc ON r.arcref = pc.arcref\n    WHERE r.reg_accuracy IS NOT NULL \n      AND pc.cloud_metrics IS NOT NULL\n    GROUP BY s.zoneref\n)\nSELECT\n    st.zoneregistry AS site_code,\n    st.zonelabel AS site_name,\n    ROUND(CAST(\n        (0.3 * COALESCE(adc.adc, 0.0)) +\n        (0.3 * COALESCE(mfs.mfs, 0.0)) +\n        (0.2 * COALESCE(rar.rar, 0.0)) +\n        (0.2 * COALESCE(sce.sce, 0.0)) -\n        (2.0 * sqrt(GREATEST(0.0, COALESCE(err.error_ratio, 0.0))))\n    AS NUMERIC), 2) AS dpq\nFROM sites st\nLEFT JOIN adc_data adc ON st.zoneregistry = adc.zoneref\nLEFT JOIN mfs_data mfs ON st.zoneregistry = mfs.zoneref\nLEFT JOIN rar_data rar ON st.zoneregistry = rar.zoneref\nLEFT JOIN sce_data sce ON st.zoneregistry = sce.zoneref\nLEFT JOIN err_data err ON st.zoneregistry = err.zoneref\nORDER BY dpq DESC;"], "external_knowledge": [1, 6, 9, 33, 38], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "archeology_scan_6", "selected_database": "archeology_scan", "query": "How good are our 3D models based on the criteria for high-fidelity standard? Please generate a comprehensive report that shows each site's ID, name, total mesh count, high-fidelity mesh count and proportion (as a percentage), average ratio of mesh complexity, average resolution parameters (in mm), average geometric accuracy measurements and Mesh Quality category. Present the data with the highest-fidelity results first.", "normal_query": "Would you generate a comprehensive report categorizing sites based on High Fidelity Mesh standard? For each archaeological location, please include the site code, side name, total mesh count, high-fidelity mesh count and proportion (as a percentage), the average Mesh Complexity Ratio, average resolution parameters (in mm), average geometric accuracy measurements and Mesh Quality Classification. The data should be presented in descending order of high-fidelity percentage.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH mesh_analysis AS (\n    SELECT \n        m.zoneref AS site_code,\n        m.facetregistry AS mesh_id,\n        -- MCR calculation with proper NULL handling and division protection\n        COALESCE((m.mesh_specs->>'Facet_Faces')::bigint, 0)::real / \n        GREATEST(1.0, \n            COALESCE((m.mesh_specs->>'Facet_Verts')::bigint, 1)::real * \n            POWER(COALESCE((m.mesh_specs->>'Facet_Res_Mm')::real, 1.0), 2)\n        ) * 1000.0 AS mcr,\n        COALESCE((m.mesh_specs->>'Facet_Res_Mm')::real, 0.0) AS facet_res_mm,\n        COALESCE((m.mesh_specs->>'Geom_Delta_Mm')::real, 0.0) AS geom_delta_mm,\n        -- High Fidelity Mesh criteria: MCR > 5.0, Resolution < 1.0mm, Geometric Accuracy < 0.5mm\n        CASE \n            WHEN (COALESCE((m.mesh_specs->>'Facet_Faces')::bigint, 0)::real / \n                  GREATEST(1.0, \n                      COALESCE((m.mesh_specs->>'Facet_Verts')::bigint, 1)::real * \n                      POWER(COALESCE((m.mesh_specs->>'Facet_Res_Mm')::real, 1.0), 2)\n                  ) * 1000.0) > 5.0 \n                 AND COALESCE((m.mesh_specs->>'Facet_Res_Mm')::real, 999.0) < 1.0 \n                 AND COALESCE((m.mesh_specs->>'Geom_Delta_Mm')::real, 999.0) < 0.5 \n            THEN TRUE\n            ELSE FALSE\n        END AS is_high_fidelity\n    FROM mesh m\n    WHERE m.mesh_specs IS NOT NULL\n      AND COALESCE((m.mesh_specs->>'Facet_Verts')::bigint, 0) > 0\n      AND COALESCE((m.mesh_specs->>'Facet_Faces')::bigint, 0) > 0\n      AND COALESCE((m.mesh_specs->>'Facet_Res_Mm')::real, 0.0) > 0.0\n)\nSELECT \n    s.zoneregistry AS site_code,\n    s.zonelabel AS site_name,\n    COALESCE(COUNT(ma.mesh_id), 0) AS total_meshes,\n    COALESCE(SUM(CASE WHEN ma.is_high_fidelity THEN 1 ELSE 0 END), 0) AS high_fidelity_count,\n    CASE \n        WHEN COUNT(ma.mesh_id) = 0 THEN 0.00\n        ELSE ROUND(\n            (SUM(CASE WHEN ma.is_high_fidelity THEN 1 ELSE 0 END)::numeric / \n             COUNT(ma.mesh_id)::numeric * 100.0), 2\n        )\n    END AS high_fidelity_percentage,\n    CASE \n        WHEN COUNT(ma.mesh_id) = 0 THEN NULL\n        ELSE ROUND(AVG(ma.mcr)::numeric, 2)\n    END AS avg_mcr,\n    CASE \n        WHEN COUNT(ma.mesh_id) = 0 THEN NULL\n        ELSE ROUND(AVG(ma.facet_res_mm)::numeric, 2)\n    END AS avg_mesh_resolution_mm,\n    CASE \n        WHEN COUNT(ma.mesh_id) = 0 THEN NULL\n        ELSE ROUND(AVG(ma.geom_delta_mm)::numeric, 2)\n    END AS avg_geometric_accuracy_mm,\n    -- Mesh Quality Classification based on knowledge bank #53\n    CASE \n        WHEN COUNT(ma.mesh_id) = 0 THEN 'No Mesh Data'\n        WHEN SUM(CASE WHEN ma.is_high_fidelity THEN 1 ELSE 0 END) > 0 THEN 'Has High-Fidelity Meshes'\n        ELSE 'Standard Mesh Quality'\n    END AS mesh_quality_classification\nFROM sites s\nLEFT JOIN mesh_analysis ma ON s.zoneregistry = ma.site_code\nGROUP BY s.zoneregistry, s.zonelabel\nORDER BY \n    high_fidelity_percentage DESC, \n    high_fidelity_count DESC,\n    total_meshes DESC;"], "external_knowledge": [4, 13, 53], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "archeology_scan_7", "selected_database": "archeology_scan", "query": "What are the scanning conditions like at each site? Show me each location's code and name, along with weather averages (temperature, humidity, and illumination levels), environment  suitability score, and corresponding quartile ranking and environmental condition category based on the score.", "normal_query": "Show me each site's code and name, along with the average temperature, humidity, and illumination levels. I'd also like to see the average Environmental Suitability Index for each site, classified into quartiles, to understand the range of conditions. Finally, classify each site into Environmental Condition Classification System according to average ESI value.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH environment_analysis AS (\n    SELECT \n        e.zoneref AS site_code,\n        COALESCE((e.ambient_cond->>'Ambic_Temp')::real, 20.0) AS ambic_temp,\n        COALESCE((e.ambient_cond->>'Hume_Pct')::real, 50.0) AS hume_pct,\n        COALESCE((e.ambient_cond->>'Illume_Lux')::bigint, 10000) AS illume_lux,\n        -- ESI calculation with proper NULL handling\n        100 - 2.5 * ABS(COALESCE((e.ambient_cond->>'Ambic_Temp')::real, 20.0) - 20) - \n        POWER(ABS((COALESCE((e.ambient_cond->>'Hume_Pct')::real, 50.0) - 50) / 2), 1.5) - \n        600.0 / (COALESCE((e.ambient_cond->>'Illume_Lux')::bigint, 10000) + 100) AS esi\n    FROM environment e\n    WHERE e.ambient_cond IS NOT NULL\n)\nSELECT \n    s.zoneregistry AS site_code,\n    s.zonelabel AS site_name,\n    CASE \n        WHEN COUNT(ea.ambic_temp) = 0 THEN NULL\n        ELSE ROUND(AVG(ea.ambic_temp)::numeric, 1)\n    END AS avg_temperature_c,\n    CASE \n        WHEN COUNT(ea.hume_pct) = 0 THEN NULL\n        ELSE ROUND(AVG(ea.hume_pct)::numeric, 1)\n    END AS avg_humidity_pct,\n    CASE \n        WHEN COUNT(ea.illume_lux) = 0 THEN NULL\n        ELSE ROUND(AVG(ea.illume_lux)::numeric, 1)\n    END AS avg_illumination_lux,\n    CASE \n        WHEN COUNT(ea.esi) = 0 THEN NULL\n        ELSE ROUND(AVG(ea.esi)::numeric, 1)\n    END AS avg_esi,\n    CASE \n        WHEN COUNT(ea.esi) = 0 THEN NULL\n        ELSE NTILE(4) OVER (ORDER BY AVG(ea.esi))\n    END AS esi_quartile,\n    CASE \n        WHEN COUNT(ea.esi) = 0 THEN 'No Environmental Data'\n        WHEN AVG(ea.esi) > 85 THEN 'Optimal Scanning Conditions'\n        WHEN AVG(ea.esi) > 70 THEN 'Good Scanning Conditions'\n        WHEN AVG(ea.esi) > 50 THEN 'Acceptable Scanning Conditions'\n        ELSE 'Challenging Scanning Conditions'\n    END AS scanning_condition_class\nFROM sites s\nLEFT JOIN environment_analysis ea ON s.zoneregistry = ea.site_code\nGROUP BY s.zoneregistry, s.zonelabel\nORDER BY avg_esi DESC NULLS LAST;"], "external_knowledge": [7, 15, 50], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 1, "distinct": false, "order": true}}
{"instance_id": "archeology_scan_8", "selected_database": "archeology_scan", "query": "I'd like to analyze how efficiently each scan processing workflow performs and spot any bottlenecks. For every software and stage combination, show me the software, processing stage, average hours needed for processing, average CPU and GPU usage percentages, average data size in GB, the ratio of the processing efficiency, and whether it's running efficiently or hitting bottlenecks ('Bottleneck Detected' if it is qualified as processing bottleneck, 'Efficient' if it is not). Also include how many workflows we're looking at for each combination. Sort the results by bottleneck status first, followed by the ratio value from lowest to highest.", "normal_query": "I want to evaluate each scan processing workflow's Processing Efficiency Ratio and identify whether it qualifies as a Processing Bottleneck. For each combination of processing software and stage, please include the software, stage, average processing hours, average CPU and GPU usage percentages, average data size in GB, the average PER value, and the the efficiency status ('Bottleneck Detected' if it is qualified as processing bottleneck, 'Efficient' if it is not). Additionally, provide the total count of workflows for each combination. Sort the results by bottleneck status first, followed by the PER value in ascending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH workflow_analysis AS (\n    SELECT \n        p.flowregistry,\n        p.flowsoft AS processing_software,\n        p.flowstage AS processing_stage,\n        COALESCE((p.system_usage->>'Flow_Hrs')::real, 1.0) AS flow_hrs,\n        COALESCE((p.system_usage->>'Proc_CPU')::bigint, 50) AS proc_cpu,\n        COALESCE((p.system_usage->>'Proc_GPU')::bigint, 50) AS proc_gpu,\n        COALESCE(REGEXP_REPLACE(s.size, '[^0-9.]', '', 'g')::real, 1.0) AS data_size_gb,\n        COALESCE((pc.cloud_metrics->>'Total_Pts')::bigint, 1000000) AS total_pts,\n        -- PER calculation with proper NULL handling and division protection\n        COALESCE(REGEXP_REPLACE(s.size, '[^0-9.]', '', 'g')::real, 1.0) * LOG(10, GREATEST(1.0, COALESCE((pc.cloud_metrics->>'Total_Pts')::bigint, 1000000))) /\n        GREATEST(0.1, \n            COALESCE((p.system_usage->>'Flow_Hrs')::real, 1.0) * \n            GREATEST(1.0, (COALESCE((p.system_usage->>'Proc_CPU')::bigint, 50) + COALESCE((p.system_usage->>'Proc_GPU')::bigint, 50)) / 200.0)\n        ) AS per_value\n    FROM processing p\n    LEFT JOIN scans s ON p.zoneref = s.zoneref\n    LEFT JOIN pointcloud pc ON s.arcref = pc.arcref\n    WHERE p.system_usage IS NOT NULL\n      AND COALESCE((p.system_usage->>'Flow_Hrs')::real, 0) > 0\n      AND (COALESCE((p.system_usage->>'Proc_CPU')::bigint, 0) + COALESCE((p.system_usage->>'Proc_GPU')::bigint, 0)) > 0\n)\nSELECT \n    wa.processing_software,\n    wa.processing_stage,\n    CASE \n        WHEN COUNT(*) = 0 THEN NULL\n        ELSE ROUND(AVG(wa.flow_hrs)::numeric, 1)\n    END AS avg_processing_hours,\n    CASE \n        WHEN COUNT(*) = 0 THEN NULL\n        ELSE ROUND(AVG(wa.proc_cpu)::numeric, 1)\n    END AS avg_cpu_usage_pct,\n    CASE \n        WHEN COUNT(*) = 0 THEN NULL\n        ELSE ROUND(AVG(wa.proc_gpu)::numeric, 1)\n    END AS avg_gpu_usage_pct,\n    CASE \n        WHEN COUNT(*) = 0 THEN NULL\n        ELSE ROUND(AVG(wa.data_size_gb)::numeric, 1)\n    END AS avg_data_size_gb,\n    CASE \n        WHEN COUNT(*) = 0 THEN NULL\n        ELSE ROUND(AVG(wa.per_value)::numeric, 1)\n    END AS avg_per,\n    CASE \n        WHEN COUNT(*) = 0 THEN 'No Data'\n        WHEN AVG(wa.per_value) < 0.5 THEN 'Bottleneck Detected'\n        ELSE 'Efficient'\n    END AS efficiency_status,\n    COUNT(*) AS workflow_count\nFROM workflow_analysis wa\nGROUP BY wa.processing_software, wa.processing_stage\nORDER BY \n    CASE \n        WHEN COUNT(*) = 0 THEN 3\n        WHEN AVG(wa.per_value) < 0.5 THEN 1 \n        ELSE 2 \n    END,\n    avg_per ASC NULLS LAST;"], "external_knowledge": [8, 17], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 1, "distinct": false, "order": true}}
{"instance_id": "archeology_scan_9", "selected_database": "archeology_scan", "query": "Which sites are best for finding artifacts? Show me each location's ID along with the average ratio between total points and cloud density, and the average efficiency of feature identification. I need all sites included, even if some data might be missing. Sort the results by average feature identification efficiency in descending order.", "normal_query": "For each archaeological site, I need its Point Cloud Density Ratio and Feature Extraction Efficiency to identify sites with high potential for feature extraction. Please include the site code, average PCDR value, and average FEE value. Ensure that all sites are included, even if some data might be missing. Sort the results by average FEE in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH site_pc_sp_metrics AS (SELECT sc.zoneref AS site_code, AVG(COALESCE((pc.cloud_metrics->>'Total_Pts')::bigint, 0)) AS avg_total_pts, AVG(COALESCE((pc.cloud_metrics->>'Cloud_Dense')::bigint, 0)) AS avg_cloud_dense, AVG(COALESCE((sp.spatial_dims->>'Area_M2')::real, 0)) AS avg_area_m2 FROM scans sc LEFT JOIN pointcloud pc ON sc.arcref = pc.arcref LEFT JOIN spatial sp ON sc.arcref = sp.arcref WHERE sc.zoneref IS NOT NULL GROUP BY sc.zoneref), site_feature_metrics AS (SELECT f.zoneref AS site_code, SUM(COALESCE((f.feature_analysis->>'Trait_Count')::bigint, 0)) AS total_trait_count, SUM(COALESCE((f.feature_analysis->>'Arti_Count')::bigint, 0)) AS total_arti_count FROM features f WHERE f.zoneref IS NOT NULL GROUP BY f.zoneref), site_calculations AS (SELECT s.zoneregistry AS site_code, met.avg_cloud_dense, feat.total_trait_count, feat.total_arti_count, COALESCE(met.avg_total_pts / NULLIF(met.avg_cloud_dense * met.avg_area_m2, 0), 0) AS pcdr FROM sites s LEFT JOIN site_pc_sp_metrics met ON s.zoneregistry = met.site_code LEFT JOIN site_feature_metrics feat ON s.zoneregistry = feat.site_code) SELECT sc.site_code, ROUND(sc.pcdr::numeric, 2) AS avg_pcdr, ROUND(COALESCE((COALESCE(sc.total_trait_count, 0) + COALESCE(sc.total_arti_count, 0)) * 1000.0 / NULLIF(sc.pcdr * SQRT(NULLIF(sc.avg_cloud_dense, 0)), 0), 0)::numeric, 2) AS avg_fee FROM site_calculations sc ORDER BY avg_fee DESC;"], "external_knowledge": [2, 32], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "archeology_scan_10", "selected_database": "archeology_scan", "query": "Hey, can you help me figure out how efficient our archaeological scanning gear is? I need to know the equipments' IDs, their efficiency of computing resource utilization (rounded to two decimal places), the average processing time in hours, their efficiency rankings, and their workflow efficiency status. Also, please include CPU usage (named 'cpu_usage'), GPU usage (named 'gpu_usage'), and processing hours (named 'processing_hours') as JSON in the resource details. Make sure to include all equipments, even if the data's incomplete, and sort everything by PRU value from lowest to highest. Thanks!", "normal_query": "My purpose is to analyze the Processing Resource Utilization (PRU) of our archaeological scanning equipment and categorize workflows according to the Workflow Efficiency Classification system. Please provide the equipments' IDs, PRU values (rounded to two decimal places), average processing time in hours, efficiency rankings, workflow efficiency status, and include the CPU usage (named 'cpu_usage'), GPU usage (named 'gpu_usage'), and processing hours (named 'processing_hours') in json format as resource details. I'd like all equipment to be included in the analysis, even those with incomplete data. Please sort the results by PRU value in ascending order to help identify the most efficient setups.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH site_avg_scan_size AS (\n    -- This CTE now correctly handles the text format of the 'size' column\n    -- by splitting the string to extract only the numeric part before averaging.\n    SELECT\n        s.zoneref,\n        AVG(COALESCE(SPLIT_PART(s.size, ' ', 1)::real, 1.0)) AS avg_file_size_gb\n    FROM scans s\n    WHERE s.zoneref IS NOT NULL AND s.size IS NOT NULL\n    GROUP BY s.zoneref\n),\nprocessing_data AS (\n    -- This CTE gathers all necessary data, starting from 'equipment' and using LEFT JOINs\n    -- to ensure all equipment is included, even if related data is missing.\n    SELECT\n        e.equipregistry,\n        p.flowregistry,\n        COALESCE((p.system_usage->>'Flow_Hrs')::real, 0) AS flow_hrs,\n        COALESCE((p.system_usage->>'Proc_CPU')::bigint, 0) AS proc_cpu,\n        COALESCE((p.system_usage->>'Proc_GPU')::bigint, 0) AS proc_gpu,\n        -- The query now uses the average file size for the site.\n        COALESCE(ss.avg_file_size_gb, 1.0) AS file_size_gb,\n        COALESCE((m.mesh_specs->>'Facet_Verts')::bigint, 0) AS mesh_vertices\n    FROM equipment e\n    LEFT JOIN processing p ON e.equipregistry = p.equipref\n    -- The join to the 'site_avg_scan_size' CTE provides the corrected average file size.\n    LEFT JOIN site_avg_scan_size ss ON p.zoneref = ss.zoneref\n    LEFT JOIN mesh m ON p.zoneref = m.zoneref AND p.equipref = m.equipref\n),\npru_calculations AS (\n    -- This CTE calculates the PRU for each processing workflow using the cleaned data.\n    SELECT\n        d.*,\n        -- This is the corrected PRU formula based on the knowledge definition.\n        COALESCE(\n            (d.flow_hrs * (d.proc_cpu + d.proc_gpu) / 2.0) /\n            NULLIF(d.file_size_gb * 10 * LOG(10, d.mesh_vertices + 10000.0), 0),\n            0\n        ) AS pru\n    FROM processing_data d\n)\n-- The final SELECT statement formats the output as requested.\nSELECT\n    pc.equipregistry AS equipment_id,\n    ROUND(pc.pru::numeric, 2) AS processing_resource_utilization,\n    -- A window function calculates the average processing hours for each piece of equipment.\n    ROUND(AVG(pc.flow_hrs) OVER (PARTITION BY pc.equipregistry)::numeric, 2) AS avg_processing_hours,\n    -- DENSE_RANK provides a ranking for each workflow based on its efficiency.\n    DENSE_RANK() OVER (ORDER BY pc.pru ASC) AS efficiency_rank,\n    -- The CASE statement classifies each workflow according to the defined efficiency tiers.\n    CASE\n        WHEN pc.pru < 5.0 THEN 'Optimized'\n        WHEN pc.pru >= 5.0 AND pc.pru < 10.0 THEN 'Acceptable'\n        ELSE 'Needs Optimization'\n    END AS workflow_status,\n    -- The resource details are built into a JSON object, or set to NULL if no processing data exists.\n    CASE\n        WHEN pc.flowregistry IS NOT NULL THEN\n            JSONB_BUILD_OBJECT(\n                'cpu_usage', pc.proc_cpu,\n                'gpu_usage', pc.proc_gpu,\n                'processing_hours', pc.flow_hrs\n            )\n        ELSE NULL\n    END AS resource_details\nFROM pru_calculations pc\nORDER BY processing_resource_utilization ASC;"], "external_knowledge": [37, 51], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "archeology_scan_M_1", "selected_database": "archeology_scan", "query": "For our analysis work, let's create a special, pre-calculated table called high_fidelity_meshes to keep track of our best 3D models. In this table, I want to see the mesh's unique ID, the site it belongs to, the equipment used, the vertex and face counts, its resolution in millimeters, and its geometric accuracy. Also, please add a column for the ratio of its topological complexity to resolution. Only include the high fidelity meshes.", "normal_query": "We need to create a persistent table of all High Fidelity Mesh data for our archaeological analysis. Please set up a materialized view called 'high_fidelity_meshes'. The view should include the mesh's registry ID, site reference, equipment used, vertex and face counts, resolution in millimeters, geometric accuracy, and the calculated MCR value. Only include meshes that meet all the High Fidelity Mesh criteria.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n/* \n * SQL Intent: Create materialized view for high fidelity meshes\n * Knowledge: #4 (Mesh Complexity Ratio), #13 (High Fidelity Mesh)\n * Advanced Features: Materialized view, JSONB access, Type casting\n */\n\nCREATE MATERIALIZED VIEW high_fidelity_meshes AS\nSELECT\n    m.facetregistry,\n    m.zoneref,\n    m.equipref,\n    (m.mesh_specs->>'Facet_Verts')::bigint AS facetverts,\n    (m.mesh_specs->>'Facet_Faces')::bigint AS facetfaces,\n    (m.mesh_specs->>'Facet_Res_Mm')::real AS facetresmm,\n    (m.mesh_specs->>'Geom_Delta_Mm')::real AS geomdeltamm,\n    -- Calculate MCR per knowledge #4, accessing fields from JSONB\n    ((m.mesh_specs->>'Facet_Faces')::float / NULLIF((m.mesh_specs->>'Facet_Verts')::float, 0) / POWER((m.mesh_specs->>'Facet_Res_Mm')::real, 2)) * 1000 AS mcr\nFROM mesh m\nWHERE \n    -- Apply High Fidelity Mesh criteria from knowledge #13, accessing fields from JSONB\n    ((m.mesh_specs->>'Facet_Faces')::float / NULLIF((m.mesh_specs->>'Facet_Verts')::float, 0) / POWER((m.mesh_specs->>'Facet_Res_Mm')::real, 2)) * 1000 > 5.0 AND\n    (m.mesh_specs->>'Facet_Res_Mm')::real < 1.0 AND\n    (m.mesh_specs->>'Geom_Delta_Mm')::real < 0.5;\n"], "external_knowledge": [4, 13], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Note: This test validates the view creation and structure.\n    # Specific data values depend on the underlying dataset.\n    pred_query_result = execute_queries(pred_sqls, db_name, conn)\n    query= [\n    \"\"\"\n    SELECT facetregistry, zoneref, mcr::text FROM high_fidelity_meshes LIMIT 1\n    \"\"\"\n    ]\n    test_result = execute_queries(query, db_name, conn)\n    assert test_result, \"The view should return results.\"\n    assert len(test_result[0][0]) == 3, \"The view should have the correct number of columns.\"\n    # The following are example assertions and may need to be adjusted to your data.\n    # assert test_result[0][0][0] == 20\n    # assert test_result[0][0][1] == 'SC3210'\n    # assert '755326' in str(test_result[0][0][2])\n    return 1\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "archeology_scan_M_3", "selected_database": "archeology_scan", "query": "Can you create a view for me called view_premium_quality_scans to identify high-quality archaeological scans? For each of these scans, please display its ID, project and site refs, the scan timestamp, scan resolution (mm), point density (points/m²), coverage percentage, overlap percentage, and noise level (dB). The main thing is to only include scans that meet our standards: high resolution, comprehensive coverage, and the noise level is below 1.5 dB.", "normal_query": "Create a view called view_premium_quality_scans that identifies high-quality archaeological scans. This view should include the Scan ID, Project Reference, Site Reference, Scan Timestamp, Scan Resolution (mm), Point Density (points/m²), Coverage (%), Overlap (%), and Noise Level (dB). The view should identify scans that meet the criteria for both a High Resolution Scan and Comprehensive Coverage, and also have a Noise Level less than 1.5.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Intent: Provide an easy way to query scans meeting the criteria for 'Premium Quality'.\n-- KB Used: KB 10 (High Resolution), KB 11 (Comprehensive Coverage)\n-- Advanced Features: CREATE VIEW, LEFT JOIN, JSONB access\n\nCREATE OR REPLACE VIEW view_premium_quality_scans AS\nSELECT\n    s.QuestRegistry,\n    s.ArcRef,\n    s.ZoneRef,\n    s.ChronoTag,\n    (spc.cloud_metrics->>'Scan_Resol_Mm')::real AS ScanResolMm,\n    (spc.cloud_metrics->>'Point_Dense')::bigint AS PointDense,\n    (spc.cloud_metrics->>'Cover_Pct')::real AS CoverPct,\n    (spc.cloud_metrics->>'Lap_Pct')::real AS LapPct,\n    (spc.cloud_metrics->>'Noise_Db')::real AS NoiseDb\nFROM\n    scans s\nLEFT JOIN -- Use LEFT JOIN in case some scans don't have point cloud data yet\n    pointcloud spc ON s.arcref = spc.arcref AND s.crewref = spc.crewref\nWHERE\n    -- Apply High Resolution criteria (KB 10)\n    (spc.cloud_metrics->>'Scan_Resol_Mm')::real <= 1.0 AND (spc.cloud_metrics->>'Point_Dense')::bigint >= 1000\n    -- Apply Comprehensive Coverage criteria (KB 11)\n    AND (spc.cloud_metrics->>'Cover_Pct')::real >= 95.0 AND (spc.cloud_metrics->>'Lap_Pct')::real >= 30.0\n    -- Add a basic noise check\n    AND (spc.cloud_metrics->>'Noise_Db')::real < 1.5;\n"], "external_knowledge": [10, 11], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    pred_query_result = execute_queries(pred_sqls, db_name, conn)\n    query= [\n    \"\"\"\n    SELECT QuestRegistry, ArcRef, ZoneRef FROM view_premium_quality_scans LIMIT 1\n    \"\"\"\n    ]\n    test_result = execute_queries(query, db_name, conn)\n    assert test_result, \"View should return results if matching data exists.\"\n    if test_result[0]:\n        assert len(test_result[0][0]) == 3, \"View should have the correct columns.\"\n        # Example assertions, adjust to your data\n        # assert test_result[0][0][0] == 'ASD996783'\n        # assert test_result[0][0][1] == 'PR9945'\n        # assert test_result[0][0][2] == 'SC5229'\n    return 1\n"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "archeology_scan_M_4", "selected_database": "archeology_scan", "query": "I need a way to quickly check how good the scanning conditions were for our different sites. Can you create a view called site_esi that calculates how suitable environmental conditions were for scanning operations? For each site, just show its zone reference ID and the calculated ESI score, rounded to two decimal places.", "normal_query": "A view named site_esi is required. This view should determine the Environmental Suitability Index for each site. The output should include the Zone Reference and the calculated ESI value, rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Intent: Create view for calculating Environmental Suitability Index\n-- Knowledge Used: \"Environmental Suitability Index (ESI)\" (#7)\n-- Advanced Functions: ABS(), POWER(), JSONB access\n\nCREATE OR REPLACE VIEW site_esi AS\nSELECT \n    zoneref,\n    ROUND((\n        100 - 2.5 * ABS((ambient_cond->>'Ambic_Temp')::real - 20) - \n        POWER(ABS((ambient_cond->>'Hume_Pct')::real - 50) / 2, 1.5) - \n        600 / ((ambient_cond->>'Illume_Lux')::bigint + 100)\n    )::numeric, 2) AS esi\nFROM environment;\n"], "external_knowledge": [7], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    pred_query_result = execute_queries(pred_sqls, db_name, conn)\n    query= [\n    \"\"\"\n    SELECT zoneref, esi::text FROM site_esi ORDER BY zoneref LIMIT 1\n    \"\"\"\n    ]\n    test_result = execute_queries(query, db_name, conn)\n    assert test_result, \"View should produce results.\"\n    # Example assertion, adjust based on your data\n    # assert test_result[0][0][0] == 'SC9016'\n    # assert str(test_result[0][0][1]) == '74.89'\n    return 1\n"], "category": "Management", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cross_border_1", "selected_database": "cross_border", "query": "Let's check out the top 5 riskiest data flows. For each one, show me the flow ID, how risky it is, and how sensitive the data is. Sort them by the most sensitive data first, and make sure to round everything to two decimal places.", "normal_query": "List the top 5 high-risk data flows, showing each flow's ID, Risk Exposure Score, and Data Sensitivity Index, including all flows even if risk or profile data is missing. Sort by Data Sensitivity Index from highest to lowest, rounding scores to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH highrisk AS ( SELECT df.\"RecordRegistry\", COALESCE((rm.risk_management_profile->'assessment'->>'risk_score')::numeric, 0) AS risk_score, CASE WHEN (rm.risk_management_profile->'assessment'->>'control_effectiveness_pct') IS NULL THEN NULL WHEN (rm.risk_management_profile->'assessment'->>'control_effectiveness_pct')::numeric > 1 THEN ((rm.risk_management_profile->'assessment'->>'control_effectiveness_pct')::numeric / 100.0) ELSE (rm.risk_management_profile->'assessment'->>'control_effectiveness_pct')::numeric END AS ce_frac, COALESCE((rm.risk_management_profile->'assessment'->>'risk_score')::numeric, 0) * (1 / COALESCE(NULLIF(CASE WHEN (rm.risk_management_profile->'assessment'->>'control_effectiveness_pct') IS NULL THEN NULL WHEN (rm.risk_management_profile->'assessment'->>'control_effectiveness_pct')::numeric > 1 THEN ((rm.risk_management_profile->'assessment'->>'control_effectiveness_pct')::numeric / 100.0) ELSE (rm.risk_management_profile->'assessment'->>'control_effectiveness_pct')::numeric END, 0), 1)) AS res, COALESCE(dp.\"vol_gb\", 0) * CASE WHEN LOWER(TRIM(dp.\"dataSense\")) = 'critical' THEN 4 WHEN LOWER(TRIM(dp.\"dataSense\")) = 'high' THEN 3 WHEN LOWER(TRIM(dp.\"dataSense\")) = 'medium' THEN 2 WHEN LOWER(TRIM(dp.\"dataSense\")) = 'low' THEN 1 ELSE 0 END AS dsi FROM \"DataFlow\" df LEFT JOIN \"RiskManagement\" rm ON df.\"RecordRegistry\" = rm.flow_link LEFT JOIN \"DataProfile\" dp ON df.\"RecordRegistry\" = dp.\"Flow_Sign\" ), ranked AS ( SELECT \"RecordRegistry\", res, dsi, RANK() OVER (ORDER BY dsi DESC NULLS LAST, res DESC NULLS LAST, \"RecordRegistry\") AS dsi_rank FROM highrisk ) SELECT \"RecordRegistry\", ROUND(res::numeric, 2) AS risk_exposure_score, ROUND(dsi::numeric, 2) AS data_sensitivity_index FROM ranked WHERE dsi_rank <= 5 ORDER BY dsi_rank LIMIT 5;"], "external_knowledge": [2, 4, 10], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "cross_border_2", "selected_database": "cross_border", "query": "Let’s see how vendors are distributed across different risk tiers. For each tier, tell me the tier name, how many vendors fall into it, and what percentage of the total that is (rounded to two decimals). Sort them so the tier with the most vendors comes first.", "normal_query": "Group all vendors by their Vendor Risk Tier. For each tier, return the tier name, the number of vendors in that tier, and the percentage of total vendors (rounded to two decimals). Sort the results by the number of vendors in each tier from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH vendor_vri AS ( SELECT vm.\"Vendor_Trace\", (CASE WHEN LOWER(TRIM(vm.\"vendSecRate\"))='a' THEN 4 WHEN LOWER(TRIM(vm.\"vendSecRate\"))='b' THEN 3 WHEN LOWER(TRIM(vm.\"vendSecRate\"))='c' THEN 2 WHEN LOWER(TRIM(vm.\"vendSecRate\"))='d' THEN 1 ELSE NULL END) * (CASE WHEN LOWER(TRIM(vm.\"contrState\"))='active' THEN 1 ELSE 0.5 END) AS vri FROM \"VendorManagement\" vm ), tiered AS ( SELECT CASE WHEN vri < 2 THEN 'High Risk' WHEN vri >= 2 AND vri < 3 THEN 'Medium Risk' WHEN vri >= 3 THEN 'Low Risk' END AS risk_tier FROM vendor_vri WHERE vri IS NOT NULL ) SELECT risk_tier, COUNT(*) AS vendor_count, ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) AS percentage FROM tiered GROUP BY risk_tier ORDER BY vendor_count DESC;"], "external_knowledge": [6, 16], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "cross_border_3", "selected_database": "cross_border", "query": "Let’s find the top 10 overloaded data flows. For each, show me the flow ID, how much of the available bandwidth is being used compared to the total possible, and how efficient the transfer was based on the success rate and error count. We’ll sort them by bandwidth usage, from highest to lowest, and round the numbers to two decimal places.", "normal_query": "Find the top 10 Overloaded Data Flows, and list each flow's ID, its Bandwidth Saturation Index, and its Data Transfer Efficiency, with both metrics rounded to two decimal places. Sort by BSI from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FlowMetrics AS (\n  SELECT\n    df.\"RecordRegistry\",\n    ((df.flow_overview->'performance'->>'bandwidth_util_pct')::numeric * \n     ((df.flow_overview->'performance'->>'data_size_mb')::numeric / NULLIF((df.flow_overview->'performance'->>'duration_min')::numeric, 0))) AS bsi,\n    COALESCE((df.flow_overview->'performance'->>'success_pct')::numeric / NULLIF((df.flow_overview->'performance'->>'error_count')::numeric + 1, 0), 0) AS dte\n  FROM \"DataFlow\" df\n  WHERE ((df.flow_overview->'performance'->>'bandwidth_util_pct')::numeric * \n     ((df.flow_overview->'performance'->>'data_size_mb')::numeric / NULLIF((df.flow_overview->'performance'->>'duration_min')::numeric, 0))) > 50\n    AND COALESCE((df.flow_overview->'performance'->>'success_pct')::numeric / NULLIF((df.flow_overview->'performance'->>'error_count')::numeric + 1, 0), 0) < 1.0\n)\nSELECT \"RecordRegistry\", ROUND(bsi::numeric, 2) AS bandwidth_saturation_index, ROUND(dte::numeric, 2) AS data_transfer_efficiency\nFROM FlowMetrics\nORDER BY bsi DESC\nLIMIT 10;"], "external_knowledge": [0, 1, 18], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "cross_border_4", "selected_database": "cross_border", "query": "Let’s find the 5 data profiles most at risk for sensitive data exposure. For each one, tell me the profile ID, how sensitive the data is, and how strong the security protections are. Round the sensitivity score to two decimals and sort highest-to-lowest sensitivity. Use the scale High=3, Medium=2, Low=1; treat any other label (including 'Critical') as Low.", "normal_query": "Find the top 5 data profiles with potential Sensitive Data Exposure. For each one, show the profile ID, the data sensitivity score, and the security score. Round the sensitivity score to two decimal places and list them from highest to lowest sensitivity. Use the existing sensitivity scale (High=3, Medium=2, Low=1); treat any other label (including 'Critical') as Low.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH scored AS ( SELECT dp.\"profileTrace\", (dp.\"vol_gb\" * CASE WHEN LOWER(TRIM(dp.\"dataSense\"))='high' THEN 3 WHEN LOWER(TRIM(dp.\"dataSense\"))='medium' THEN 2 ELSE 1 END) AS dsi_raw, CASE WHEN LOWER(TRIM(COALESCE(sp.\"enc_state\",'')))='full' AND LOWER(TRIM(COALESCE(sp.\"acl_state\",'')))='strong' THEN 3 WHEN LOWER(TRIM(COALESCE(sp.\"enc_state\",'')))='full' OR LOWER(TRIM(COALESCE(sp.\"acl_state\",'')))='strong' THEN 2 ELSE 1 END AS srs FROM \"DataProfile\" dp LEFT JOIN \"SecurityProfile\" sp ON sp.\"profile_key\"=dp.\"profileTrace\" ) SELECT \"profileTrace\", ROUND(dsi_raw::numeric,2) AS \"DataSensitivityScore\", srs AS \"SecurityScore\" FROM scored ORDER BY dsi_raw DESC LIMIT 5;"], "external_knowledge": [4, 5, 14], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "cross_border_5", "selected_database": "cross_border", "query": "Let’s find the top 10 compliance records where there are issues with data moving between countries—like mismatched or missing origin and destination—and either GDPR or local law compliance is marked as failed. For each, I want the compliance ID, GDPR and local law status, and the data transfer route. Sort them by ID from smallest to biggest.", "normal_query": "Find the top 10 records where data is moving between different countries (the two countries don’t match or one is missing) and either GDPR or local-law status is marked Non-compliant. Show the record ID, the GDPR status, the local-law status, and the transfer route (origin to destination). Sort by ID from smallest to largest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH \"NonCompliantCrossBorder\" AS (\n  SELECT\n    com.\"complianceTrace\" AS \"compliance_id\",\n    com.\"gdprComp\"        AS \"gdpr_status\",\n    com.\"loc_law_comp\"    AS \"local_law_status\",\n    df.\"flow_overview\"->'routing'->>'origin_country'      AS \"origin_country\",\n    df.\"flow_overview\"->'routing'->>'destination_country' AS \"destination_country\"\n  FROM \"Compliance\" com\n  JOIN \"DataFlow\" df\n    ON TRIM(com.\"recordRegistry\") = TRIM(df.\"RecordRegistry\")\n  WHERE (\n    (\n      (df.\"flow_overview\"->'routing'->>'origin_country') IS NULL\n      AND (df.\"flow_overview\"->'routing'->>'destination_country') IS NOT NULL\n    )\n    OR (\n      (df.\"flow_overview\"->'routing'->>'origin_country') IS NOT NULL\n      AND (df.\"flow_overview\"->'routing'->>'destination_country') IS NULL\n    )\n    OR (\n      LOWER(TRIM(df.\"flow_overview\"->'routing'->>'origin_country'))\n      <> LOWER(TRIM(df.\"flow_overview\"->'routing'->>'destination_country'))\n    )\n  )\n  AND (\n    LOWER(TRIM(com.\"gdprComp\")) = 'non-compliant'\n    OR LOWER(TRIM(com.\"loc_law_comp\")) = 'non-compliant'\n  )\n)\nSELECT\n  \"compliance_id\",\n  \"gdpr_status\",\n  \"local_law_status\",\n  COALESCE(\"origin_country\", '[missing]') || ' -> ' ||\n  COALESCE(\"destination_country\", '[missing]') AS \"transfer_path\"\nFROM \"NonCompliantCrossBorder\"\nORDER BY \"compliance_id\" ASC\nLIMIT 10;"], "external_knowledge": [15], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cross_border_6", "selected_database": "cross_border", "query": "Let’s find the top 3 months with the highest average severity for audit findings, but only include audits where the severity score was over 0.5. For each month, I need the month (in 'year-month' format), the average severity (rounded to two decimal places), and how severe it was compared to other months. We’ll sort everything from the earliest to the latest month.", "normal_query": "Find the top 3 months with the highest average Audit Finding Severity for audits with a Critical Audit Issue. List each month ('year-month'), the average AFS (rounded to two decimal places), and its severity rank. Sort by month from earliest to latest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH AuditSeverity AS (SELECT ac.\"AUDIT_TRACE\", COALESCE(ac.\"critFindNum\"::float / NULLIF(ac.\"FINDTALLY\" + 1, 0), 0) AS afs, ac.\"REMED_DUE\" FROM \"AuditAndCompliance\" ac WHERE ac.\"REMED_DUE\" IS NOT NULL), MonthlyTrends AS (SELECT TO_CHAR(\"REMED_DUE\", 'YYYY-MM') AS audit_month, AVG(afs) AS avg_severity, DENSE_RANK() OVER (ORDER BY AVG(afs) DESC) AS severity_rank FROM AuditSeverity WHERE afs > 0.5 GROUP BY TO_CHAR(\"REMED_DUE\", 'YYYY-MM')) SELECT audit_month, ROUND(avg_severity::numeric, 2) AS average_afs, severity_rank FROM MonthlyTrends WHERE severity_rank <= 3 ORDER BY audit_month;"], "external_knowledge": [7, 13], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "cross_border_7", "selected_database": "cross_border", "query": "Find audits where the pressure from data subject requests is greater than 50. For each of them, I need the audit ID, the pressure score (rounded to two decimal places), and a breakdown of the request types, such as how many requests for access, deletion, rectification, and portability were made. Sort the results by the pressure score from highest to lowest, and show up to 100 records.", "normal_query": "Find audits with a Data Subject Request Pressure greater than 50. List each audits ID, the DSRP (rounded to two decimal places), and a breakdown of request types (access, deletion, rectification, portability). Sort by DSRP from highest to lowest, and show up to 100 records.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH RequestPressure AS (SELECT ac.\"AUDIT_TRACE\", (ac.\"AccReqNum\" + ac.\"DEL_REQ_NUM\" + ac.\"rect_req_num\" + ac.\"PORTREQNUM\") AS dsrl, (ac.\"AccReqNum\" + ac.\"DEL_REQ_NUM\" + ac.\"rect_req_num\" + ac.\"PORTREQNUM\") * ac.\"resp_time_day\" AS dsrp, ARRAY[('Access: ' || ac.\"AccReqNum\")::text, ('Deletion: ' || ac.\"DEL_REQ_NUM\")::text, ('Rectification: ' || ac.\"rect_req_num\")::text, ('Portability: ' || ac.\"PORTREQNUM\")::text] AS request_types FROM \"AuditAndCompliance\" ac WHERE ac.\"resp_time_day\" IS NOT NULL) SELECT \"AUDIT_TRACE\", ROUND(dsrp::numeric, 2) AS request_pressure, UNNEST(request_types) AS request_breakdown FROM RequestPressure WHERE dsrp > 50 ORDER BY dsrp DESC LIMIT 100;"], "external_knowledge": [8, 34, 71], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "cross_border_8", "selected_database": "cross_border", "query": "Let's look at data flows that cross borders and calculate their associated risk based on their volume. For each flow, I need to see the flow ID, its risk factor (rounded to two decimal places), the total risk (rounded to two decimal places), and how each flow ranks based on its total risk. Give me the flows where the total risk exceeds 1000, and sort them from highest to lowest. Please limit the results to the top 5 flows.", "normal_query": "For cross-border data flows, calculate the Cross-Border Data Volume Risk and list the flow ID, Cross-Border Risk Factor (rounded to two decimal places), CDVR (rounded to two decimal places), and the rank of CDVR. Show only flows where CDVR is greater than 1000, sort by CDVR from highest to lowest, and limit to the top 5.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH cross_border AS (\n  SELECT\n    df.\"RecordRegistry\",\n    (rm.\"risk_management_profile\"->'assessment'->>'risk_score')::numeric AS risk_score,\n    NULLIF((rm.\"risk_management_profile\"->'assessment'->>'control_effectiveness_pct')::numeric, 0) AS ctrl_eff_pct,\n    dp.\"vol_gb\" AS vol_gb\n  FROM \"DataFlow\" df\n  JOIN \"RiskManagement\" rm ON df.\"RecordRegistry\" = rm.\"flow_link\"\n  JOIN \"DataProfile\" dp ON df.\"RecordRegistry\" = dp.\"Flow_Sign\"\n  WHERE\n    df.\"flow_overview\"->'routing'->>'origin_country' IS NOT NULL\n    AND df.\"flow_overview\"->'routing'->>'destination_country' IS NOT NULL\n    AND LOWER(TRIM(df.\"flow_overview\"->'routing'->>'origin_country'))\n        <> LOWER(TRIM(df.\"flow_overview\"->'routing'->>'destination_country'))\n    AND (rm.\"risk_management_profile\"->'assessment'->>'control_effectiveness_pct') IS NOT NULL\n),\nscored AS (\n  SELECT\n    \"RecordRegistry\",\n    (risk_score * (100 / ctrl_eff_pct) * 2) AS cbrf,\n    (risk_score * (100 / ctrl_eff_pct) * 2) * vol_gb AS cdvr\n  FROM cross_border\n)\nSELECT\n  \"RecordRegistry\",\n  ROUND(cbrf::numeric, 2) AS \"cross_border_risk_factor\",\n  ROUND(cdvr::numeric, 2) AS \"cross_border_data_volume_risk\",\n  ROW_NUMBER() OVER (ORDER BY cdvr DESC) AS \"risk_rank\"\nFROM scored\nWHERE cdvr > 1000\nORDER BY cdvr DESC\nLIMIT 5;"], "external_knowledge": [2, 9, 33], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "cross_border_9", "selected_database": "cross_border", "query": "Let’s find the data profiles that have failed their integrity checks. For each profile, I need the profile ID, the count of integrity failures, and a list of failure types (like 'Integrity Check' or 'Checksum Verification') in a single string, separated by commas. Sort the profiles by the failure count, starting with the highest, and show me just the top 10.", "normal_query": "Find data profiles with a Data Integrity Failure, and calculate their Integrity Failure Count. List each profiles ID, its IFC, and the types of failures (like 'Integrity Check' or 'Checksum Verification') in a single string, separated by commas. Sort by IFC from highest to lowest, and show only the top 10 profiles.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH \"IntegrityIssues\" AS ( SELECT dp.\"profileTrace\", dp.\"Int_check\", dp.\"CSUMVERIFY\", CASE WHEN LOWER(TRIM(dp.\"Int_check\")) = 'failed' THEN 1 ELSE 0 END + CASE WHEN LOWER(TRIM(dp.\"CSUMVERIFY\")) = 'failed' THEN 1 ELSE 0 END AS ifc FROM \"DataProfile\" dp WHERE LOWER(TRIM(dp.\"Int_check\")) = 'failed' OR LOWER(TRIM(dp.\"CSUMVERIFY\")) = 'failed' ) SELECT \"profileTrace\", ifc AS integrity_failure_count, TRIM(BOTH ', ' FROM CONCAT_WS(', ', CASE WHEN LOWER(TRIM(\"Int_check\")) = 'failed' THEN 'Integrity Check' END, CASE WHEN LOWER(TRIM(\"CSUMVERIFY\")) = 'failed' THEN 'Checksum Verification' END )) AS failures FROM \"IntegrityIssues\" ORDER BY ifc DESC LIMIT 10;"], "external_knowledge": [17, 72, 73], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cross_border_10", "selected_database": "cross_border", "query": "Let’s find cross-border data flows that are under high audit compliance pressure. Focus on those with slow remediation timelines and remediation deadlines approaching within the next 5 days (assuming today is 2025-04-01). For each of these flows, I need the flow ID, the audit compliance pressure (rounded to two decimal places), and how many days the remediation is overdue. Sort these by the most overdue flows first, followed by audit compliance pressure from highest to lowest. Limit the results to the top 10 flows.", "normal_query": "I want to find cross-border data flows with High Audit Compliance Pressure. Focus on flows with slow remediation timelines and remediation deadlines within the next 5 days (assuming today is 2025-04-01). Show the flow ID, the Audit Compliance Pressure rounded to 2 decimal places, and the days overdue. Sort by days overdue from most overdue to least, then by Audit Compliance Pressure from highest to lowest, and limit to the top 10 flows.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH \"AuditMetrics\" AS ( SELECT df.\"RecordRegistry\" AS flow_id, (ac.\"critFindNum\"::float / (ac.\"FINDTALLY\" + 1)) AS afs, (ac.\"AccReqNum\" + ac.\"DEL_REQ_NUM\" + ac.\"rect_req_num\" + ac.\"PORTREQNUM\") AS dsrl, ac.\"REMED_DUE\" AS remed_due, GREATEST((DATE '2025-04-01' - ac.\"REMED_DUE\"), 0) AS days_overdue, (ac.\"REMED_DUE\" - DATE '2025-04-01') AS days_until_due FROM \"DataFlow\" df JOIN \"RiskManagement\" rm ON df.\"RecordRegistry\" = rm.\"flow_link\" JOIN \"DataProfile\" dp ON dp.\"Flow_Sign\" = df.\"RecordRegistry\" JOIN \"AuditAndCompliance\" ac ON ac.\"profjoin\" = dp.\"profileTrace\" WHERE ac.\"REMED_DUE\" IS NOT NULL AND LOWER(TRIM(df.\"flow_overview\"->'routing'->>'origin_country')) <> LOWER(TRIM(df.\"flow_overview\"->'routing'->>'destination_country')) ), \"ACPcalc\" AS ( SELECT flow_id, (afs * dsrl) * afs AS acp, days_overdue, days_until_due FROM \"AuditMetrics\" ) SELECT flow_id, ROUND(acp::numeric, 2) AS audit_compliance_pressure, days_overdue FROM \"ACPcalc\" WHERE acp > 5 AND ( days_overdue > 0 OR (days_overdue = 0 AND days_until_due BETWEEN 0 AND 5) ) ORDER BY days_overdue DESC, acp DESC LIMIT 10;"], "external_knowledge": [57, 74, 7, 8, 36, 75, 76, 77], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "cross_border_M_1", "selected_database": "cross_border", "query": "Find the systems that work with a lot of sensitive stuff but don’t have strong protection in place. If something fits that risk profile, mark it for review. For each one, show its ID, whether we flagged it, and key details about how it’s secured.", "normal_query": "Identify systems that should be flagged for review if they have a high Data Sensitivity Index (DSI) and a low Security Robustness Score (SRS). For each, return the system ID, whether it's marked for review, and key security settings.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH SensitiveDataProfiles AS ( SELECT dp.\"profileTrace\", COALESCE(dp.\"vol_gb\", 0) * CASE LOWER(TRIM(dp.\"dataSense\")) WHEN 'critical' THEN 4 WHEN 'high' THEN 3 WHEN 'medium' THEN 2 WHEN 'low' THEN 1 ELSE 0 END AS dsi FROM \"DataProfile\" dp ) UPDATE \"SecurityProfile\" sp SET \"APISECSTATE\" = 'Review Required' FROM SensitiveDataProfiles sdp WHERE sp.\"profile_key\" = sdp.\"profileTrace\" AND sdp.dsi > 100 AND ( CASE WHEN LOWER(TRIM(sp.\"enc_state\")) = 'full' AND LOWER(TRIM(sp.\"acl_state\")) = 'strong' THEN 3 WHEN LOWER(TRIM(sp.\"enc_state\")) = 'full' OR LOWER(TRIM(sp.\"acl_state\")) = 'strong' THEN 2 ELSE 1 END ) < 2 AND sp.\"APISECSTATE\" IS DISTINCT FROM 'Review Required' RETURNING sp.\"profile_key\", sp.\"APISECSTATE\", sp.\"enc_state\", sp.\"acl_state\";"], "external_knowledge": [4, 5, 14], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs): test_result = execute_queries([\"SELECT sp.\\\"profile_key\\\", sp.\\\"APISECSTATE\\\", sp.\\\"enc_state\\\", sp.\\\"acl_state\\\", dp.\\\"vol_gb\\\", dp.\\\"dataSense\\\" FROM \\\"SecurityProfile\\\" sp JOIN \\\"DataProfile\\\" dp ON sp.\\\"profile_key\\\" = dp.\\\"profileTrace\\\" WHERE COALESCE(dp.\\\"vol_gb\\\", 0) * CASE LOWER(TRIM(dp.\\\"dataSense\\\")) WHEN 'critical' THEN 4 WHEN 'high' THEN 3 WHEN 'medium' THEN 2 WHEN 'low' THEN 1 ELSE 0 END > 100 AND (CASE WHEN LOWER(TRIM(sp.\\\"enc_state\\\")) = 'full' AND LOWER(TRIM(sp.\\\"acl_state\\\")) = 'strong' THEN 3 WHEN LOWER(TRIM(sp.\\\"enc_state\\\")) = 'full' OR LOWER(TRIM(sp.\\\"acl_state\\\")) = 'strong' THEN 2 ELSE 1 END) < 2\"], db_name, conn); assert test_result, 'Expected non-empty test result for qualified records'; assert all(row[1] == 'Review Required' for row in test_result[0]), 'Some rows did not have APISECSTATE set to Review Required'; return 1"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cross_border_M_2", "selected_database": "cross_border", "query": "We need to keep an updated summary of how well the data flows are performing. Make sure we have a place to store the record ID, the success rate, the error count, and a timestamp showing when the data was last updated. For every record, calculate how efficient the data transfer was. Then, if we don’t already have a record for it, add a new one, or if it’s already there, update it with the latest success rate, error count, and timestamp.", "normal_query": "We need to maintain a reliable summary that tracks the performance of each data flow. For every data transfer, calculate its Data Transfer Efficiency (DTE) and make sure this value is stored in a dedicated record, along with the original success rate, the number of errors, and the timestamp when this performance summary was last refreshed. If there’s already a summary for a data flow, make sure it gets updated with the latest numbers; if not, create a new one with all the required information. The DTE value should be rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE IF NOT EXISTS data_flow_metrics (\n  recordregistry CHARACTER(8) PRIMARY KEY,\n  dte NUMERIC,\n  success_pct NUMERIC,\n  error_tally SMALLINT,\n  last_updated TIMESTAMPTZ DEFAULT NOW()\n);\n\nWITH FlowEfficiency AS (\n  SELECT\n    df.\"RecordRegistry\" AS recordregistry,\n    (df.flow_overview->'performance'->>'success_pct')::numeric AS success_pct,\n    (df.flow_overview->'performance'->>'error_count')::int AS error_count,\n    CASE\n      WHEN ((df.flow_overview->'performance'->>'error_count')::int + 1) = 0 THEN NULL\n      ELSE ROUND(((df.flow_overview->'performance'->>'success_pct')::numeric / ((df.flow_overview->'performance'->>'error_count')::numeric + 1)), 2)\n    END AS calculated_dte\n  FROM \"DataFlow\" df\n)\n\nINSERT INTO data_flow_metrics (recordregistry, dte, success_pct, error_tally, last_updated)\nSELECT\n  fe.recordregistry,\n  fe.calculated_dte,\n  fe.success_pct,\n  fe.error_count,\n  NOW()\nFROM FlowEfficiency fe\nON CONFLICT (recordregistry) DO UPDATE\nSET\n  dte = EXCLUDED.dte,\n  success_pct = EXCLUDED.success_pct,\n  error_tally = EXCLUDED.error_tally,\n  last_updated = EXCLUDED.last_updated;"], "external_knowledge": [0], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    def is_close_enough(a, b, rel_tol=1e-9, abs_tol=1e-9):\n        return abs(float(a) - float(b)) <= max(rel_tol * max(abs(float(a)), abs(float(b))), abs_tol)\n\n    sample_query = \"\"\"\n        SELECT \"RecordRegistry\",\n               (flow_overview->'performance'->>'success_pct')::numeric AS success_pct,\n               (flow_overview->'performance'->>'error_count')::int AS error_count\n        FROM \"DataFlow\"\n        WHERE (flow_overview->'performance'->>'success_pct') IS NOT NULL\n          AND (flow_overview->'performance'->>'error_count') IS NOT NULL\n        ORDER BY \"RecordRegistry\" LIMIT 1;\n    \"\"\"\n    res, _, _ = execute_queries([sample_query], db_name, conn)\n    record_registry, success_pct, error_count = res[0]\n    expected_dte = round(success_pct / (error_count + 1), 2)\n\n    verification_query = f\"\"\"\n        SELECT recordregistry,\n               ROUND(dte::numeric, 2) AS dte,\n               success_pct::numeric,\n               error_tally::numeric\n        FROM data_flow_metrics\n        WHERE recordregistry = '{record_registry}';\n    \"\"\"\n    res, _, _ = execute_queries([verification_query], db_name, conn)\n    assert len(res) > 0, \"No record was inserted into data_flow_metrics\"\n    assert res[0][1] == expected_dte, f\"DTE value mismatch. Expected {expected_dte}, got {res[0][1]}\"\n    assert res[0][2] == success_pct, f\"Success percentage mismatch. Expected {success_pct}, got {res[0][2]}\"\n    assert res[0][3] == error_count, f\"Error tally mismatch. Expected {error_count}, got {res[0][3]}\"\n\n    update_source = f\"\"\"\n        UPDATE \"DataFlow\"\n        SET flow_overview = jsonb_set(\n            jsonb_set(flow_overview, '{{performance,success_pct}}', '\"95.0\"', true),\n            '{{performance,error_count}}', '\"2\"', true)\n        WHERE \"RecordRegistry\" = '{record_registry}';\n    \"\"\"\n    execute_queries([update_source], db_name, conn)\n    execute_queries(pred_sqls, db_name, conn)\n\n    new_success_pct = 95.0\n    new_error_count = 2\n    new_expected_dte = round(new_success_pct / (new_error_count + 1), 2)\n    res, _, _ = execute_queries([verification_query], db_name, conn)\n    assert is_close_enough(res[0][1], new_expected_dte), f\"Updated DTE mismatch. Expected {new_expected_dte}, got {res[0][1]}\"\n    assert is_close_enough(float(res[0][2]), new_success_pct), f\"Updated success_pct mismatch. Expected {new_success_pct}, got {res[0][2]}\"\n    assert res[0][3] == new_error_count, f\"Updated error_tally mismatch. Expected {new_error_count}, got {res[0][3]}\"\n    return True"], "category": "Management", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cross_border_M_3", "selected_database": "cross_border", "query": "Let’s find all data transfers that go between two different countries and clearly fail to meet legal requirements. We’ll only consider it a serious compliance gap if either the general data protection rules or the local laws are explicitly marked as not being followed — not just partially done, but fully non-compliant. For each one, show the countries involved, some identifying info about the flow, and who the vendor is if we know it.", "normal_query": "Please create a materialized view named cross_border_compliance_gap_view. This view should act as a pre-computed list identifying all data flows that exhibit a Cross-Border Compliance Gap, defined as flows where the origin and destination countries differ, and where either GDPR compliance or local law compliance is marked as 'Non-compliant'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE MATERIALIZED VIEW IF NOT EXISTS cross_border_compliance_gap_view AS\nSELECT\n  df.\"RecordRegistry\" AS recordregistry,\n  df.\"flow_overview\"->'classification'->>'flow_tag' AS flowtag,\n  TRIM(LOWER(df.\"flow_overview\"->'routing'->>'origin_country')) AS orignation,\n  TRIM(LOWER(df.\"flow_overview\"->'routing'->>'destination_country')) AS destnation,\n  c.\"gdprComp\" AS gdprcomp,\n  c.\"loc_law_comp\" AS loclawcomp,\n  vm.\"Vendor_Trace\" AS vendortrace\nFROM \"DataFlow\" df\nJOIN \"RiskManagement\" rm ON df.\"RecordRegistry\" = rm.\"flow_link\"\nJOIN \"Compliance\" c ON rm.\"RISKTRACE\" = c.\"risk_tie\"\nLEFT JOIN \"VendorManagement\" vm ON c.\"vendorTie\" = vm.\"Vendor_Trace\"\nWHERE\n  TRIM(LOWER(df.\"flow_overview\"->'routing'->>'origin_country')) <>\n  TRIM(LOWER(df.\"flow_overview\"->'routing'->>'destination_country'))\n  AND (\n    TRIM(LOWER(c.\"gdprComp\"::text)) = 'non-compliant' OR\n    TRIM(LOWER(c.\"loc_law_comp\"::text)) = 'non-compliant'\n  );\n"], "external_knowledge": [3, 7, 8, 36, 40], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    execute_queries(sol_sqls, db_name, conn)\n    test_result = execute_queries([\"SELECT recordregistry, flowtag, orignation, destnation, gdprcomp, loclawcomp, vendortrace FROM cross_border_compliance_gap_view LIMIT 1;\"] , db_name, conn)\n    print(test_result)\n    assert test_result and ('Non-compliant' in (test_result[0][0][4], test_result[0][0][5])), \"Compliance condition not satisfied (gdprComp or locLawComp must be 'Non-compliant')\"\n    assert test_result and test_result[0][0][2] != test_result[0][0][3], \"Origin and destination countries should differ for cross-border validation\"\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cross_border_M_4", "selected_database": "cross_border", "query": "Let's update the dataflow table by adding a column called transfer_path. For all the data flows that cross borders, I want you to create a string that shows the journey from the origin to the destination country in this format: 'OrigNation -> DestNation'. Make sure the column gets filled in for all the existing records.", "normal_query": "Please modify the dataflow table by adding a new column called transfer_path. Once the column is added, populate it for all existing Cross-Border Data Flows by creating their Transfer Path string, which combines the origin and destination nations.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["ALTER TABLE \"DataFlow\" ADD COLUMN transfer_path VARCHAR(255);\n\nCOMMENT ON COLUMN \"DataFlow\".transfer_path IS\n  'Stores the transfer path (OriginCountry -> DestinationCountry) as defined in KB ID 70.';\n\nUPDATE \"DataFlow\"\nSET transfer_path = \n  TRIM(flow_overview::jsonb->'routing'->>'origin_country') || ' -> ' ||\n  TRIM(flow_overview::jsonb->'routing'->>'destination_country')\nWHERE\n  flow_overview::jsonb->'routing'->>'origin_country' IS NOT NULL\n  AND flow_overview::jsonb->'routing'->>'destination_country' IS NOT NULL\n  AND TRIM(flow_overview::jsonb->'routing'->>'origin_country') <> \n      TRIM(flow_overview::jsonb->'routing'->>'destination_country');"], "external_knowledge": [70, 75], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    sample_query = \"\"\"\n        SELECT \"RecordRegistry\"\n        FROM \"DataFlow\"\n        WHERE flow_overview->'routing'->>'origin_country' IS NOT NULL\n          AND flow_overview->'routing'->>'destination_country' IS NOT NULL\n          AND flow_overview->'routing'->>'origin_country' <> flow_overview->'routing'->>'destination_country'\n        LIMIT 1;\n    \"\"\"\n    res, _, _ = execute_queries([sample_query], db_name, conn)\n    assert len(res) > 0, \"No cross-border records found in DataFlow\"\n    record_registry = res[0][0]\n\n    validation_query = f\"\"\"\n        SELECT\n            flow_overview->'routing'->>'origin_country' AS origin,\n            flow_overview->'routing'->>'destination_country' AS destination,\n            transfer_path\n        FROM \"DataFlow\"\n        WHERE \"RecordRegistry\" = '{record_registry}';\n    \"\"\"\n    res, _, _ = execute_queries([validation_query], db_name, conn)\n    origin, destination, actual_path = res[0]\n    expected_path = f\"{origin} -> {destination}\"\n\n    assert origin != destination, f\"Record {record_registry} is not cross-border ({origin} == {destination})\"\n    assert actual_path == expected_path, f\"Transfer path mismatch for {record_registry}. Expected '{expected_path}', got '{actual_path}'\"\n    print(\"Verified transfer path.\")"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cross_border_M_5", "selected_database": "cross_border", "query": "Let’s go through the audit records. If a record has a lot of critical findings — meaning the number of critical issues is more than half of the total findings — and the remediation deadline has already passed, change its status to 'Overdue'. But only do that if the current status isn’t already set to 'Complete' or 'Overdue'.", "normal_query": "Please update the AuditAndCompliance table. For the purpose of this operation, define a 'Critical Audit Issue' as any audit where the number of critical findings is greater than 50% of total findings. For any such audit record where the remediation due date is earlier than today, set its remediation status to 'Overdue'. This should only apply if the current status is not already marked as 'Complete' or 'Overdue'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE \"AuditAndCompliance\" SET \"remed_state\" = 'Overdue' WHERE (\"critFindNum\"::numeric / (\"FINDTALLY\" + 1)) > 0.5 AND \"REMED_DUE\" < CURRENT_DATE AND LOWER(TRIM(\"remed_state\")) IS DISTINCT FROM 'complete' AND LOWER(TRIM(\"remed_state\")) IS DISTINCT FROM 'overdue';"], "external_knowledge": [13], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, *args): execute_queries(pred_sqls, db_name, conn); result, _, _ = execute_queries([\"SELECT COUNT(*) FROM \\\"AuditAndCompliance\\\" WHERE \\\"remed_state\\\" = 'Overdue' AND (\\\"critFindNum\\\"::numeric / (\\\"FINDTALLY\\\" + 1)) > 0.5 AND \\\"REMED_DUE\\\" < CURRENT_DATE;\"], db_name, conn); assert result is not None and result[0][0] > 0, f'No rows were updated to Overdue. Got count = {result[0][0] if result else None}'; return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cross_border_11", "selected_database": "cross_border", "query": "Let’s figure out which data flows are really feeling the heat from audits and compliance costs. First, how heavy is the audit load? For each flow, take the number of critical findings, divide by total findings plus one, and multiply that by the total number of data subject requests. Then, how costly is it to stay compliant? Divide the compliance cost by the penalties plus one. Now show me the flows where both numbers are high—specifically, audit load over 10 and cost pressure over 0.8. For each of those, give me the flow ID, the audit load, and the cost pressure, both rounded nicely. Just make sure to link everything properly across the tables using the flow ID.", "normal_query": "I want to identify data flows with both high audit remediation load and high compliance cost pressure. Calculate the remediation load as audit severity (critical findings over findings + 1) times total data subject requests. Compute cost pressure as total compliance cost divided by penalties plus 1. List the flow ID along with both values, rounded to two decimal places, but only include flows where remediation load is over 10 and cost pressure exceeds 0.8. Ensure to join the relevant tables using the flow ID.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ARLCalc AS ( SELECT ac.record_registry, COALESCE(ac.\"critFindNum\"::float / NULLIF(ac.\"FINDTALLY\" + 1, 0), 0) * COALESCE(ac.\"AccReqNum\" + ac.\"DEL_REQ_NUM\" + ac.rect_req_num + ac.\"PORTREQNUM\", 0) AS arl FROM \"AuditAndCompliance\" ac ), ComplianceCost AS ( SELECT rm.flow_link, rm.flow_link AS record_registry, COALESCE((rm.risk_management_profile->'financial_impact'->>'cost_usd')::numeric, 0) / (COALESCE((rm.risk_management_profile->'financial_impact'->>'penalties_usd')::numeric, 0) + 1) AS ccr FROM \"RiskManagement\" rm ) SELECT ar.record_registry, ROUND(ar.arl::numeric, 2) AS audit_remediation_load, ROUND(cc.ccr::numeric, 2) AS compliance_cost_ratio FROM ARLCalc ar JOIN ComplianceCost cc ON ar.record_registry = cc.record_registry WHERE ar.arl > 10 AND cc.ccr > 0.8 ORDER BY ar.arl DESC;"], "external_knowledge": [15, 75], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "cross_border_12", "selected_database": "cross_border", "query": "I want to find data flows that seem both risky and unreliable. I'm looking at how dangerous they are based on how well protections are working, and how often the data transfers succeed without problems. Just show me the ID, how much risk is involved, and whether they usually work well.", "normal_query": "I want to identify data transfers with high RES and low DFRS. Please return the unique identifier, RES, and DFRS for each qualifying transfer.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH DFRS AS ( SELECT df.\"RecordRegistry\", COALESCE(((df.flow_overview->'performance'->>'success_pct')::float / ((df.flow_overview->'performance'->>'error_count')::float + 1)) * (1 - ((df.flow_overview->'performance'->>'retry_count')::float / ((df.flow_overview->'performance'->>'error_count')::float + 1))), 0) AS dfrs FROM \"DataFlow\" df), RES AS ( SELECT rm.\"flow_link\", COALESCE((rm.risk_management_profile->'assessment'->>'risk_score')::float / NULLIF((rm.risk_management_profile->'assessment'->>'control_effectiveness_pct')::float, 0), 0) AS res FROM \"RiskManagement\" rm) SELECT r.\"flow_link\", ROUND(r.res::numeric, 2) AS res, ROUND(f.dfrs::numeric, 2) AS dfrs FROM RES r JOIN DFRS f ON r.\"flow_link\" = f.\"RecordRegistry\" WHERE r.res > 0.7 AND f.dfrs < 0.5;"], "external_knowledge": [0, 2, 27, 30, 39], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cross_border_13", "selected_database": "cross_border", "query": "Let’s create a list that keeps track of data flows with compliance issues when data crosses borders. For each flow that has this compliance gap, we need to include details like the record ID, the flow tag, the countries involved (origin and destination), the compliance status with relevant data protection laws, the status of compliance with local regulations, and the vendor trace ID. This list will be called cross_border_compliance_gap_view.", "normal_query": "Please create a materialized view named cross_border_compliance_gap_view. This view should act as a pre-computed list identifying all data flows exhibiting a Cross-Border Compliance Gap. For each identified data flow, include the following details in the view: the record registry ID, flow tag, origin nation, destination nation, GDPR compliance status, local law compliance status, and the vendor trace ID.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n  df.\"RecordRegistry\" AS recordregistry,\n  df.\"flow_overview\"->'classification'->>'flow_tag' AS flowtag,\n  df.\"flow_overview\"->'routing'->>'origin_country' AS origin_nation,\n  df.\"flow_overview\"->'routing'->>'destination_country' AS destination_nation,\n  c.\"gdprComp\" AS gdprcomp,\n  c.\"loc_law_comp\" AS loclawcomp,\n  vm.\"Vendor_Trace\" AS vendortrace\nFROM \"DataFlow\" df\nJOIN \"RiskManagement\" rm ON df.\"RecordRegistry\" = rm.\"flow_link\"\nJOIN \"Compliance\" c ON rm.\"RISKTRACE\" = c.\"risk_tie\"\nLEFT JOIN \"VendorManagement\" vm ON c.\"vendorTie\" = vm.\"Vendor_Trace\"\nWHERE\n  df.\"flow_overview\"->'routing'->>'origin_country' IS NOT NULL AND\n  df.\"flow_overview\"->'routing'->>'destination_country' IS NOT NULL AND\n  df.\"flow_overview\"->'routing'->>'origin_country' <> df.\"flow_overview\"->'routing'->>'destination_country'\n  AND (\n    LOWER(TRIM(c.\"gdprComp\"::text)) = 'non-compliant' OR\n    LOWER(TRIM(c.\"loc_law_comp\"::text)) = 'non-compliant'\n  );"], "external_knowledge": [3, 7, 8, 36, 40], "test_cases": [], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cross_border_14", "selected_database": "cross_border", "query": "Let’s check out the data flows that might be considered high-risk because they involve sensitive data. For each of these, I need the flow ID, the sensitivity score, and the destination country. The sensitivity score is calculated by multiplying the data size by a factor: if the data is highly sensitive, it gets a 3x factor, otherwise it gets a 1x factor. Show me the flows where the sensitivity score is above 100 and sort them with the highest sensitivity first.", "normal_query": "I want to find data flows that could be considered high-risk based on their sensitivity. For each data flow, show me the flow ID, the calculated sensitivity score (called DSI), and the country where the data is going. The DSI is calculated by taking the data volume (in GB) and multiplying it by a factor based on how sensitive the data is: if the data is marked as 'High' sensitivity, the factor is 3, and for any other sensitivity, it’s 1. Only show the data flows where the DSI is more than 100, and sort them by DSI from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH dsi_calc AS ( SELECT dp.\"Flow_Sign\" AS \"RecordRegistry\", dp.\"vol_gb\" * CASE WHEN LOWER(TRIM(dp.\"dataSense\")) = 'high' THEN 3 ELSE 1 END AS dsi FROM \"DataProfile\" dp ) SELECT df.\"RecordRegistry\", dc.dsi, df.flow_overview->'routing'->>'destination_country' AS \"Dest_Nation\" FROM \"DataFlow\" df JOIN dsi_calc dc ON df.\"RecordRegistry\" = dc.\"RecordRegistry\" WHERE dc.dsi > 100 ORDER BY dc.dsi DESC;"], "external_knowledge": [14, 16, 4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cross_border_15", "selected_database": "cross_border", "query": "I want to get a general idea of how trustworthy our vendors are. Can you give me a single number that reflects their overall reliability based on things like how secure they seem and whether they’re still actively working with us?", "normal_query": "Calculate the average Vendor Reliability Index (VRI) using the standard definition, across all vendors.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT AVG((CASE WHEN LOWER(TRIM(vm.\"vendSecRate\")) = 'a' THEN 4 WHEN LOWER(TRIM(vm.\"vendSecRate\")) = 'b' THEN 3 WHEN LOWER(TRIM(vm.\"vendSecRate\")) = 'c' THEN 2 ELSE 1 END) * (CASE WHEN LOWER(TRIM(vm.\"contrState\")) = 'active' THEN 1 ELSE 0.5 END)) AS avg_vri FROM \"VendorManagement\" vm;"], "external_knowledge": [6, 16], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cross_border_16", "selected_database": "cross_border", "query": "Let’s figure out which data profile has the highest sensitivity score. For each profile, we’ll calculate a score by multiplying how much data it has (in GB) with a factor based on how sensitive it is—3 for High sensitivity, 2 for Medium, and 1 for Low. I just need the highest score from all the profiles.", "normal_query": "I’m trying to find out which data profile has the highest sensitivity score based on how much data it holds and how sensitive the data is. Each profile has a volume in gigabytes and a sensitivity level—either High, Medium, or Low. I want to multiply the volume by a factor depending on sensitivity: 3 if it's High, 2 if it's Medium, and 1 if it's Low. Then give me maximum of these calculated values across all data profiles.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT MAX(dp.\"vol_gb\" * CASE WHEN LOWER(TRIM(dp.\"dataSense\")) = 'high' THEN 3 WHEN LOWER(TRIM(dp.\"dataSense\")) = 'medium' THEN 2 ELSE 1 END) AS max_dsi FROM \"DataProfile\" dp;"], "external_knowledge": [4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cross_border_17", "selected_database": "cross_border", "query": "Let’s see which audits are under the most pressure from user data requests. For each one, add up the number of access, deletion, rectification, and portability requests—use zero if any values are missing—then multiply that total by the average response time. I just want to know which audit ends up with the highest result.", "normal_query": "I’m trying to find the maximum Data Subject Request Pressure (DSRP) from the audit records. To get this, I’ll calculate the Data Subject Request Load (DSRL) by adding up the number of access, deletion, rectification, and portability requests—treating any missing values as zero. Then I’ll multiply that total by the average response time (also defaulting to zero) and return the highest result.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT MAX((COALESCE(\"AccReqNum\", 0) + COALESCE(\"DEL_REQ_NUM\", 0) + COALESCE(\"rect_req_num\", 0) + COALESCE(\"PORTREQNUM\", 0)) * COALESCE(\"resp_time_day\", 0)) AS max_dsrp FROM \"AuditAndCompliance\";"], "external_knowledge": [34], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cross_border_18", "selected_database": "cross_border", "query": "Let’s check out which vendors are carrying the biggest compliance burden. First, we’ll measure how bad their audit issues are by dividing critical findings by total findings plus one. Then we turn their security rating into a score—4 for ‘A’, 3 for ‘B’, 2 for ‘C’, and 1 for anything else. To get the compliance burden, we multiply the audit severity by (5 minus the security score). Show me the vendors with a burden over 1.5, and list their ID, compliance burden, and security score—sorted from highest to lowest. Include vendors even if they don’t have audit data.", "normal_query": "I’m looking for vendors with a high Vendor Compliance Burden (VCB). To get that, first compute their Audit Finding Severity (AFS) by dividing critical findings by total findings plus one. Then turn their security rating into a number: 4 for ‘A’, 3 for ‘B’, 2 for ‘C’, and 1 for anything else. Multiply AFS by (5 minus the security score) to get the VCB. Show only vendors with a VCB above 1.5, and return their vendor ID, the VCB rounded to two decimals, and their numeric security rating—sorted from highest to lowest. Include all vendors with ratings, even if they don’t have audit data.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH AuditSeverity AS ( SELECT LOWER(TRIM(ac.\"VendJoin\"::TEXT)) AS vendor_id, COALESCE(ac.\"critFindNum\", 0)::FLOAT / NULLIF(ac.\"FINDTALLY\" + 1, 0) AS afs FROM \"AuditAndCompliance\" ac ), VendorSecurityScore AS ( SELECT LOWER(TRIM(vm.\"Vendor_Trace\"::TEXT)) AS vendor_id, CASE WHEN LOWER(TRIM(vm.\"vendSecRate\")) = 'a' THEN 4 WHEN LOWER(TRIM(vm.\"vendSecRate\")) = 'b' THEN 3 WHEN LOWER(TRIM(vm.\"vendSecRate\")) = 'c' THEN 2 ELSE 1 END AS sec_rating FROM \"VendorManagement\" vm ), VCBCalc AS ( SELECT v.vendor_id, v.sec_rating, COALESCE(a.afs, 0) * (5 - v.sec_rating) AS vcb FROM VendorSecurityScore v LEFT JOIN AuditSeverity a ON v.vendor_id = a.vendor_id ) SELECT vcb.vendor_id, ROUND(vcb.vcb::numeric, 2) AS vendor_compliance_burden, vcb.sec_rating AS vendor_security_rating FROM VCBCalc vcb WHERE vcb.vcb > 1.5 ORDER BY vendor_compliance_burden DESC;"], "external_knowledge": [4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cross_border_19", "selected_database": "cross_border", "query": "Let's look at countries where the data is super sensitive but the encryption's a bit lacking. For each country, I need the number of profiles, the average sensitivity score, average security strength, how well data is encrypted, and how long it’s being kept. Only show me the countries where the sensitivity score is above 100, and the encryption coverage is below 2. Make sure to sort them by encryption first (lowest to highest) and then by sensitivity (highest to lowest), and give me the top 20. You’ll need to work out the sensitivity from data volume, the security score from encryption and access settings, and the coverage ratio by combining both.", "normal_query": "I’m looking to assess countries where data sensitivity is high but encryption coverage is weak. For each destination country, calculate the number of profiles, the average Data Sensitivity Index (DSI), average Security Robustness Score (SRS), average Encryption Coverage Ratio (ECR), and average retention days. Only include destinations where the average DSI is over 100 and the ECR is below 2. Sort the results by ECR in ascending order, then DSI in descending order, and return the top 20. You’ll need to compute DSI from data volume and sensitivity, SRS from encryption and access control settings, and ECR by combining both. Be sure to link the profiles and flow data properly using their shared identifiers.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH \"DataProfile\" AS ( SELECT * FROM ( VALUES ('flowA', 60.0, 'High', 365), ('flowB', 40.0, 'Medium', 180), ('flowC', 30.0, 'Low', 90), ('flowD', 80.0, 'High', 200) ) AS dp(\"Flow_Sign\", \"vol_gb\", \"dataSense\", \"ret_days\") ), \"SecurityProfile\" AS ( SELECT * FROM ( VALUES ('flowA', 'Partial', 'Weak'), ('flowB', 'Full', 'Weak'), ('flowC', 'Full', 'Strong'), ('flowD', 'Partial', 'Weak') ) AS sp(\"flowKey\", \"enc_state\", \"acl_state\") ), \"DataFlow\" AS ( SELECT * FROM ( VALUES ('flowA', '{\"routing\": {\"destination_country\": \"Norway\"}}'::jsonb), ('flowB', '{\"routing\": {\"destination_country\": \"Norway\"}}'::jsonb), ('flowC', '{\"routing\": {\"destination_country\": \"Japan\"}}'::jsonb), ('flowD', '{\"routing\": {\"destination_country\": \"Norway\"}}'::jsonb) ) AS df(\"RecordRegistry\", \"flow_overview\") ), DataSensitivity AS ( SELECT dp.\"Flow_Sign\", dp.\"vol_gb\", dp.\"dataSense\", dp.\"ret_days\", CASE WHEN LOWER(TRIM(dp.\"dataSense\")) = 'high' THEN dp.\"vol_gb\" * 3 WHEN LOWER(TRIM(dp.\"dataSense\")) = 'medium' THEN dp.\"vol_gb\" * 2 ELSE dp.\"vol_gb\" * 1 END AS dsi FROM \"DataProfile\" dp ), SecurityScore AS ( SELECT sp.\"flowKey\", CASE WHEN LOWER(TRIM(sp.\"enc_state\")) = 'full' AND LOWER(TRIM(sp.\"acl_state\")) = 'strong' THEN 3 WHEN LOWER(TRIM(sp.\"enc_state\")) = 'full' OR LOWER(TRIM(sp.\"acl_state\")) = 'strong' THEN 2 ELSE 1 END AS srs FROM \"SecurityProfile\" sp ), EncryptionCoverage AS ( SELECT ds.\"Flow_Sign\", ds.dsi, ss.srs, ds.\"ret_days\", (ds.dsi * ss.srs) AS ecr FROM DataSensitivity ds JOIN SecurityScore ss ON ds.\"Flow_Sign\" = ss.\"flowKey\" ), AggregatedByDestNation AS ( SELECT LOWER(TRIM(df.flow_overview->'routing'->>'destination_country')) AS dest_nation, COUNT(*) AS profile_count, ROUND(AVG(ec.dsi)::NUMERIC, 2) AS avg_dsi, ROUND(AVG(ec.srs)::NUMERIC, 2) AS avg_srs, ROUND(AVG(ec.ecr)::NUMERIC, 2) AS avg_ecr, ROUND(AVG(ec.\"ret_days\")::NUMERIC, 2) AS avg_retention_days FROM EncryptionCoverage ec JOIN \"DataFlow\" df ON ec.\"Flow_Sign\" = df.\"RecordRegistry\" GROUP BY LOWER(TRIM(df.flow_overview->'routing'->>'destination_country')) ) SELECT dest_nation AS \"Dest_Nation\", profile_count, avg_dsi, avg_srs, avg_ecr, avg_retention_days FROM AggregatedByDestNation WHERE avg_dsi > 100 AND avg_ecr < 200 ORDER BY avg_ecr ASC, avg_dsi DESC LIMIT 20;"], "external_knowledge": [35], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cross_border_20", "selected_database": "cross_border", "query": "Let’s find data flows that look risky and involve a lot of sensitive data. For each one, can you tell me when it happened, who sent it, where it went, and which protocol it used? I only want the ones where the risk exposure score is above 0.7 and the data sensitivity index is over 100. Show me the top 50, sorted from highest risk to lowest, and use the sensitivity score as a tiebreaker. You’ll need to pull info from different places using the flow ID, even if some values are missing.", "normal_query": "I want to find data flows with high Risk Exposure Score and high Data Sensitivity Index. For each of these flows, show the timestamp, origin actor, destination country, protocol used, the computed risk exposure (rounded), and data sensitivity (rounded). A flow qualifies if its risk exposure is greater than 0.7 and its sensitivity index exceeds 100. Sort the results by risk exposure and sensitivity, both in descending order, and return the top 50 flows. Use the flow identifier to combine data across the necessary tables, even if not all values are present.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH RiskExposure AS ( SELECT rm.\"flow_link\", COALESCE((rm.\"risk_management_profile\"->'assessment'->>'control_effectiveness_pct')::FLOAT, 0) AS ctrl_eff, COALESCE((rm.\"risk_management_profile\"->'assessment'->>'risk_score')::FLOAT, 0) AS risk_score, COALESCE((rm.\"risk_management_profile\"->'assessment'->>'risk_score')::FLOAT, 0) / NULLIF((rm.\"risk_management_profile\"->'assessment'->>'control_effectiveness_pct')::FLOAT, 0) AS res FROM \"RiskManagement\" rm ), DataSensitivity AS ( SELECT dp.\"Flow_Sign\", dp.\"vol_gb\", (CASE WHEN LOWER(TRIM(dp.\"dataSense\")) = 'high' THEN 3 WHEN LOWER(TRIM(dp.\"dataSense\")) = 'medium' THEN 2 ELSE 1 END * dp.\"vol_gb\") AS dsi FROM \"DataProfile\" dp ), FlowDetails AS ( SELECT df.\"FLOWSTAMP\", LOWER(TRIM(df.flow_overview->'routing'->>'origin_actor')) AS \"origActor\", LOWER(TRIM(df.flow_overview->'routing'->>'destination_country')) AS \"Dest_Nation\", LOWER(TRIM(df.flow_overview->'routing'->>'protocol')) AS \"ChanProto\", df.\"RecordRegistry\" FROM \"DataFlow\" df ) SELECT fd.\"FLOWSTAMP\", fd.\"origActor\", fd.\"Dest_Nation\", fd.\"ChanProto\", ROUND(re.res::NUMERIC, 2) AS risk_exposure, ROUND(ds.dsi::NUMERIC, 2) AS data_sensitivity FROM RiskExposure re JOIN DataSensitivity ds ON re.\"flow_link\" = ds.\"Flow_Sign\" JOIN FlowDetails fd ON fd.\"RecordRegistry\" = ds.\"Flow_Sign\" WHERE re.res > 0.7 AND ds.dsi > 100 ORDER BY re.res DESC, ds.dsi DESC LIMIT 50;"], "external_knowledge": [2, 4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cross_border_M_6", "selected_database": "cross_border", "query": "Let’s create a quick summary view called `DataFlowSummary` for data flows with 'High' or 'Critical' sensitivity levels. For each flow, I need details like the record ID, destination country, the actor that started the flow, the data size, the duration, and the sensitivity level. This is for summarizing only those flows with the specific sensitivity levels.", "normal_query": "I want to create a view called `DataFlowSummary` that summarizes the data flows from the DataFlow record, specifically for flows with 'High' or 'Critical' sensitivity levels. The view should include details like the record identifier, destination country, originating actor, data size, duration, and sensitivity level. This involves filtering based on the sensitivity level of the data flows.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE VIEW DataFlowSummary AS SELECT \"RecordRegistry\", flow_overview->'routing'->>'destination_country' AS \"Dest_Nation\", flow_overview->'routing'->>'origin_actor' AS \"origActor\", (flow_overview->'performance'->>'data_size_mb')::FLOAT AS \"DATA_SIZE_MB\", (flow_overview->'performance'->>'duration_min')::FLOAT AS \"durMin\", flow_overview->'classification'->>'sensitivity_level' AS \"DATA_SENSITIVITY_LEVEL\" FROM \"DataFlow\" WHERE LOWER(TRIM(flow_overview->'classification'->>'sensitivity_level')) IN ('high', 'critical');"], "external_knowledge": [4, 79], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    execute_queries([\n        \"DROP VIEW IF EXISTS DataFlowSummary;\",\n        \"\"\"\n        CREATE VIEW DataFlowSummary AS\n        SELECT\n            \\\"RecordRegistry\\\",\n            flow_overview->'routing'->>'destination_country' AS \\\"Dest_Nation\\\",\n            flow_overview->'routing'->>'origin_actor' AS \\\"origActor\\\",\n            (flow_overview->'performance'->>'data_size_mb')::FLOAT AS \\\"DATA_SIZE_MB\\\",\n            (flow_overview->'performance'->>'duration_min')::FLOAT AS \\\"durMin\\\",\n            flow_overview->'classification'->>'sensitivity_level' AS \\\"DATA_SENSITIVITY_LEVEL\\\"\n        FROM \\\"DataFlow\\\"\n        WHERE flow_overview->'classification'->>'sensitivity_level' IN ('High', 'Critical');\n        \"\"\"\n    ], db_name, conn)\n    test_result = execute_queries([\"SELECT \\\"RecordRegistry\\\" FROM DataFlowSummary WHERE \\\"DATA_SENSITIVITY_LEVEL\\\" IN ('High', 'Critical');\"], db_name, conn)\n    assert test_result, \"No results returned from DataFlowSummary\"\n    record_registry = str(test_result[0][0])\n    assert isinstance(record_registry, str), f\"Expected a string for RecordRegistry, but got {type(record_registry)}\"\n    return record_registry"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cross_border_M_7", "selected_database": "cross_border", "query": "So, I need a list of data flows that are marked as 'Critical' and have a risk score above 50. For each one, I’d like to see the flow ID, sensitivity level, risk score, mitigation state, encryption status and method, the vendor assessment, and when the contract expires. Make sure the list is sorted by the highest risk score first.", "normal_query": "Generate a report for data flows with a sensitivity level of 'Critical' and a Risk Exposure Score (RES) greater than 50. The report should include the flow identifier, sensitivity level, risk assessment, risk mitigation state, encryption status and method, vendor assessment, and contract expiry date. The results should be ordered by risk assessment in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION create_high_risk_critical_flows() RETURNS TABLE ( FlowID TEXT, SensitivityLevel TEXT, RiskAssessment REAL, RiskMitigationState TEXT, EncryptionState TEXT, EncryptionMethod TEXT, VendorAssessment TEXT, ContractExpiryDate DATE ) AS $$ BEGIN RETURN QUERY SELECT df.\"RecordRegistry\" AS FlowID, df.flow_overview->'classification'->>'sensitivity_level' AS SensitivityLevel, (rm.risk_management_profile->'assessment'->>'risk_score')::REAL AS RiskAssessment, rm.risk_management_profile->'mitigation'->>'mitigation_state' AS RiskMitigationState, sp.enc_state AS EncryptionState, sp.\"ENCMETH\" AS EncryptionMethod, vp.\"VENDASSESS\" AS VendorAssessment, vp.\"CONTR_EXPIRE\" AS ContractExpiryDate FROM \"DataFlow\" df JOIN \"RiskManagement\" rm ON df.\"RecordRegistry\" = rm.flow_link JOIN \"SecurityProfile\" sp ON df.\"RecordRegistry\" = sp.\"flowKey\" JOIN \"VendorManagement\" vp ON df.\"RecordRegistry\" = vp.recordregistry WHERE LOWER(TRIM(df.flow_overview->'classification'->>'sensitivity_level')) = 'critical' AND (rm.risk_management_profile->'assessment'->>'risk_score')::REAL > 50 ORDER BY RiskAssessment DESC; END; $$ LANGUAGE plpgsql;"], "external_knowledge": [2, 4], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn): execute_queries(pred_sqls, db_name, conn); test_result = execute_queries([\"\"\" SELECT FlowID, SensitivityLevel, RiskAssessment FROM create_high_risk_critical_flows() WHERE SensitivityLevel = 'Critical' AND RiskAssessment > 50 LIMIT 1; \"\"\"], db_name, conn); assert test_result, \"No results returned from create_high_risk_critical_flows\"; flow_id = str(test_result[0][0]); assert isinstance(flow_id, str), f\"Expected a string for FlowID, but got {type(flow_id)}\"; return flow_id"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cross_border_M_8", "selected_database": "cross_border", "query": "Let’s pretend we just had a critical data transfer. It happened on 2025-06-29 at 10:30 AM, 'ActorA' sent 150.5 MB of data to 'ActorB' in the USA over TCP. It took 60 minutes, and the data was marked as “Critical”. Let’s log that with ID 'UUID-1236'.", "normal_query": "Please add a new data exchange event to the system. Use ID 'UUID-1236', timestamp '2025-06-29T10:30:00', initiated by 'ActorA', received by 'ActorB', sent to 'USA', over 'TCP'. The data volume was 150.5 MB, it lasted 60 minutes, and the sensitivity level is 'Critical'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["INSERT INTO \"DataFlow\" (\"RecordRegistry\", \"FLOWSTAMP\", flow_overview)\nVALUES (\n  'UUID-1236',\n  '2025-06-29T10:30:00',\n  '{\n    \"routing\": {\n      \"origin_actor\": \"ActorA\",\n      \"destination_actor\": \"ActorB\",\n      \"destination_country\": \"USA\",\n      \"origin_country\": \"Canada\",\n      \"protocol\": \"TCP\"\n    },\n    \"performance\": {\n      \"data_size_mb\": 150.5,\n      \"duration_min\": 60\n    },\n    \"classification\": {\n      \"sensitivity_level\": \"Critical\"\n    }\n  }'::jsonb\n);"], "external_knowledge": [6], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, *args):\n    execute_queries(pred_sqls, db_name, conn)\n    result, _, _ = execute_queries([\"SELECT COUNT(*) FROM \\\"DataFlow\\\" WHERE \\\"RecordRegistry\\\" = 'UUID-1236' AND \\\"FLOWSTAMP\\\" = '2025-06-29T10:30:00' AND flow_overview->'routing'->>'origin_actor' = 'ActorA' AND flow_overview->'routing'->>'destination_actor' = 'ActorB' AND flow_overview->'routing'->>'destination_country' = 'USA' AND flow_overview->'routing'->>'origin_country' = 'Canada' AND flow_overview->'routing'->>'protocol' = 'TCP' AND (flow_overview->'performance'->>'data_size_mb')::float = 150.5 AND (flow_overview->'performance'->>'duration_min')::float = 60 AND flow_overview->'classification'->>'sensitivity_level' = 'Critical';\"], db_name, conn)\n    assert result is not None and result[0][0] > 0, f'Record not inserted or values do not match. Got count = {result[0][0] if result else None}'\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cross_border_M_10", "selected_database": "cross_border", "query": "Let’s clean up the data a bit. I need you to delete any records where the success rate is under 50% and the sensitivity level is 'Low.' But only delete those if they’re also linked to records with a risk score under 20, and if they’re tied to records where the GDPR status is 'Non-compliant.'", "normal_query": "I want to delete records where the success percentage is below 50 and the data sensitivity level is 'Low.' Additionally, only delete these records if they are linked to entries with a risk assessment score under 20 (this is related to the Risk Exposure Score calculation) and if they are also linked to records with non-compliant GDPR status (this refers to GDPR compliance).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["DELETE FROM \"DataFlow\"\nWHERE\n  (flow_overview->'performance'->>'success_pct')::FLOAT < 50\n  AND LOWER(TRIM(flow_overview->'classification'->>'sensitivity_level')) = 'low'\n  AND EXISTS (\n    SELECT 1 FROM \"RiskManagement\" rm\n    WHERE rm.\"flow_link\" = \"DataFlow\".\"RecordRegistry\"\n      AND (rm.risk_management_profile->'assessment'->>'risk_score')::FLOAT < 20\n  )\n  AND EXISTS (\n    SELECT 1 FROM \"Compliance\" c\n    WHERE c.\"recordRegistry\" = \"DataFlow\".\"RecordRegistry\"\n      AND LOWER(TRIM(c.\"gdprComp\")) = 'non-compliant'\n  );"], "external_knowledge": [3, 25], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    execute_queries(pred_sqls, db_name, conn)\n    test_result, _, _ = execute_queries([\"\"\"\n        SELECT * FROM \\\"DataFlow\\\"\n        WHERE (flow_overview->'performance'->>'success_pct')::FLOAT < 50\n          AND LOWER(TRIM(flow_overview->'classification'->>'sensitivity_level')) = 'low'\n          AND EXISTS (\n            SELECT 1 FROM \\\"RiskManagement\\\" rm\n            WHERE rm.flow_link = \\\"DataFlow\\\".\\\"RecordRegistry\\\"\n              AND (rm.risk_management_profile->'assessment'->>'risk_score')::FLOAT < 20\n          )\n          AND EXISTS (\n            SELECT 1 FROM \\\"Compliance\\\" c\n            WHERE c.\\\"recordRegistry\\\" = \\\"DataFlow\\\".\\\"RecordRegistry\\\"\n              AND LOWER(TRIM(c.\\\"gdprComp\\\")) = 'non-compliant'\n          );\n    \"\"\"], db_name, conn)\n    assert not test_result, \"Expected no records matching DELETE conditions to exist in the DataFlow table\"\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_1", "selected_database": "crypto_exchange", "query": "What's the current spread percentage of the midpoint price? Show me the exchange code, the timestamp of the snapshot, and the calculated spread for the latest market data.", "normal_query": "Could you calculate the Spread Percentage for the most recent market snapshot. Show me the exchange code of the most recent market snapshot with the timestamp of the snapshot, and the calculated percentage?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH snapshot AS (\n  SELECT\n    (orderbook_metrics->>'best_ask')::numeric AS best_ask,\n    (orderbook_metrics->>'best_bid')::numeric AS best_bid,\n    (orderbook_metrics->>'mid_price')::numeric AS mid_price,\n    \"EXCH_SPOT\" AS exchange,\n    \"TimeTrack\" AS timetrack\n  FROM marketdata\n  ORDER BY \"TimeTrack\" DESC\n  LIMIT 1\n)\nSELECT\n  exchange,\n  timetrack,\n  ((best_ask - best_bid) / mid_price * 100) AS spread_percentage\nFROM snapshot;\n"], "external_knowledge": [0], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "crypto_exchange_2", "selected_database": "crypto_exchange", "query": "Show me how much of each order has been filled by checking the most recent execution record. Please include the order ID, total order quantity, remaining quantity, and the calculated rate.", "normal_query": "For each order, calculate the Order Fill Rate based on its latest execution record. Display the order ID, total order quantity, remaining quantity, and the calculated order fill rate.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH latest_exec AS (\n  SELECT \n    \"Ord_Link\",\n    remain_amt,\n    ROW_NUMBER() OVER (PARTITION BY \"Ord_Link\" ORDER BY \"ExecMARK\" DESC) AS rn\n  FROM \"orderExecutions\"\n)\nSELECT \n  o.\"ORD_STAMP\",\n  (o.order_attributes->>'quantity')::numeric AS order_quantity,\n  le.remain_amt,\n  (((o.order_attributes->>'quantity')::numeric - le.remain_amt) / \n   (o.order_attributes->>'quantity')::numeric * 100) AS order_fill_rate\nFROM \"orders\" o\nJOIN latest_exec le ON o.\"ORD_STAMP\" = le.\"Ord_Link\"\nWHERE le.rn = 1;\n"], "external_knowledge": [8], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "crypto_exchange_3", "selected_database": "crypto_exchange", "query": "What's the risk exposure for our top 5 positions right now? Show me the margin-form identifier, the position's notional value, the volatility measure used and the calculated risk value.", "normal_query": "Calculate the Position Value at Risk (PVaR) for the top 5 positions, using their notional value from risk and margin data and the single latest market volatility reading. Show me the margin-form identifier, the position's notional value, the volatility measure used, and the calculated PVaR.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH latest_volatility AS (SELECT \"priceShiftDay\" AS volatility_measure FROM \"marketstats\" ORDER BY \"FundSpot\" DESC LIMIT 1) SELECT rm.\"MARG_FORM\", (rm.margin_risk_profile ->> 'position_notional')::numeric AS notional_value, lv.volatility_measure, ((rm.margin_risk_profile ->> 'position_notional')::numeric * (REPLACE(lv.volatility_measure, '%', '')::numeric) * 0.01) AS pvar FROM \"riskandmargin\" AS rm, latest_volatility AS lv ORDER BY notional_value DESC LIMIT 5;"], "external_knowledge": [2], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "crypto_exchange_4", "selected_database": "crypto_exchange", "query": "Show me the risk and margin pivot ID, the associated order ID, the account balance node ID, the initial margin hold value, the margin account balance, and the percentage of margin being utilized.", "normal_query": "Please display the risk and margin pivot ID, the associated order ID, the account balance node ID, the initial margin hold value, the margin account balance, and the calculated margin utilization.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n  r.\"MARG_FORM\",\n  r.\"ordStamp\",\n  ab.\"ACCTBAL_NODE\",\n  (r.margin_risk_profile->>'initial_margin_pct')::numeric AS initial_margin,\n  ab.marg_sum,\n  ((r.margin_risk_profile->>'initial_margin_pct')::numeric / ab.marg_sum * 100) AS margin_utilization\nFROM \"riskandmargin\" r\nJOIN \"orders\" o ON r.\"ordStamp\" = o.\"ORD_STAMP\"\nJOIN \"accountbalances\" ab ON o.\"UserRef\" = ab.\"userTAG\";\n"], "external_knowledge": [7], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_5", "selected_database": "crypto_exchange", "query": "What's our overall profit ratio when comparing winning and losing trades across all accounts? Display the total sum of positive realized PnL, the total sum of negative realized PnL, and the calculated ratio of profitable trades to losing trades.", "normal_query": "Can you calculate the Profit Factor based on the realized PnL across all account balances? Display the total sum of positive realized PnL, the total sum of negative realized PnL, and the calculated Profit Factor.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH pnl AS (\n  SELECT \n    SUM(CASE WHEN \"REAL_LINE\" > 0 THEN \"REAL_LINE\" ELSE 0 END) AS sum_positive,\n    SUM(CASE WHEN \"REAL_LINE\" < 0 THEN \"REAL_LINE\" ELSE 0 END) AS sum_negative\n  FROM \"accountbalances\"\n)\nSELECT \n  sum_positive,\n  sum_negative,\n  CASE \n    WHEN sum_negative = 0 THEN NULL\n    ELSE sum_positive / ABS(sum_negative)\n  END AS profit_factor\nFROM pnl;\n"], "external_knowledge": [34], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_6", "selected_database": "crypto_exchange", "query": "How do trading spreads vary with market mood across different currency pairs? Show me the the market pair name, the calculated percentage, the overall market sentiment, the buy force, the average percentage for that sentiment, and the percentile rank of the percentage.", "normal_query": "Analyze the Spread Percentage across different markets and correlate it with market sentiment indicators. For each market pair, display the market pair name, the calculated spread percentage, the overall market sentiment, the buy force, the average spread percentage for that sentiment, and the percentile rank of the spread percentage.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH spread_calc AS (\n    SELECT \n        \"EXCH_SPOT\",\n        \"mktCombo\" AS market_pair,\n        ((orderbook_metrics->>'best_ask')::numeric - \n         (orderbook_metrics->>'best_bid')::numeric) / \n        (orderbook_metrics->>'mid_price')::numeric * 100 AS spread_pct,\n        \"TimeTrack\"\n    FROM \"marketdata\"\n),\nsentiment_data AS (\n    SELECT \n        md_ref,\n        (indicator_bundle->'sentiment'->>'market_sentiment') AS sentiment,\n        (indicator_bundle->'order_flow'->>'buy_pressure')::real AS buy_force\n    FROM \"analyticsindicators\"\n)\nSELECT \n    s.market_pair,\n    s.spread_pct,\n    d.sentiment,\n    d.buy_force,\n    AVG(s.spread_pct) OVER (PARTITION BY d.sentiment) AS avg_spread_by_sentiment,\n    PERCENT_RANK() OVER (ORDER BY s.spread_pct) AS spread_percentile\nFROM spread_calc s\nJOIN sentiment_data d ON s.\"EXCH_SPOT\" = d.md_ref\nORDER BY s.spread_pct DESC;\n"], "external_knowledge": [0], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "crypto_exchange_7", "selected_database": "crypto_exchange", "query": "How well does smart money predict price changes? I'd like to see the dominance category, the level of 'Whale-Driven Market' activity, the market pair, the average price change over 1 hour, average price change over 4 hours, average price change over 24 hours for different market pairs and calculate the success rate of smart money flow. Please group the results by flow dominance, whale activity, and market pair, and sort them by the successful smart money flow rate, from highest to lowest.", "normal_query": "I want to understand the impact of 'Smart Money Flow' on price movements across different market pairs. Can you provide the 'flow dominance' category, the level of 'Whale-Driven Market' activity, the market pair, the average price change over 1 hour, average price change over 4 hours, average price change over 24 hours for different market pairs and calculate the 'smart money accuracy' rate. Please group the results by flow dominance, whale activity, and market pair, and sort them by smart money accuracy, from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH money_flows AS (\n    SELECT \n        md.\"TimeTrack\" AS flow_time,\n        md.\"mktCombo\" AS market_pair,\n        -- Fetch all required flow metrics in the first CTE\n        (a.indicator_bundle->'participant_flow'->>'smart_money')::real AS smart_force,\n        (a.indicator_bundle->'participant_flow'->>'retail_flow')::real AS retail_flow,\n        (a.indicator_bundle->'participant_flow'->>'institutional_flow')::real AS institutional_flow,\n        (a.indicator_bundle->'participant_flow'->>'whale_activity') AS whale_activity,\n        (md.orderbook_metrics->>'mid_price')::numeric AS mid_price\n    FROM \"analyticsindicators\" a\n    -- The initial join is correct\n    JOIN \"marketdata\" md ON a.md_ref = md.\"EXCH_SPOT\"\n),\nprice_changes AS (\n    SELECT \n        mf.*,\n        LEAD(mf.mid_price, 4) OVER (PARTITION BY mf.market_pair ORDER BY mf.flow_time) AS next_price_4h,\n        LEAD(mf.mid_price, 24) OVER (PARTITION BY mf.market_pair ORDER BY mf.flow_time) AS next_price_24h,\n        -- This logic correctly implements \"Flow Dominance\" \n        CASE \n            WHEN mf.smart_force > mf.retail_flow * 1.2 AND mf.smart_force > mf.institutional_flow * 1.2 THEN 'Smart Money Dominant'\n            WHEN mf.retail_flow > mf.smart_force * 1.2 AND mf.retail_flow > mf.institutional_flow * 1.2 THEN 'Retail Dominant'\n            WHEN mf.institutional_flow > mf.smart_force * 1.2 AND mf.institutional_flow > mf.retail_flow * 1.2 THEN 'Institutional Dominant'\n            ELSE 'Mixed'\n        END AS flow_dominance\n    FROM money_flows mf\n    -- The incorrect second JOIN is removed\n),\n-- Final CTE to calculate hourly changes for clarity\nfinal_data AS (\n    SELECT\n        pc.*,\n        (LEAD(pc.mid_price, 1) OVER (PARTITION BY pc.market_pair ORDER BY pc.flow_time) - pc.mid_price) AS change_1h,\n        (pc.next_price_4h - pc.mid_price) AS change_4h,\n        (pc.next_price_24h - pc.mid_price) AS change_24h\n    FROM price_changes pc\n)\nSELECT \n    fd.flow_dominance,\n    fd.whale_activity,\n    fd.market_pair,\n    AVG(fd.change_1h) AS avg_1h_change,\n    AVG(fd.change_4h) AS avg_4h_change,\n    AVG(fd.change_24h) AS avg_24h_change,\n    -- This logic correctly implements \"Smart Money Accuracy\" using available columns [cite: 51]\n    SUM(CASE \n            WHEN (fd.smart_force > fd.retail_flow AND fd.smart_force > fd.institutional_flow AND fd.next_price_4h > fd.mid_price) OR\n                 (fd.smart_force < fd.retail_flow AND fd.smart_force < fd.institutional_flow AND fd.next_price_4h < fd.mid_price)\n            THEN 1 ELSE 0 \n        END)::float / COUNT(*) AS smart_money_accuracy\nFROM final_data fd\n-- Filtering out rows where future price data is not available\nWHERE fd.next_price_24h IS NOT NULL\nGROUP BY fd.flow_dominance, fd.whale_activity, fd.market_pair\nORDER BY smart_money_accuracy DESC;\n"], "external_knowledge": [50, 51], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "crypto_exchange_8", "selected_database": "crypto_exchange", "query": "I want to know the real leverage traders are using. Can you provide the notional value of position, position leverage multiplier, the total wallet balance, and the resulting effective leverage for each relevant position?", "normal_query": "To analyze the 'Effective Leverage' for positions, please provide the notional value of position, position leverage multiplier, the total wallet balance, and the resulting effective leverage for each relevant position.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH risk AS (\n  SELECT \n    r.\"MARG_FORM\", \n    (r.margin_risk_profile ->> 'position_notional')::numeric AS position_notional, \n    (r.margin_risk_profile ->> 'leverage_multiplier')::numeric AS leverage_multiplier, \n    o.\"UserRef\" \n  FROM \n    \"riskandmargin\" r \n    JOIN \"orders\" o ON r.\"ordStamp\" = o.\"ORD_STAMP\"\n) \nSELECT \n  position_notional, \n  leverage_multiplier, \n  -- Corrected Line: Sanitize the string before casting to numeric\n  REPLACE(\n    REPLACE(ab.\"walletSum\", '$', ''), \n    ',', \n    ''\n  )::numeric AS wallet_balance, \n  -- Corrected Line: Use the sanitized value in the calculation\n  leverage_multiplier * (\n    position_notional / NULLIF(\n      REPLACE(\n        REPLACE(ab.\"walletSum\", '$', ''), \n        ',', \n        ''\n      )::numeric, \n      0\n    )\n  ) AS effective_leverage \nFROM \n  risk \n  JOIN \"accountbalances\" ab ON risk.\"UserRef\" = ab.\"userTAG\";\n"], "external_knowledge": [33], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_9", "selected_database": "crypto_exchange", "query": "I want to determine the strength of technical signals in the market. Please provide the RSI(14) value, MACD line value, Bollinger Band width, the technical meter direction, and the calculated strength.", "normal_query": "I want to determine the 'Technical Signal Strength' in the market. Please provide the RSI(14) value, MACD line value, Bollinger Band width, the technical meter direction, and the calculated technical signal strength.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH signals AS (\n  SELECT \n    (indicator_bundle->'technical'->>'rsi_14')::numeric AS rsi14,\n    (indicator_bundle->'technical'->>'macd_hist')::numeric AS macd_hist,\n    (indicator_bundle->'technical'->>'bollinger_width_pct')::numeric AS bollinger_width,\n    indicator_bundle->'sentiment'->>'signal' AS tech_signal\n  FROM \"analyticsindicators\"\n)\nSELECT \n  rsi14,\n  macd_hist,\n  bollinger_width,\n  tech_signal,\n  ((ABS(rsi14 - 50) + ABS(macd_hist) + (bollinger_width * 0.01)) / 3) *\n    CASE \n      WHEN LOWER(TRIM(tech_signal)) = 'buy' THEN 1 \n      WHEN LOWER(TRIM(tech_signal)) = 'sell' THEN -1 \n      ELSE 0 \n    END AS technical_signal_strength\nFROM signals;\n"], "external_knowledge": [39], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_10", "selected_database": "crypto_exchange", "query": "I need to identify the large orders that could significantly impact market prices. Please include the order ID, the trade side (Buy or Sell), the order quantity, and the depth volume in units of both bid and ask.", "normal_query": "Help me find the Whale Orders, including the order ID, the trade side (Buy or Sell), the order quantity, and the depth volume in units of both bid and ask for any order that qualifies as a Whale Order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n  o.\"ORD_STAMP\",\n  (o.order_attributes->>'side') AS trade_side,\n  (o.order_attributes->>'quantity')::numeric AS order_quantity,\n  md.orderbook_metrics->>'bid_size' AS bid_size,\n  md.orderbook_metrics->>'ask_size' AS ask_size\nFROM \"orders\" o\nJOIN \"marketdata\" md ON md.\"EXCH_SPOT\" = o.\"exchSpot\"\nWHERE (\n  (LOWER(TRIM(o.order_attributes->>'side')) = 'sell' AND (o.order_attributes->>'quantity')::numeric > 0.1 * (md.orderbook_metrics->>'bid_size')::numeric) OR\n  (LOWER(TRIM(o.order_attributes->>'side')) = 'buy' AND (o.order_attributes->>'quantity')::numeric > 0.1 * (md.orderbook_metrics->>'ask_size')::numeric)\n);\n"], "external_knowledge": [10], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_M_1", "selected_database": "crypto_exchange", "query": "Clean up our executed orders data by removing all records for orders that were cancelled.", "normal_query": "We need to clean up our 'orderExecutions' table by removing all orders with a 'Cancelled' orderflow status. Can you create such query?", "preprocess_sql": ["CREATE table \"orderexecutions_bak\" as select * from \"orderExecutions\";"], "clean_up_sqls": ["\nINSERT INTO \"orderExecutions\"\nSELECT * FROM \"orderexecutions_bak\"\nWHERE ordersmark IN (\n    SELECT RecordVault\n    FROM \"orders\"\n    WHERE LOWER(TRIM(order_attributes->>'status')) = 'cancelled') AND (order_attributes->>'quantity')::real > 5;"], "sol_sql": ["DELETE FROM \"orderExecutions\"\nWHERE \"Ord_Link\" IN (\n    SELECT \"ORD_STAMP\"\n    FROM orders\n    WHERE LOWER(TRIM(order_attributes->>'status')) = 'cancelled'\n);"], "external_knowledge": [21], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    verification_sql = \"\"\"\n    SELECT COUNT(*) \n    FROM \"orderExecutions\"\n    WHERE \"Ord_Link\" IN (\n        SELECT \"ORD_STAMP\"\n        FROM orders\n        WHERE LOWER(TRIM(order_attributes->>'status')) = 'cancelled'\n    );\n    \"\"\"\n    pred_result, _, _ = execute_queries(verification_sql, db_name, conn)\n    assert pred_result[0][0] == 0, \\\n        f\"DELETE failed: Found {pred_result[0][0]} cancelled orders remaining\"\n"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_M_2", "selected_database": "crypto_exchange", "query": "Make a function called 'calc_effective_leverage' that figures out how leveraged a position really is by comparing its size to the trader's wallet balance.", "normal_query": "Create a function called 'calc_effective_leverage' that takes position leverage (as text), position value, and wallet balance to calculate Effective Leverage.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION calc_effective_leverage(\n  p_posmagn TEXT,     -- Leverage multiplier\n  p_possum NUMERIC,   -- Notional position value\n  p_walletsum NUMERIC -- Total wallet balance\n) RETURNS NUMERIC AS $$\nBEGIN\n  RETURN p_posmagn::NUMERIC * (p_possum / p_walletsum);\nEND;\n$$ LANGUAGE plpgsql;"], "external_knowledge": [33], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    sql = \"SELECT calc_effective_leverage('10', 200, 100);\"\n    result, _, _ = execute_queries(sql, db_name, conn)\n    assert abs(float(result[0][0]) - 20.0) < 0.001, f\"Expected 20, got {result[0][0]}\"\n"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_M_3", "selected_database": "crypto_exchange", "query": "Can you help me create a recalc_market_impact_cost procedure that grabs the current market impact factor and calculates impact costs for all our 'New' orders, then saves the results with timestamps? We'll need a special log table for this, named market_impact_cost_log, which should have columns for a unique auto-incrementing ID (primary key), the order's reference text field, the calculated impact cost as a number, and when it was calculated with timezone info defaulting to current time. We don't need to run the procedure just yet.", "normal_query": "We need to track and calculate Market Impact Cost for all new orders. Please create a procedure called 'recalc_market_impact_cost' that gets the current market impact factor, calculates MIC for all orders with 'New' status using the formula, and logs the results with timestamps. Besides, create a log table 'market_impact_cost_log' to store the impact costs with columns for ID, order reference, calculated MIC, and timestamp (log_id SERIAL PRIMARY KEY, ordersmark TEXT, mic NUMERIC, calculated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()). No need to call the procedure now.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE market_impact_cost_log (\n    log_id SERIAL PRIMARY KEY,\n    ordersmark TEXT NOT NULL,\n    mic NUMERIC,\n    calculated_at TIMESTAMPTZ DEFAULT NOW()\n);\n\nCREATE OR REPLACE PROCEDURE recalc_market_impact_cost()\nLANGUAGE plpgsql AS $$\nDECLARE\n  rec RECORD;\n  v_mkteffect REAL;\nBEGIN\n  SELECT \"mktEffect\" INTO v_mkteffect\n  FROM \"systemmonitoring\"\n  ORDER BY \"SYS_MON_PIVOT\" DESC\n  LIMIT 1;\n\n  FOR rec IN\n    SELECT o.\"ORD_STAMP\", \n           (o.order_attributes->>'quantity')::real AS quantity,\n           (o.order_attributes->>'limit_price')::real AS price\n    FROM orders o\n    WHERE LOWER(TRIM(o.order_attributes->>'status')) = 'new'\n  LOOP\n    INSERT INTO market_impact_cost_log (ordersmark, mic)\n    VALUES (\n        rec.\"ORD_STAMP\",\n        rec.quantity * rec.price * v_mkteffect * 0.01\n    );\n  END LOOP;\nEND;\n$$;"], "external_knowledge": [4, 21], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    execute_queries(\"CALL recalc_market_impact_cost();\", db_name, conn)\n    verify_sql = \"\"\"\n    SELECT m.mic, \n           (o.order_attributes->>'quantity')::real * \n           (o.order_attributes->>'limit_price')::real * \n           s.\"mktEffect\" * 0.01 AS expected\n    FROM market_impact_cost_log m\n    JOIN orders o ON m.ordersmark = o.\"ORD_STAMP\"\n    CROSS JOIN (\n        SELECT \"mktEffect\" \n        FROM \"systemmonitoring\"\n        ORDER BY \"SYS_MON_PIVOT\" DESC \n        LIMIT 1\n    ) s\n    \"\"\"\n    results, _, _ = execute_queries(verify_sql, db_name, conn)\n    for mic, expected in results:\n        assert abs(float(mic) - expected) < 0.001, f\"MIC calculation error: {mic} vs {expected}\"\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "crypto_exchange_M_4", "selected_database": "crypto_exchange", "query": "Make a view called 'whale_orders' that flags really big orders by comparing their size to the market's available liquidity, showing the order ID, market note, order quantity, and available liquidity.", "normal_query": "Could you create a view called 'whale_orders' that identifies all Whale Orders in our system? We need to see the order ID, market note, order quantity, and available liquidity for orders.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE VIEW whale_orders AS\nSELECT\n    o.\"ORD_STAMP\" AS orderspivot,\n    o.\"market_note\" AS mktnote,\n    (o.order_attributes->>'quantity')::real AS dealcount,\n    (md.orderbook_metrics->>'bid_size')::real + (md.orderbook_metrics->>'ask_size')::real AS available_liquidity\nFROM orders o\nJOIN \"marketdata\" md ON o.\"exchSpot\" = md.\"EXCH_SPOT\"\nWHERE (o.order_attributes->>'quantity')::real > 0.1 * (\n    (md.orderbook_metrics->>'bid_size')::real + \n    (md.orderbook_metrics->>'ask_size')::real\n);"], "external_knowledge": [10], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    verify_sql = \"\"\"\n    SELECT COUNT(*) \n    FROM whale_orders\n    WHERE dealcount <= 0.1 * available_liquidity;\n    \"\"\"\n    result, _, _ = execute_queries(verify_sql, db_name, conn)\n    assert result[0][0] == 0, \\\n        f\"Found {result[0][0]} non-whale orders in whale_orders view\"\n"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_M_5", "selected_database": "crypto_exchange", "query": "Add a new field called 'spread_percentage' to show the spread percentage calculation of all market data records by updating their JSON fields for orderbook metrics.", "normal_query": "Please update all market data records to include the Spread Percentage as a new field 'spread_percentage' in the orderbook_metrics JSON in table 'marketdata'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE \"marketdata\" SET orderbook_metrics = jsonb_set(orderbook_metrics, '{spread_percentage}', to_jsonb((((orderbook_metrics->>'best_ask')::numeric - (orderbook_metrics->>'best_bid')::numeric) / (orderbook_metrics->>'mid_price')::numeric) * 100)) WHERE orderbook_metrics IS NOT NULL AND (orderbook_metrics->>'best_ask') IS NOT NULL AND (orderbook_metrics->>'best_bid') IS NOT NULL AND (orderbook_metrics->>'mid_price') IS NOT NULL AND (orderbook_metrics->>'mid_price')::numeric <> 0;"], "external_knowledge": [0], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    verify_sql = \"\"\"\n    SELECT \n        ((orderbook_metrics->>'best_ask')::numeric - \n         (orderbook_metrics->>'best_bid')::numeric) /\n        (orderbook_metrics->>'mid_price')::numeric * 100 AS expected,\n        (orderbook_metrics->>'spread_percentage')::numeric AS actual\n    FROM \"marketdata\"\n    WHERE \"EXCH_SPOT\" = 'BTC-USDT';\n    \"\"\"\n    result, _, _ = execute_queries(verify_sql, db_name, conn)\n    for expected, actual in result:\n        assert abs(expected - actual) < 0.001, \\\n            f\"Spread calc error: {expected} vs {actual}\"\n"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_11", "selected_database": "crypto_exchange", "query": "I need to understand our platform's overall risk level. Can you tell me, on average, what percentage of their available margin our users have currently tied up in positions?", "normal_query": "Help me calculate the platform-wide average for 'Margin Utilization'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Step 1: Gather and prepare margin data for each account.\n-- This CTE joins the risk and account balance tables to get the necessary components for calculating margin utilization for each user: the initial margin required for their positions and their total margin account balance.\nWITH \"AccountMarginMetrics\" AS (\n    SELECT\n        (rm.margin_risk_profile->>'initial_margin_pct')::REAL AS initial_margin_required,\n        ab.\"AVAIL_SUM\" AS margin_account_balance\n    FROM public.riskandmargin rm\n    JOIN public.orders o ON rm.\"ordStamp\" = o.\"ORD_STAMP\"\n    JOIN public.accountbalances ab ON o.\"UserRef\" = ab.\"userTAG\"\n)\n-- Final Step: Calculate the average utilization percentage.\n-- This final SELECT computes the average margin utilization across all accounts. The formula from the knowledge base is applied to each account, and then the average is taken. COALESCE ensures a 0 is returned if there's no data.\nSELECT\n    COALESCE(\n        AVG((initial_margin_required / NULLIF(margin_account_balance, 0)) * 100),\n    0.0) AS \"AverageMarginUtilization\"\nFROM \"AccountMarginMetrics\";\n"], "external_knowledge": [7], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_12", "selected_database": "crypto_exchange", "query": "I need a quick risk assessment of how many of our users are in the danger zone of getting a margin call.", "normal_query": "Generate a count of all accounts that are currently at 'Margin Call Risk'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH \"AccountMarginUtilization\" AS ( SELECT o.\"UserRef\", ( rm.margin_risk_profile ->> 'initial_margin_pct' ) :: REAL / NULLIF(ab.\"AVAIL_SUM\", 0) * 100 AS utilization_pct FROM public.riskandmargin rm JOIN public.orders o ON rm.\"ordStamp\" = o.\"ORD_STAMP\" JOIN public.accountbalances ab ON o.\"UserRef\" = ab.\"userTAG\" ) SELECT COUNT(DISTINCT \"UserRef\") AS \"AccountsAtMarginCallRisk\" FROM \"AccountMarginUtilization\" WHERE utilization_pct > 80;"], "external_knowledge": [7, 18], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_13", "selected_database": "crypto_exchange", "query": "Can you count how many enormous trades have occurred on our platform? I'm looking for the total number of single orders that were so large they were more than 10% of the market's depth at that moment.", "normal_query": "Provide a total count of all orders that are classified as a 'Whale Order'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Step 1: Classify each order based on the corrected 'Whale Order' definition.\n-- This CTE joins orders with market data. It now correctly checks the order's 'side' ('Buy' or 'Sell')\n-- to compare its quantity against the appropriate side of the order book liquidity ('ask_size' for buys, 'bid_size' for sells).\nWITH \"OrderClassification\" AS (\n    SELECT\n        CASE\n            -- For a Buy order, check against the ask side liquidity\n            WHEN LOWER(TRIM(o.order_attributes->>'side')) = 'buy' AND\n                 (o.order_attributes->>'quantity')::REAL > 0.1 * (md.orderbook_metrics->>'ask_size')::REAL\n            THEN 1\n            -- For a Sell order, check against the bid side liquidity\n            WHEN LOWER(TRIM(o.order_attributes->>'side')) = 'sell' AND\n                 (o.order_attributes->>'quantity')::REAL > 0.1 * (md.orderbook_metrics->>'bid_size')::REAL\n            THEN 1\n            ELSE 0\n        END AS is_whale_order\n    FROM public.orders o\n    JOIN public.marketdata md ON o.\"exchSpot\" = md.\"EXCH_SPOT\"\n)\n-- Final Step: Sum the classifications to get a total count.\n-- This sums the 'is_whale_order' flags to produce a final scalar count of all correctly identified whale orders.\nSELECT\n    SUM(is_whale_order) AS \"TotalWhaleOrders\"\nFROM \"OrderClassification\";\n"], "external_knowledge": [10], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_14", "selected_database": "crypto_exchange", "query": "Can you calculate the average spread as a percentage of the midpoint price?", "normal_query": "What is the average 'Spread Percentage' across all of our markets?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Step 1: Calculate the spread percentage for each market.\n-- This CTE accesses the marketdata table and unnests the JSONB order book metrics. It then applies\n-- the 'Spread Percentage' formula from the knowledge base to calculate the spread for each market listed.\nWITH \"MarketSpread\" AS (\n    SELECT\n      ((orderbook_metrics->>'best_ask')::REAL - (orderbook_metrics->>'best_bid')::REAL) /\n      NULLIF((orderbook_metrics->>'mid_price')::REAL, 0) * 100 AS spread_percentage\n    FROM public.marketdata\n)\n-- Final Step: Compute the average spread across all markets.\n-- This final SELECT calculates the average of all the individual spread percentages computed in the CTE.\nSELECT\n    COALESCE(AVG(spread_percentage), 0.0) AS \"AverageSpreadPercentage\"\nFROM \"MarketSpread\";\n"], "external_knowledge": [0], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_15", "selected_database": "crypto_exchange", "query": "How risky is order OR6015391 in terms of getting liquidated?", "normal_query": "What is the Liquidation Risk Level for order OR6015391?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    CASE \n        WHEN COALESCE(\n            NULLIF(REGEXP_REPLACE(rm.margin_risk_profile->>'liquidation_price', '[^0-9.]', '', 'g'), '')::numeric, 0) >=\n        COALESCE((md.orderbook_metrics->>'mark_price')::numeric * 0.95, 0)\n        THEN 'High Risk'\n        ELSE 'Safe'\n    END AS liquidation_risk_level\nFROM riskandmargin rm\nJOIN orders o ON rm.\"ordStamp\" = o.\"ORD_STAMP\"\nJOIN marketdata md ON o.\"exchSpot\"= md.\"EXCH_SPOT\" WHERE LOWER(TRIM(rm.\"ordStamp\")) = 'or6015391';\n"], "external_knowledge": [11], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_16", "selected_database": "crypto_exchange", "query": "If we execute order OR6015391 right now, what is the cost of its impact on market? Please rounded to 2 decimals", "normal_query": "What is the Market Impact Cost for order OR6015391, rounded to 2 decimals?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    ROUND(\n        (o.order_attributes->>'quantity')::numeric * \n        (o.order_attributes->>'limit_price')::numeric * \n        COALESCE((a.indicator_bundle->'order_flow'->>'large_order_ratio')::numeric, 0) * 0.01,\n    2) AS estimated_market_impact\nFROM orders o\nJOIN analyticsindicators a ON o.\"exchSpot\" = a.\"md_ref\"\nWHERE LOWER(TRIM(o.\"ORD_STAMP\")) = 'or6015391';\n"], "external_knowledge": [4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_17", "selected_database": "crypto_exchange", "query": "Is the EX203 market drying up right now? Tell me if we're in a liquidity crunch where it's hard to trade without moving prices by returning the categorical status 'Liquidity Crisis' or 'Normal Market Conditions'.", "normal_query": "Our trading strategy requires large transactions in liquid markets. Is market EX203 experiencing a Liquidity Crisis? Return the categorical status 'Liquidity Crisis' or 'Normal Market Conditions'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    CASE \n        WHEN (\n            COALESCE((m.orderbook_metrics->>'bid_size')::numeric, 0) + \n            COALESCE((m.orderbook_metrics->>'ask_size')::numeric, 0)\n        ) * COALESCE((m.orderbook_metrics->>'mid_price')::numeric, 0) / \n        NULLIF(ms.\"VOLday\", 0) < 0.01\n        THEN 'Liquidity Crisis'\n        ELSE 'Normal Market Conditions'\n    END AS liquidity_status\nFROM marketdata m\nJOIN marketstats ms ON m.\"EXCH_SPOT\" = ms.\"md_link\"\nWHERE LOWER(TRIM(m.\"EXCH_SPOT\")) = 'ex203';\n"], "external_knowledge": [16], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_18", "selected_database": "crypto_exchange", "query": "How good are the average returns on order OR6015391 adjusted for risk exposure?", "normal_query": "What are the average Risk-Adjusted Returns for order OR6015391?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH OrderData AS (SELECT ab.\"REAL_LINE\" AS realized_pnl, (rm.margin_risk_profile ->> 'position_risk_pct') :: numeric AS position_risk_ratio, (rm.margin_risk_profile ->> 'position_notional') :: numeric AS position_notional, ms.\"priceShiftDay\" AS volatility_rating_text FROM public.orders o JOIN public.riskandmargin rm ON o.\"ORD_STAMP\" = rm.\"ordStamp\" JOIN public.accountbalances ab ON o.\"UserRef\" = ab.\"userTAG\" JOIN public.marketdata md ON o.\"exchSpot\" = md.\"EXCH_SPOT\" JOIN public.marketstats ms ON md.\"EXCH_SPOT\" = ms.\"md_link\" WHERE o.\"ORD_STAMP\" = 'OR6015391'), CalculatedPVaR AS (SELECT d.realized_pnl, d.position_risk_ratio, (d.position_notional * (REPLACE(d.volatility_rating_text, '%', '') :: numeric / 100) * 0.01) AS pvar FROM OrderData d) SELECT AVG(d.realized_pnl / NULLIF((d.pvar * d.position_risk_ratio), 0)) AS \"RiskAdjustedReturn\" FROM CalculatedPVaR d;"], "external_knowledge": [30], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_19", "selected_database": "crypto_exchange", "query": "Our arbitrage strategy robot needs to identify cross-market spread opportunities. Does EX203 have significant arbitrage opportunities across markets? Return 'Arbitrage Opportunity' if the value exceeds the threshold, otherwise 'Normal Market'.", "normal_query": "Our arbitrage strategy robot needs to identify cross-market spread opportunities. According to our arbitrage strategy, when the cross-market spread exceeds the threshold, an Arbitrage Window exists, triggering automated trading. Please determine whether EX203 presents an arbitrage opportunity. Return 'Arbitrage Opportunity' if the value exceeds the threshold, otherwise 'Normal Market'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["/* Validated: analyticsindicators.indicator_bundle contains arbitrage_pct */\nSELECT \n    CASE \n        WHEN (indicator_bundle->'cross_venue'->>'arbitrage_pct')::numeric > 0.05 \n        THEN 'Arbitrage Opportunity'\n        ELSE 'Normal Market'\n    END AS arbitrage_status\nFROM analyticsindicators\nWHERE LOWER(TRIM(md_ref)) = 'ex203';\n"], "external_knowledge": [12], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_20", "selected_database": "crypto_exchange", "query": "What percentage of order OR6015391 has been filled?", "normal_query": "What is the Order Fill Rate for order OR6015391?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT (COALESCE((o.\"order_attributes\"->>'quantity')::numeric, 0) - \n         COALESCE(e.\"remain_amt\", 0)) / \n        NULLIF(COALESCE((o.\"order_attributes\"->>'quantity')::numeric, 0), 0) * 100 AS exact_fill_rate_percentage\nFROM orders o\nLEFT JOIN (\n    SELECT \"Ord_Link\", \"remain_amt\"\n    FROM \"orderExecutions\"\n    WHERE LOWER(TRIM(\"Ord_Link\")) = 'or6015391'\n    ORDER BY \"ExecMARK\" DESC \n    LIMIT 1\n) e ON o.\"ORD_STAMP\" = e.\"Ord_Link\"\nWHERE LOWER(TRIM(o.\"ORD_STAMP\")) = 'or6015391';\n"], "external_knowledge": [8], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_M_6", "selected_database": "crypto_exchange", "query": "Clean up old execution records that have passed their expiration date, but only for those quick-fire orders that either fill immediately or cancel.", "normal_query": "Purge expired execution records for IOC/FOK orders where expireSpot timestamp is before current time.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["/* SIMPLE DELETE: Remove expired execution records */\n-- Knowledge: Value illustration (ID 22) - IOC/FOK order handling\n-- Features: Date functions, JSONB filtering\nDELETE FROM \"orderExecutions\"\nWHERE \"expireSpot\" < NOW() \nAND \"Ord_Link\" IN (\n    SELECT \"ORD_STAMP\" \n    FROM orders \n    WHERE order_attributes->>'time_in_force' IN ('IOC','FOK')\n);\n"], "external_knowledge": [22], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Test 1: Verify no expired IOC/FOK execution records remain\n    expired_ioc_fok_check_sql = \"\"\"\n    SELECT COUNT(*)\n    FROM \"orderExecutions\" oe\n    JOIN orders o ON oe.\"Ord_Link\" = o.\"ORD_STAMP\"\n    WHERE oe.\"expireSpot\" < NOW() \n    AND o.order_attributes->>'time_in_force' IN ('IOC','FOK');\n    \"\"\"\n    \n    result, _, _ = execute_queries(expired_ioc_fok_check_sql, db_name, conn)\n    assert result[0][0] == 0, (\n        f\"Found {result[0][0]} expired execution records for IOC/FOK orders that should have been deleted\"\n    )\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Test 2: Verify expired execution records for non-IOC/FOK orders are NOT deleted\n    expired_non_ioc_fok_check_sql = \"\"\"\n    SELECT COUNT(*)\n    FROM \"orderExecutions\" oe\n    JOIN orders o ON oe.\"Ord_Link\" = o.\"ORD_STAMP\"\n    WHERE oe.\"expireSpot\" < NOW() \n    AND (o.order_attributes->>'time_in_force' NOT IN ('IOC','FOK') \n         OR o.order_attributes->>'time_in_force' IS NULL);\n    \"\"\"\n    \n    result, _, _ = execute_queries(expired_non_ioc_fok_check_sql, db_name, conn)\n    # These records should still exist (not deleted)\n    assert result[0][0] >= 0, \"Expired execution records for non-IOC/FOK orders should not be deleted\"\n    "], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_M_7", "selected_database": "crypto_exchange", "query": "Build a live liquidity dashboard 'market_liquidity_dashboard' showing the exchange spot market symbol, snapshot timestamp, and the corresponding liquidity ratio.", "normal_query": "Create a view market_liquidity_dashboard showing the exchange spot market symbol, the timestamp when the snapshot was taken, and the corresponding liquidity ratio for each market.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["/* SIMPLE VIEW: Create liquidity ratio monitor */\n-- Knowledge: Liquidity Ratio (ID 5)\n-- Features: JSONB operators, materialized view\nCREATE VIEW market_liquidity_dashboard AS\nSELECT\n    md.\"EXCH_SPOT\",\n    md.\"TimeTrack\" AS snapshot_time,\n    ((md.orderbook_metrics->>'bid_size')::numeric + \n     (md.orderbook_metrics->>'ask_size')::numeric) *\n    (md.orderbook_metrics->>'mid_price')::numeric / \n    NULLIF(ms.\"VOLday\", 0) AS liquidity_ratio\nFROM marketdata md\nJOIN marketstats ms ON md.\"EXCH_SPOT\" = ms.md_link;\n"], "external_knowledge": [5], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Test 1: Verify the view exists\n    view_exists_sql = \"SELECT COUNT(*) FROM pg_views WHERE viewname = 'market_liquidity_dashboard';\"\n    \n    result, _, _ = execute_queries(view_exists_sql, db_name, conn)\n    assert result[0][0] == 1, \"Materialized view 'market_liquidity_dashboard' was not created\"\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Test 2: Verify the view has the expected columns\n    column_check_sql = \"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'market_liquidity_dashboard' ORDER BY ordinal_position;\"\n    \n    result, _, _ = execute_queries(column_check_sql, db_name, conn)\n    actual_columns = [row[0].lower() for row in result]\n    assert len(actual_columns) == 3, f\"Expected 3 columns, got {len(actual_columns)}\"\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):  \n    # Test 4: Verify the liquidity ratio formula is applied correctly\n    formula_verification_sql = \"\"\"\n    SELECT \n        mld.\"EXCH_SPOT\",\n        mld.\"liquidity_ratio\",\n        ((md.\"orderbook_metrics\"->>'bid_size')::numeric + \n         (md.\"orderbook_metrics\"->>'ask_size')::numeric) *\n        (md.\"orderbook_metrics\"->>'mid_price')::numeric / \n        NULLIF(ms.\"VOLday\", 0) AS expected_ratio\n    FROM market_liquidity_dashboard mld\n    JOIN marketdata md ON mld.\"EXCH_SPOT\" = md.\"EXCH_SPOT\" \n                       AND mld.\"snapshot_time\" = md.\"TimeTrack\"\n    JOIN marketstats ms ON md.\"EXCH_SPOT\" = ms.\"md_link\"\n    WHERE mld.\"liquidity_ratio\" IS NOT NULL\n    LIMIT 1;\n    \"\"\"\n    \n    result, _, _ = execute_queries(formula_verification_sql, db_name, conn)\n    \n    # Verify that calculated ratios match expected ratios (within floating point precision)\n    for row in result:\n        actual_ratio = float(row[1])\n        expected_ratio = float(row[2])\n        assert abs(actual_ratio - expected_ratio) < 0.0001, (\n            f\"Liquidity ratio calculation mismatch for {row[0]}: \"\n            f\"got {actual_ratio}, expected {expected_ratio}\"\n        )\n"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_M_8", "selected_database": "crypto_exchange", "query": "Can you make a helper calc_spread_pct() that takes the JSONB for fast order-book analytics and returns the calculated spread percentage?", "normal_query": "Create function calc_spread_pct() that takes the JSONB for fast order-book analytics and returns the calculated spread percentage.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["/* SIMPLE FUNCTION: Calculate spread percentage */\n-- Knowledge: Spread Percentage (ID 0)\n-- Features: PL/pgSQL function, JSONB input\nCREATE OR REPLACE FUNCTION calc_spread_pct(book_metrics JSONB)\nRETURNS NUMERIC AS $$\nBEGIN\n    RETURN (\n        ((book_metrics->>'best_ask')::NUMERIC - \n         (book_metrics->>'best_bid')::NUMERIC) / \n        (book_metrics->>'mid_price')::NUMERIC * 100\n    );\nEND;\n$$ LANGUAGE plpgsql;\n"], "external_knowledge": [0], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Test 1: Verify the function exists\n    function_exists_sql = \"\"\"\n    SELECT COUNT(*)\n    FROM pg_proc p\n    JOIN pg_namespace n ON p.pronamespace = n.oid\n    WHERE p.proname = 'calc_spread_pct'\n    AND n.nspname = 'public';\n    \"\"\"\n    \n    result, _, _ = execute_queries(function_exists_sql, db_name, conn)\n    assert result[0][0] == 1, \"Function 'calc_spread_pct' was not created\"\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Test with decimal values\n    decimal_test_sql = \"\"\"\n    SELECT calc_spread_pct('{\n        \"best_ask\": 100.25,\n        \"best_bid\": 99.75,\n        \"mid_price\": 100.00\n    }'::JSONB);\n    \"\"\"\n    \n    result, _, _ = execute_queries(decimal_test_sql, db_name, conn)\n    expected_spread = ((100.25 - 99.75) / 100.00) * 100  # Should be 0.5%\n    actual_spread = float(result[0][0])\n    assert abs(actual_spread - expected_spread) < 0.001, (\n        f\"Decimal calculation failed. Expected {expected_spread}, got {actual_spread}\"\n    )\n    "], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_M_9", "selected_database": "crypto_exchange", "query": "I want to create a trg_margin_util that automatically update how much percentage of margin being utilized whenever their risk profile changes. Please store the result in a new JSONB key named margin_util_pct inside the margin_risk_profile column.", "normal_query": "Create trigger trg_margin_util that auto-calculates Margin Utilization whenever margin profile changes. The result should be stored in a new JSONB key named margin_util_pct inside the margin_risk_profile column.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["/* Auto-update margin utilization */\nCREATE OR REPLACE FUNCTION update_margin_util()\nRETURNS TRIGGER AS $$\nBEGIN\n    NEW.margin_risk_profile = jsonb_set(\n        NEW.margin_risk_profile,\n        '{margin_util_pct}',\n        to_jsonb(ROUND((\n            (NEW.margin_risk_profile->>'initial_margin_pct')::numeric / \n            NULLIF((\n                SELECT \"AVAIL_SUM\" \n                FROM accountbalances \n                WHERE \"userTAG\" = (\n                    SELECT \"UserRef\" FROM orders \n                    WHERE \"ORD_STAMP\" = NEW.\"ordStamp\"\n                )\n            ), 0) * 100\n        ), 2))\n    );\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER trg_margin_util\nBEFORE INSERT OR UPDATE ON riskandmargin\nFOR EACH ROW EXECUTE FUNCTION update_margin_util();\n"], "external_knowledge": [7], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    trigger_exists_sql = \"\"\"\n    SELECT COUNT(*)\n    FROM pg_trigger\n    WHERE tgname = 'trg_margin_util'\n    AND tgrelid = (SELECT oid FROM pg_class WHERE relname = 'riskandmargin');\n    \"\"\"\n    \n    result, _, _ = execute_queries(trigger_exists_sql, db_name, conn)\n    assert result[0][0] == 1, \"Trigger 'trg_margin_util' was not created on riskandmargin table\"\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # First, ensure we have test data setup\n    test_insert_sql = \"\"\"\n    INSERT INTO riskandmargin (\"ordStamp\", margin_risk_profile)\n    VALUES (\n        (SELECT \"ORD_STAMP\" FROM orders LIMIT 1),\n        '{\"initial_margin_pct\": 5.0, \"other_field\": \"test\"}'::jsonb\n    )\n    RETURNING \n        \"ordStamp\",\n        margin_risk_profile->>'margin_util_pct' as calculated_util_pct,\n        margin_risk_profile->>'initial_margin_pct' as initial_margin;\n    \"\"\"\n    \n    result, _, _ = execute_queries(test_insert_sql, db_name, conn)\n    \n    if result:  # If insert was successful\n        ord_stamp = result[0][0]\n        calculated_util = result[0][1]\n        initial_margin = result[0][2]\n        \n        # Verify that margin_util_pct was calculated and added\n        assert calculated_util is not None, \"margin_util_pct should be calculated by trigger\"\n        assert initial_margin == '5.0', f\"initial_margin_pct should be preserved, got {initial_margin}\"\n        \n        # Verify calculation manually\n        manual_calc_sql = f\"\"\"\n        SELECT \n            ROUND((\n                5.0 / \n                NULLIF((\n                    SELECT \"AVAIL_SUM\"::numeric \n                    FROM accountbalances \n                    WHERE \"userTAG\" = (\n                        SELECT \"UserRef\" FROM orders \n                        WHERE LOWER(TRIM(\"ORD_STAMP\")) = '{ord_stamp}'\n                    )\n                ), 0) * 100\n            )::numeric, 2) as expected_util_pct;\n        \"\"\"\n        \n        manual_result, _, _ = execute_queries(manual_calc_sql, db_name, conn)\n        if manual_result and manual_result[0][0] is not None:\n            expected_util = str(manual_result[0][0])\n            assert calculated_util == expected_util, (\n                f\"Margin utilization calculation mismatch. Expected {expected_util}, got {calculated_util}\"\n            )\n    "], "category": "Management", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "crypto_exchange_M_10", "selected_database": "crypto_exchange", "query": "Emergency brake on dangerous bets! Cancel orders classified as Critically Over-Leveraged Position and set the cancellation reason to 'Critical Leverage'.", "normal_query": "Please cancel executions for positions classified as Critically Over-Leveraged Position and set the cancellation reason to 'Critical Leverage'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH AccountMetrics AS (\n    SELECT\n      o.\"ORD_STAMP\",\n      (rm.margin_risk_profile ->> 'leverage') :: numeric AS leverage,\n      (rm.margin_risk_profile ->> 'leverage_multiplier') :: numeric AS leverage_multiplier,\n      (rm.margin_risk_profile ->> 'position_notional') :: numeric AS position_notional,\n      (rm.margin_risk_profile ->> 'initial_margin_pct') :: numeric AS initial_margin_pct,\n      ab.\"walletSum\",\n      ab.\"AVAIL_SUM\",\n      ms.\"priceShiftDay\"\n    FROM\n      public.orders o\n      JOIN public.riskandmargin rm ON o.\"ORD_STAMP\" = rm.\"ordStamp\"\n      JOIN public.accountbalances ab ON o.\"UserRef\" = ab.\"userTAG\"\n      JOIN public.marketdata md ON o.\"exchSpot\" = md.\"EXCH_SPOT\"\n      JOIN public.marketstats ms ON md.\"EXCH_SPOT\" = ms.\"md_link\"\n  ),\n  CalculatedMetrics AS (\n    SELECT\n      am.\"ORD_STAMP\",\n      (\n        am.initial_margin_pct / NULLIF(am.\"AVAIL_SUM\", 0)\n      ) * 100 AS margin_utilization,\n      am.leverage_multiplier * (\n        am.position_notional / NULLIF(\n          REPLACE(REPLACE(am.\"walletSum\", '$', ''), ',', '') :: numeric,\n          0\n        )\n      ) AS effective_leverage,\n      (\n        am.leverage * (\n          REPLACE(am.\"priceShiftDay\", '%', '') :: numeric / 100\n        )\n      ) > 500 AS is_over_leveraged\n    FROM\n      AccountMetrics am\n  ),\n  CriticallyOverLeveragedOrders AS (\n    SELECT\n      \"ORD_STAMP\"\n    FROM\n      CalculatedMetrics\n    WHERE\n      is_over_leveraged = TRUE\n      AND effective_leverage > 20\n      AND margin_utilization > 90\n  )\nUPDATE\n  \"orderExecutions\"\nSET\n  \"CancelNote\" = 'Critical Leverage'\nWHERE\n  \"Ord_Link\" IN (\n    SELECT\n      \"ORD_STAMP\"\n    FROM\n      CriticallyOverLeveragedOrders\n  );"], "external_knowledge": [40], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    \"\"\"\n    This test case validates that the 'Critical Leverage' cancellation logic was applied correctly.\n\n    It performs two checks:\n    1.  Verifies that the total number of executions marked with 'Critical Leverage'\n        exactly matches the number of executions that meet the complex criteria for being\n        critically over-leveraged.\n    2.  Verifies that no executions were marked as 'Critical Leverage' incorrectly by ensuring\n        the count of such errors is zero.\n    \"\"\"\n\n    # This CTE-based query is the single source of truth for what should be marked.\n    # It perfectly mirrors the logic from the main UPDATE query.\n    source_of_truth_sql = \"\"\"\n    WITH\n      AccountMetrics AS (\n        SELECT\n          o.\"ORD_STAMP\",\n          (rm.margin_risk_profile ->> 'leverage') :: numeric AS leverage,\n          (rm.margin_risk_profile ->> 'leverage_multiplier') :: numeric AS leverage_multiplier,\n          (rm.margin_risk_profile ->> 'position_notional') :: numeric AS position_notional,\n          (rm.margin_risk_profile ->> 'initial_margin_pct') :: numeric AS initial_margin_pct,\n          ab.\"walletSum\",\n          ab.\"AVAIL_SUM\",\n          ms.\"priceShiftDay\"\n        FROM\n          public.orders o\n          JOIN public.riskandmargin rm ON o.\"ORD_STAMP\" = rm.\"ordStamp\"\n          JOIN public.accountbalances ab ON o.\"UserRef\" = ab.\"userTAG\"\n          JOIN public.marketdata md ON o.\"exchSpot\" = md.\"EXCH_SPOT\"\n          JOIN public.marketstats ms ON md.\"EXCH_SPOT\" = ms.\"md_link\"\n      ),\n      CalculatedMetrics AS (\n        SELECT\n          am.\"ORD_STAMP\",\n          (am.initial_margin_pct / NULLIF(am.\"AVAIL_SUM\", 0)) * 100 AS margin_utilization,\n          am.leverage_multiplier * (\n            am.position_notional / NULLIF(REPLACE(REPLACE(am.\"walletSum\", '$', ''), ',', '') :: numeric, 0)\n          ) AS effective_leverage,\n          (am.leverage * (REPLACE(am.\"priceShiftDay\", '%', '') :: numeric / 100)) > 500 AS is_over_leveraged\n        FROM\n          AccountMetrics am\n      ),\n      CriticallyOverLeveragedOrders AS (\n        SELECT \"ORD_STAMP\"\n        FROM CalculatedMetrics\n        WHERE\n          is_over_leveraged = TRUE\n          AND effective_leverage > 20\n          AND margin_utilization > 90\n      )\n    -- This query selects the primary keys (RecordVault) of all executions that SHOULD be canceled.\n    SELECT oe.\"RecordVault\"\n    FROM \"orderExecutions\" oe\n    JOIN CriticallyOverLeveragedOrders coo ON oe.\"Ord_Link\" = coo.\"ORD_STAMP\";\n    \"\"\"\n\n    result, _, _ = execute_queries(source_of_truth_sql, db_name, conn)\n    # Create a set of the primary keys for all executions that should have been marked.\n    expected_marked_executions = {row[0] for row in result}\n    expected_count = len(expected_marked_executions)\n\n    # Get the primary keys of all executions ACTUALLY marked in the database.\n    actually_marked_sql = \"\"\"\n    SELECT \"RecordVault\" FROM \"orderExecutions\" WHERE \"CancelNote\" = 'Critical Leverage';\n    \"\"\"\n    result, _, _ = execute_queries(actually_marked_sql, db_name, conn)\n    actually_marked_executions = {row[0] for row in result}\n    actual_count = len(actually_marked_executions)\n\n    # Test 1: Assert that the set of expected executions is identical to the set of actual executions.\n    # This is a more precise check than just comparing counts.\n    assert expected_marked_executions == actually_marked_executions, (\n        f\"Mismatch between expected and actual canceled executions. \"\n        f\"Expected {expected_count} executions to be marked, but found {actual_count}. \"\n        f\"Executions that should have been marked but weren't: {expected_marked_executions - actually_marked_executions}. \"\n        f\"Executions that were marked but shouldn't have been: {actually_marked_executions - expected_marked_executions}.\"\n    )\n"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_1", "selected_database": "polar_equipment", "query": "Let's compare how efficient our equipment is versus how safe it is. Can you show me a list with the equipment type, its code, its efficiency score, and its safety score? Then, for each equipment type, rank them by efficiency and by safety. I also want to see how big the gap is between those two ranks. Sort everything by type, and then by the best efficiency score.", "normal_query": "Show me the equipment type, equipment code, equipment efficiency rx   ating, safety index, efficiency rank, safety rank, and the absolute rank difference between them. Sort the results by equipment type and then by EER in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH equipment_efficiency AS (\n    SELECT e.\"EQUIP_CODE\" AS equip_code, e.\"EquipType\" AS equiptype, ROUND(( ((et.type_indices ->> 'performance_score')::numeric + e.\"RELIAB_IDX\"::numeric) / 2 * (1 - ((et.type_indices ->> 'environmental_impact_idx')::numeric / 100)) ), 2) AS eer, (et.type_indices ->> 'safety_idx')::numeric AS safety_index FROM \"Equipment\" AS e JOIN \"EquipmentType\" AS et ON e.\"EquipType\" = et.\"EquipType\"\n)\nSELECT equiptype, equip_code, eer, safety_index, RANK() OVER (PARTITION BY equiptype ORDER BY eer DESC) AS efficiency_rank, RANK() OVER (PARTITION BY equiptype ORDER BY safety_index DESC) AS safety_rank, ABS(RANK() OVER (PARTITION BY equiptype ORDER BY eer DESC) - RANK() OVER (PARTITION BY equiptype ORDER BY safety_index DESC)) AS rank_difference FROM equipment_efficiency ORDER BY equiptype, eer DESC"], "external_knowledge": [0], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "polar_equipment_2", "selected_database": "polar_equipment", "query": "I need to know which of our gear is ready for a bad storm. Can you check everything and give me a list of all equipment that's up to our 'extreme weather readiness' standard? For each item, I want to see its code and type, whether the heater, insulation, and emergency lights are good to go, its structural safety score, and the final 'Ready' or 'Not Ready' label.", "normal_query": "Could you identify all equipment that meets the extreme weather readiness criteria in our polar database? Show me the equipment code, equipment type, heater status, insulation status, emergency light status, the calculated structural safety factor, and the extreme weather readiness status. Make sure to include all equipment with available structural safety data, even if some equipment might be missing cabin environment, lighting safety, or thermal insulation information.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH structural_safety AS (\nSELECT ws.\"loc_link\", ROUND(((100 - ws.\"structLoadPct\") / 100 * CASE WHEN ws.\"snowLoad_kgm2\" > 100 OR ws.\"windSpeed_ms\" > 20 THEN 0.5 WHEN ws.\"snowLoad_kgm2\" > 50 OR ws.\"windSpeed_ms\" > 10 THEN 0.8 ELSE 1.0 END)::numeric, 2) AS ssf FROM \"WeatherAndStructure\" ws\n)\nSELECT e.\"EQUIP_CODE\" AS equipment_code, e.\"EquipType\" AS equipment_type, (ce.\"cabin_env_snapshot\" -> 'hvac' ->> 'heater_state') AS heater_status, tswg.\"insulationStat\" AS insulation_status, ls.\"emerLightStat\" AS emergency_light_status, ss.ssf, CASE WHEN ss.ssf > 0.7 AND (ce.\"cabin_env_snapshot\" -> 'hvac' ->> 'heater_state') != 'Off' AND tswg.\"insulationStat\" NOT IN ('Poor', 'Good') AND ls.\"emerLightStat\" IN ('Testing', 'On') THEN 'Extreme Weather Ready' ELSE 'Not Ready' END AS ewr_status FROM \"Equipment\" e LEFT JOIN \"CabinEnvironment\" ce ON e.\"EQUIP_CODE\" = ce.\"equipRef\" LEFT JOIN \"LightingAndSafety\" ls ON e.\"EQUIP_CODE\" = ls.\"equipRef\" LEFT JOIN \"Communication\" c ON e.\"EQUIP_CODE\" = c.\"equipRef\" LEFT JOIN \"ThermalSolarWindAndGrid\" tswg ON c.\"COMM_ID\" = tswg.\"comm_link\" LEFT JOIN structural_safety ss ON ce.\"loc_link\" = ss.\"loc_link\" WHERE ss.ssf IS NOT NULL;"], "external_knowledge": [10, 50], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_3", "selected_database": "polar_equipment", "query": "Time for a safety check on our life support gear. Can you create a report for me? I need to see the equipment's code and type, its current life support status, and its calculated reliability score. Based on that score, tell me if it's 'High', 'Moderate', or 'Low' reliability. Also, toss in a little JSON summary showing the status of the oxygen, medical, and safety systems with fields names: 'oxygen_status', 'medical_status', 'safety_system_status'. Let's just focus on the 'Safety' type equipment and sort it by the best reliability score.", "normal_query": "For our polar safety assessment, I need to evaluate the safety equipment's life support system reliability. Please provide a report showing the equipment code, equipment type, life support status, calculated LSSR score (rounded to 2 decimal places), and reliability classification based on life support reliability classification. Also include a JSON summary of oxygen status , medical status, and safety system status as support systems status with fields names: 'oxygen_status', 'medical_status', 'safety_system_status'. Focus only on safety equipment and sort the results by LSSR in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ors_calc AS (\n    SELECT \n        om.\"equipRef\", -- 1 6 33 51\n        CASE --1\n            WHEN om.\"OPER_status\" = 'Active' THEN 10 * (1 - om.\"OPER_hours\" / om.\"maintCycleHrs\")\n            WHEN om.\"OPER_status\" = 'Standby' THEN 5 * (1 - om.\"OPER_hours\" / om.\"maintCycleHrs\")\n            ELSE 0 \n        END AS ors\n    FROM \"OperationMaintenance\" om\n),\ntie_calc AS (\n    SELECT \n        c.\"equipRef\",\n        CASE \n            WHEN tswg.\"insulationStat\" = 'Good' THEN 0.9 - (tswg.\"heatLoss_kwh\" / 10)\n            WHEN tswg.\"insulationStat\" = 'Fair' THEN 0.6 - (tswg.\"heatLoss_kwh\" / 10)\n            WHEN tswg.\"insulationStat\" = 'Poor' THEN 0.3 - (tswg.\"heatLoss_kwh\" / 10)\n            ELSE 0\n        END AS tie\n    FROM \"ThermalSolarWindAndGrid\" tswg\n    JOIN \"Communication\" c ON tswg.\"comm_link\" = c.\"COMM_ID\"\n),\nlssr_calc AS (\n    SELECT \n        oc.\"equipRef\",\n        0.7 * oc.ors + 0.3 * COALESCE(tc.tie, 0) AS lssr\n    FROM ors_calc oc\n    LEFT JOIN tie_calc tc ON oc.\"equipRef\" = tc.\"equipRef\"\n)\nSELECT \n    e.\"EQUIP_CODE\",\n    e.\"EquipType\",\n    ls.\"lifeSupportStat\",\n    ROUND(lc.lssr::numeric, 2) AS lssr,\n    CASE \n        WHEN lc.lssr >= 0.8 THEN 'High Reliability'\n        WHEN lc.lssr >= 0.6 THEN 'Moderate Reliability'\n        ELSE 'Low Reliability' \n    END AS reliability_classification,\n    jsonb_build_object(\n        'oxygen_status', ls.\"O2SupplyStat\",\n        'medical_status', ls.\"medEquipStat\",\n        'safety_system_status', ls.\"safetySysStat\"\n    ) AS support_systems_status\nFROM \"Equipment\" e\nJOIN lssr_calc lc ON e.\"EQUIP_CODE\" = lc.\"equipRef\"\nJOIN \"LightingAndSafety\" ls ON e.\"EQUIP_CODE\" = ls.\"equipRef\"\nWHERE e.\"EquipType\" = 'Safety'\nORDER BY lssr DESC LIMIT 100"], "external_knowledge": [1, 6, 33, 51], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "polar_equipment_4", "selected_database": "polar_equipment", "query": "How green are our stations? I want a report showing each station's type and name, how many pieces of gear are there, and how much they rely on renewable energy. Please show the percentage of renewable use, the total renewable power in watts, and a simple classification according to the classification system of energy sustainability. Only look at stations with solar or wind data, and please sort them to show the greenest stations first.", "normal_query": "Provide the location type, station name, number of equipment at each station, their renewable energy contribution values (rounded to 2 decimal places), total renewable energy output in watts, and how they're categorized according to the energy sustainability classification System? Only include equipment that has measurable solar or wind output data, and sort the results from highest to lowest REC value.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH renewable_contribution_per_unit AS (\n    SELECT \n        l.\"locType\",\n        l.\"STATION_name\",\n        (COALESCE(tswg.\"solarOutput_w\", 0) + COALESCE(tswg.\"windOutput_w\", 0)) AS renewable_w,\n        (COALESCE(tswg.\"solarOutput_w\", 0) + COALESCE(tswg.\"windOutput_w\", 0) + COALESCE(tswg.\"fuelCellOutput_w\", 0)) AS total_w\n    FROM \"ThermalSolarWindAndGrid\" tswg\n    JOIN \"Communication\" c ON tswg.\"comm_link\" = c.\"COMM_ID\"\n    JOIN \"Location\" l ON c.\"loc_link\" = l.\"STATION_name\"\n    WHERE \n        tswg.\"solarOutput_w\" IS NOT NULL \n        OR tswg.\"windOutput_w\" IS NOT NULL\n)\nSELECT \n    \"locType\" AS location_type,\n    \"STATION_name\" AS station_name,\n    COUNT(*) AS equipment_count,\n    ROUND(SUM(renewable_w)::numeric, 2) AS total_renewable_output_w,\n    ROUND(\n        (SUM(renewable_w) / NULLIF(SUM(total_w), 0) * 100)::numeric, 2\n    ) AS station_rec_percent,\n    CASE \n        WHEN (SUM(renewable_w) / NULLIF(SUM(total_w), 0) * 100) > 70 THEN 'Energy-Sustainable'\n        WHEN (SUM(renewable_w) / NULLIF(SUM(total_w), 0) * 100) > 50 THEN 'Moderately Sustainable'\n        ELSE 'Low Sustainability'\n    END AS sustainability_classification\nFROM renewable_contribution_per_unit\nGROUP BY \"locType\", \"STATION_name\"\nORDER BY station_rec_percent DESC;"], "external_knowledge": [9, 52], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "polar_equipment_5", "selected_database": "polar_equipment", "query": "Let's get a handle on our water situation at each station. For each station, can you tell me its name and location type? I need to see the average water quality score, the average water management score, and a count of how many systems are in 'conservation needed' mode. Also, give me a simple classification for both the water quality and the overall management status. Sort the list with the best-managed stations at the top.", "normal_query": "For each combination of station name and location type, I need to see station names, location types, average water quality indices, average water resource management index scores (both rounded to 2 decimal places), count of systems with water conservation requirement, water quality classification, and water resource management status. Sort by highest WRMI first, then by water quality.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH wrmi_calc AS (\n    SELECT \n        waw.\"equipRef\",\n        waw.\"waterLevelPct\" * \n        (waw.\"waterQualityIdx\" / 100.0) * \n        (1 - waw.\"wasteTankPct\" / 100.0) AS wrmi\n    FROM \"WaterAndWaste\" waw\n)\nSELECT \n    l.\"STATION_name\",\n    l.\"locType\",\n    ROUND(AVG(waw.\"waterQualityIdx\")::numeric, 2) AS avg_water_quality_index,\n    ROUND(AVG(wc.wrmi)::numeric, 2) AS avg_wrmi,\n    COUNT(*) FILTER (WHERE wc.wrmi < 0.5) AS systems_needing_conservation,\n    CASE \n        WHEN AVG(waw.\"waterQualityIdx\") >= 91 THEN 'High-Quality'\n        WHEN AVG(waw.\"waterQualityIdx\") >= 71 THEN 'Good'\n        WHEN AVG(waw.\"waterQualityIdx\") >= 51 THEN 'Moderate'\n        WHEN AVG(waw.\"waterQualityIdx\") >= 26 THEN 'Poor'\n        ELSE 'Unsafe'\n    END AS quality_classification,\n    CASE \n        WHEN AVG(wc.wrmi) < 0.5 THEN 'Conservation Needed'\n        WHEN AVG(wc.wrmi) < 0.7 THEN 'Monitoring Advised'\n        ELSE 'Sustainable Management'\n    END AS management_status\nFROM \"WaterAndWaste\" waw\nJOIN \"Equipment\" e ON waw.\"equipRef\" = e.\"EQUIP_CODE\"\nJOIN \"CabinEnvironment\" ce ON e.\"EQUIP_CODE\" = ce.\"equipRef\"\nJOIN \"Location\" l ON ce.\"loc_link\" = l.\"STATION_name\"\nJOIN wrmi_calc wc ON waw.\"equipRef\" = wc.\"equipRef\"\nGROUP BY l.\"STATION_name\", l.\"locType\"\nORDER BY avg_wrmi DESC, avg_water_quality_index DESC;"], "external_knowledge": [7, 18, 25, 53], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "polar_equipment_6", "selected_database": "polar_equipment", "query": "I need to check how ready our equipment is. Can you go through all the maintenance records and calculate the score for its operational readiness for each one? Just show me a list with the record ID, its operating hours, maintenance cycle hours, its current status, and the final readiness score.", "normal_query": "Could you calculate the operational readiness score for all our equipment maintenance records? I'd like to see the registry ID, operation hours, maintenance cycle hours, operational status, and the calculated ORS value for each record.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    \"OP_MAINT_ID\", \n    \"OPER_hours\",\n    \"maintCycleHrs\",\n    \"OPER_status\",\n    CASE \n        WHEN \"OPER_status\" = 'Active' THEN 10 * (1 - \"OPER_hours\" / \"maintCycleHrs\")\n        WHEN \"OPER_status\" = 'Standby' THEN 5 * (1 - \"OPER_hours\" / \"maintCycleHrs\")\n        ELSE 0 \n    END AS ORS\nFROM \"OperationMaintenance\";"], "external_knowledge": [1], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_7", "selected_database": "polar_equipment", "query": "Let's figure out how sustainable our power gear is. Can you calculate the index of energy sustainability for every power device? I need a list showing the device's code, its energy efficiency percentage, what its power source is, and the final index score you calculated.", "normal_query": "I want to calculate the energy sustainability index for each power device in our database. Please retrieve the equipment reference code, energy efficiency percentage, power source, and then calculate the corresponding ESI value for each device.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    \"equip_ref\", \n    (\"battery_telemetry\" -> 'power_state' ->> 'conversion_eff_pct')::numeric AS energy_efficiency_percent,\n    (\"battery_telemetry\" -> 'power_state' ->> 'primary_source') AS power_source,\n    CASE \n        WHEN (\"battery_telemetry\" -> 'power_state' ->> 'primary_source') IN ('Solar', 'Wind', 'Hybrid')\n            THEN (\"battery_telemetry\" -> 'power_state' ->> 'conversion_eff_pct')::numeric * 1.5\n        WHEN (\"battery_telemetry\" -> 'power_state' ->> 'primary_source') = 'Battery'\n            THEN (\"battery_telemetry\" -> 'power_state' ->> 'conversion_eff_pct')::numeric * 1.0\n        WHEN (\"battery_telemetry\" -> 'power_state' ->> 'primary_source') = 'Diesel'\n            THEN (\"battery_telemetry\" -> 'power_state' ->> 'conversion_eff_pct')::numeric * 0.7\n        ELSE 0 \n    END AS ESI\nFROM \"PowerBattery\""], "external_knowledge": [2], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_8", "selected_database": "polar_equipment", "query": "How stable are our comms systems? I need a report that calculates the stability index for each communication unit. Can you show me the unit's ID, antenna status, signal strength, and network lag? Then, using that, calculate both the simple reliability index and the more complex stability index. Please round the numbers to make them easier to read.", "normal_query": "I would like to assess our polar base communication systems by calculating the base station communication stability index for each communication unit. Please extract the registry ID, antenna status, radio signal strength, and network latency from our communication records, then calculate both the communication reliability index and BSCSI for each unit. Make sure to round all values to two decimal places for clarity in reporting.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n    \"COMM_ID\",\n    \"antenna_stat\",\n    ROUND(\"radioSignal_dBm\"::numeric, 2) AS radio_strength,\n    ROUND(\"netLatency_ms\"::numeric, 2) AS latency_ms,\n    ROUND(CASE WHEN \"antenna_stat\" = 'Error' THEN 0 WHEN \"antenna_stat\" = 'Warning' THEN 5 WHEN \"antenna_stat\" = 'Normal' THEN 10 ELSE 0 END * (1 - \"netLatency_ms\"::numeric / 1000), 2) AS CRI,\n    ROUND(CASE WHEN \"netLatency_ms\" IS NOT NULL THEN ((CASE WHEN \"antenna_stat\" = 'Error' THEN 0 WHEN \"antenna_stat\" = 'Warning' THEN 5 WHEN \"antenna_stat\" = 'Normal' THEN 10 ELSE 0 END * (1 - \"netLatency_ms\"::numeric / 1000)) * (1 + 0.2 * (\"radioSignal_dBm\"::numeric / 100)) * (1 - 0.01 * (1000 - \"netLatency_ms\"::numeric))) ELSE NULL END, 2) AS BSCSI\nFROM \"Communication\"\nLIMIT 100;"], "external_knowledge": [4, 32], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_9", "selected_database": "polar_equipment", "query": "I need a list of our safest and best-performing equipment. Can you find all the gear with a top-tier performance index of overall safety-say, anything over 0.75? For each item on the list, show me its equipment code, its calculated efficiency rating, and the final safety/performance score.", "normal_query": "Could you list all equipment with high overall safety performance index scores greater than 0.75? Please display the equipment code, calculate the equipment efficiency rating, and show the OSPI value for each item.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH equipment_metrics AS (\n  SELECT\n    e.\"EQUIP_CODE\",\n    e.\"RELIAB_IDX\", -- Instance-level reliability index (0-100 scale)\n    (et.type_indices ->> 'performance_score')::REAL AS performance_score,        -- Type-level performance index (0-100 scale)\n    (et.type_indices ->> 'environmental_impact_idx')::REAL AS env_impact_idx, -- Type-level environmental index (0-100 scale)\n    (et.type_indices ->> 'safety_idx')::REAL AS safety_idx                  -- Type-level safety index (0-100 scale)\n  FROM\n    \"Equipment\" AS e\n    JOIN \"EquipmentType\" AS et ON e.\"EquipType\" = et.\"EquipType\"\n),\ncalculated_eer AS (\n  SELECT\n    \"EQUIP_CODE\",\n    safety_idx,\n    -- Compute EER: Averaging two 0-100 scores. Normalizing the 0-100 env_impact_idx to a 0-1 multiplier.\n    ( (performance_score + \"RELIAB_IDX\") / 2.0 ) * (1 - (env_impact_idx / 100.0)) AS eer\n  FROM\n    equipment_metrics\n)\nSELECT\n  \"EQUIP_CODE\",\n  eer,\n  -- Compute OSPI: Normalizing the 0-100 safety_idx to a 0-1 multiplier before using it in the formula.\n  (safety_idx / 100.0) * eer * 0.8 AS ospi\nFROM\n  calculated_eer\n-- Filter records where the final OSPI score is > 0.75\nWHERE\n  (safety_idx / 100.0) * eer * 0.8 > 0.75;"], "external_knowledge": [0, 30], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_10", "selected_database": "polar_equipment", "query": "Let's assess how our vehicles are performing. Can you calculate the coefficient for the vehicle's performance for every chassis we have? I just need a simple report with the chassis ID and its calculated performance score. Make sure to check all of them, even if some have missing engine data.", "normal_query": "For each chassis in our database, calculate the vehicle performance coefficient. I need a report showing the chassis registry ID first, followed by the calculated VPC value. Please include all chassis records in your analysis, even those without corresponding engine data.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n cv.\"CHASSIS_ID\", \n ( \n 1 - ( \n ( \n (cv.ground_vehicle_status -> 'brake_system' ->> 'pad_wear_pct')::REAL + \n (cv.ground_vehicle_status -> 'tracks' ->> 'wear_pct')::REAL \n ) / 200.0 \n ) \n ) * \n ( \n ( \n NULLIF( \n substring(cv.ground_vehicle_status -> 'vehicle_motion' ->> 'speed_kmh' from '^[0-9\\.]+'), \n '' \n )::REAL * 3.6 / 50.0 \n ) \n ) * \n ( \n (ef.engine_fluids_snapshot -> 'engine_core' ->> 'load_pct')::REAL / 100.0 \n ) AS VPC \n FROM \n \"ChassisAndVehicle\" AS cv \n LEFT JOIN \n \"EngineAndFluids\" AS ef \n ON cv.\"engine_link\" = ef.\"ENGINE_ID\";"], "external_knowledge": [5], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_11", "selected_database": "polar_equipment", "query": "I need a quick number: how many of our shelters are actually ready if a big storm hits? Even if we're missing some sensor data for a shelter, it should still be part of the initial check. Just give me the final tally.", "normal_query": "I need to get a total count of all shelters that are prepared for severe weather. Please determine this by applying the extreme weather readiness status standard. The analysis should include all shelters, even if some weather or thermal data is missing. Provide the final result as a single number representing the total count.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH shelter_readiness_data AS (\n    SELECT\n        e.\"EQUIP_CODE\",\n        -- Calculate Structural Safety Factor (SSF)\n        (100 - ws.\"structLoadPct\") / 100.0 *\n        CASE\n            WHEN ws.\"snowLoad_kgm2\" > 100 OR ws.\"windSpeed_ms\" > 20 THEN 0.5\n            WHEN ws.\"snowLoad_kgm2\" > 50 OR ws.\"windSpeed_ms\" > 10 THEN 0.8\n            ELSE 1.0\n        END AS ssf,\n        -- Get heater, insulation, and emergency light status\n        (ce.cabin_env_snapshot -> 'hvac' ->> 'heater_state') AS heater_status,\n        tswg.\"insulationStat\",\n        ls.\"emerLightStat\"\n    FROM \"Equipment\" e\n    JOIN \"CabinEnvironment\" ce ON e.\"EQUIP_CODE\" = ce.\"equipRef\"\n    LEFT JOIN \"WeatherAndStructure\" ws ON ce.loc_link = ws.loc_link\n    LEFT JOIN \"ThermalSolarWindAndGrid\" tswg ON ce.comm_link = tswg.comm_link\n    LEFT JOIN \"LightingAndSafety\" ls ON e.\"EQUIP_CODE\" = ls.\"equipRef\"\n    WHERE e.\"EquipType\" = 'Shelter'\n)\n-- Step 2: Apply the 'Extreme Weather Ready' logic and count the results.\nSELECT COUNT(*)\nFROM shelter_readiness_data\nWHERE\n    -- Apply EWRS knowledge\n    ssf > 0.7\n    AND heater_status != 'Off'\n    AND \"insulationStat\" != 'Good' \n    AND \"emerLightStat\" IN ('On', 'Testing')\n"], "external_knowledge": [3, 50], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_12", "selected_database": "polar_equipment", "query": "What's our best-case scenario for getting good science data from the Arctic? Looking only at our equipment up north, find the highest chance of success for a mission from any single instrument. Just give me that single, top-line probability score rounded to two decimal places.", "normal_query": "I want to assess our top-end capability for research in the Arctic. Could you please calculate the maximum scientific mission success probability for any single piece of scientific equipment operating in the 'Arctic' region? Please provide the final result as a single value rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH arctic_mission_data AS (\n    SELECT\n        -- Calculate Scientific Equipment Reliability (SER)\n        s.\"measureAccPct\" *\n        CASE s.\"calibrStat\"\n            WHEN 'Valid' THEN 1.0\n            WHEN 'Due' THEN 0.7\n            WHEN 'Expired' THEN 0.3\n            ELSE 0.0\n        END AS ser,\n        -- Calculate Communication Reliability Index (CRI)\n        CASE c.antenna_stat\n            WHEN 'Error' THEN 0\n            WHEN 'Warning' THEN 5\n            WHEN 'Normal' THEN 10\n            ELSE 0\n        END * (1 - c.\"netLatency_ms\" / 1000.0) AS cri\n    FROM \"Scientific\" s\n    JOIN \"Communication\" c ON s.\"equipRef\" = c.\"equipRef\"\n    JOIN \"Location\" l ON c.loc_link = l.\"STATION_name\"\n    WHERE l.\"locType\" = 'Arctic'\n)\n-- Step 2: Calculate SMSP for each instrument and find the maximum value.\nSELECT ROUND(MAX(ser * (0.8 + 0.2 * cri / 10.0))::NUMERIC, 2) AS max_arctic_smsp\nFROM arctic_mission_data;"], "external_knowledge": [34, 4, 8], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_13", "selected_database": "polar_equipment", "query": "Let's find our safest and most efficient truck. Just calculate their safety performance overall, and I want to see the single highest score out of the entire fleet. Just give me that top number, rounded.", "normal_query": "I need to identify the absolute best-performing vehicle in our fleet from a safety perspective. Please calculate the overall safety performance index for every vehicle. From all the calculated OSPI scores, find the single maximum value, rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH vehicle_safety_performance AS (\n    SELECT\n        -- EER Calculation\n        ((et.type_indices ->> 'performance_score')::REAL + e.\"RELIAB_IDX\") / 2.0 *\n        (1 - (et.type_indices ->> 'environmental_impact_idx')::REAL / 100.0) AS eer,\n        -- Get safety index for OSPI\n        (et.type_indices ->> 'safety_idx')::REAL AS safety_idx\n    FROM \"Equipment\" e\n    JOIN \"EquipmentType\" et ON e.\"EquipType\" = et.\"EquipType\"\n    WHERE e.\"EquipType\" = 'Vehicle'\n)\n-- Step 2: Calculate the final OSPI and find the maximum value.\nSELECT ROUND(MAX(safety_idx / 100.0 * eer * 0.8)::NUMERIC, 2) AS max_vehicle_ospi\nFROM vehicle_safety_performance LIMIT 100\n"], "external_knowledge": [30, 0], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_14", "selected_database": "polar_equipment", "query": "I want to see our best and worst stability of equipment during long-term operation. For each category of equipment, can you show me a list of the top 5 most stable machines and the 5 least stable ones? Show me the equipment's ID, its category, and its stability score, and group the results by category, with the best ones on top.", "normal_query": "I need you, for each equipment type, please identify the 5 units with the highest long-term operational stability score (LOSS) and the 5 units with the lowest LOSS. Please display the equipment code, its type, and the calculated LOSS, ordered first by equipment type and then by the LOSS score in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH equipment_stability_scores AS (\n    SELECT\n        e.\"EQUIP_CODE\",\n        e.\"EquipType\",\n        -- LOSS Calculation with COALESCE to prevent NULL results\n        0.5 * ( -- EER Part\n            (\n                (COALESCE((et.type_indices ->> 'performance_score')::REAL, 0) + COALESCE(e.\"RELIAB_IDX\", 0)) / 2.0\n            ) *\n            (1 - (COALESCE((et.type_indices ->> 'environmental_impact_idx')::REAL, 0) / 100.0))\n        ) +\n        0.5 * ( \n            CASE om.\"OPER_status\"\n                WHEN 'Active' THEN 10\n                WHEN 'Standby' THEN 5\n                ELSE 0\n            END *\n            (CASE \n                WHEN COALESCE(om.\"maintCycleHrs\", 0) > 0 THEN (1 - COALESCE(om.\"OPER_hours\", 0) / om.\"maintCycleHrs\")\n                ELSE 0 \n            END)\n        ) * (1 - COALESCE(om.\"OPER_hours\", 0) / 20000.0) AS loss,\n        ROW_NUMBER() OVER(PARTITION BY e.\"EquipType\" ORDER BY\n            0.5 * (((COALESCE((et.type_indices ->> 'performance_score')::REAL, 0) + COALESCE(e.\"RELIAB_IDX\", 0)) / 2.0) * (1 - (COALESCE((et.type_indices ->> 'environmental_impact_idx')::REAL, 0) / 100.0))) +\n            0.5 * (CASE om.\"OPER_status\" WHEN 'Active' THEN 10 WHEN 'Standby' THEN 5 ELSE 0 END * (CASE WHEN COALESCE(om.\"maintCycleHrs\", 0) > 0 THEN (1 - COALESCE(om.\"OPER_hours\", 0) / om.\"maintCycleHrs\") ELSE 0 END)) *\n            (1 - COALESCE(om.\"OPER_hours\", 0) / 20000.0)\n        DESC) AS rank_desc,\n\n        ROW_NUMBER() OVER(PARTITION BY e.\"EquipType\" ORDER BY\n            0.5 * (((COALESCE((et.type_indices ->> 'performance_score')::REAL, 0) + COALESCE(e.\"RELIAB_IDX\", 0)) / 2.0) * (1 - (COALESCE((et.type_indices ->> 'environmental_impact_idx')::REAL, 0) / 100.0))) +\n            0.5 * (CASE om.\"OPER_status\" WHEN 'Active' THEN 10 WHEN 'Standby' THEN 5 ELSE 0 END * (CASE WHEN COALESCE(om.\"maintCycleHrs\", 0) > 0 THEN (1 - COALESCE(om.\"OPER_hours\", 0) / om.\"maintCycleHrs\") ELSE 0 END)) *\n            (1 - COALESCE(om.\"OPER_hours\", 0) / 20000.0)\n        ASC) AS rank_asc\n\n    FROM \"Equipment\" e\n    JOIN \"EquipmentType\" et ON e.\"EquipType\" = et.\"EquipType\"\n    JOIN \"OperationMaintenance\" om ON e.\"EQUIP_CODE\" = om.\"equipRef\"\n)\nSELECT \"EQUIP_CODE\", \"EquipType\", loss\nFROM equipment_stability_scores\nWHERE rank_desc <= 5 OR rank_asc <= 5\nORDER BY \"EquipType\", loss DESC LIMIT 100\n"], "external_knowledge": [37, 0, 1], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "polar_equipment_15", "selected_database": "polar_equipment", "query": "Let's see how much the antenna's condition matters for our comms. Can you group everything by the antenna status—like normal, warning, or error—and tell me the average communication stability for each? Also, show me how many links are in each group. Make sure to only use records where we have all the necessary data, and list the results with the most stable antenna status on top.", "normal_query": "I want to perform an analysis of communication link stability grouped by antenna status. For each antenna stat category, please calculate the average base station communication stability index. The final report should display the antenna status, the total number of links for that status, and the average bscsi rounded to two decimal places. For this analysis, please ensure you are using a complete data set. Sort the results by the average BSCSI in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH comm_stability AS (\n    SELECT\n        antenna_stat,\n        -- BSCSI Calculation\n        (CASE antenna_stat\n            WHEN 'Error' THEN 0 WHEN 'Warning' THEN 5 WHEN 'Normal' THEN 10 ELSE 0\n         END * (1 - \"netLatency_ms\" / 1000.0)) -- This is the CRI part\n         * (1 + 0.2 * \"radioSignal_dBm\" / 100.0)\n         * (1 - 0.01 * (1000 - \"netLatency_ms\")) AS bscsi\n    FROM \"Communication\"\n    WHERE antenna_stat IS NOT NULL AND \"netLatency_ms\" IS NOT NULL AND \"radioSignal_dBm\" IS NOT NULL\n)\n-- Step 2: Group by the antenna status and calculate the average BSCSI.\nSELECT\n    antenna_stat,\n    COUNT(*) as number_of_links,\n    ROUND(AVG(bscsi)::numeric, 2) AS average_bscsi\nFROM comm_stability\nGROUP BY antenna_stat\nORDER BY average_bscsi DESC;"], "external_knowledge": [32, 4, 54], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "polar_equipment_16", "selected_database": "polar_equipment", "query": "I want to rank our most efficient vehicles. For each truck, calculate its overall transportation efficiency number. Please show me a list of the top 100 vehicles, with the vehicle's ID, the coefficient for vehicle performance, the index for energy sustainability, and the overall transportation efficiency number, ordered from most efficient to least.", "normal_query": "I need to generate a comprehensive vehicle efficiency and sustainability report. For all vehicles, please calculate the polar transportation efficiency coefficient. The report should display the equipment ref for each vehicle, along with its calculated VPC, ESI, and the final PTEC. Please round the VPC, ESI and PTEC scores to two decimal places. Sort the results by the PTEC in descending order and show only the top 100 vehicles.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH vehicle_vpc AS (\n    SELECT\n        cv.\"equipRef\" as equipRef,\n        -- VPC Calculation\n        ROUND(((1 - (COALESCE((cv.ground_vehicle_status -> 'brake_system' ->> 'pad_wear_pct')::REAL, 0) + COALESCE((cv.ground_vehicle_status -> 'tracks' ->> 'wear_pct')::REAL, 0)) / 200.0) *\n        (NULLIF(substring(cv.ground_vehicle_status -> 'vehicle_motion' ->> 'speed_kmh' from '^[0-9\\.]+'), '')::REAL / 50.0) *\n        ((ef.engine_fluids_snapshot -> 'engine_core' ->> 'load_pct')::REAL / 100.0))::NUMERIC, 2) AS vpc\n    FROM \"ChassisAndVehicle\" cv\n    JOIN \"EngineAndFluids\" ef ON cv.engine_link = ef.\"ENGINE_ID\"\n),\n-- Step 2: Calculate ESI for all equipment.\nequipment_esi AS (\n    SELECT\n        e.\"EQUIP_CODE\" as equip_code,\n        -- ESI Calculation\n        (pb.battery_telemetry -> 'power_state' ->> 'conversion_eff_pct')::REAL *\n        CASE (pb.battery_telemetry -> 'power_state' ->> 'primary_source')\n            WHEN 'Solar' THEN 1.5 WHEN 'Wind' THEN 1.5 WHEN 'Hybrid' THEN 1.5\n            WHEN 'Battery' THEN 1.0\n            WHEN 'Diesel' THEN 0.7\n            ELSE 0\n        END AS esi\n    FROM \"Equipment\" e\n    JOIN \"PowerBattery\" pb ON e.\"EQUIP_CODE\" = pb.equip_ref\n)\n-- Step 3: Join the results and calculate the final PTEC.\nSELECT\n    v.equipRef,\n    v.vpc,\n    ROUND(s.esi::NUMERIC, 2) AS esi,\n    -- PTEC Calculation\n    ROUND(v.vpc * (0.6 + 0.4 * s.esi / 100.0)::NUMERIC, 2) AS ptec\nFROM vehicle_vpc v\nJOIN equipment_esi s ON v.equipRef = s.EQUIP_CODE\nORDER BY ptec DESC LIMIT 100\n"], "external_knowledge": [31, 5, 2, 55], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "polar_equipment_17", "selected_database": "polar_equipment", "query": "What's the overall reliability score for all the gear we have running right now? I need the average comprehensive score, but only for the active equipment. Make sure you factor in the efficiency, readiness, and communication scores. Just give me that one final number, rounded.", "normal_query": "I need a high-level summary of our fleet's current operational state. Please calculate the average comprehensive operational reliability indicator for all equipment that is currently in an 'Active' operational status. Present the final result as a single value, rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH all_metrics AS (\n    SELECT\n        om.\"OPER_status\",\n        -- EER Calculation\n        (((et.type_indices ->> 'performance_score')::REAL + e.\"RELIAB_IDX\") / 2.0 * (1 - (et.type_indices ->> 'environmental_impact_idx')::REAL / 100.0)) AS eer,\n        -- ORS Calculation\n        CASE om.\"OPER_status\"\n            WHEN 'Active' THEN 10 * (1 - om.\"OPER_hours\" / om.\"maintCycleHrs\")\n            WHEN 'Standby' THEN 5 * (1 - om.\"OPER_hours\" / om.\"maintCycleHrs\")\n            ELSE 0\n        END AS ors,\n        -- CRI Calculation\n        CASE c.antenna_stat\n            WHEN 'Error' THEN 0 WHEN 'Warning' THEN 5 WHEN 'Normal' THEN 10 ELSE 0\n        END * (1 - c.\"netLatency_ms\" / 1000.0) AS cri\n    FROM \"Equipment\" e\n    JOIN \"EquipmentType\" et ON e.\"EquipType\" = et.\"EquipType\"\n    JOIN \"OperationMaintenance\" om ON e.\"EQUIP_CODE\" = om.\"equipRef\"\n    JOIN \"Communication\" c ON e.\"EQUIP_CODE\" = c.\"equipRef\"\n)\n-- Step 2: Calculate the CORI for active equipment and find the average.\nSELECT\n    ROUND(AVG(0.4 * eer + 0.4 * ors + 0.2 * cri)::NUMERIC, 2) AS avg_cori_for_active_equipment\nFROM\n    all_metrics\nWHERE\n    \"OPER_status\" = 'Active' LIMIT 100\n"], "external_knowledge": [0, 1, 4, 39], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_18", "selected_database": "polar_equipment", "query": "How does the cold affect our batteries? I want to see a breakdown of battery health based on how cold it is outside. Group the gear into a few temperature buckets like 'Extreme Cold,' 'Standard Cold,' and 'Mild Cold,' and for each bucket, show how many pieces of equipment are in it and what their average battery health is.", "normal_query": "I need to analyze battery performance under thermal stress. Please calculate the temperature-zoned average battery health for all equipment. The report should group equipment by the standard external temperature ranges and display the equipment count and the average battery health for each zone, rounded to two decimal places. Order the results by the temperature range.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n CASE \n  WHEN ws.\"extTempC\" < -40 THEN 'Extreme Cold (<-40C)' \n  WHEN ws.\"extTempC\" >= -40 AND ws.\"extTempC\" < -20 THEN 'Standard Cold (-40C to -20C)' \n  ELSE 'Mild Cold (>=-20C)' \n END AS temp_range, \n COUNT(DISTINCT pb.equip_ref) AS equipment_count, \n ROUND(AVG((pb.battery_telemetry -> 'battery_pack' ->> 'health_pct')::REAL)::NUMERIC, 2) AS avg_battery_health \nFROM \"PowerBattery\" pb \nJOIN \"Communication\" c ON pb.equip_ref = c.\"equipRef\" \nJOIN \"WeatherAndStructure\" ws ON c.loc_link = ws.loc_link \nWHERE ws.\"extTempC\" IS NOT NULL \nGROUP BY temp_range \nORDER BY temp_range;"], "external_knowledge": [22, 56], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": true, "order": true}}
{"instance_id": "polar_equipment_19", "selected_database": "polar_equipment", "query": "I want a list of our most unreliable comms hubs. Figure out which stations have consistently bad link resilience. For each of those problem stations, show me the station's name, its average reliability and stability scores rounded by 2 decimals, and a list of all the equipment there so we know what to check. Only use complete data for this, and show me the worst stations at the top of the list.", "normal_query": "Please generate a report on stations with poor communication links. Use the communication network resilience assessment to identify all stations with 'Low Resilience'. For each of these stations, I need to see the station name, the average communication reliability index rounded by 2 decimals, the average base station communication stability index rounded by 2 decimals, and a list of all equipment at station contributing to the low score. Please ensure you use a complete data set for the calculations. Order the results by the average BSCSI, with the lowest first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH comm_resilience AS (\n    SELECT\n        c.loc_link,\n        c.\"equipRef\",\n        -- CRI\n        CASE c.antenna_stat WHEN 'Error' THEN 0 WHEN 'Warning' THEN 5 WHEN 'Normal' THEN 10 ELSE 0 END * (1 - c.\"netLatency_ms\" / 1000.0) AS cri,\n        -- BSCSI\n       (CASE c.antenna_stat WHEN 'Error' THEN 0 WHEN 'Warning' THEN 5 WHEN 'Normal' THEN 10 ELSE 0 END * (1 - c.\"netLatency_ms\" / 1000.0)) * (1 + 0.2 * c.\"radioSignal_dBm\" / 100.0) * (1 - 0.01 * (1000 - c.\"netLatency_ms\")) AS bscsi\n    FROM \"Communication\" c\n    WHERE c.loc_link IS NOT NULL AND c.antenna_stat IS NOT NULL AND c.\"netLatency_ms\" IS NOT NULL AND c.\"radioSignal_dBm\" IS NOT NULL\n)\n-- Step 2: Group by station and filter for those classified as 'Low Resilience'.\nSELECT\n    loc_link as station_name,\n    ROUND(AVG(cri)::NUMERIC, 2) as avg_cri,\n    ROUND(AVG(bscsi)::NUMERIC, 2) as avg_bscsi,\n    ARRAY_AGG(\"equipRef\") as equipment_at_station\nFROM comm_resilience\nGROUP BY loc_link\n-- Apply CNRA classification logic in the HAVING clause\nHAVING NOT (AVG(cri) > 0.6 AND AVG(bscsi) > 0.7) -- This identifies the 'Low Resilience' stations\nORDER BY avg_bscsi\n"], "external_knowledge": [54, 45, 32, 4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "polar_equipment_20", "selected_database": "polar_equipment", "query": "I need to see the water situation at all of our stations. Can you give me a list showing each station's name, its water quality score, and the water tank level? Also, add a simple category based on our standard classification system for water quality. Make sure every station shows up, even the ones we don't have water readings for, and list them alphabetically.", "normal_query": "Please generate a comprehensive water quality report for each station. For every station, show its name, the raw water quality index, and the water level percentage. Additionally, apply the water quality classification system to categorize the water. Ensure that all stations are included in the report, even if they have no associated water data. Order the results by station name.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nSELECT\n    l.\"STATION_name\",\n    ww.\"waterQualityIdx\",\n    ww.\"waterLevelPct\",\n    -- Apply WQCS classification logic\n    CASE\n        WHEN ww.\"waterQualityIdx\" >= 91 THEN 'High-Quality'\n        WHEN ww.\"waterQualityIdx\" >= 71 THEN 'Good'\n        WHEN ww.\"waterQualityIdx\" >= 51 THEN 'Moderate'\n        WHEN ww.\"waterQualityIdx\" >= 26 THEN 'Poor'\n        ELSE 'Unsafe'\n    END AS water_quality_class\nFROM\n    \"Location\" l\nLEFT JOIN \"Communication\" c ON l.\"STATION_name\" = c.loc_link\nLEFT JOIN \"WaterAndWaste\" ww ON c.\"equipRef\" = ww.\"equipRef\"\nGROUP BY l.\"STATION_name\", ww.\"waterQualityIdx\", ww.\"waterLevelPct\"\nORDER BY l.\"STATION_name\""], "external_knowledge": [25], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "polar_equipment_M_1", "selected_database": "polar_equipment", "query": "To make things easier, can we build a reusable tool to figure out the index of energy sustainability? I need a function called 'calculate_esi' that takes an efficiency number and a power source name, and then just spits out the ESI score.", "normal_query": "I want to create a function called 'calculate_esi' taking two inputs, efficiency and resource, that returns the energy sustainability index for our equipment. Please make this a reusable PostgreSQL function that our team can call whenever needed.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Intent: Create a function to automatically calculate ESI (Energy Sustainability Index)\n-- Knowledge used: #2 Energy Sustainability Index (ESI)\n-- Advanced features: CASE expression, function creation\n\nCREATE OR REPLACE FUNCTION calculate_esi(\n    efficiency NUMERIC, \n    source VARCHAR\n) RETURNS NUMERIC AS $$\nBEGIN\n    -- Step 1: Calculate ESI value based on energy sustainability formula\n    -- Using CASE expression to implement different weights for energy source types\n    RETURN efficiency * CASE \n        WHEN source = 'Solar' THEN 1.5\n        WHEN source = 'Wind' THEN 1.5\n        WHEN source = 'Hybrid' THEN 1.5\n        WHEN source = 'Battery' THEN 1.0\n        WHEN source = 'Diesel' THEN 0.7\n        ELSE 0\n    END;\nEND;\n$$ LANGUAGE plpgsql;\n"], "external_knowledge": [2], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    pred_query_result = execute_queries(pred_sqls, db_name, conn)\n    query = [\"\"\"\n    SELECT calculate_esi(0.8, 'Solar') AS esi_solar;\n             \"\"\"]\n    test_result = execute_queries(query, db_name, conn)\n    print (test_result)\n    assert test_result\n    assert str(test_result[0][0][0]) == \"1.20\"\n    return 1\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_M_2", "selected_database": "polar_equipment", "query": "Our queries filtering scientific equipment by reliability are slow. Can you create a special index called 'idx_scientific_reliability' to speed things up? It should be built directly on the reliability score calculation so we can find our most reliable gear faster.", "normal_query": "Create a function-based index called 'idx_scientific_reliability' to optimize queries that filter scientific equipment based on their scientific equipment reliability. This index should directly implement the SER formula.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE INDEX idx_scientific_reliability ON \"Scientific\" \n    ((\n        \"measureAccPct\" * CASE \n            WHEN \"calibrStat\" = 'Valid' THEN 1.0\n            WHEN \"calibrStat\" = 'Due' THEN 0.7\n            WHEN \"calibrStat\" = 'Expired' THEN 0.3\n            ELSE 0.0\n        END\n    ));"], "external_knowledge": [8], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    pred_query_result = execute_queries(pred_sqls, db_name, conn)\n    query = [\"\"\"\n    SELECT 1\n    FROM pg_indexes\n    WHERE indexname = 'idx_scientific_reliability';\n    \"\"\"]\n    test_result = execute_queries(query, db_name, conn)\n    print (test_result)\n    assert test_result\n    assert test_result[0][0][0] == 1    \n    return 1\n"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_M_3", "selected_database": "polar_equipment", "query": "Let's reward the well-maintained cabins. For any equipment that's in a cabin meeting our 'habitability standard', can you give its reliability index a 15% boost?", "normal_query": "Increase the reliability index by 15% for all equipment associated with cabins that meet our cabin habitability standard.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Update equipment that meets cabin habitability standards\n-- Note: This query is updated for the new schema, referencing nested JSONB fields and removing unnecessary joins.\n\nUPDATE \"Equipment\"\nSET \"RELIAB_IDX\" = \"RELIAB_IDX\" * 1.15\nWHERE \"EQUIP_CODE\" IN (\n    -- Step 1: Identify equipment that meets cabin habitability standards using the new JSONB structure.\n    SELECT c.\"equipRef\"\n    FROM \"CabinEnvironment\" AS c\n    WHERE\n        -- Extract air quality metrics from the 'air_metrics' nested object\n        (c.cabin_env_snapshot -> 'air_metrics' ->> 'temperature_c')::REAL BETWEEN 18 AND 24\n        AND (c.cabin_env_snapshot -> 'air_metrics' ->> 'o2_pct')::REAL > 19.5\n        AND (c.cabin_env_snapshot -> 'air_metrics' ->> 'co2_ppm')::REAL < 1000\n        -- Extract HVAC metrics from the 'hvac' nested object\n        AND (c.cabin_env_snapshot -> 'hvac' ->> 'vent_state') != 'Off'\n        AND (c.cabin_env_snapshot -> 'hvac' ->> 'heater_state') != 'Off'\n);"], "external_knowledge": [17], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    query = [\"\"\"\n    SELECT \\\"EQUIP_CODE\\\", \\\"RELIAB_IDX\\\"\n    FROM \\\"Equipment\\\"\n    WHERE \\\"EQUIP_CODE\\\" IN (\n        -- Subquery now uses the correct JSONB paths for the habitability standard check\n        SELECT c.\\\"equipRef\\\"\n        FROM \\\"CabinEnvironment\\\" AS c\n        WHERE\n            (c.cabin_env_snapshot -> 'air_metrics' ->> 'temperature_c')::REAL BETWEEN 18 AND 24\n            AND (c.cabin_env_snapshot -> 'air_metrics' ->> 'o2_pct')::REAL > 19.5\n            AND (c.cabin_env_snapshot -> 'air_metrics' ->> 'co2_ppm')::REAL < 1000\n            AND (c.cabin_env_snapshot -> 'hvac' ->> 'vent_state') != 'Off'\n            AND (c.cabin_env_snapshot -> 'hvac' ->> 'heater_state') != 'Off'\n    )\n    LIMIT 3;\n    \"\"\"]\n\n    test_result = execute_queries(query, db_name, conn)\n    assert test_result, \"Query should return results\"\n    assert len(test_result[0]) > 0, \"At least one equipment should meet the cabin habitability criteria\"\n    return 1", "def test_case(pred_sqls, sol_sqls, db_name, conn):\n    query = [\"\"\"\n    SELECT COUNT(*) > 0\n    FROM \\\"Equipment\\\"\n    WHERE\n        MOD(\\\"RELIAB_IDX\\\"::NUMERIC, 0.01) = 0\n        AND \\\"EQUIP_CODE\\\" IN (\n            -- Subquery now uses the correct JSONB paths for the habitability standard check\n            SELECT c.\\\"equipRef\\\"\n            FROM \\\"CabinEnvironment\\\" AS c\n            WHERE\n                (c.cabin_env_snapshot -> 'air_metrics' ->> 'temperature_c')::REAL BETWEEN 18 AND 24\n                AND (c.cabin_env_snapshot -> 'air_metrics' ->> 'o2_pct')::REAL > 19.5\n                AND (c.cabin_env_snapshot -> 'air_metrics' ->> 'co2_ppm')::REAL < 1000\n                AND (c.cabin_env_snapshot -> 'hvac' ->> 'vent_state') != 'Off'\n                AND (c.cabin_env_snapshot -> 'hvac' ->> 'heater_state') != 'Off'\n        );\n    \"\"\"]\n\n    update_result = execute_queries(query, db_name, conn)\n    assert update_result[0][0][0], \"Reliability values should be updated by multiplying by 1.15\"\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_M_4", "selected_database": "polar_equipment", "query": "I need a simple way to check on water usage for our dashboards. Can you create a view called 'water_management_view'? It should show the equipment ID, its calculated water management score, and categorize each of them based on the status classification of water resource management. Let's base it on all equipment that has water level data.", "normal_query": "Create a dashboard view called 'water_management_view' that calculates the water resource management index for all equipment with water level data. The view should display the equipment reference, the calculated WRMI value, and categorize each item according to the water resource management status classification.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Provide an easily queryable view for water resource status, adapted for the new schema.\n-- Knowledge used: #7 Water Resource Management Index (WRMI), #53\n-- Advanced features: View creation with calculation and data normalization.\n\nCREATE OR REPLACE VIEW water_management_view AS\nSELECT \n    w.\"equipRef\",\n    -- Step 1: Calculate WRMI, normalizing all percentage-based columns to a 0-1 scale.\n    (w.\"waterLevelPct\" / 100.0) * (w.\"waterQualityIdx\" / 100.0) * (1 - (w.\"wasteTankPct\" / 100.0)) AS wrmi,\n    -- Step 2: Classify water management status using the same normalized formula.\n    CASE\n        WHEN (w.\"waterLevelPct\" / 100.0) * (w.\"waterQualityIdx\" / 100.0) * (1 - (w.\"wasteTankPct\" / 100.0)) < 0.5\n            THEN 'Conservation Needed'\n        WHEN (w.\"waterLevelPct\" / 100.0) * (w.\"waterQualityIdx\" / 100.0) * (1 - (w.\"wasteTankPct\" / 100.0)) < 0.7\n            THEN 'Monitoring Advised'\n        ELSE 'Sustainable Management'\n    END AS management_status\nFROM \n    \"WaterAndWaste\" w\nWHERE \n    w.\"waterLevelPct\" IS NOT NULL;"], "external_knowledge": [7, 53], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    execute_queries(pred_sqls, db_name, conn)\n\n    sample_query = [\"\"\"\n        SELECT \n            \\\"equipRef\\\", \n            \\\"waterLevelPct\\\", \n            \\\"waterQualityIdx\\\", \n            \\\"wasteTankPct\\\"\n        FROM \\\"WaterAndWaste\\\"\n        WHERE \\\"waterLevelPct\\\" IS NOT NULL\n        LIMIT 1;\n    \"\"\"]\n    sample_result = execute_queries(sample_query, db_name, conn)\n    assert sample_result and sample_result[0], \"No data in WaterAndWaste\"\n\n    equip_ref, level, quality, waste = sample_result[0][0]\n    wrmi = (level / 100.0) * (quality / 100.0) * (1 - (waste / 100.0))\n\n    if wrmi < 0.5:\n        expected_status = 'Conservation Needed'\n    elif wrmi < 0.7:\n        expected_status = 'Monitoring Advised'\n    else:\n        expected_status = 'Sustainable Management'\n\n    view_query = [f\"\"\"\n        SELECT wrmi, management_status\n        FROM water_management_view\n        WHERE \\\"equipRef\\\" = '{equip_ref}';\n    \"\"\"]\n\n    view_result = execute_queries(view_query, db_name, conn)\n    assert view_result and view_result[0], f\"No data returned from view for equipRef {equip_ref}\"\n\n    wrmi_view, status_view = view_result[0][0]\n\n    assert abs(wrmi - wrmi_view) < 0.0001, f\"WRMI mismatch: expected {wrmi}, got {wrmi_view}\"\n    assert status_view == expected_status, f\"Status mismatch: expected {expected_status}, got {status_view}\"\n\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_M_5", "selected_database": "polar_equipment", "query": "We need a standard way to calculate the performance coefficient for vehicles. Can you build a function called 'calculate_vpc' that takes brake wear, track wear, speed, and engine load as inputs? It's important that it's robust, so please make sure it throws an error if any of the input values are out of the expected range.", "normal_query": "For our polar vehicles, we need a utility function 'calculate_vpc' to calculate the vehicle performance coefficient for performance assessment. Create a PostgreSQL function that takes four parameters: brake pad wear percentage (0-100), track wear percentage (0-100), vehicle speed (km/h, non-negative), and engine load percentage (0-100). The function should validate these inputs with clear error messages.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Intent: Create a function to calculate VPC for vehicles\n-- Knowledge used: #5 Vehicle Performance Coefficient (VPC)\n-- Advanced features: Function creation, error handling with EXCEPTION\n\nCREATE OR REPLACE FUNCTION calculate_vpc(\n    brake_wear REAL,\n    track_wear DOUBLE PRECISION,\n    vehicle_speed REAL,\n    engine_load REAL\n) RETURNS NUMERIC AS $$\nBEGIN\n    -- Step 1: Validate input parameters to prevent calculation errors\n    -- Using defensive programming to handle edge cases\n    IF brake_wear < 0 OR brake_wear > 100 OR \n       track_wear < 0 OR track_wear > 100 OR\n       vehicle_speed < 0 OR engine_load < 0 OR engine_load > 100 THEN\n        RAISE EXCEPTION 'Invalid input parameters for VPC calculation';\n    END IF;\n    \n    -- Step 2: Calculate VPC using the formula from knowledge point #5\n    -- VPC = (1 - (brake_wear + track_wear)/200) * (vehicle_speed/50) * (engine_load/100)\n    RETURN (1 - (brake_wear + track_wear) / 200.0) * \n           (vehicle_speed / 50.0) * \n           (engine_load / 100.0);\nEND;\n$$ LANGUAGE plpgsql;\n"], "external_knowledge": [5], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n        execute_queries(pred_sqls, db_name, conn)\n\n        valid_inputs = (20, 30, 40, 80)\n        brake_wear, track_wear, vehicle_speed, engine_load = valid_inputs\n        expected_vpc = (1 - (brake_wear + track_wear) / 200.0) * (vehicle_speed / 50.0) * (engine_load / 100.0)\n\n        query_valid = [f\"\"\"\n            SELECT calculate_vpc({brake_wear}, {track_wear}, {vehicle_speed}, {engine_load}) AS vpc_result;\n        \"\"\"]\n\n        result_valid = execute_queries(query_valid, db_name, conn)\n        assert result_valid and result_valid[0], \"No result returned for valid input\"\n\n        actual_vpc = float(result_valid[0][0][0])\n        assert abs(actual_vpc - expected_vpc) < 0.0001, f\"Incorrect VPC calculation: expected {expected_vpc}, got {actual_vpc}\"\n\n        query_invalid = [\"\"\"\n            SELECT calculate_vpc(120, 30, 40, 80);\n        \"\"\"]\n\n        _, execution_error, _ = execute_queries(query_invalid, db_name, conn)\n        assert execution_error, \"Expected an exception for invalid input but function executed without error\"\n\n        return 1\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_M_6", "selected_database": "polar_equipment", "query": "I want a standard way to figure out how reliable our life support systems are. Can you build a reusable function called get_lssr that takes an equipment ID and gives back its life support score? It should be based on our formulas for how ready the gear is and how well its insulation is holding up. Make sure the calculator doesn't break if some of the sensor data is missing; it should just return zero in that case.", "normal_query": "Please create a reusable function named get_lssr to standardize the calculation of the life support system reliability for any given piece of equipment. This function should take an equipment code as input and calculate the LSSR. The function must also handle cases where component data might be missing, returning 0 in such instances.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Step 1: Define a function that accepts an equipment code and returns the LSSR score.\nCREATE OR REPLACE FUNCTION get_lssr(p_equip_code TEXT)\nRETURNS REAL AS $$\nDECLARE\n    v_ors REAL;\n    v_tie REAL;\nBEGIN\n    -- Step 2: Calculate ORS by selecting from OperationMaintenance.\n    SELECT\n        CASE \"OPER_status\"\n            WHEN 'Active' THEN 10 * (1 - \"OPER_hours\" / NULLIF(\"maintCycleHrs\", 0))\n            WHEN 'Standby' THEN 5 * (1 - \"OPER_hours\" / NULLIF(\"maintCycleHrs\", 0))\n            ELSE 0\n        END INTO v_ors\n    FROM \"OperationMaintenance\" WHERE \"equipRef\" = p_equip_code;\n\n    -- Step 3: Calculate TIE by joining through Communication to ThermalSolarWindAndGrid.\n    SELECT\n        CASE tsg.\"insulationStat\"\n            WHEN 'Good' THEN 0.9 - COALESCE(tsg.\"heatLoss_kwh\", 0) / 10.0\n            WHEN 'Fair' THEN 0.6 - COALESCE(tsg.\"heatLoss_kwh\", 0) / 10.0\n            ELSE 0.3 - COALESCE(tsg.\"heatLoss_kwh\", 0) / 10.0\n        END INTO v_tie\n    FROM \"ThermalSolarWindAndGrid\" tsg\n    JOIN \"Communication\" c ON tsg.comm_link = c.\"COMM_ID\"\n    WHERE c.\"equipRef\" = p_equip_code;\n\n    -- Step 4: Return the final LSSR, handling cases where a component might be missing.\n    RETURN 0.7 * COALESCE(v_ors, 0) + 0.3 * COALESCE(v_tie, 0);\n\nEXCEPTION\n    WHEN NO_DATA_FOUND THEN\n        RETURN 0;\nEND;\n$$ LANGUAGE plpgsql;\n"], "external_knowledge": [1, 6, 33], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n        # Run the prediction SQL first\n        execute_queries(pred_sqls, db_name, conn)\n\n        fetch_query = [\"\"\"\n            SELECT o.\"equipRef\", o.\"OPER_status\", o.\"OPER_hours\", o.\"maintCycleHrs\",\n                t.\"insulationStat\", t.\"heatLoss_kwh\"\n            FROM \"OperationMaintenance\" o\n            JOIN \"Communication\" c ON o.\"equipRef\" = c.\"equipRef\"\n            JOIN \"ThermalSolarWindAndGrid\" t ON c.\"COMM_ID\" = t.\"comm_link\"\n            WHERE o.\"maintCycleHrs\" > 0\n            LIMIT 1;\n        \"\"\"]\n        result = execute_queries(fetch_query, db_name, conn)\n        assert result and result[0], \"No suitable equipment found for LSSR test.\"\n\n        row = result[0][0]\n        equip_ref, status, hours, cycle, insulation, heat_loss = row\n        heat_loss = heat_loss or 0  \n\n        if status == 'Active':\n            ors = 10 * (1 - hours / cycle)\n        elif status == 'Standby':\n            ors = 5 * (1 - hours / cycle)\n        else:\n            ors = 0\n\n        if insulation == 'Good':\n            tie = 0.9 - heat_loss / 10.0\n        elif insulation == 'Fair':\n            tie = 0.6 - heat_loss / 10.0\n        else:\n            tie = 0.3 - heat_loss / 10.0\n\n        expected_lssr = 0.7 * ors + 0.3 * tie\n\n        lssr_query = [f\"\"\"SELECT get_lssr('{equip_ref}');\"\"\"]\n        lssr_result = execute_queries(lssr_query, db_name, conn)\n        assert lssr_result and lssr_result[0], \"No result from get_lssr.\"\n\n        actual_lssr = float(lssr_result[0][0][0])\n        assert abs(actual_lssr - expected_lssr) < 0.0001, f\"Expected {expected_lssr}, got {actual_lssr}\"\n\n        return 1\n\n"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_M_7", "selected_database": "polar_equipment", "query": "Let's find our most problematic equipment. I need a list of all assets that are both performing poorly—let's use an a rating to calculate equipment efficiency below 40 as the cutoff—and are also more expensive to maintain than other gear in their same category. For each one that meets these criteria, please log its equipment code in our review system table called EquipmentReviewLog, create if it doesn't exists, and note the reason it was flagged.", "normal_query": "Please identify all equipment with an equipment efficiency rating below 40 that also have a maintenance cost higher than the average for their specific equipment type. For each identified piece of equipment, create a new record in the EquipmentReviewLog table, create if the table doesn't exists. You should also insert its equipment code and a reason for the review.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- DDL to create the log table (run once)\nCREATE TABLE EquipmentReviewLog (log_id SERIAL PRIMARY KEY, equip_code TEXT, reason TEXT);\n\n-- Step 1: In a CTE, calculate EER for each item and also use a window function to find the average maintenance cost for its specific equipment type.\nWITH equipment_performance_cost AS (\n    SELECT\n        e.\"EQUIP_CODE\",\n        om.\"MAINT_COST_usd\",\n        -- EER Calculation\n        ((COALESCE((et.type_indices ->> 'performance_score')::REAL, 0) + COALESCE(e.\"RELIAB_IDX\", 0)) / 2.0 * (1 - COALESCE((et.type_indices ->> 'environmental_impact_idx')::REAL, 0) / 100.0)) AS eer,\n        -- Window function to get the average cost for the equipment's type, used for comparison.\n        AVG(om.\"MAINT_COST_usd\") OVER (PARTITION BY e.\"EquipType\") as avg_type_maint_cost\n    FROM \"Equipment\" e\n    JOIN \"EquipmentType\" et ON e.\"EquipType\" = et.\"EquipType\"\n    JOIN \"OperationMaintenance\" om ON e.\"EQUIP_CODE\" = om.\"equipRef\"\n)\n-- Step 2: INSERT records into the log table by SELECTing from the CTE where EER is low and cost is higher than the type average.\nINSERT INTO EquipmentReviewLog (equip_code, reason)\nSELECT\n    \"EQUIP_CODE\",\n    'Low EER (' || ROUND(eer::NUMERIC, 2) || ') and High Maintenance Cost (' || ROUND(\"MAINT_COST_usd\"::NUMERIC, 2) || ' vs avg ' || ROUND(avg_type_maint_cost::NUMERIC, 2) || ')'\nFROM\n    equipment_performance_cost\nWHERE\n    eer < 40 AND \"MAINT_COST_usd\" > avg_type_maint_cost;\n"], "external_knowledge": [0], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n        query = [\"\"\"\n            WITH equipment_metrics AS (\n                SELECT\n                    e.\"EQUIP_CODE\",\n                    om.\"MAINT_COST_usd\",\n                    ((COALESCE((et.type_indices ->> 'performance_score')::REAL, 0) + \n                    COALESCE(e.\"RELIAB_IDX\", 0)) / 2.0 *\n                    (1 - COALESCE((et.type_indices ->> 'environmental_impact_idx')::REAL, 0) / 100.0)\n                    ) AS eer,\n                    AVG(om.\"MAINT_COST_usd\") OVER (PARTITION BY e.\"EquipType\") AS avg_cost\n                FROM \"Equipment\" e\n                JOIN \"EquipmentType\" et ON e.\"EquipType\" = et.\"EquipType\"\n                JOIN \"OperationMaintenance\" om ON e.\"EQUIP_CODE\" = om.\"equipRef\"\n            )\n            SELECT \"EQUIP_CODE\", eer, \"MAINT_COST_usd\", avg_cost\n            FROM equipment_metrics\n            WHERE eer < 40 AND \"MAINT_COST_usd\" > avg_cost\n        \"\"\"]\n        expected = execute_queries(query, db_name, conn)\n        assert expected and expected[0], \"No qualifying equipment found for test.\"\n\n        fetch_log = [\"\"\"SELECT equip_code, reason FROM EquipmentReviewLog;\"\"\"]\n        log_result = execute_queries(fetch_log, db_name, conn)\n        assert log_result and log_result[0], \"Log table should have entries\"\n\n        logged_codes = {row[0] for row in log_result[0]}\n        expected_codes = {row[0] for row in expected[0]}\n\n        assert logged_codes == expected_codes, f\"Mismatch in logged equipment. Expected: {expected_codes}, Got: {logged_codes}\"\n\n        return 1\n"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_M_8", "selected_database": "polar_equipment", "query": "We need to add a hard safety stop to our system. Can you set things up so that if a piece of gear has failed its inspection, nobody can mark it as 'Active' and put it back in service? The system should block the update and give an error message. We can't have people using equipment that we know is broken.", "normal_query": "I need to implement a critical safety protocol. Please create a failed inspection activation lockout rule in the database. If this condition is met, the transaction should be blocked and an exception raised to prevent using unsafe equipment.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Step 1: Create the function that the trigger will execute.\nCREATE OR REPLACE FUNCTION check_inspection_before_activation()\nRETURNS TRIGGER AS $$\nBEGIN\n    -- Step 2: Check if the status is being changed to 'Active' and if the inspection status is 'Failed'.\n    IF NEW.\"OPER_status\" = 'Active' AND OLD.\"OPER_status\" != 'Active' AND OLD.inspect_status = 'Failed' THEN\n        -- Step 3: If the conditions are met, raise an exception to block the UPDATE.\n        RAISE EXCEPTION 'Cannot activate equipment %. Inspection status is \"Failed\".', NEW.\"equipRef\";\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Step 4: Create the trigger and attach it to the OperationMaintenance table.\nCREATE TRIGGER trigger_prevent_activation_if_failed_inspection\nBEFORE UPDATE ON \"OperationMaintenance\"\nFOR EACH ROW EXECUTE FUNCTION check_inspection_before_activation();\n"], "external_knowledge": [57], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n        fetch_query = [\"\"\"\n            SELECT \"equipRef\"\n            FROM \"OperationMaintenance\"\n            WHERE inspect_status = 'Failed' AND \"OPER_status\" != 'Active'\n            LIMIT 1;\n        \"\"\"]\n        result = execute_queries(fetch_query, db_name, conn)\n        assert result and result[0], \"No suitable test row with Failed inspection found.\"\n\n        equip_code = result[0][0][0]\n\n        update_query = [f\"\"\"\n            UPDATE \"OperationMaintenance\"\n            SET \"OPER_status\" = 'Active'\n            WHERE \"equipRef\" = '{equip_code}';\n        \"\"\"]\n\n        _, execution_error, _ = execute_queries(update_query, db_name, conn)\n        assert execution_error, \"Expected trigger to block activation for failed inspection, but no error occurred.\"\n\n        verify_query = [f\"\"\"\n            SELECT \"OPER_status\" FROM \"OperationMaintenance\"\n            WHERE \"equipRef\" = '{equip_code}';\n        \"\"\"]\n        status_result = execute_queries(verify_query, db_name, conn)\n        assert status_result and status_result[0][0][0] != 'Active', \"Status should not have been updated to Active.\"\n\n        return 1\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_M_9", "selected_database": "polar_equipment", "query": "Let's make a standard tool for measuring how well our energy and water systems work together. Can you build a calculator function called get_ewrii? It should take an equipment ID and give back the a single score based on its energy sustainability and its water management performance. It's important that this tool is reliable; if it can't find some of the data it needs for a calculation, it should just return zero instead of breaking.", "normal_query": "I need to create a reusable function named get_ewrii to standardize our energy-water resource integration index calculation. The function should accept an equipment code and calculate the EWRII. The function must return a value of 0 if any of the underlying data for a calculation component is not found, thereby preventing query failures.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Step 1: Define a function that accepts an equipment code and returns its EWRII.\nCREATE OR REPLACE FUNCTION get_ewrii(p_equip_code TEXT) RETURNS REAL AS $$\nDECLARE\n v_esi REAL;\n v_wrmi REAL;\n v_heater_temp REAL;\nBEGIN\n SELECT (pb.battery_telemetry -> 'power_state' ->> 'conversion_eff_pct')::REAL * CASE (pb.battery_telemetry -> 'power_state' ->> 'primary_source') WHEN 'Solar' THEN 1.5 WHEN 'Wind' THEN 1.5 WHEN 'Hybrid' THEN 1.5 WHEN 'Battery' THEN 1.0 WHEN 'Diesel' THEN 0.7 ELSE 0 END INTO v_esi FROM \"PowerBattery\" pb WHERE pb.equip_ref = p_equip_code;\n SELECT (ww.\"waterLevelPct\" / 100.0) * (ww.\"waterQualityIdx\" / 100.0) * (1 - (ww.\"wasteTankPct\" / 100.0)) INTO v_wrmi FROM \"WaterAndWaste\" ww WHERE ww.\"equipRef\" = p_equip_code;\n SELECT (ce.cabin_env_snapshot -> 'hvac' ->> 'heater_temp_c')::REAL INTO v_heater_temp FROM \"CabinEnvironment\" ce WHERE ce.\"equipRef\" = p_equip_code;\n IF v_esi IS NULL OR v_wrmi IS NULL OR v_heater_temp IS NULL THEN RETURN 0; END IF;\n RETURN 0.5 * v_esi + 0.5 * v_wrmi * (1 - v_heater_temp / 100.0);\nEXCEPTION WHEN NO_DATA_FOUND THEN RETURN 0;\nEND;\n$$ LANGUAGE plpgsql;"], "external_knowledge": [38, 2, 7], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n        manual_calc_query = [f\"\"\"\n            SELECT\n                equip_ref,\n                (pb.battery_telemetry -> 'power_state' ->> 'conversion_eff_pct')::REAL *\n                CASE (pb.battery_telemetry -> 'power_state' ->> 'primary_source')\n                    WHEN 'Solar' THEN 1.5\n                    WHEN 'Wind' THEN 1.5\n                    WHEN 'Hybrid' THEN 1.5\n                    WHEN 'Battery' THEN 1.0\n                    WHEN 'Diesel' THEN 0.7\n                    ELSE 0\n                END AS esi,\n\n                -- WRMI\n                (ww.\"waterLevelPct\" / 100.0) *\n                (ww.\"waterQualityIdx\" / 100.0) *\n                (1 - (ww.\"wasteTankPct\" / 100.0)) AS wrmi,\n\n                -- Heater temp\n                (ce.cabin_env_snapshot -> 'hvac' ->> 'heater_temp_c')::REAL AS heater_temp\n            FROM \"PowerBattery\" pb\n            JOIN \"WaterAndWaste\" ww ON pb.equip_ref = ww.\"equipRef\"\n            JOIN \"CabinEnvironment\" ce ON pb.equip_ref = ce.\"equipRef\"\n            LIMIT 1\n        \"\"\"]\n\n        manual_result = execute_queries(manual_calc_query, db_name, conn)\n        assert manual_result and manual_result[0], f\"Failed to manually fetch values\"\n        ref, esi, wrmi, heater_temp = manual_result[0][0]\n\n        manual_ewrii = 0.5 * (esi or 0) + 0.5 * (wrmi or 0) * (1 - (heater_temp or 20) / 100.0)\n\n        func_query = [f\"SELECT get_ewrii('{ref}') AS ewrii;\"]\n        func_result = execute_queries(func_query, db_name, conn)\n        assert func_result and func_result[0][0], f\"get_ewrii returned no value for {ref}\"\n        func_ewrii = float(func_result[0][0][0])\n\n        tolerance = 0.0001\n        assert abs(func_ewrii - manual_ewrii) < tolerance, (\n            f\"Mismatch in EWRII. Function: {func_ewrii}, Manual: {manual_ewrii}\"\n        )\n\n        print(f\"get_ewrii() = {func_ewrii:.4f}, Manual = {manual_ewrii:.4f} — Match within tolerance.\")\n        return 1\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "polar_equipment_M_10", "selected_database": "polar_equipment", "query": "We need a database failsafe to protect our most important gear. Can you set up a trigger named trigger_prevent_delete_active_critical that stops anyone from deleting a piece of critical equipment from the system if it's currently running? The system should throw an error and block the deletion automatically.", "normal_query": "I need to enforce a database-level safety protocol for critical equipment. Please create a trigger named trigger_prevent_delete_active_critical that prevents the deletion of any equipment record that is currently 'Active' and also meets the definition of critical equipment. This trigger should fire before any delete operation on the Equipment table and raise an exception if the conditions are met, ensuring that essential, in-use assets cannot be accidentally removed.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nCREATE OR REPLACE FUNCTION prevent_delete_active_critical_equipment()\nRETURNS TRIGGER AS $$\nDECLARE\n    is_critical BOOLEAN;\n    is_active BOOLEAN;\nBEGIN\n    -- Step 2: Check if the equipment being deleted (represented by OLD) meets the criteria for being \"Critical\".\n    SELECT TRUE INTO is_critical FROM polar.\"EquipmentType\" WHERE \"EquipType\" = OLD.\"EquipType\" AND (type_indices->>'safety_idx')::REAL > 75;\n    -- Step 3: Check if the equipment is \"Active\".\n    SELECT TRUE INTO is_active FROM polar.\"OperationMaintenance\" WHERE \"equipRef\" = OLD.\"EQUIP_CODE\" AND \"OPER_status\" = 'Active';\n\n    -- Step 4: If both conditions are true, raise an exception to block the DELETE.\n    IF is_critical AND is_active THEN\n        RAISE EXCEPTION 'Cannot delete equipment %: It is a critical asset and currently active.', OLD.\"EQUIP_CODE\";\n    END IF;\n    RETURN OLD;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Step 5: Create the trigger and attach it to the Equipment table.\nCREATE TRIGGER trigger_prevent_delete_active_critical\nBEFORE DELETE ON \"Equipment\"\nFOR EACH ROW EXECUTE FUNCTION prevent_delete_active_critical_equipment();\n"], "external_knowledge": [11], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n        setup_queries = [\n            \"\"\"\n            INSERT INTO \"EquipmentType\" (\"EquipType\", type_indices)\n            VALUES ('Safety', jsonb_build_object('safety_idx', 90))\n            ON CONFLICT (\"EquipType\") DO UPDATE\n            SET type_indices = EXCLUDED.type_indices;\n            \"\"\",\n            \"\"\"\n            INSERT INTO \"Equipment\" (\"EQUIP_CODE\", \"EquipType\", \"RELIAB_IDX\")\n            VALUES ('CRIT-EQ-DEL', 'Safety', 88)\n            ON CONFLICT (\"EQUIP_CODE\") DO NOTHING;\n            \"\"\",\n            \"\"\"\n            INSERT INTO \"OperationMaintenance\" (\n                \"equipRef\", \"OPER_status\", \"OPER_hours\", \"maintCycleHrs\"\n            ) VALUES (\n                'CRIT-EQ-DEL', 'Active', 50, 100\n            )\n            ON CONFLICT (\"equipRef\") DO UPDATE\n            SET \"OPER_status\" = 'Active';\n            \"\"\"\n        ]\n\n        for query in setup_queries:\n            execute_queries([query], db_name, conn)\n\n        delete_query = [\"\"\"DELETE FROM \"Equipment\" WHERE \"EQUIP_CODE\" = 'CRIT-EQ-DEL';\"\"\"]\n        _, execution_error, _ = execute_queries(delete_query, db_name, conn)\n\n        assert execution_error, \"Expected deletion to be blocked by trigger, but it succeeded\"\n        print(\"Trigger correctly prevented deletion of active critical equipment.\")\n\n        return 1\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "sports_events_1", "selected_database": "sports_events", "query": "Please show me the average age of all sprint session winners at the time they won. The result should be a single age in years.", "normal_query": "Calculate the average age of all Sprint Winners at the time they won. Show the result as a single value representing the average age in years.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: To calculate the average age of all drivers at the time they won a race.\n-- Advanced Functions: Aggregate Function (AVG), Date/Time Function (AGE), JSONB operators\nSELECT\n    EXTRACT(YEAR FROM AVG(AGE((r.event_schedule->>'date_set')::date, (d.driver_identity->>'birth_date')::date))) AS average_winner_age\nFROM sprint_results sr\nJOIN drivers d ON sr.unitDrive = d.DRV_MAIN\nJOIN races r ON sr.matchRef = r.RAK_ID\nWHERE (sr.sprint_performance->>'ranking_order')::integer = 1;"], "external_knowledge": [15], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "sports_events_2", "selected_database": "sports_events", "query": "Our team is studying how thin air affects car performance at racing venues. Can you pull up a list of all tracks that are located high enough above sea level to create high-altitude circuit? I need to see the track names and their exact elevations, with the highest altitude venues listed first.", "normal_query": "I need to identify all High-Altitude Circuits in our database for aerodynamics research. Please retrieve the circuit name and elevation for all circuits that qualify as High-Altitude Circuits. Sort the results by elevation in descending order to show the highest circuits first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: To retrieve a list of all circuits that are considered high-altitude.\n-- Knowledge Used: \"High-Altitude Circuit\" (ID: 14)\n-- Advanced Functions: Basic SELECT with WHERE clause, JSONB operators\nSELECT\n    -- Step 1: Select the name and elevation of the circuit from the 'location_metadata' JSON.\n    -- Step 2: Filter for circuits where the elevation is greater than 800 meters.\n    location_metadata->>'name' AS circuit_name,\n    (location_metadata->'coordinates'->>'elevation_m')::integer AS elevation\nFROM circuits\nWHERE (location_metadata->'coordinates'->>'elevation_m')::integer > 800\nORDER BY elevation DESC;"], "external_knowledge": [14], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "sports_events_3", "selected_database": "sports_events", "query": "To analyze team performance, can you calculate the rate of constructor reliability for each team? I need to see the team names, their total races started, total races finished, the reliability rate as a percentage, and give them a reliability rank. Only include constructors with significant participation so we get meaningful data, and sort them from most reliable to least reliable.", "normal_query": "I need to analyze team performance by calculating the Constructor Reliability Rate for all constructors in our championship database. Please provide a ranking that shows each constructor's name, total races started, total races finished, their reliability rate as a percentage, and their reliability rank. Only include Constructors with Significant Participation to ensure statistical validity. Sort the results by reliability rate from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ConstructorRaceStats AS (SELECT cr.unitNode, c.NameLabel, COUNT(cr.matchRef) AS total_races_started, SUM(CASE WHEN cr.ST_mark is NULL THEN 1 ELSE 0 END) AS total_races_finished FROM constructor_results AS cr JOIN constructors AS c ON cr.unitNode = c.CSTR_Key GROUP BY cr.unitNode, c.NameLabel), ConstructorReliability AS (SELECT NameLabel, total_races_started, total_races_finished, (CAST(total_races_finished AS REAL) * 100 / total_races_started) AS reliability_rate FROM ConstructorRaceStats WHERE total_races_started > 10) SELECT NameLabel AS constructor_name, total_races_started, total_races_finished, reliability_rate, RANK() OVER (ORDER BY reliability_rate DESC) AS reliability_rank FROM ConstructorReliability ORDER BY reliability_rate DESC;"], "external_knowledge": [27, 50], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "sports_events_4", "selected_database": "sports_events", "query": "I'm curious about the dominant wins in sprint races, which are characterized by large margins. Just give me the total number of these landslide wins.", "normal_query": "Please count how many Dominant Victory events occurred in sprint races. Just return the total count.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: To count the total number of races that were won by a dominant margin of over 5 seconds.\n-- Advanced Functions: CTE, Window Function (LEAD), COUNT, JSONB operators\nWITH RaceFinishers AS (\n    -- Step 1: Get finish times and positions, extracting data from JSON.\n    SELECT\n        matchRef,\n        (sprint_performance->>'ranking_order')::integer AS OrdMark,\n        (sprint_performance->'timing'->>'duration_ms')::bigint AS milSec,\n        LEAD((sprint_performance->'timing'->>'duration_ms')::bigint, 1) OVER (PARTITION BY matchRef ORDER BY (sprint_performance->>'ranking_order')::integer) as second_place_time\n    FROM sprint_results\n    WHERE (sprint_performance->>'ranking_order')::integer <= 2 AND (sprint_performance->'timing'->>'duration_ms') IS NOT NULL\n)\n-- Step 2: Calculate winning margin and count races where it exceeds 5 seconds.\nSELECT\n    COUNT(*) AS number_of_dominant_victories\nFROM RaceFinishers\nWHERE OrdMark = 1 AND (milSec - second_place_time) / 1000.0 < -5; -- Corrected logic: second place time is larger"], "external_knowledge": [15, 29, 42], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "sports_events_5", "selected_database": "sports_events", "query": "I want to see how McLaren's overall team performance develops over seasons - like watching their report card get updated after each race. Just show me the year, race ID, constructor name, and the cumulative constructor's performance score after each race event, so I can track how their performance score changes as the season progresses.", "normal_query": "Our team need to analyze how McLaren's Constructor's Performance Score (CPS) evolves throughout different seasons. Show the year, race ID, constructor name, and the cumulative CPS score after each race event, so I can track how their performance score changes as the season progresses.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH McLarenRaceData AS (SELECT r.Yr, r.RAK_ID, r.rNUM, c.NameLabel, cr.scoreVal, cr.ST_mark FROM races AS r JOIN constructor_results AS cr ON r.RAK_ID = cr.matchRef JOIN constructors AS c ON cr.unitNode = c.CSTR_Key WHERE LOWER(TRIM(c.NameLabel)) = 'mclaren'), CumulativeStats AS (SELECT Yr, RAK_ID, rNUM, NameLabel, SUM(COALESCE(scoreVal, 0)) OVER (PARTITION BY Yr ORDER BY rNUM) AS cumulative_season_points, COUNT(*) OVER (PARTITION BY Yr ORDER BY rNUM) AS cumulative_starts, SUM(CASE WHEN ST_mark IS NULL THEN 1 ELSE 0 END) OVER (PARTITION BY Yr ORDER BY rNUM) AS cumulative_finishes FROM McLarenRaceData) SELECT Yr AS year, RAK_ID AS race_id, NameLabel AS constructor_name, (cumulative_season_points * (cumulative_finishes * 1.0 / cumulative_starts)) AS cumulative_cps FROM CumulativeStats ORDER BY year, rNUM;"], "external_knowledge": [25, 27, 31], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "sports_events_6", "selected_database": "sports_events", "query": "I'm curious about how Hamilton's value as a driver changes as he gets older and more experienced. Please give me the race IDs and his performance values.", "normal_query": "I need to analyze Lewis Hamilton's Driver Performance Value throughout his career. Can you show the race ID and the calculated DPV value?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: To track the performance value of a specific driver over a season.\n-- Knowledge Used: \"Driver Age\" (ID: 20), \"Driver's Points Per Race (PPR)\" (ID: 26), \"Driver Performance Value\" (ID: 37)\n-- Advanced Functions: CTE, Window Functions, Date/Time functions, JSONB operators\nWITH DriverData AS (\n    -- Step 1: Get driver's age and PPR before each race, extracting dates and names from JSON.\n    SELECT\n        ds.rlink AS race_id,\n        ds.Drive_Link AS driver_id,\n        EXTRACT(YEAR FROM AGE((r.event_schedule->>'date_set')::date, (d.driver_identity->>'birth_date')::date)) as driver_age,\n        (LAG(ds.acc_pt, 1, 0) OVER (PARTITION BY ds.Drive_Link ORDER BY ds.rlink)) /\n            NULLIF((ROW_NUMBER() OVER (PARTITION BY ds.Drive_Link ORDER BY ds.rlink)) - 1, 0) as ppr\n    FROM driver_standings ds\n    JOIN drivers d ON ds.Drive_Link = d.DRV_MAIN\n    JOIN races r ON ds.rlink = r.RAK_ID\n    WHERE LOWER(TRIM(d.driver_identity->>'reference')) = 'hamilton' \n)\n-- Step 2: Calculate DPV for each race.\nSELECT\n    race_id,\n    ppr / NULLIF(driver_age, 0) AS dpv\nFROM DriverData\nWHERE driver_age > 0;"], "external_knowledge": [20, 26, 37], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "sports_events_7", "selected_database": "sports_events", "query": "Can you rank the drivers based on the stability of their average lap time? Please show me each driver's surname and first_name in a JSON format, their average consistency score, and the number of Races Analyzed. Just focus on drivers who have competed in more than five races, and list the most consistent ones at the top.", "normal_query": "Can you rank the drivers based on their Average Lap Time Consistency? Please show me each driver's surname and first_name in a JSON format, their average consistency score, and the number of Races Analyzed. Just focus on drivers who have competed in more than five races, and list the most consistent ones at the top.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["/* Intent: Measure driver consistency using Lap Time Consistency knowledge */\nWITH driver_laps AS (\n    /* Step 1: Get all lap times in seconds */\n    SELECT wheel_unit AS driver_id,\n           msec_val/1000.0 AS lap_time_sec,\n           rc_index AS race_id\n    FROM lap_times\n),\nstats AS (\n    /* Step 2: Calculate mean and standard deviation per driver */\n    SELECT driver_id,\n           race_id,\n           AVG(lap_time_sec) AS avg_lap,\n           STDDEV(lap_time_sec) AS lap_stddev\n    FROM driver_laps\n    GROUP BY driver_id, race_id\n)\n/* Step 3: Rank drivers by consistency */\nSELECT d.driver_identity->>'name' AS driver_name,\n       AVG(s.lap_stddev) AS avg_consistency,\n       COUNT(DISTINCT s.race_id) AS races_analyzed\nFROM stats s\nJOIN drivers d ON s.driver_id = d.drv_main\nGROUP BY d.drv_main, d.driver_identity->>'name'\nHAVING COUNT(DISTINCT s.race_id) > 5\nORDER BY avg_consistency;"], "external_knowledge": [32], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "sports_events_8", "selected_database": "sports_events", "query": "I'm interested in the achievements of veterans. Could you pull up a list of the race year, official race event name, driver's full name, their podium position, and their age at the time of the race. Please show accomplishments by oldest drivers first, and for same-age drivers, show most recent results first.", "normal_query": "Retrieve all instances of a Veteran's Podium. For each occurrence, please provide the race year, the official race event name, the driver's full name, their specific podium position, and their calculated age at the time of the race. The results should be ordered first in descending order by the driver's age at the time of the race, and then in descending order by the race year.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Step 1: Directly query the driver standings, filtering for podium finishes (positions 1, 2, or 3).\n-- Step 2: Join with the races table to get the year of the race and the drivers table to get their birth date.\n-- Step 3: In the WHERE clause, calculate the driver's age at the time of the race by subtracting their birth year from the race year.\n-- Step 4: Filter for instances where this calculated age is 35 or greater.\nSELECT\n    r.yr AS race_year,\n    r.event_schedule->>'event_name' AS race_event,\n    (d.driver_identity->'name'->>'first_name') || ' ' || (d.driver_identity->'name'->>'surname') AS driver_name,\n    ds.px AS podium_position,\n    (r.yr - EXTRACT(YEAR FROM (d.driver_identity->>'birth_date')::date)) AS age_at_race\nFROM\n    driver_standings ds\nJOIN\n    drivers d ON ds.drive_link = d.drv_main\nJOIN\n    races r ON ds.rlink = r.rak_id\nWHERE\n    ds.px IN (1, 2, 3)\nAND\n    (r.yr - EXTRACT(YEAR FROM (d.driver_identity->>'birth_date')::date)) >= 35\nORDER BY\n    age_at_race DESC, race_year DESC;"], "external_knowledge": [49], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "sports_events_9", "selected_database": "sports_events", "query": "I need to generate a list that ranks the drivers' overall performance in a Sprint session. The output should include the event name, the driver's ID, and their performance index score. Please make sure the best performances are right at the top.", "normal_query": "I need to generate a report that calculates the Sprint Performance Index for every completed driver result in a sprint session. The output should include the event name, the driver's reference code, and the calculated Sprint Performance Index. Sort the results in descending order based on the index to feature the highest-scoring performances first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n    r.event_schedule->>'event_name' as event,\n    d.driver_identity->>'reference' AS driver,\n    (9 - (sr.sprint_performance->>'final_position')::integer) + (sr.sprint_performance->>'points')::integer AS sprint_performance_index\nFROM sprint_results sr\nJOIN races r ON sr.matchref = r.rak_id\nJOIN drivers d ON sr.unitdrive = d.drv_main\nWHERE sr.sprint_performance->>'final_position' IS NOT NULL -- Ensure the driver finished\nORDER BY sprint_performance_index DESC;"], "external_knowledge": [36], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "sports_events_10", "selected_database": "sports_events", "query": "For each qualifying session, calculate the average percentage of qualifying specialists and tell me the average of those percentages across all sessions, rounded to two decimal places.", "normal_query": "For each qualifying session, calculate the average percentage of drivers who meet the Qualifying Specialist criteria: specifically and output the average of those percentages across all sessions, rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Calculate a composite score indicating qualifying specialist behavior\n-- Step 1: Find pole position times using window functions and time conversion\nWITH pole_times AS (\n    SELECT \n        rbind,\n        MIN(\n            CASE \n                WHEN q3_r IS NOT NULL AND q3_r != '' THEN \n                    EXTRACT(EPOCH FROM q3_r::TIME) * 1000\n                WHEN q2_r IS NOT NULL AND q2_r != '' THEN \n                    EXTRACT(EPOCH FROM q2_r::TIME) * 1000\n                ELSE EXTRACT(EPOCH FROM q1_r::TIME) * 1000\n            END\n        ) AS pole_time_ms\n    FROM qualifying\n    WHERE q1_r IS NOT NULL\n    GROUP BY rbind\n),\n-- Step 2: Calculate qualifying deficits and identify specialists using domain knowledge\nqualifying_analysis AS (\n    SELECT \n        q.pilotrec,\n        q.rbind,\n        CASE \n            WHEN q.q3_r IS NOT NULL AND q.q3_r != '' THEN \n                EXTRACT(EPOCH FROM q.q3_r::TIME) * 1000\n            WHEN q.q2_r IS NOT NULL AND q.q2_r != '' THEN \n                EXTRACT(EPOCH FROM q.q2_r::TIME) * 1000\n            ELSE EXTRACT(EPOCH FROM q.q1_r::TIME) * 1000\n        END - pt.pole_time_ms AS time_deficit_ms,\n        CASE WHEN (\n            CASE \n                WHEN q.q3_r IS NOT NULL AND q.q3_r != '' THEN \n                    EXTRACT(EPOCH FROM q.q3_r::TIME) * 1000\n                WHEN q.q2_r IS NOT NULL AND q.q2_r != '' THEN \n                    EXTRACT(EPOCH FROM q.q2_r::TIME) * 1000\n                ELSE EXTRACT(EPOCH FROM q.q1_r::TIME) * 1000\n            END - pt.pole_time_ms\n        ) < 200 THEN 1 ELSE 0 END AS is_close_to_pole\n    FROM qualifying q\n    JOIN pole_times pt ON q.rbind = pt.rbind\n    WHERE q.q1_r IS NOT NULL\n)\n-- Step 3: Calculate specialist score using statistical measures\nSELECT ROUND(\n    AVG(is_close_to_pole) * 100.0, 2\n) AS qualifying_specialist_score\nFROM qualifying_analysis;"], "external_knowledge": [41, 28], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "sports_events_11", "selected_database": "sports_events", "query": "Using historical data, estimate the average probability that a driver achieves hat trick achievements, given that they start from pole position. Use the simplified probability assumptions for calculation, which estimate the chance to win and the chance to set the fastest lap if they're on pole, and the result should be rounded to four decimal places.", "normal_query": "Using historical data, estimate the average probability that a driver achieves a Hat Trick, given that they start from Pole Position. Base your calculation on assumed Pole-Based Race Win Probability and Pole-Based Fastest Lap Probability, and the result should be rounded to four decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Calculate the probability of achieving a hat trick based on historical data\n-- Step 1: Identify pole positions from qualifying results using positional analysis\nWITH pole_positions AS (\n    SELECT \n        rbind,\n        pilotrec,\n        CASE WHEN px_pos = 1 THEN 1 ELSE 0 END as has_pole\n    FROM qualifying\n    WHERE px_pos IS NOT NULL\n),\n-- Step 2: Simulate hat trick components using available data and probability theory\nhat_trick_components AS (\n    SELECT \n        pp.rbind,\n        pp.pilotrec,\n        pp.has_pole,\n        -- Simulate race win probability (simplified)\n        CASE WHEN pp.has_pole = 1 THEN 0.35 ELSE 0.05 END as win_probability,\n        -- Simulate fastest lap probability\n        CASE WHEN pp.has_pole = 1 THEN 0.25 ELSE 0.08 END as fastest_lap_probability\n    FROM pole_positions pp\n)\n-- Step 3: Calculate expected hat trick probability using mathematical probability\nSELECT ROUND(\n    AVG(has_pole * win_probability * fastest_lap_probability) * 100.0, 4\n) AS hat_trick_probability_percentage\nFROM hat_trick_components\nWHERE has_pole = 1;"], "external_knowledge": [18, 10, 16, 13, 51, 52], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 4, "distinct": false, "order": false}}
{"instance_id": "sports_events_12", "selected_database": "sports_events", "query": "Can you calculate how well the top 8 finishers perform on average in sprint sessions? Round the result to two decimal places.", "normal_query": "I need to analyze the average Sprint Performance Index (SPI) across top 8 finishers in sprint sessions. Please round the result to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Calculate average sprint performance index across all sprint participants\n-- Step 1: Extract sprint positions and points from JSON performance data\nWITH sprint_analysis AS (\n    SELECT \n        unitdrive,\n        (sprint_performance->>'final_position')::INTEGER as finish_position,\n        (sprint_performance->>'points')::INTEGER as points_earned,\n        -- Step 2: Apply Sprint Performance Index formula from knowledge base\n        CASE \n            WHEN (sprint_performance->>'final_position')::INTEGER IS NOT NULL \n            THEN (9 - (sprint_performance->>'final_position')::INTEGER) + \n                 COALESCE((sprint_performance->>'points')::INTEGER, 0)\n            ELSE NULL\n        END AS sprint_performance_index\n    FROM sprint_results\n    WHERE sprint_performance->>'final_position' IS NOT NULL\n      AND (sprint_performance->>'final_position')::INTEGER <= 8\n)\n-- Step 3: Calculate average SPI using statistical aggregation\nSELECT ROUND(\n    AVG(sprint_performance_index), 2\n) AS average_sprint_performance_index\nFROM sprint_analysis\nWHERE sprint_performance_index IS NOT NULL;"], "external_knowledge": [36, 9, 15], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "sports_events_13", "selected_database": "sports_events", "query": "Which constructor has the best track record for finishing races? Calculate the reliability rate among all constructors who have participated in at least 5 races, which shows the races finished out of races started, and return the highest reliability percentage, rounded to two decimal places.", "normal_query": "Which constructor has the best track record for finishing races? Help me find the highest Constructor Reliability Rate among all constructors who have participated in at least 5 races. The result should be rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Find the highest constructor reliability rate in the dataset\n-- Step 1: Count total starts and finishes per constructor using advanced grouping\nWITH constructor_reliability AS (\n    SELECT \n        cr.unitnode,\n        COUNT(*) as total_starts,\n        -- Step 2: Count finishes (assuming non-null scoreval indicates finish)\n        COUNT(*) FILTER (WHERE cr.scoreval IS NOT NULL) as total_finishes,\n        -- Step 3: Calculate reliability rate using the knowledge base formula\n        CASE \n            WHEN COUNT(*) > 0 \n            THEN (COUNT(*) FILTER (WHERE cr.scoreval IS NOT NULL))::DECIMAL / COUNT(*) * 100\n            ELSE 0\n        END as reliability_rate\n    FROM constructor_results cr\n    GROUP BY cr.unitnode\n    HAVING COUNT(*) >= 5  -- Minimum sample size for statistical significance\n)\n-- Step 4: Find maximum reliability rate using advanced ranking\nSELECT ROUND(MAX(reliability_rate), 2) AS highest_constructor_reliability_rate\nFROM constructor_reliability;"], "external_knowledge": [27], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "sports_events_14", "selected_database": "sports_events", "query": "Find drivers with at least 10 laps recorded, and identify the highest stability of a driver's lap times during a race. Give me the top score rounded to two decimals.", "normal_query": "I want to identify the best Lap Time Consistency performance from drivers who have completed at least 10 laps. Please provide me with the highest consistency score rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Find the best lap time consistency score (lowest standard deviation)\n-- Step 1: Calculate individual driver average lap times using window functions\nWITH driver_lap_stats AS (\n    SELECT \n        wheel_unit,\n        AVG(msec_val / 1000.0) as avg_lap_time_seconds,\n        STDDEV(msec_val / 1000.0) as lap_time_stddev,\n        COUNT(*) as lap_count\n    FROM lap_times\n    WHERE msec_val > 0\n    GROUP BY wheel_unit\n    HAVING COUNT(*) >= 10  -- Minimum laps for meaningful consistency measurement\n),\n-- Step 2: Apply Lap Time Consistency formula from knowledge base\nconsistency_scores AS (\n    SELECT \n        wheel_unit,\n        -- Step 3: Calculate consistency score (lower is better, so invert for champion score)\n        CASE \n            WHEN lap_time_stddev > 0 \n            THEN 100.0 / lap_time_stddev  -- Inverted for \"champion\" score\n            ELSE 0\n        END as consistency_champion_score\n    FROM driver_lap_stats\n    WHERE lap_time_stddev IS NOT NULL\n)\n-- Step 4: Find the highest consistency champion score\nSELECT ROUND(MAX(consistency_champion_score), 2) AS lap_consistency_champion_score\nFROM consistency_scores;"], "external_knowledge": [32, 21, 23], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "sports_events_15", "selected_database": "sports_events", "query": "What's the absolute fastest lap time ever recorded in our database, measured in seconds? Make sure to ignore any zero or negative times.", "normal_query": "What is the fastest single Lap Time in Seconds recorded in the database? Exclude any zero or negative lap times.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Identify the fastest single lap time recorded in the database\n-- Step 1: Filter out invalid lap times (zero or negative values)\n-- Step 2: Find minimum lap time using MIN function\nSELECT MIN(msec_val / 1000.0) AS fastest_lap_seconds\nFROM lap_times\nWHERE msec_val > 0;"], "external_knowledge": [21], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "sports_events_16", "selected_database": "sports_events", "query": "To do the performance analysis, please help me calculate the average duration of our pit stops (in seconds), excluding any records where the duration is not a positive value. I want a single output, rounded to three decimal places.", "normal_query": "For our performance analysis, please calculate the Average Pit Stop Duration (in seconds), excluding any records where the duration is not a positive value. The final output should be a single value, rounded to three decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Calculate the average time cars spend in pit stops\n-- Step 1: Convert milliseconds to seconds using division\n-- Step 2: Calculate average using AVG function with NULL filtering\nSELECT ROUND(AVG(ms_count / 1000.0), 3) AS average_pit_stop_seconds\nFROM pit_stops\nWHERE ms_count > 0 AND ms_count IS NOT NULL;"], "external_knowledge": [22], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": false}}
{"instance_id": "sports_events_17", "selected_database": "sports_events", "query": "I need to know if we have any circuits with specific environmental characteristics regarded as 'high-altitude'. Can you just give me a simple 'Yes' or 'No' answer?", "normal_query": "I need to know if we have any circuits that are considered a High-Altitude Circuit. Can you just give me a simple 'Yes' or 'No' answer?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Determine if the database contains any high-altitude circuits (>800m elevation)\n-- Step 1: Extract elevation data from JSON metadata\n-- Step 2: Check if any circuit exceeds 800m threshold using conditional logic\nSELECT CASE WHEN EXISTS(SELECT 1 FROM circuits WHERE (location_metadata->'coordinates'->>'elevation_m')::NUMERIC > 800) \n        THEN 'Yes' \n        ELSE 'No' \n    END AS has_high_altitude_circuits;"], "external_knowledge": [14], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "sports_events_18", "selected_database": "sports_events", "query": "Please analyze every session that constitute a championship race weekend and count the occurrences of unavailable date or time information. What's the session name with the highest total count of indeterminate entries?", "normal_query": "Please analyze every session within the standard Race Weekend Structure and count the occurrences of Indeterminate Event Timings for each session type. What's the session name with the highest total count of indeterminate entries?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH AllIndeterminateSessions AS (SELECT session_data.key AS session_name FROM races AS T1, LATERAL jsonb_each(T1.event_schedule -> 'sessions') AS session_data WHERE session_data.value ->> 'date' IS NULL OR session_data.value ->> 'time' IS NULL UNION ALL SELECT 'Race' AS session_name FROM races WHERE event_schedule ->> 'start_time' IS NULL OR event_schedule ->> 'date_set' IS NULL) SELECT session_name FROM AllIndeterminateSessions GROUP BY session_name ORDER BY COUNT(*) DESC LIMIT 1;"], "external_knowledge": [7, 0], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "sports_events_19", "selected_database": "sports_events", "query": "Calculates the time difference between each driver's qualifying lap and the pole sitter's lap, and then categorize drivers into three groups based on their qualifying performance. Return driver IDs, average deficits (rounded to 3 decimal places), and their qualifying cluster.", "normal_query": "Calculating each driver's Qualifying Time Deficit to Pole, and then categorize drivers into three groups based on Qualifying Performance Cluster. Return driver IDs, average deficits (rounded to 3 decimal places), and their qualifying cluster.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH pole_times AS (\n    SELECT \n        rbind,\n        MIN(EXTRACT(EPOCH FROM \n            COALESCE(NULLIF(q3_r, '')::TIME, NULLIF(q2_r, '')::TIME, q1_r::TIME)\n        ) * 1000) AS pole_ms\n    FROM qualifying\n    GROUP BY rbind\n),\ndriver_deficits AS (\n    SELECT \n        q.pilotrec,\n        ROUND(AVG(\n            EXTRACT(EPOCH FROM \n                COALESCE(NULLIF(q.q3_r, '')::TIME, NULLIF(q.q2_r, '')::TIME, q.q1_r::TIME)\n            ) * 1000 - pt.pole_ms\n        ) / 1000.0, 3) AS avg_deficit_sec\n    FROM qualifying q\n    JOIN pole_times pt ON q.rbind = pt.rbind\n    GROUP BY q.pilotrec\n)\nSELECT *,\n    CASE \n        WHEN avg_deficit_sec < 0.15 THEN 'Pole Threat'\n        WHEN avg_deficit_sec < 0.4 THEN 'Mid Gap'\n        ELSE 'Backmarker'\n    END AS qualifying_cluster\nFROM driver_deficits;"], "external_knowledge": [28, 53], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": false}}
{"instance_id": "sports_events_20", "selected_database": "sports_events", "query": "Can you calculate the average stops per car for each event, and just show me the total count of races classified as a 'Single-Stop Race' based on the pit strategy classification criteria?", "normal_query": "Can you calculate the Average Stops Per Car for each event and just show me the total count of races classified as a 'Single-Stop Race' based on Pit Strategy Cluster?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH stop_counts AS (\n    SELECT \n        matchidx,\n        COUNT(*) * 1.0 / COUNT(DISTINCT wunit) AS avg_stops_per_car\n    FROM pit_stops\n    GROUP BY matchidx\n),\nstrategy_classification AS (\n    SELECT \n        matchidx,\n        CASE \n            WHEN avg_stops_per_car < 1.5 THEN 'Single-Stop Race'\n            WHEN avg_stops_per_car < 2.5 THEN 'Standard Two-Stop'\n            ELSE 'High-Strategy Event'\n        END AS pit_strategy_cluster\n    FROM stop_counts\n)\nSELECT \n    COUNT(*) AS single_stop_race_count\nFROM strategy_classification\nWHERE LOWER(TRIM(pit_strategy_cluster)) = 'single-stop race';"], "external_knowledge": [54, 55], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "sports_events_M_1", "selected_database": "sports_events", "query": "Please add a boolean column to the table that records pit stops to identify whether the pit stops are efficient. The new column should be named 'is_efficient' and contain TRUE for efficient stops and FALSE otherwise. Besides, the value should remain NULL if the millisecond count is NULL.", "normal_query": "Please add a boolean column to the pit_stops table based on the Efficient Pit Stop criteria for our analysis. The new column should be named 'is_efficient' and contain TRUE for efficient stops and FALSE otherwise. Besides, the value should remain NULL if the millisecond count is NULL.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- SQL Intent: Add a new column 'is_efficient' to the pit_stops table and populate it based on whether the pit stop duration was under 2.5 seconds.\n-- Knowledge Used: #17 Efficient Pit Stop, #22 Pit Stop Duration in Seconds\n-- Advanced Functions/Operations: ALTER TABLE, UPDATE, CAST\n\n-- Step 1: Add a boolean column named 'is_efficient' to the 'pit_stops' table. \nALTER TABLE pit_stops ADD COLUMN is_efficient BOOLEAN;\n-- Step 2: Update all rows in the 'pit_stops' table.\nUPDATE pit_stops\nSET is_efficient = (ms_count < 2500);"], "external_knowledge": [17, 22], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # First, verify that the is_efficient column was added to the pit_stops table\n    column_check_sql = \"\"\"\n    SELECT column_name \n    FROM information_schema.columns \n    WHERE table_schema = 'public' \n    AND table_name = 'pit_stops' \n    AND column_name = 'is_efficient';\n    \"\"\"\n    \n    column_result, _, _ = execute_queries(column_check_sql, db_name, conn)\n    assert len(column_result) == 1, \"The 'is_efficient' column was not added to the pit_stops table\"\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Verify that efficient pit stops (< 2500ms) are marked as TRUE\n    efficient_check_sql = \"\"\"\n    SELECT COUNT(*) \n    FROM pit_stops \n    WHERE ms_count < 2500 AND is_efficient = TRUE;\n    \"\"\"\n    \n    efficient_result, _, _ = execute_queries(efficient_check_sql, db_name, conn)\n    \n    # Count total pit stops under 2500ms\n    total_efficient_sql = \"\"\"\n    SELECT COUNT(*) \n    FROM pit_stops \n    WHERE ms_count < 2500;\n    \"\"\"\n    \n    total_efficient_result, _, _ = execute_queries(total_efficient_sql, db_name, conn)\n    \n    assert efficient_result[0][0] == total_efficient_result[0][0], (\n        f\"Efficient pit stops not correctly marked: {efficient_result[0][0]} marked as efficient \"\n        f\"but {total_efficient_result[0][0]} pit stops are actually under 2500ms\"\n    )\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Verify that non-efficient pit stops (>= 2500ms) are marked as FALSE\n    non_efficient_check_sql = \"\"\"\n    SELECT COUNT(*) \n    FROM pit_stops \n    WHERE ms_count >= 2500 AND is_efficient = FALSE;\n    \"\"\"\n    \n    non_efficient_result, _, _ = execute_queries(non_efficient_check_sql, db_name, conn)\n    \n    # Count total pit stops 2500ms or more\n    total_non_efficient_sql = \"\"\"\n    SELECT COUNT(*) \n    FROM pit_stops \n    WHERE ms_count >= 2500;\n    \"\"\"\n    \n    total_non_efficient_result, _, _ = execute_queries(total_non_efficient_sql, db_name, conn)\n    \n    assert non_efficient_result[0][0] == total_non_efficient_result[0][0], (\n        f\"Non-efficient pit stops not correctly marked: {non_efficient_result[0][0]} marked as non-efficient \"\n        f\"but {total_non_efficient_result[0][0]} pit stops are actually 2500ms or more\"\n    )\n"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "sports_events_M_2", "selected_database": "sports_events", "query": "Can you create a function named get_driver_age that takes the driver information (JSONB) as input and calculates their current driver age?", "normal_query": "Can you create a function named get_driver_age that takes a driver_identity JSONB parameter as input, extracts the birth_date from it, and returns the driver's current age in years as an INTEGER?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- SQL Intent: Create a function that computes the current age of a driver based on their birth date.\n-- Knowledge Used: #20 Driver Age\n-- Advanced Functions/Operations: CREATE OR REPLACE FUNCTION, JSONB Extraction (->>), AGE(), EXTRACT()\nCREATE OR REPLACE FUNCTION get_driver_age(driver_identity JSONB)\nRETURNS INTEGER AS $$\nBEGIN\n    RETURN EXTRACT(YEAR FROM AGE(NOW(), (driver_identity->>'birth_date')::DATE));\nEND;\n$$ LANGUAGE plpgsql;"], "external_knowledge": [20], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # First, verify that the get_driver_age function was created\n    function_check_sql = \"\"\"\n    SELECT COUNT(*) \n    FROM information_schema.routines \n    WHERE routine_schema = 'public' \n    AND routine_name = 'get_driver_age' \n    AND routine_type = 'FUNCTION';\n    \"\"\"\n    \n    function_result, _, _ = execute_queries(function_check_sql, db_name, conn)\n    assert function_result[0][0] == 1, \"The 'get_driver_age' function was not created\"\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Test the function with a known driver from the sample data\n    # Using the sample driver with birth_date \"1937-11-11\"\n    test_function_sql = \"\"\"SELECT get_driver_age('{\"code\": null, \"name\": {\"surname\": \"Brambilla\", \"first_name\": \"Vittorio\"}, \"info_link\": \"http://en.wikipedia.org/wiki/Vittorio_Brambilla\", \"reference\": \"brambilla\", \"birth_date\": \"1937-11-11\", \"nationality\": \"Italian\", \"racing_number\": null}'::JSONB) AS calculated_age;\"\"\"\n    \n    test_result, _, _ = execute_queries(test_function_sql, db_name, conn)\n    calculated_age = test_result[0][0]\n    \n    # Verify the age calculation is reasonable (should be around 87-88 years as of 2025)\n    assert calculated_age >= 87 and calculated_age <= 88, (\n        f\"Age calculation appears incorrect: got {calculated_age}, expected around 87-88 years\"\n    )\n    "], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "sports_events_M_3", "selected_database": "sports_events", "query": "Create a high_altitude_circuits view that shows all circuits that can be classified as high-altitude. I want to see their circuit ID, name, and elevation.", "normal_query": "Create a view called high_altitude_circuits showing all High-Altitude Circuit entries. Include the circuit key, name, and elevation from the circuits table.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- SQL Intent: Create a view to show all circuits that are classified as \"High-Altitude\".\n-- Knowledge Used: #14 High-Altitude Circuit\n-- Advanced Functions/Operations: CREATE VIEW, JSONB path operator\n\n-- Step 1: Define a new view named 'high_altitude_circuits'.\n-- Step 2: Select key information from the 'circuits' table.\n-- Step 3: Filter the circuits where the elevation, extracted from the 'location_metadata' JSONB field, is greater than 800 meters. [cite: 34]\nCREATE OR REPLACE VIEW high_altitude_circuits AS\nSELECT\n    cctkey,\n    location_metadata->>'name' AS circuit_name,\n    (location_metadata->'coordinates'->>'elevation_m')::INTEGER AS elevation\nFROM\n    circuits\nWHERE\n    (location_metadata->'coordinates'->>'elevation_m')::INTEGER > 800;"], "external_knowledge": [14], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # First, verify that the high_altitude_circuits view was created\n    view_check_sql = \"\"\"\n    SELECT COUNT(*) \n    FROM information_schema.views \n    WHERE table_schema = 'public' \n    AND table_name = 'high_altitude_circuits';\n    \"\"\"\n    view_result, _, _ = execute_queries(view_check_sql, db_name, conn)\n    assert view_result[0][0] == 1, \"The 'high_altitude_circuits' view was not created\"\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Test that the view only contains circuits with elevation > 800\n    elevation_check_sql = \"SELECT COUNT(*) FROM high_altitude_circuits WHERE elevation <= 800;\"\n    elevation_result, _, _ = execute_queries(elevation_check_sql, db_name, conn)\n    assert elevation_result[0][0] == 0, (\n        f\"Found {elevation_result[0][0]} circuits with elevation <= 800 in the high_altitude_circuits view\"\n    )\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Cross-check with original table to ensure filtering is correct\n    cross_check_sql = \"\"\"\n    SELECT COUNT(*) FROM circuits \n    WHERE (location_metadata->'coordinates'->>'elevation_m')::INTEGER > 800\n    AND location_metadata->'coordinates'->>'elevation_m' IS NOT NULL;\n    \"\"\"\n    cross_check_result, _, _ = execute_queries(cross_check_sql, db_name, conn)\n    view_count_sql = \"SELECT COUNT(*) FROM high_altitude_circuits;\"\n    view_count_result, _, _ = execute_queries(view_count_sql, db_name, conn)\n    # The counts should match (accounting for NULL elevation values)\n    assert view_count_result[0][0] == cross_check_result[0][0], (\n        f\"View contains more circuits ({view_count_result[0][0]}) than expected from source table ({cross_check_result[0][0]})\"\n    )\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    view_count_sql = \"SELECT * FROM high_altitude_circuits;\"\n    view_count_result, _, _ = execute_queries(view_count_sql, db_name, conn)\n    l = len(view_count_result[0])\n    assert l == 3, (f\"View contains {l} columns\")\n    "], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "sports_events_M_4", "selected_database": "sports_events", "query": "Update the race records with a victory type marker using the sprint results timing data, which set victory_type to 'Dominant Victory' if the cretiria is satisfied.", "normal_query": "Update the races table to flag Dominant Victory events in the event_schedule JSONB field (set victory_type to 'Dominant Victory').", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- SQL Intent: Update the 'event_schedule' JSONB data in the 'races' table to add a 'victory_type' flag for \"Dominant Victory\".\n-- Knowledge Used: #42 Dominant Victory, #29 Race Time Delta to Winner\n-- Advanced Functions/Operations: UPDATE FROM, CTE, Window Function (LEAD), JSONB concatenation\nUPDATE races r\nSET event_schedule = event_schedule || '{\"victory_type\": \"Dominant Victory\"}'\nFROM (\n    WITH RaceTimeDeltas AS (\n        SELECT\n            matchref,\n            (sprint_performance->'timing'->>'duration_ms')::BIGINT AS finish_time_ms,\n            LEAD((sprint_performance->'timing'->>'duration_ms')::BIGINT, 1) OVER(PARTITION BY matchref ORDER BY (sprint_performance->>'ranking_order')::INT) AS next_driver_finish_time_ms\n        FROM sprint_results\n        WHERE (sprint_performance->>'ranking_order')::INT <= 2\n    )\n    SELECT\n        matchref,\n        (next_driver_finish_time_ms - finish_time_ms) / 1000.0 AS delta_to_second\n    FROM RaceTimeDeltas\n    WHERE finish_time_ms IS NOT NULL AND next_driver_finish_time_ms IS NOT NULL\n) AS dominant_wins\nWHERE r.rak_id = dominant_wins.matchref AND dominant_wins.delta_to_second > 5;"], "external_knowledge": [42, 29], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Verification query to check if races with >5 second gaps have been correctly flagged\n    verification_sql = \"\"\"\n    WITH RaceTimeDeltas AS (\n        SELECT\n            matchref,\n            (sprint_performance->'timing'->>'duration_ms')::BIGINT AS finish_time_ms,\n            LEAD((sprint_performance->'timing'->>'duration_ms')::BIGINT, 1) \n                OVER(PARTITION BY matchref ORDER BY (sprint_performance->>'ranking_order')::INT) AS next_driver_finish_time_ms\n        FROM sprint_results\n        WHERE (sprint_performance->>'ranking_order')::INT <= 2\n    ),\n    DominantRaces AS (\n        SELECT\n            matchref,\n            (next_driver_finish_time_ms - finish_time_ms) / 1000.0 AS delta_to_second\n        FROM RaceTimeDeltas\n        WHERE finish_time_ms IS NOT NULL \n          AND next_driver_finish_time_ms IS NOT NULL\n          AND (next_driver_finish_time_ms - finish_time_ms) / 1000.0 > 5\n    )\n    SELECT COUNT(*) as expected_updates\n    FROM DominantRaces;\n    \"\"\"\n    \n    expected_result, _, _ = execute_queries(verification_sql, db_name, conn)\n    expected_count = expected_result[0][0]\n    \n    # Query to count how many races actually got the victory_type flag\n    actual_count_sql = \"\"\"\n    SELECT COUNT(*) FROM races \n    WHERE event_schedule->>'victory_type' = 'Dominant Victory';\n    \"\"\"\n    \n    actual_result, _, _ = execute_queries(actual_count_sql, db_name, conn)\n    actual_count = actual_result[0][0]\n    \n    # Assert that the number of updated races matches the expected number\n    assert actual_count == expected_count, (\n        f\"Expected {expected_count} races to be flagged as Dominant Victory, \"\n        f\"but {actual_count} races were actually updated.\"\n    )\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Additional verification: ensure that all flagged races actually have >5 second gaps\n    validation_sql = \"\"\"\n    WITH RaceTimeDeltas AS (\n        SELECT\n            matchref,\n            (sprint_performance->'timing'->>'duration_ms')::BIGINT AS finish_time_ms,\n            LEAD((sprint_performance->'timing'->>'duration_ms')::BIGINT, 1) \n                OVER(PARTITION BY matchref ORDER BY (sprint_performance->>'ranking_order')::INT) AS next_driver_finish_time_ms\n        FROM sprint_results\n        WHERE (sprint_performance->>'ranking_order')::INT <= 2\n    ),\n    FlaggedRaces AS (\n        SELECT r.rak_id\n        FROM races r\n        WHERE r.event_schedule->>'victory_type' = 'Dominant Victory'\n    )\n    SELECT COUNT(*)\n    FROM FlaggedRaces fr\n    JOIN RaceTimeDeltas rtd ON fr.rak_id = rtd.matchref\n    WHERE rtd.finish_time_ms IS NOT NULL \n      AND rtd.next_driver_finish_time_ms IS NOT NULL\n      AND (rtd.next_driver_finish_time_ms - rtd.finish_time_ms) / 1000.0 <= 5;\n    \"\"\"\n    \n    validation_result, _, _ = execute_queries(validation_sql, db_name, conn)\n    incorrectly_flagged = validation_result[0][0]\n    \n    # Assert that no races were incorrectly flagged (should be 0)\n    assert incorrectly_flagged == 0, (\n        f\"Found {incorrectly_flagged} races incorrectly flagged as Dominant Victory \"\n        f\"that do not meet the >5 second criteria.\"\n    )\n    "], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "sports_events_M_5", "selected_database": "sports_events", "query": "Create a stored procedure named award_hat_trick that takes driver ID and race ID as parameters to verify and record the three key achievements in a single race weekend. If all three conditions are met, insert a record with achievement type being 'Hat Trick' into the table that records all achievements.", "normal_query": "Create a stored procedure named award_hat_trick that takes driver_id and race_id as parameters to verify and record if a specified driver accomplished a 'Hat Trick'. If all three conditions are met, insert a Hat Trick record (achievement_type should be 'Hat Trick') into the achievements table.", "preprocess_sql": ["-- Pre-process SQL to create achievements table\nCREATE TABLE IF NOT EXISTS achievements (\n    achievement_id SERIAL PRIMARY KEY,\n    driver_id INTEGER NOT NULL,\n    race_id INTEGER NOT NULL,\n    achievement_type VARCHAR(50) NOT NULL,\n    recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (driver_id) REFERENCES drivers(drv_main),\n    FOREIGN KEY (race_id) REFERENCES races(rak_id),\n    CONSTRAINT unique_achievement UNIQUE (driver_id, race_id, achievement_type)\n);"], "clean_up_sqls": [], "sol_sql": ["-- SQL Intent: Define a procedure to verify and record a 'Hat Trick' achievement for a driver in a specific race.\n-- Knowledge Used: #18 Hat Trick, #10 Pole Position, #13 Fastest Lap Award, #16 Race Winner\n-- Advanced Functions/Operations: CREATE PROCEDURE, PL/pgSQL, EXISTS logic\n\n-- Assuming the 'achievements' table from query #8 exists.\nCREATE OR REPLACE PROCEDURE award_hat_trick(p_driver_id INTEGER, p_race_id INTEGER)\nLANGUAGE plpgsql\nAS $$\nDECLARE\n    is_pole BOOLEAN;\n    is_winner BOOLEAN;\n    has_fastest_lap BOOLEAN;\nBEGIN\n    -- Step 1: Check for Pole Position. Join qualifying and races to find if the driver was in position 1 for the given race.\n    SELECT EXISTS (\n        SELECT 1 FROM qualifying \n        WHERE pilotrec = p_driver_id AND rbind = p_race_id AND px_pos = 1\n    ) INTO is_pole;\n\n    -- Step 2: Check for Race Winner. Find if the driver finished in position 1 in the driver standings for the race.\n    SELECT EXISTS (\n        SELECT 1 FROM driver_standings \n        WHERE drive_link = p_driver_id AND rlink = p_race_id AND px = 1\n    ) INTO is_winner;\n\n    -- Step 3: Check for Fastest Lap. Find if the driver has the rank 1 lap time for the given race.\n    -- This assumes a ranking of lap times is available, e.g., in a results table's JSONB or by finding the minimum lap time.\n    -- Here we query lap_times for the fastest lap (pp=1, assuming 'pp' is position).\n    SELECT EXISTS (\n      SELECT 1 FROM lap_times \n      WHERE wheel_unit = p_driver_id AND rc_index = p_race_id AND pp = 1\n    ) INTO has_fastest_lap;\n\n    -- Step 4: If all three conditions are true, insert the achievement. \n    IF is_pole AND is_winner AND has_fastest_lap THEN\n        INSERT INTO achievements (driver_id, race_id, achievement_type)\n        VALUES (p_driver_id, p_race_id, 'Hat Trick');\n    END IF;\nEND;\n$$;\n"], "external_knowledge": [18, 10, 13, 16], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Verify the procedure exists\n    procedure_check_sql = \"\"\"\n    SELECT COUNT(*) FROM information_schema.routines \n    WHERE routine_name = 'award_hat_trick' AND routine_type = 'PROCEDURE';\n    \"\"\"\n    proc_result, _, _ = execute_queries(procedure_check_sql, db_name, conn)\n    assert proc_result[0][0] == 1, \"Procedure 'award_hat_trick' was not created\"\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Test case 1: Driver who achieves Hat Trick   \n    # Find a driver who has pole position, race win, and fastest lap in the same race\n    hat_trick_candidate_sql = \"\"\"\n    WITH pole_positions AS (\n        SELECT pilotrec as driver_id, rbind as race_id\n        FROM qualifying \n        WHERE px_pos = 1\n    ),\n    race_winners AS (\n        SELECT drive_link as driver_id, rlink as race_id\n        FROM driver_standings \n        WHERE px = 1\n    ),\n    fastest_laps AS (\n        SELECT wheel_unit as driver_id, rc_index as race_id\n        FROM lap_times \n        WHERE pp = 1\n    )\n    SELECT pp.driver_id, pp.race_id\n    FROM pole_positions pp\n    JOIN race_winners rw ON pp.driver_id = rw.driver_id AND pp.race_id = rw.race_id\n    JOIN fastest_laps fl ON pp.driver_id = fl.driver_id AND pp.race_id = fl.race_id\n    LIMIT 1;\n    \"\"\"\n    candidate_result, _, _ = execute_queries(hat_trick_candidate_sql, db_name, conn)\n    if candidate_result:\n        # Test with a valid Hat Trick candidate\n        test_driver_id, test_race_id = candidate_result[0]\n        # Call the procedure\n        call_procedure_sql = f\"CALL award_hat_trick({test_driver_id}, {test_race_id});\"\n        execute_queries(call_procedure_sql, db_name, conn)\n        # Verify the achievement was recorded\n        verify_achievement_sql = f\"\"\"\n        SELECT COUNT(*) \n        FROM achievements \n        WHERE driver_id = {test_driver_id} \n          AND race_id = {test_race_id} \n          AND achievement_type = 'Hat Trick';\n        \"\"\"\n        achievement_result, _, _ = execute_queries(verify_achievement_sql, db_name, conn)\n        assert achievement_result[0][0] == 1, f\"Hat Trick achievement not recorded for driver {test_driver_id} in race {test_race_id}\"\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Test case 2: Driver who does NOT achieve Hat Trick (missing one condition)\n    # Find a driver with pole position but not race winner\n    non_hat_trick_sql = \"\"\"\n    WITH pole_positions AS (\n        SELECT pilotrec as driver_id, rbind as race_id\n        FROM qualifying \n        WHERE px_pos = 1\n    ),\n    race_winners AS (\n        SELECT drive_link as driver_id, rlink as race_id\n        FROM driver_standings \n        WHERE px = 1\n    )\n    SELECT pp.driver_id, pp.race_id\n    FROM pole_positions pp\n    LEFT JOIN race_winners rw ON pp.driver_id = rw.driver_id AND pp.race_id = rw.race_id\n    WHERE rw.driver_id IS NULL\n    LIMIT 1;\n    \"\"\"\n    non_candidate_result, _, _ = execute_queries(non_hat_trick_sql, db_name, conn)\n    if non_candidate_result:\n        test_driver_id2 = non_candidate_result[0][0]\n        test_race_id2 = non_candidate_result[0][1]\n        # Record count before calling procedure\n        count_before_sql = \"SELECT COUNT(*) FROM achievements WHERE achievement_type = 'Hat Trick';\"\n        before_result, _, _ = execute_queries(count_before_sql, db_name, conn)\n        count_before = before_result[0][0]\n        # Call the procedure\n        call_procedure_sql2 = f\"CALL award_hat_trick({test_driver_id2}, {test_race_id2});\"\n        execute_queries(call_procedure_sql2, db_name, conn)\n        # Verify no new achievement was recorded\n        count_after_sql = \"SELECT COUNT(*) FROM achievements WHERE achievement_type = 'Hat Trick';\"\n        after_result, _, _ = execute_queries(count_after_sql, db_name, conn)\n        count_after = after_result[0][0]\n        assert count_after == count_before, f\"Hat Trick achievement incorrectly recorded for driver {test_driver_id2} who did not meet all criteria\"\n    "], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "sports_events_M_6", "selected_database": "sports_events", "query": "Build a view called podium_finishes showing all podium finishes in season standings. Display the driver's last name, race year, and their finishing position.", "normal_query": "Create a view named podium_finishes that displays all Podium Finish achievements in season standings. Show the driver surname (from driver_identity JSONB), race year, and final position.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- SQL Intent: Create a read-only view named 'podium_finishes' that shows all drivers who achieved a 1st, 2nd, or 3rd place finish in a race.\n-- Knowledge Used: #11 Podium Finish\n-- Advanced Functions/Operations: CREATE VIEW, JOIN, JSONB Extraction\nCREATE OR REPLACE VIEW podium_finishes AS\nSELECT\n    d.driver_identity->'name'->>'surname' AS driver_surname,\n    r.yr AS race_year,\n    ds.px AS final_position\nFROM driver_standings ds\nJOIN drivers d ON ds.drive_link = d.drv_main\nJOIN races r ON ds.rlink = r.rak_id\nWHERE ds.px <= 3;\n"], "external_knowledge": [11], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Verify the view exists\n    view_check_sql = \"\"\"\n    SELECT COUNT(*) \n    FROM information_schema.views \n    WHERE table_name = 'podium_finishes';\n    \"\"\"\n    view_result, _, _ = execute_queries(view_check_sql, db_name, conn)\n    assert view_result[0][0] == 1, \"View 'podium_finishes' was not created\"\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Test: Verify that the view only contains positions 1, 2, and 3\n    position_check_sql = \"\"\"\n    SELECT DISTINCT final_position \n    FROM podium_finishes \n    ORDER BY final_position;\n    \"\"\"\n    position_result, _, _ = execute_queries(position_check_sql, db_name, conn)\n    valid_positions = {1, 2, 3}\n    actual_positions = {row[0] for row in position_result}\n    assert actual_positions.issubset(valid_positions), f\"View contains invalid positions: {actual_positions - valid_positions}\"\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Test: Verify that all records in the view match the manual query\n    manual_query_sql = \"\"\"\n    SELECT\n        d.driver_identity->'name'->>'surname' AS driver_surname,\n        r.yr AS race_year,\n        ds.px AS final_position\n    FROM driver_standings ds\n    JOIN drivers d ON ds.drive_link = d.drv_main\n    JOIN races r ON ds.rlink = r.rak_id\n    WHERE ds.px <= 3\n    ORDER BY race_year, final_position, driver_surname;\n    \"\"\"\n    manual_result, _, _ = execute_queries(manual_query_sql, db_name, conn)\n\n    # Query the view with the same ordering\n    view_query_sql = \"\"\"\n    SELECT driver_surname, race_year, final_position\n    FROM podium_finishes\n    ORDER BY race_year, final_position, driver_surname;\n    \"\"\"\n    view_query_result, _, _ = execute_queries(view_query_sql, db_name, conn)\n    # Compare the results\n    assert len(manual_result) == len(view_query_result), f\"Manual query returned {len(manual_result)} rows, view returned {len(view_query_result)} rows\"\n    for i, (manual_row, view_row) in enumerate(zip(manual_result, view_query_result)):\n        assert manual_row == view_row, f\"Row {i} differs: manual={manual_row}, view={view_row}\"\n    "], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "sports_events_M_8", "selected_database": "sports_events", "query": "Please add a new true/false column called is_pole_position to the qualifying table with a default value of FALSE, then mark it as TRUE for whoever got the pole position.", "normal_query": "Add a new boolean column named is_pole_position to the qualifying table with a default value of FALSE. Then update this column to TRUE for all records having achieved Pole Position.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- SQL Intent: Add a new column 'is_pole_position' to the qualifying table and set it to true for the driver who achieved the fastest time.\n-- Knowledge Used: #10 Pole Position\n-- Advanced Functions/Operations: ALTER TABLE, UPDATE\n\n-- Step 1: Add a boolean column named 'is_pole_position' to the 'qualifying' table, with a default value of false.\nALTER TABLE qualifying ADD COLUMN is_pole_position BOOLEAN DEFAULT FALSE;\n\n-- Step 2: Update the 'qualifying' table. Set the new 'is_pole_position' column to TRUE only for the record corresponding to the driver who finished in position 1 ('px_pos' = 1).\n-- This indicates they achieved Pole Position by setting the fastest time.\nUPDATE qualifying\nSET is_pole_position = TRUE\nWHERE px_pos = 1;\n"], "external_knowledge": [10], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n# Verify column was added with correct properties\n    verify_column_sql = \"\"\"\n    SELECT column_name, data_type, column_default \n    FROM information_schema.columns \n    WHERE table_schema = 'public' \n    AND table_name = 'qualifying' \n    AND column_name = 'is_pole_position';\n    \"\"\"\n    column_info, _, _ = execute_queries(verify_column_sql, db_name, conn)\n    assert len(column_info) == 1, \"Column was not added\"\n    assert column_info[0][1] == 'boolean', \"Column has wrong data type\"\n    assert column_info[0][2] == 'false', \"Column has wrong default value\"\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Verify only position 1 qualifiers are marked as pole\n    check_pole_positions_sql = \"\"\"\n    SELECT COUNT(*) FROM qualifying\n    WHERE px_pos = 1 AND is_pole_position = FALSE;\n    \"\"\"\n    missing_poles, _, _ = execute_queries(check_pole_positions_sql, db_name, conn)\n    assert missing_poles[0][0] == 0, \"Some position 1 qualifiers not marked as pole\"\n    \n    check_non_pole_positions_sql = \"\"\"\n    SELECT COUNT(*) FROM qualifying\n    WHERE px_pos != 1 AND is_pole_position = TRUE;\n    \"\"\"\n    incorrect_poles, _, _ = execute_queries(check_non_pole_positions_sql, db_name, conn)\n    assert incorrect_poles[0][0] == 0, \"Some non-position 1 qualifiers incorrectly marked as pole\"\n    "], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "sports_events_29", "selected_database": "sports_events", "query": "I need to create a custom domain called championship_points based on the REAL data type that only allows zero or positive numbers for championship points.", "normal_query": "I want to create a custom domain named championship_points based on the REAL data type to store Championship Points System (Race) values. The domain should include a CHECK constraint to ensure all values are non-negative.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- SQL Intent: Create a custom domain 'championship_points' that can be used for columns storing points, ensuring no negative point values can ever be inserted.\n-- Knowledge Used: #8 Championship Points System (Race)\n-- Advanced Functions/Operations: CREATE DOMAIN, CHECK constraint\nCREATE DOMAIN championship_points AS REAL\nCHECK (VALUE >= 0);\n"], "external_knowledge": [8], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Verify domain was created with correct properties\n    verify_domain_sql = \"\"\"\n    SELECT domain_name, data_type, domain_default \n    FROM information_schema.domains \n    WHERE domain_schema = 'public' \n    AND domain_name = 'championship_points';\n    \"\"\"\n    domain_info, _, _ = execute_queries(verify_domain_sql, db_name, conn)\n    assert len(domain_info) == 1, \"Domain was not created\"\n    assert domain_info[0][1] == 'real', \"Domain has wrong base data type\"\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Test domain constraint by attempting to use it\n    test_table_sql = \"CREATE TABLE test_points (test_value championship_points);\"\n    execute_queries(test_table_sql, db_name, conn)\n    \n    # Verify rejects negative values\n    try:\n        insert_negative_sql = \"INSERT INTO test_points VALUES (-1);\"\n        _, error, _ = execute_queries(insert_negative_sql, db_name, conn)\n        assert error is not None, \"Domain accepted negative value\"\n    finally:\n        pass\n    \n    # Verify accepts positive values\n    test_table_sql = \"INSERT INTO test_points VALUES (25), (0), (18.5);\"\n    result, _, _ = execute_queries(test_table_sql + \"SELECT COUNT(*) FROM test_points;\", db_name, conn)\n    assert result[0][0] == 3, \"Domain rejected valid point values\"\n    execute_queries(\"DROP TABLE test_points;\", db_name, conn)\n    "], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "sports_events_30", "selected_database": "sports_events", "query": "I need to clean up our constructor database by handling missing nationality information. Can you find all the teams where we don't know what country they're from and mark those entries as 'Not Recorded' instead of leaving them empty?.", "normal_query": "I need to clean up our constructor database by handling missing nationality information. For all constructors where the nationality field shows Data Unavailability, please update these records to explicitly indicate 'Not Recorded' instead of leaving them blank.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- SQL Intent: Update the 'constructors' table to set the 'naty' field to 'Not Recorded' for any constructor where the nationality is currently unknown (NULL).\n-- Knowledge Used: #6 Data Unavailability for Constructor Nationality\n-- Advanced Functions/Operations: UPDATE, IS NULL\nUPDATE constructors SET naty = 'Not Recorded' WHERE naty IS NULL;\n"], "external_knowledge": [6], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Verification query 1: Check that no constructors have NULL nationality\n    verification_sql_1 = \"\"\"\n    SELECT COUNT(*)\n    FROM constructors\n    WHERE naty IS NULL;\n    \"\"\"\n    # Execute the verification query\n    result_1, _, _ = execute_queries(verification_sql_1, db_name, conn)\n    # Assert that no constructors have NULL nationality after the update\n    assert result_1[0][0] == 0, (\n        f\"Update failed: {result_1[0][0]} constructors still have NULL nationality. \"\n        f\"All NULL values should have been updated to 'Not Recorded'.\"\n    )\n    ", "def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Verification query 2: Check that constructors with previously NULL nationality now have 'Not Recorded' as their nationality\n    verification_sql_2 = \"\"\"\n    SELECT COUNT(*)\n    FROM constructors\n    WHERE naty = 'Not Recorded';\n    \"\"\"\n    # Execute the second verification query\n    result_2, _, _ = execute_queries(verification_sql_2, db_name, conn)\n    # Assert that at least one constructor has 'Not Recorded' nationality\n    # (assuming there were NULL values before the update)\n    assert result_2[0][0] > 0, (\n        f\"Update may not have worked correctly: {result_2[0][0]} constructors \"\n        f\"have 'Not Recorded' nationality. Expected at least 1 if there were NULL values.\"\n    )\n    "], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_1", "selected_database": "labor_certification_applications", "query": "I'm trying to figure out which visas take the longest to get approved. Can you give me a breakdown of the average wait time for each type of visa? Just show me the ones that actually got certified, and list them from longest to shortest wait time.", "normal_query": "I'm curious about how long it takes for different visa applications to get approved. Could you show me the average Application Processing Time for each of the Visa Classification Types? Please only include applications that were certified, and sort the list to show the visa types that take the longest on top.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT LOWER(TRIM(c.visacls::TEXT)) AS visa_class,\n       AVG(EXTRACT(EPOCH FROM (TO_DATE(c.decisionday, 'YYYY-MM-DD')::timestamp - TO_DATE(c.recvday, 'YYYY/MM/DD')::timestamp)) / 86400) AS average_processing_time_days\nFROM public.cases c\nWHERE c.decisionday IS NOT NULL AND c.recvday IS NOT NULL AND c.statustag ILIKE 'Certified%'\nGROUP BY LOWER(TRIM(c.visacls::TEXT))\nORDER BY average_processing_time_days DESC;"], "external_knowledge": [1, 12], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "labor_certification_applications_2", "selected_database": "labor_certification_applications", "query": "What's the percentage of H-1B applications that are successful?", "normal_query": "I want to know the Approval Rate for H-1B Visa Classification Types. Can you calculate the percentage of H-1B visa applications that end up being certified?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT ( COUNT(*) FILTER ( WHERE LOWER(TRIM(c.statustag::TEXT)) ILIKE 'certified%' AND LOWER(TRIM(c.visacls::TEXT)) = 'h-1b' )::DECIMAL / NULLIF(COUNT(*) FILTER ( WHERE LOWER(TRIM(c.visacls::TEXT)) = 'h-1b' ), 0) * 100 ) AS h1b_approval_rate_percentage FROM public.cases c;"], "external_knowledge": [1, 13], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_3", "selected_database": "labor_certification_applications", "query": "I'm trying to see if bigger companies have an easier time getting visas approved. Can you break down companies based on different employer sizes? Then, for each size, tell me the average application success rate for getting those visas approved. I want to see how many companies are in each size group, and what their average approval rate is, from the highest approval rate to the lowest.", "normal_query": "I'm looking to understand how the size of an employer, based on their application volume, relates to their success in getting visa applications approved. Can you categorize employers into Employer Size Classifications, and then calculate the average Application Success Rate for each of these categories? I'd like to see the number of employers in each size category, and their average success rate, sorted from highest success rate to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH app_per_year AS ( SELECT homefirm AS employer_name, EXTRACT(YEAR FROM TO_DATE(recvday, 'YYYY/MM/DD')) AS year, COUNT(*) AS total_applications, COUNT(*) FILTER (WHERE statustag ILIKE 'Certified%') AS certified_applications FROM public.cases WHERE recvday IS NOT NULL GROUP BY homefirm, EXTRACT(YEAR FROM TO_DATE(recvday, 'YYYY/MM/DD')) ), emp_size_category AS ( SELECT employer_name, year, total_applications, certified_applications, CASE WHEN total_applications < 5 THEN 'Small-scale User' WHEN total_applications BETWEEN 5 AND 25 THEN 'Medium-scale User' ELSE 'Large-scale User' END AS employer_size_category FROM app_per_year ) SELECT employer_size_category, COUNT(DISTINCT employer_name) AS number_of_employers_in_category, AVG((certified_applications::DECIMAL / total_applications) * 100) AS average_success_rate_percentage FROM emp_size_category GROUP BY employer_size_category ORDER BY average_success_rate_percentage DESC;"], "external_knowledge": [18, 46], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "labor_certification_applications_4", "selected_database": "labor_certification_applications", "query": "I'm curious to know which soc code categories are most frequently associated with successful H-1B visa applications. Can you list the top 5 job soc titles that appear most often in certified H-1B visa cases? I want to see which jobs are most commonly approved for H-1B visas.", "normal_query": "I'm interested in identifying the most frequently certified occupations for H-1B visas. Could you provide a list of the top 5 SOC Code Framework that appear most often in certified H-1B visa applications? The output should include the job title and the number of certified H-1B applications for each title, sorted in descending order by the number of applications.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT c.soctitle AS job_soc_title, COUNT(*) AS number_of_certified_h1b_applications FROM public.cases c WHERE LOWER(TRIM(c.visacls::TEXT)) = 'h-1b' AND LOWER(TRIM(c.statustag::TEXT)) ILIKE 'certified%' GROUP BY c.soctitle ORDER BY number_of_certified_h1b_applications DESC LIMIT 5;"], "external_knowledge": [7], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "labor_certification_applications_5", "selected_database": "labor_certification_applications", "query": "The goal is to calculate the average wage differential ratio for top-level positions that are compensated on an annual basis. When a pay range is provided, the midpoint between the lower and upper amounts should be used. If only a single value is available, that value will be used as the offered amount. This ensures consistency in how compensation is interpreted across all relevant records.", "normal_query": "I want to analyze the Wage Differential Rate specifically for top-tier, high-skill roles that are paid on a yearly basis. For these roles, when a salary range is given, the midpoint between the lower and upper values should be used; if only one value is available, that value should be used directly. The calculation should focus only on entries where all required wage information is clearly provided and valid. Finally, I want to compute the average percentage difference between the offered pay and the standard market rate for these positions.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH wage_data AS ( SELECT c.filekey, ( REPLACE(pw.wage_details->'offered_wage'->>'from', '$', '')::NUMERIC + COALESCE( NULLIF(REPLACE(pw.wage_details->'offered_wage'->>'to', '$', ''), '')::NUMERIC, REPLACE(pw.wage_details->'offered_wage'->>'from', '$', '')::NUMERIC ) ) / 2 AS avg_offered_wage, REPLACE( SPLIT_PART(pw.wage_details->'prevailing_wage'->>'value', ' ', 2), ',', '' )::NUMERIC AS prevailing_wage_value FROM public.cases c JOIN public.case_worksite cw ON c.filekey = cw.dockkey JOIN public.prevailing_wage pw ON cw.wagetrack = pw.trackno WHERE LOWER(TRIM(pw.wage_details->'prevailing_wage'->>'level')) = 'iv' AND LOWER(TRIM(pw.wage_details->'offered_wage'->>'unit')) = 'year' AND LOWER(TRIM(pw.wage_details->'prevailing_wage'->>'unit')) = 'year' AND pw.wage_details->'prevailing_wage'->>'value' IS NOT NULL AND pw.wage_details->'offered_wage'->>'from' IS NOT NULL AND REPLACE( SPLIT_PART(pw.wage_details->'prevailing_wage'->>'value', ' ', 2), ',', '' )::NUMERIC > 0 ) SELECT ROUND(AVG((avg_offered_wage - prevailing_wage_value) / prevailing_wage_value * 100), 2) AS average_wdr_for_level_iv_annual_positions FROM wage_data;"], "external_knowledge": [5, 11], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_6", "selected_database": "labor_certification_applications", "query": "Within the custom computer programming services industry where NAICS code equals to 541511, how many annually paid positions qualify as significantly high wage positions? I'm looking for a count of positions where the wage differential ratio exceeds 20%.", "normal_query": "I am analyzing compensation trends within the Custom Computer Programming Services industry, specifically NAICS code 541511. I need to determine the number of Premium Wage Positions that are paid annually. Can you provide a count of the positions within this industry where the Wage Differential Rate exceeds 20%?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Intent: Identify how many annually paid positions in a specific tech industry (NAICS 541511) offer wages significantly above prevailing (WDR > 20%), counting actual worker positions (headcount).\n-- Knowledge Used: ID 40 (Premium Wage Position), ID 6 (NAICS Code Purpose), ID 11 (Wage Differential Rate)\n-- Advanced PSQL: CTEs, JSONB operators, string functions, CAST(), SUM() with FILTER\n-- Difficulty: Advanced\n-- Type: Result\n\nWITH industry_wage_data AS (\n    SELECT\n        c.filekey,\n        cw.wsHeads AS headcount,\n        (REPLACE(pw.wage_details->'offered_wage'->>'from', '$', '')::NUMERIC) AS offered_wage_from,\n        (REPLACE(SPLIT_PART(pw.wage_details->'prevailing_wage'->>'value', ' ', 2), ',', '')::NUMERIC) AS prevailing_wage_value,\n        e.employer_contact_info->>'naics_code' as naics_code\n    FROM\n        public.cases c\n    JOIN\n        public.employer e ON c.homefirm = e.corphandle AND c.homezip = e.zipref\n    JOIN\n        public.case_worksite cw ON c.filekey = cw.dockkey\n    JOIN\n        public.prevailing_wage pw ON cw.wagetrack = pw.trackno\n    WHERE\n        pw.wage_details->'offered_wage'->>'unit' = 'Year'\n        AND pw.wage_details->'prevailing_wage'->>'unit' = 'Year'\n        AND (pw.wage_details->'prevailing_wage'->>'value') IS NOT NULL\n        AND (pw.wage_details->'offered_wage'->>'from') IS NOT NULL\n        AND (REPLACE(SPLIT_PART(pw.wage_details->'prevailing_wage'->>'value', ' ', 2), ',', '')::NUMERIC) > 0\n        AND (e.employer_contact_info->>'naics_code')::TEXT = '541511'\n)\nSELECT\n    SUM(headcount) FILTER (\n        WHERE (offered_wage_from - prevailing_wage_value) / prevailing_wage_value * 100 > 20\n    ) AS count_of_premium_wage_positions\nFROM\n    industry_wage_data iwd;\n"], "external_knowledge": [6, 11, 40], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_7", "selected_database": "labor_certification_applications", "query": "I need to identify attorneys with high level performance who specialize in E-3 Australian visas. Can you provide a count of attorneys who meet the criteria of a high performer and for whom E-3 Australian visa cases constitute more than 50% of their total caseload?", "normal_query": "I am interested in identifying attorneys who are highly proficient in handling E-3 Australian visa applications. Can you provide a count of attorneys who qualify as High Performers, based on the Attorney Performance Rating, and for whom E-3 Australian visa cases constitute a significant portion of their practice? Specifically, I need the number of attorneys where more than 50% of their caseload consists of E-3 Australian visa applications.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Identify how many top-performing attorneys (success rate > 95%) have a significant focus ( > 50% of their cases) on E-3 Australian visas.\n-- Knowledge Used: ID 52 (Attorney Performance Rating), ID 1 (Visa Classification Types)\n-- Advanced PSQL: CTEs, COUNT() with FILTER, AVG(CASE WHEN...), SUM(CASE WHEN...), GROUP BY, HAVING\n-- Difficulty: Advanced\n-- Type: Result\n\nWITH attorney_performance AS (\n    SELECT\n        ca.counselmail,\n        COUNT(*) AS total_cases,\n        SUM(CASE WHEN cs.visacls = 'E-3 Australian' THEN 1 ELSE 0 END) AS e3_cases,\n        AVG(CASE WHEN cs.statustag ILIKE 'Certified%' THEN 1 ELSE 0 END) AS success_rate\n    FROM\n        public.case_attorney ca\n    JOIN\n        public.cases cs ON ca.docketkey = cs.filekey\n    GROUP BY\n        ca.counselmail\n    HAVING COUNT(*) > 0\n)\nSELECT\n    COUNT(*) AS count_of_high_perf_e3_focused_attorneys\nFROM\n    attorney_performance ap\nWHERE\n    ap.success_rate > 0.95\n    AND (ap.e3_cases::DECIMAL / ap.total_cases) > 0.50;\n"], "external_knowledge": [1, 52], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_8", "selected_database": "labor_certification_applications", "query": "I want to see how competitive the salaries are for software quality assurance analysts and testers jobs that pay yearly. Group these jobs into the wage competitiveness levels, and for each level, show the total number of positions, where positions means the total worker count from the dataset's head count information, not the number of rows. Sort so the most common category is at the top.", "normal_query": "I am preparing to analyze the distribution of Wage Competitiveness Tiers for Software Quality Assurance Analysts and Testers positions. Specifically, I would like to see a breakdown of annually paid positions in this occupation, categorized by their wage competitiveness level. For clarity, positions here are defined strictly as the total number of worker positions calculated by summing the head count information from the dataset, rather than counting individual records. Please provide the total number of worker positions in each tier, and sort the results by the number of positions in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH job_wdr AS ( SELECT ((REPLACE(pw.wage_details->'offered_wage'->>'from', '$', '')::NUMERIC) - (REPLACE(SPLIT_PART(pw.wage_details->'prevailing_wage'->>'value', ' ', 2), ',', '')::NUMERIC)) / NULLIF((REPLACE(SPLIT_PART(pw.wage_details->'prevailing_wage'->>'value', ' ', 2), ',', '')::NUMERIC), 0) * 100 AS wdr_percentage, COALESCE(c.headCt, cw.wsHeads) AS position_count FROM public.cases c JOIN public.case_worksite cw ON c.filekey = cw.dockkey JOIN public.prevailing_wage pw ON cw.wagetrack = pw.trackno WHERE c.soccd = '15-1253.00' AND pw.wage_details->'offered_wage'->>'unit' = 'Year' AND pw.wage_details->'prevailing_wage'->>'unit' = 'Year' AND pw.wage_details->'prevailing_wage'->>'value' IS NOT NULL AND pw.wage_details->'offered_wage'->>'from' IS NOT NULL ) SELECT CASE WHEN wdr_percentage < 0 THEN 'Below-Market' WHEN wdr_percentage BETWEEN 0 AND 10 THEN 'Market-Competitive' WHEN wdr_percentage > 10 THEN 'Premium' ELSE 'N/A' END AS wage_competitiveness_tier, SUM(position_count) AS number_of_positions FROM job_wdr GROUP BY wage_competitiveness_tier ORDER BY number_of_positions DESC;"], "external_knowledge": [7, 48], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "labor_certification_applications_10", "selected_database": "labor_certification_applications", "query": "I'm looking to compare how long visa applications take to process based on how complex they are. Could you sort the applications into application complexity levels and for each group, tell me how many applications there are along with the average processing time in days? Please round the averages to two decimals and list the groups starting with the ones that take the longest.", "normal_query": "I require a comparative analysis of visa application processing times segmented by application complexity. Please classify each application using the Application Complexity Tiers. Then, for each complexity category, compute the number of applications and the average processing time in days. Round the average processing time to two decimal places and present the results in descending order of average processing time.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH app_complexity AS ( SELECT filekey, CASE WHEN h1bdep = 'Yes' OR willfulv = 'Yes' THEN 'Complex' ELSE 'Standard' END AS complexity_category, ( CASE WHEN decisionday LIKE '%/%' THEN TO_DATE(decisionday, 'YYYY/MM/DD') ELSE TO_DATE(decisionday, 'YYYY-MM-DD') END - CASE WHEN recvday LIKE '%/%' THEN TO_DATE(recvday, 'YYYY/MM/DD') ELSE TO_DATE(recvday, 'YYYY-MM-DD') END )::NUMERIC AS processing_days FROM public.cases WHERE decisionday IS NOT NULL AND recvday IS NOT NULL ) SELECT ac.complexity_category, COUNT(*) AS number_of_applications, ROUND(AVG(ac.processing_days)::DECIMAL, 2) AS average_processing_time_days FROM app_complexity ac GROUP BY ac.complexity_category ORDER BY average_processing_time_days DESC;"], "external_knowledge": [12, 51], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "labor_certification_applications_11", "selected_database": "labor_certification_applications", "query": "I'm trying to figure out when people are submitting their visa applications to see if they're doing it at the best time. Could you help me break down the applications based on different visa filing window? And I also need to know how many applications are in each category and what percentage of the total they make up and round it to two decimal places. Please sort the results from most to least applications.", "normal_query": "I am eager to create a report detailing the Visa Filing Window Distribution for all visa applications. The report should categorize applications based on Visa Filing Window. The output should include the category name, the number of applications falling into each category, and the percentage of total applications represented by each category, rounded to two decimal places. Please present the results in descending order by the number of applications.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Query 1: Calculate Monthly Visa Filing Window Distribution \n-- Intent: Identify if applications are being submitted during optimal filing periods (6 months before start date)\n-- Knowledge Used: ID 41 (Visa Filing Window)\n-- Advanced PSQL: DATE_PART(), TO_DATE(), interval subtraction, GROUP BY\n-- Difficulty: Normal\n-- Type: Normal Table\n\nSELECT \n    CASE\n        WHEN months_before_start < 0 THEN 'After Start Date'\n        WHEN months_before_start = 0 THEN 'Same Month'\n        WHEN months_before_start BETWEEN 1 AND 3 THEN '1-3 Months Before'\n        WHEN months_before_start BETWEEN 4 AND 6 THEN 'Optimal Window (4-6 Months)'\n        ELSE 'Early Filing (>6 Months)'\n    END AS filing_time_category,\n    COUNT(*) AS application_count,\n    ROUND((COUNT(*)::DECIMAL / SUM(COUNT(*)) OVER ()) * 100, 2) AS percentage\nFROM (\n    SELECT \n        filekey,\n        DATE_PART('month', TO_DATE(beginday, 'DD/MM/YYYY')) - \n        DATE_PART('month', TO_DATE(recvday, 'YYYY/MM/DD')) + \n        (DATE_PART('year', TO_DATE(beginday, 'DD/MM/YYYY')) - \n         DATE_PART('year', TO_DATE(recvday, 'YYYY/MM/DD'))) * 12 AS months_before_start\n    FROM \n        public.cases\n    WHERE \n        recvday IS NOT NULL AND beginday IS NOT NULL\n) AS filing_window\nGROUP BY \n    filing_time_category\nORDER BY \n    application_count DESC;\n"], "external_knowledge": [41], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "labor_certification_applications_12", "selected_database": "labor_certification_applications", "query": "Find the jobs where the offered pay is over 10% higher than the going market rate and there are at least 30 applications. Only include cases where both pay figures are available and in the same pay unit, like both per hour or both per year. For each job, list its title, number of applications, average percentage pay difference (rounded to two decimals), and mark it as “Skill Shortage Occupation”. Show only the top five with the biggest average pay differences, starting from the highest.", "normal_query": "For each occupation, identify those that meet the definition of Skill Shortage Occupations — having a Wage Differential Rate (WDR) greater than 10% and at least 30 applications. Include only situations where both the offered wage and the prevailing wage are available and measured in the same pay unit. Show the occupation title, total applications, average WDR (rounded to two decimals), and mark them with the category “Skill Shortage Occupation”. List only the top five with the highest average WDR in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH wage_data AS (\n    SELECT c.filekey, c.soccd, LOWER(TRIM(c.soctitle)) AS soctitle,\n        (REPLACE(pw.wage_details->'offered_wage'->>'from', '$', '')::NUMERIC - REPLACE(SPLIT_PART(pw.wage_details->'prevailing_wage'->>'value', ' ', 2), ',', '')::NUMERIC) / NULLIF(REPLACE(SPLIT_PART(pw.wage_details->'prevailing_wage'->>'value', ' ', 2), ',', '')::NUMERIC, 0) * 100 AS wdr\n    FROM public.cases c\n    JOIN public.case_worksite cw ON c.filekey = cw.dockkey\n    JOIN public.prevailing_wage pw ON cw.wagetrack = pw.trackno\n    WHERE pw.wage_details->'offered_wage'->>'unit' = pw.wage_details->'prevailing_wage'->>'unit'\n      AND (pw.wage_details->'prevailing_wage'->>'value') IS NOT NULL\n      AND (pw.wage_details->'offered_wage'->>'from') IS NOT NULL\n), occupation_stats AS (\n    SELECT soctitle, COUNT(*) AS application_count, AVG(wdr) AS avg_wdr\n    FROM wage_data\n    GROUP BY soctitle\n    HAVING COUNT(*) >= 30\n)\nSELECT soctitle AS occupation_title, application_count, ROUND(avg_wdr::NUMERIC, 2) AS average_wage_differential_rate, 'Skill Shortage Occupation' AS category\nFROM occupation_stats\nWHERE avg_wdr > 10\nORDER BY avg_wdr DESC\nLIMIT 5;"], "external_knowledge": [11, 43], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "labor_certification_applications_13", "selected_database": "labor_certification_applications", "query": "List the states that have at least 1.5× the national average of visa applications, and tell me how many such hotspot states there are in total.", "normal_query": "For each U.S. state, identify Geographic Application Hotspots based on visa application counts exceeding 1.5 times the national average. The output should include the list of hotspot states and the total count of such hotspot states.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH state_applications AS ( SELECT LOWER(TRIM(w.wstate)) AS wstate, COUNT(cw.dockkey) AS state_app_count FROM public.worksite w JOIN public.case_worksite cw ON w.w_addr1 = cw.ws_addr1 AND w.wzip = cw.wszip GROUP BY LOWER(TRIM(w.wstate)) ), hotspot_analysis AS ( SELECT wstate, state_app_count, AVG(state_app_count) OVER () AS national_avg FROM state_applications ) SELECT wstate FROM hotspot_analysis WHERE state_app_count > national_avg * 1.5;"], "external_knowledge": [19], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "labor_certification_applications_14", "selected_database": "labor_certification_applications", "query": "Can you figure out which industries depend on visa? I'd like to see the NAICS code, the total number of applications, and the percentage of all applications rounded off to two decimal places. List the results starting with the industries that have the highest percentages.", "normal_query": "Please determine which industries belong to Visa-Dependent Industry. The output should include the NAICS code, the total number of applications, and the percentage of total applications (rounded to two decimal places). Sort the results from the highest to the lowest percentage.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH visa_dependent_industries AS (SELECT LOWER(TRIM(e.employer_contact_info->>'naics_code'))::TEXT AS naics_code, COUNT(c.filekey) AS industry_applications, ROUND((COUNT(c.filekey)::DECIMAL / (SELECT COUNT(*) FROM public.cases)) * 100, 2) AS industry_percentage FROM public.cases c JOIN public.employer e ON c.homefirm = e.corphandle AND c.homezip = e.zipref WHERE e.employer_contact_info->>'naics_code' IS NOT NULL GROUP BY LOWER(TRIM(e.employer_contact_info->>'naics_code')) HAVING (COUNT(c.filekey)::DECIMAL / (SELECT COUNT(*) FROM public.cases)) * 100 > 15) SELECT vdi.naics_code, vdi.industry_applications, vdi.industry_percentage, (SELECT STRING_AGG(socTitle, ', ') FROM (SELECT c.socTitle FROM public.cases c JOIN public.employer e ON c.homefirm = e.corphandle AND c.homezip = e.zipref WHERE LOWER(TRIM(e.employer_contact_info->>'naics_code'))::TEXT = vdi.naics_code AND c.socTitle IS NOT NULL GROUP BY c.socTitle ORDER BY COUNT(c.socTitle) DESC LIMIT 5) top_occupations) AS top_occupations FROM visa_dependent_industries vdi ORDER BY vdi.industry_percentage DESC;"], "external_knowledge": [20, 42], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "labor_certification_applications_15", "selected_database": "labor_certification_applications", "query": "Can you group attorneys based on the types of visa cases they usually work on? Only include attorneys who’ve worked on at least 5 cases. For each group, show the category name, how many attorneys are in it, their average specialization score rounded off to two decimals, and the average percentage of cases they handle in their main visa type rounded to two decimals. Sort the list so the categories with the most attorneys come first.", "normal_query": "For each attorney, categorize them into Attorney Specialization Categories based on their visa case specialization. Only include attorneys who have handled at least 5 cases. For each category, display the category name, the number of attorneys in each category, the average Attorney Specialization Index (ASI) (rounded to two decimal places, as handled in the SQL), and the average percentage of cases they handle in their dominant visa type (rounded to two decimal places). Sort the results by the number of attorneys in each category in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH attorney_visa_counts AS (SELECT LOWER(TRIM(ca.counselmail)) AS counselmail, LOWER(TRIM(c.visacls::TEXT)) AS visacls, COUNT(*) AS cases_per_visa_type FROM public.case_attorney ca JOIN public.cases c ON ca.docketkey = c.filekey WHERE c.visacls IS NOT NULL GROUP BY LOWER(TRIM(ca.counselmail)), LOWER(TRIM(c.visacls::TEXT))), attorney_totals AS (SELECT counselmail, SUM(cases_per_visa_type) AS total_cases_per_attorney, COUNT(*) AS unique_visa_types_per_attorney FROM attorney_visa_counts GROUP BY counselmail), attorney_specialization AS (SELECT avc.counselmail, at.total_cases_per_attorney, at.unique_visa_types_per_attorney, MAX(avc.cases_per_visa_type) AS max_cases_in_single_visa_type, 1.0 - (at.unique_visa_types_per_attorney::DECIMAL / 4.0) AS specialization_index, (MAX(avc.cases_per_visa_type)::DECIMAL / at.total_cases_per_attorney * 100) AS dominant_visa_percentage FROM attorney_visa_counts avc JOIN attorney_totals at ON avc.counselmail = at.counselmail GROUP BY avc.counselmail, at.total_cases_per_attorney, at.unique_visa_types_per_attorney) SELECT CASE WHEN at.unique_visa_types_per_attorney = 1 THEN 'Exclusive Specialists' WHEN at.dominant_visa_percentage > 80 THEN 'Specialists' WHEN at.unique_visa_types_per_attorney BETWEEN 2 AND 3 THEN 'Hybrid Practitioners' ELSE 'Generalists' END AS specialization_category, COUNT(*) AS attorney_count, ROUND(AVG(at.specialization_index), 2) AS avg_specialization_index, ROUND(AVG(at.dominant_visa_percentage), 2) AS avg_dominant_visa_percentage FROM attorney_specialization at WHERE at.total_cases_per_attorney >= 5 GROUP BY specialization_category ORDER BY attorney_count DESC;"], "external_knowledge": [34, 45], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "labor_certification_applications_16", "selected_database": "labor_certification_applications", "query": "I'm trying to figure out which states have the best lawyers when it comes to handling visa cases. Could you help me check the rate of attorney success for different states? I'd like to see: which state they're practicing in, how many cases they've handled in total, how many cases were successful and their success rate as a percentage with 2 decimal points. Let's focus on states where attorneys have handled at least 3 cases, and just show me the top 3 states with the highest success rates.", "normal_query": "Could you analyze the performance of attorneys across different court jurisdictions by calculating their Attorney Success Rate? Please show the jurisdiction state, total number of cases handled, number of certified cases, and success rate as a percentage with 2 decimal places. Only include jurisdictions with at least 3 cases and show the top 3 most successful jurisdictions.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n    LOWER(TRIM(a.attorney_profile->'highest_court'->>'state')) AS jurisdiction_state,\n    COUNT(*) AS total_cases,\n    COUNT(*) FILTER (WHERE c.statustag ILIKE 'Certified%') AS certified_cases,\n    ROUND(\n        (COUNT(*) FILTER (WHERE c.statustag ILIKE 'Certified%'))::DECIMAL /\n        NULLIF(COUNT(*), 0) * 100,\n        2\n    ) AS success_rate_percentage\nFROM\n    public.case_attorney ca\nJOIN\n    public.attorney a ON ca.counselmail = a.lawmail\nJOIN\n    public.cases c ON ca.docketkey = c.filekey\nWHERE\n    a.attorney_profile->'highest_court'->>'state' IS NOT NULL\nGROUP BY\n    LOWER(TRIM(a.attorney_profile->'highest_court'->>'state'))\nHAVING\n    COUNT(*) >= 3\nORDER BY\n    success_rate_percentage DESC\nLIMIT 3;\n"], "external_knowledge": [24], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "labor_certification_applications_17", "selected_database": "labor_certification_applications", "query": "I'm curious about the wage levels for H-1B jobs - could you help me break down the numbers? I'd like to know how many applications we have for each wage level, and what percentage of the total they make up with 2 decimal points. And rank them from most common to least common.", "normal_query": "I need an overview of Prevailing Wage Levels distribution in H-1B visa applications with valid prevailing wage level. Please show each wage level along with its application count and the percentage share of total applications, with percentages shown to 2 decimal places. Sort the results to highlight which wage levels are most commonly requested.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n    LOWER(TRIM(pw.wage_details->'prevailing_wage'->>'level')) AS wage_level,\n    COUNT(*) AS application_count,\n    ROUND(\n        (COUNT(*)::DECIMAL / SUM(COUNT(*)) OVER ()) * 100,\n        2\n    ) AS percentage\nFROM\n    public.cases c\nJOIN\n    public.case_worksite cw ON c.filekey = cw.dockkey\nJOIN\n    public.prevailing_wage pw ON cw.wagetrack = pw.trackno\nWHERE\n    c.visacls = 'H-1B'\n    AND pw.wage_details->'prevailing_wage'->>'level' IS NOT NULL\nGROUP BY\n    LOWER(TRIM(pw.wage_details->'prevailing_wage'->>'level'))\nORDER BY\n    application_count DESC;\n"], "external_knowledge": [1, 5], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "labor_certification_applications_18", "selected_database": "labor_certification_applications", "query": "Could you help me find out which industries tend to pay more than others? I'd love to see the top 5 highest-paying industries and just show me their NAICS codes, how many job applications each industry has, and average industry wage difference. Make sure we're only looking at entries with valid NAICS codes and consistent wage units, and round the wage differences to 2 decimal places to keep it clean.", "normal_query": "Can you analyze how wages differ across industries by calculating the Industry Wage Differential for each NAICS code where naics code is valid and wage unit equals? Please show: the industry NAICS code, number of applications in that industry and average wage differential (rounded to 2 decimal places). Show only the top 5 industries with the highest wage differentials.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Calculate average wage differential by industry (NAICS code)\n-- Knowledge Used: ID 31 (Industry Wage Differential), ID 6 (NAICS Code Purpose)\n-- Advanced PSQL: JSONB functions, mathematical calculations\n-- Difficulty: Advanced\n-- Type: Table\n\nWITH industry_wages AS (\n    SELECT\n        e.employer_contact_info->>'naics_code' AS naics_code,\n        ((REPLACE(pw.wage_details->'offered_wage'->>'from', '$', '')::NUMERIC) - \n         (REPLACE(SPLIT_PART(pw.wage_details->'prevailing_wage'->>'value', ' ', 2), ',', '')::NUMERIC)) /\n        NULLIF((REPLACE(SPLIT_PART(pw.wage_details->'prevailing_wage'->>'value', ' ', 2), ',', '')::NUMERIC), 0) * 100 AS wdr\n    FROM\n        public.cases c\n    JOIN\n        public.employer e ON c.homefirm = e.corphandle AND c.homezip = e.zipref\n    JOIN\n        public.case_worksite cw ON c.filekey = cw.dockkey\n    JOIN\n        public.prevailing_wage pw ON cw.wagetrack = pw.trackno\n    WHERE\n        e.employer_contact_info->>'naics_code' IS NOT NULL\n        AND pw.wage_details->'offered_wage'->>'unit' = pw.wage_details->'prevailing_wage'->>'unit'\n)\nSELECT\n    naics_code,\n    COUNT(*) AS application_count,\n    ROUND(AVG(wdr), 2) AS avg_industry_wage_differential\nFROM\n    industry_wages\nGROUP BY\n    naics_code\nORDER BY\n    avg_industry_wage_differential DESC\nLIMIT 5;"], "external_knowledge": [6, 31], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "labor_certification_applications_19", "selected_database": "labor_certification_applications", "query": "I'm wondering if having a lawyer really boosts your chances of getting a visa approved. Can you compare visa approval rates for applications filed with an attorney versus those filed without one? I want to see the total number of applications in each category, the number that were approved, and the approval rate percentage, rounded to two decimal places. Include all the attorney cases.", "normal_query": "I am working on analyzing the effectiveness of legal representation in visa application outcomes including all the attorney cases. Can you provide a report comparing the approval rate for applications that used an attorney versus those that were self-represented? I'd like to see the total number of applications in each category, the number of certified applications, and the calculated approval rate rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH representation_outcomes AS (SELECT c.filekey, TRIM(LOWER(c.statustag)) AS clean_statustag, CASE WHEN ca.counselmail IS NOT NULL THEN 'Represented' ELSE 'Self-Represented' END AS representation_status FROM public.cases c LEFT JOIN public.case_attorney ca ON c.filekey = ca.docketkey) SELECT representation_status, COUNT(*) AS total_applications, COUNT(*) FILTER (WHERE clean_statustag LIKE 'certified%') AS certified_applications, ROUND((COUNT(*) FILTER (WHERE clean_statustag LIKE 'certified%'))::DECIMAL / NULLIF(COUNT(*), 0) * 100, 2) AS approval_rate FROM representation_outcomes GROUP BY representation_status;"], "external_knowledge": [13], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_20", "selected_database": "labor_certification_applications", "query": "Which jobs are most in demand for visa applications? Could you show me the top 5 jobs that are most popular? I'd like to see the job title, how many applications there are for each, and some kind of occupational demand index for each job, rounded to two decimal places. Also, let's just stick to jobs that have valid SOC codes.", "normal_query": "I'm interested in understanding the relative demand for different occupations within the visa application process. Could you generate a report showing the top 5 occupations with the highest Occupational Demand Index? The report should include the occupation title, the number of applications for that occupation, and the calculated ODI rounded to two decimal places. Please only include occupations with valid SOC codes.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Calculate relative demand for specific occupations based on application volume\n-- Knowledge Used: ID 21 (Occupational Demand Index)\n-- Advanced PSQL: Window functions, mathematical calculations\n-- Difficulty: Normal\n-- Type: Table\n\nWITH soc_counts AS (\n    SELECT\n        c.soctitle,\n        c.soccd,\n        COUNT(*) AS application_count\n    FROM\n        public.cases c\n    WHERE\n        c.soccd IS NOT NULL\n    GROUP BY\n        c.soctitle, c.soccd\n),\nsoc_stats AS (\n    SELECT\n        AVG(application_count) AS avg_applications_per_soc\n    FROM\n        soc_counts\n)\nSELECT\n    sc.soctitle,\n    sc.application_count,\n    ROUND(\n        sc.application_count / ss.avg_applications_per_soc,\n        2\n    ) AS occupational_demand_index\nFROM\n    soc_counts sc\nCROSS JOIN\n    soc_stats ss\nORDER BY\n    occupational_demand_index DESC\nLIMIT 5;"], "external_knowledge": [21], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "labor_certification_applications_M_1", "selected_database": "labor_certification_applications", "query": "Can you make a table called 'employer_analytics' that shows how big each employer is in our visa database? I'm looking to track which companies submit lots of visa applications versus just a few. For each employer, I need their name, their employer scale indicator number, and their employer size category. If this table's already existed, just update it with the newest information.", "normal_query": "Could you create a table called 'employer_analytics' that calculates and stores the Employer Scale Indicator for each employer in our visa application database? I need the table to include the employer name, their ESI value, and categorize them according to the Employer Size Classification framework. If the table already exists, please update the records with the latest values.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE IF NOT EXISTS employer_analytics ( employer_name TEXT PRIMARY KEY, employer_scale_indicator NUMERIC, size_classification TEXT ); WITH employer_app_counts AS ( SELECT LOWER(TRIM(homeFirm)) AS employer_name, COUNT(*) AS app_count FROM cases GROUP BY LOWER(TRIM(homeFirm)) ), employer_scale_indicator AS ( SELECT employer_name, app_count, (app_count::numeric / AVG(app_count) OVER ()) AS esi FROM employer_app_counts ) INSERT INTO employer_analytics (employer_name, employer_scale_indicator, size_classification) SELECT e.employer_name, e.esi, CASE WHEN e.app_count > 25 THEN 'Large-scale users' WHEN e.app_count BETWEEN 5 AND 25 THEN 'Medium-scale users' ELSE 'Small-scale users' END AS size_classification FROM employer_scale_indicator e ON CONFLICT (employer_name) DO UPDATE SET employer_scale_indicator = EXCLUDED.employer_scale_indicator, size_classification = EXCLUDED.size_classification;"], "external_knowledge": [30, 46], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT EXISTS (SELECT FROM pg_tables WHERE tablename = 'employer_analytics')\")\n    table_exists = cursor.fetchone()[0]\n    assert table_exists, \"The employer_analytics table was not created\"\n    cursor.execute(\"SELECT COUNT(*) FROM employer_analytics\")\n    analytics_count = cursor.fetchone()[0]\n    assert analytics_count > 0, \"The employer_analytics table should contain data\"\n    cursor.execute(\"SELECT COUNT(*) FROM employer_analytics WHERE employer_name IS NULL\")\n    null_employer_count = cursor.fetchone()[0]\n    assert null_employer_count == 0, \"All employer names should be non-null\"\n    cursor.execute(\"SELECT COUNT(*) FROM employer_analytics WHERE size_classification = 'Large-scale users'\")\n    large_scale_count = cursor.fetchone()[0]\n    cursor.execute(\"SELECT COUNT(*) FROM employer_analytics WHERE size_classification = 'Medium-scale users'\")\n    medium_scale_count = cursor.fetchone()[0]\n    cursor.execute(\"SELECT COUNT(*) FROM employer_analytics WHERE size_classification = 'Small-scale users'\")\n    small_scale_count = cursor.fetchone()[0]\n    assert (large_scale_count + medium_scale_count + small_scale_count) == analytics_count, \"Sum of classifications should equal total records\"\n    cursor.execute(\"SELECT MIN(employer_scale_indicator) FROM employer_analytics\")\n    min_esi = cursor.fetchone()[0]\n    assert min_esi > 0, \"All ESI values should be positive\"\n    cursor.execute(\"\"\"\n        SELECT COUNT(*) FROM employer_analytics e\n        JOIN (\n            SELECT LOWER(TRIM(homeFirm)) AS employer_name, COUNT(*) AS app_count \n            FROM cases \n            GROUP BY LOWER(TRIM(homeFirm))\n        ) c ON e.employer_name = c.employer_name\n        WHERE \n            (c.app_count > 25 AND e.size_classification != 'Large-scale users') OR\n            (c.app_count BETWEEN 5 AND 25 AND e.size_classification != 'Medium-scale users') OR\n            (c.app_count < 5 AND e.size_classification != 'Small-scale users')\n    \"\"\")\n    misclassified = cursor.fetchone()[0]\n    assert misclassified == 0, \"Some employers were incorrectly classified\"\n    return 1"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_M_2", "selected_database": "labor_certification_applications", "query": "Hey, can you help me sort our visa attorneys into different categories based on their specialization patterns? I need to add a new column to our attorney table that shows if each lawyer is a 'Specialist,' 'Hybrid Practitioner,' or 'Generalist' depending on the different attorney specialization classification standard.", "normal_query": "will you identify and categorize attorneys in our visa database according to their Attorney Specialization Category? This requires adding a new column to the attorney table to store this classification.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["ALTER TABLE attorney ADD COLUMN IF NOT EXISTS specialization_category TEXT; WITH attorney_case_stats AS ( SELECT LOWER(TRIM(counselMail)) AS counsel_mail, SUM(case_count) AS total_cases, MAX(case_count) AS top_type_cases, COUNT(visaCls) AS distinct_visa_types FROM ( SELECT LOWER(TRIM(ca.counselMail)) AS counselMail, c.visaCls, COUNT(*) as case_count FROM case_attorney ca JOIN cases c ON ca.docketKey = c.fileKey WHERE c.visaCls IS NOT NULL GROUP BY LOWER(TRIM(ca.counselMail)), c.visaCls ) AS counts GROUP BY LOWER(TRIM(counselMail)) ) UPDATE attorney a SET specialization_category = CASE WHEN acs.top_type_cases::numeric / acs.total_cases > 0.8 THEN 'Specialist' WHEN acs.distinct_visa_types IN (2, 3) THEN 'Hybrid Practitioner' ELSE 'Generalist' END FROM attorney_case_stats acs WHERE LOWER(TRIM(a.lawMail)) = acs.counsel_mail;"], "external_knowledge": [45], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'attorney' AND column_name = 'specialization_category')\")\n    column_exists = cursor.fetchone()[0]\n    assert column_exists, \"The specialization_category column was not added to the attorney table\"\n    cursor.execute(\"SELECT COUNT(*) FROM attorney WHERE specialization_category IS NOT NULL\")\n    classified_count = cursor.fetchone()[0]\n    assert classified_count > 0, \"No attorneys were classified\"\n    cursor.execute(\"SELECT COUNT(*) FROM attorney WHERE specialization_category = 'Specialist'\")\n    specialist_count = cursor.fetchone()[0]\n    cursor.execute(\"SELECT COUNT(*) FROM attorney WHERE specialization_category = 'Hybrid Practitioner'\")\n    hybrid_count = cursor.fetchone()[0]\n    cursor.execute(\"SELECT COUNT(*) FROM attorney WHERE specialization_category = 'Generalist'\")\n    generalist_count = cursor.fetchone()[0]\n    total_classified = specialist_count + hybrid_count + generalist_count\n    assert total_classified == classified_count, \"Sum of classification types should equal total classified attorneys\"\n    cursor.execute(\"\"\"WITH attorney_case_stats AS ( SELECT counselMail, SUM(case_count) AS total_cases, MAX(case_count) AS top_type_cases FROM ( SELECT ca.counselMail, c.visaCls, COUNT(*) as case_count FROM case_attorney ca JOIN cases c ON ca.docketKey = c.fileKey WHERE c.visaCls IS NOT NULL GROUP BY ca.counselMail, c.visaCls ) AS counts GROUP BY counselMail ) SELECT COUNT(*) FROM attorney a JOIN attorney_case_stats acs ON a.lawMail = acs.counselMail WHERE a.specialization_category = 'Specialist' AND acs.top_type_cases::numeric / acs.total_cases <= 0.8\"\"\")\n    invalid_specialists = cursor.fetchone()[0]\n    assert invalid_specialists == 0, \"Some attorneys are incorrectly classified as Specialists\"\n    cursor.execute(\"\"\"WITH attorney_visa_counts AS ( SELECT ca.counselMail, COUNT(DISTINCT c.visaCls) AS distinct_visa_types FROM case_attorney ca JOIN cases c ON ca.docketKey = c.fileKey WHERE c.visaCls IS NOT NULL GROUP BY ca.counselMail ) SELECT COUNT(*) FROM attorney a JOIN attorney_visa_counts avc ON a.lawMail = avc.counselMail WHERE a.specialization_category = 'Hybrid Practitioner' AND avc.distinct_visa_types NOT IN (2, 3)\"\"\")\n    invalid_hybrids = cursor.fetchone()[0]\n    assert invalid_hybrids == 0, \"Some attorneys are incorrectly classified as Hybrid Practitioners\"\n    cursor.execute(\"\"\"WITH attorney_visa_counts AS ( SELECT ca.counselMail, COUNT(DISTINCT c.visaCls) AS distinct_visa_types FROM case_attorney ca JOIN cases c ON ca.docketKey = c.fileKey WHERE c.visaCls IS NOT NULL GROUP BY ca.counselMail ) SELECT COUNT(*) FROM attorney a JOIN attorney_visa_counts avc ON a.lawMail = avc.counselMail WHERE a.specialization_category != 'Generalist' AND avc.distinct_visa_types >= 4\"\"\")\n    misclassified_generalists = cursor.fetchone()[0]\n    assert misclassified_generalists == 0, \"Some attorneys handling 4+ visa types are not classified as Generalists\"\n    return"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_M_3", "selected_database": "labor_certification_applications", "query": "I'm trying to figure out how long our visa applications take to process. Can you make a simple procedure called 'calculate_apt' that works out the time taken for application processing for each case? After you've created the procedure, could you run it to update all our records?", "normal_query": "I intend to implement a procedure to calculate the Application Processing Time for our visa applications database. Could you create a stored procedure called calculate apt? After creating the procedure, please execute it to update all records.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["ALTER TABLE cases ADD COLUMN IF NOT EXISTS apt_days INTEGER;", "CREATE OR REPLACE PROCEDURE calculate_apt() LANGUAGE plpgsql AS $$ BEGIN UPDATE cases SET apt_days = (CAST(decisionDAY AS DATE) - CAST(recvDay AS DATE)) WHERE decisionDAY IS NOT NULL AND recvDay IS NOT NULL; END; $$;", "CALL calculate_apt();"], "external_knowledge": [12], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'cases' AND column_name = 'apt_days')\")\n    assert cursor.fetchone()[0], \"Column 'apt_days' was not added\"\n    cursor.execute(\"SELECT COUNT(*) FROM cases WHERE apt_days IS NOT NULL\")\n    non_null_count = cursor.fetchone()[0]\n    assert non_null_count > 0, \"APT was not calculated for any case\"\n    cursor.execute(\"SELECT COUNT(*) FROM cases WHERE apt_days != (CAST(decisionDAY AS DATE) - CAST(recvDay AS DATE)) AND decisionDAY IS NOT NULL AND recvDay IS NOT NULL\")\n    incorrect = cursor.fetchone()[0]\n    assert incorrect == 0, \"Some APT calculations are incorrect\"\n    cursor.execute(\"SELECT COUNT(*) FROM cases WHERE (decisionDAY IS NULL OR recvDay IS NULL) AND apt_days IS NOT NULL\")\n    invalid = cursor.fetchone()[0]\n    assert invalid == 0, \"APT was calculated for cases with missing dates\"\n    return 1"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_M_4", "selected_database": "labor_certification_applications", "query": "I'm trying to build a function for our visa database that figures out the wage differential ratio. Can you make it round to two decimal places and return null if there's no prevailing wage? I need it to take four inputs: the offered wage amount, prevailing wage amount, and both of their payment units.", "normal_query": "I am considering creating a PostgreSQL function that calculates the Wage Differential Rate in our visa application database. Please round the final percentage to two decimal places and return null if the prevailing wage is zero. The function should accept four parameters: offered wage amount, prevailing wage amount, offered wage unit, and prevailing wage unit.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Create a function to calculate Wage Differential Rate (WDR) with unit conversion\n-- Knowledge used: #11 (Wage Differential Rate) - Calculates percentage difference between offered and prevailing wage\n-- Advanced features: CASE expressions, type casting, numeric functions\n\nCREATE OR REPLACE FUNCTION calculate_wdr(\n    offered_wage TEXT, \n    prevailing_wage TEXT,\n    wage_unit enum_wage_unit_of_pay, \n    pw_unit enum_pw_unit_of_pay\n) RETURNS NUMERIC AS $$\nDECLARE\n    offered_annual NUMERIC;\n    prevailing_annual NUMERIC;\nBEGIN\n    -- Step 1: Convert offered wage to annual amount based on unit\n    CASE wage_unit\n        WHEN 'Hour' THEN offered_annual := CAST(offered_wage AS NUMERIC) * 2080;\n        WHEN 'Week' THEN offered_annual := CAST(offered_wage AS NUMERIC) * 52;\n        WHEN 'Year' THEN offered_annual := CAST(offered_wage AS NUMERIC);\n        ELSE offered_annual := 0;\n    END CASE;\n    \n    -- Step 2: Convert prevailing wage to annual amount based on unit\n    CASE pw_unit\n        WHEN 'Hour' THEN prevailing_annual := CAST(prevailing_wage AS NUMERIC) * 2080;\n        WHEN 'Week' THEN prevailing_annual := CAST(prevailing_wage AS NUMERIC) * 52;\n        WHEN 'Month' THEN prevailing_annual := CAST(prevailing_wage AS NUMERIC) * 12;\n        WHEN 'Year' THEN prevailing_annual := CAST(prevailing_wage AS NUMERIC);\n        ELSE prevailing_annual := 0;\n    END CASE;\n    \n    -- Step 3: Calculate and return the Wage Differential Rate\n    RETURN CASE WHEN prevailing_annual = 0 THEN NULL \n                ELSE ROUND(((offered_annual - prevailing_annual) / prevailing_annual) * 100, 2)\n           END;\nEND;\n$$ LANGUAGE plpgsql;\n"], "external_knowledge": [11], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    from decimal import Decimal\n    # Test that the SQL successfully creates the WDR calculation function\n    cursor = conn.cursor()\n    \n    # Check if the function exists\n    cursor.execute(\"\"\"\n        SELECT EXISTS (\n            SELECT 1 FROM pg_proc \n            WHERE proname = 'calculate_wdr'\n        )\n    \"\"\")\n    function_exists = cursor.fetchone()[0]\n    assert function_exists, \"The calculate_wdr function was not created\"\n    \n    # Test the function with various wage unit combinations\n    test_cases = [\n        (\"25\", \"20\", \"Hour\", \"Hour\", 25.00),\n        (\"1000\", \"25\", \"Week\", \"Hour\", 0.00),\n        (\"50\", \"0\", \"Hour\", \"Hour\", None)\n    ]\n    \n    for offered, prevailing, wage_unit, pw_unit, expected in test_cases:\n        cursor.execute(f\"SELECT calculate_wdr('{offered}', '{prevailing}', '{wage_unit}', '{pw_unit}')\")\n        result = cursor.fetchone()[0]\n        if expected is None:\n            assert result is None, f\"Expected NULL for calculate_wdr('{offered}', '{prevailing}', '{wage_unit}', '{pw_unit}')\"\n        else:\n            assert abs(Decimal(result) - Decimal(expected)) < 0.1, f\"Expected {expected} but got {result}\"\n    return 1"], "category": "Management", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_M_5", "selected_database": "labor_certification_applications", "query": "We want to add a new column that shows how much employers rely on visa workers, but only for those who have submitted any applications. Just sort them into Low, Moderate, or High based on their visa usage. Use 20 times their case count to estimate workforce and handle any divide-by-zero issues.", "normal_query": "We need to enhance our employer table by adding an Employer Dependency Level classification column. Please create an enumerated type with three dependency levels (Low, Moderate, High) and update the classification only for employers that have submitted at least one application. For workforce estimation, use a factor of 20 times the distinct case count per employer. Make sure to handle division by zero properly.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Step 1: Conditionally create enum type\nDO $$\nBEGIN\n    IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'enum_dependency_level') THEN\n        CREATE TYPE enum_dependency_level AS ENUM ('Low Dependency', 'Moderate Dependency', 'High Dependency');\n    END IF;\nEND\n$$;\n\n-- Step 2: Add column if not exists\nALTER TABLE employer \nADD COLUMN IF NOT EXISTS dependency_level enum_dependency_level;\n\n-- Step 3: Compute stats per employer\nWITH employer_stats AS (\n    SELECT \n        LOWER(TRIM(homeFirm)) AS firm,\n        LOWER(TRIM(homeZip)) AS zip,\n        SUM(headct) AS total_visa_workers,\n        COUNT(DISTINCT filekey) * 20 AS estimated_total_workforce,\n        MAX(CASE WHEN h1bdep = 'Yes' THEN 1 ELSE 0 END) AS has_h1b_dependency\n    FROM cases\n    GROUP BY LOWER(TRIM(homeFirm)), LOWER(TRIM(homeZip))\n),\ndependency_classification AS (\n    SELECT \n        firm, zip,\n        CASE \n            WHEN has_h1b_dependency = 1 THEN 'High Dependency'::enum_dependency_level\n            WHEN total_visa_workers::FLOAT / NULLIF(estimated_total_workforce, 0) > 0.15 THEN 'High Dependency'::enum_dependency_level\n            WHEN total_visa_workers::FLOAT / NULLIF(estimated_total_workforce, 0) BETWEEN 0.05 AND 0.15 THEN 'Moderate Dependency'::enum_dependency_level\n            ELSE 'Low Dependency'::enum_dependency_level\n        END AS derived_level\n    FROM employer_stats\n)\n\n-- Step 4: Update employer records with classification\nUPDATE employer e\nSET dependency_level = d.derived_level\nFROM dependency_classification d\nWHERE LOWER(TRIM(e.corpHandle)) = d.firm\n  AND LOWER(TRIM(e.zipref)) = d.zip;"], "external_knowledge": [54], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'enum_dependency_level')\")\n    enum_exists = cursor.fetchone()[0]\n    assert enum_exists, \"The enum_dependency_level type was not created\"\n    cursor.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'employer' AND column_name = 'dependency_level')\")\n    column_exists = cursor.fetchone()[0]\n    assert column_exists, \"The dependency_level column was not added to the employer table\"\n    cursor.execute(\"SELECT COUNT(*) FROM employer WHERE dependency_level IS NOT NULL\")\n    classified_count = cursor.fetchone()[0]\n    assert classified_count > 0, \"No employers were classified with dependency levels\"\n    cursor.execute(\"SELECT COUNT(*) FROM employer WHERE dependency_level = 'Low Dependency'\")\n    low_count = cursor.fetchone()[0]\n    cursor.execute(\"SELECT COUNT(*) FROM employer WHERE dependency_level = 'Moderate Dependency'\")\n    moderate_count = cursor.fetchone()[0]\n    cursor.execute(\"SELECT COUNT(*) FROM employer WHERE dependency_level = 'High Dependency'\")\n    high_count = cursor.fetchone()[0]\n    total_classified = low_count + moderate_count + high_count\n    assert total_classified == classified_count, \"Sum of dependency levels should equal total classified employers\"\n    cursor.execute(\"SELECT COUNT(*) FROM employer e WHERE EXISTS (SELECT 1 FROM cases c WHERE c.homeFirm = e.corpHandle AND c.homeZip = e.ZipRef AND c.h1bDep = 'Yes') AND e.dependency_level != 'High Dependency'\")\n    misclassified_h1b_dependent = cursor.fetchone()[0]\n    assert misclassified_h1b_dependent == 0, \"Some H-1B dependent employers were not classified as High Dependency\"\n    cursor.execute(\"\"\"WITH employer_stats AS ( SELECT e.corpHandle, e.ZipRef, e.dependency_level, SUM(c.headCt) AS total_visa_workers, COUNT(DISTINCT c.fileKey) * 20 AS estimated_total_workforce, (SUM(c.headCt)::FLOAT / NULLIF(COUNT(DISTINCT c.fileKey) * 20, 0)) AS visa_worker_ratio FROM employer e JOIN cases c ON c.homeFirm = e.corpHandle AND c.homeZip = e.ZipRef WHERE c.h1bDep != 'Yes' GROUP BY e.corpHandle, e.ZipRef, e.dependency_level ) SELECT COUNT(*) FROM employer_stats WHERE visa_worker_ratio > 0.15 AND dependency_level != 'High Dependency'\"\"\")\n    misclassified_high = cursor.fetchone()[0]\n    assert misclassified_high == 0, \"Some employers with >15% visa workers were not classified as High Dependency\"\n    return 1"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_M_6", "selected_database": "labor_certification_applications", "query": "Can you set up something that automatically checks how strong a wage is whenever someone adds or changes one? I want the system to figure out the difference between what’s being offered and what’s typical, then sort it into the right group. Also, make sure it works even if the wages are in different formats, like hourly versus yearly.", "normal_query": "Please build an automated system that assigns each wage entry to a category based on the Wage Competitiveness Tiers framework. This system should activate whenever a new entry is added or an existing one is modified. It needs to calculate the Wage Differential Rate (WDR) and use that value to determine the appropriate category. The process must ensure any required conversions between wage types — such as hourly or annual — are handled correctly during the calculation.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["ALTER TABLE prevailing_wage \nADD COLUMN IF NOT EXISTS wage_tier TEXT;", "CREATE OR REPLACE FUNCTION update_wage_competitiveness()\nRETURNS TRIGGER AS $$\nDECLARE\n    wage_from NUMERIC;\n    prev_wage NUMERIC;\n    wage_unit enum_wage_unit_of_pay;\n    pw_unit enum_pw_unit_of_pay;\n    offered_annual NUMERIC;\n    prevailing_annual NUMERIC;\n    wdr NUMERIC;\nBEGIN\n    wage_from := REGEXP_REPLACE(\n        NEW.wage_details -> 'offered_wage' ->> 'from', '[^0-9\\.]+', '', 'g'\n    )::NUMERIC;\n\n    prev_wage := REGEXP_REPLACE(\n        NEW.wage_details -> 'prevailing_wage' ->> 'value', '[^0-9\\.]+', '', 'g'\n    )::NUMERIC;\n\n    wage_unit := (NEW.wage_details -> 'offered_wage' ->> 'unit')::enum_wage_unit_of_pay;\n    pw_unit := (NEW.wage_details -> 'prevailing_wage' ->> 'unit')::enum_pw_unit_of_pay;\n\n    offered_annual := CASE wage_unit\n        WHEN 'Hour' THEN wage_from * 2080\n        WHEN 'Week' THEN wage_from * 52\n        WHEN 'Year' THEN wage_from\n        ELSE NULL\n    END;\n\n    prevailing_annual := CASE pw_unit\n        WHEN 'Hour' THEN prev_wage * 2080\n        WHEN 'Week' THEN prev_wage * 52\n        WHEN 'Month' THEN prev_wage * 12\n        WHEN 'Year' THEN prev_wage\n        ELSE NULL\n    END;\n\n    IF prevailing_annual IS NULL OR prevailing_annual = 0 THEN\n        wdr := NULL;\n    ELSE\n        wdr := ((offered_annual - prevailing_annual) / prevailing_annual) * 100;\n    END IF;\n\n    NEW.wage_tier := CASE\n        WHEN wdr IS NULL THEN NULL\n        WHEN wdr < 0 THEN 'Below-Market'\n        WHEN wdr BETWEEN 0 AND 10 THEN 'Market-Competitive'\n        ELSE 'Premium'\n    END;\n\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;", "DROP TRIGGER IF EXISTS wage_competitiveness_trigger ON prevailing_wage;", "CREATE TRIGGER wage_competitiveness_trigger\nBEFORE INSERT OR UPDATE ON prevailing_wage\nFOR EACH ROW\nEXECUTE FUNCTION update_wage_competitiveness();"], "external_knowledge": [11, 48], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'prevailing_wage' AND column_name = 'wage_tier')\")\n    column_exists = cursor.fetchone()[0]\n    assert column_exists, \"The wage_tier column was not added to the prevailing_wage table\"\n    cursor.execute(\"SELECT EXISTS (SELECT 1 FROM pg_proc WHERE proname = 'update_wage_competitiveness')\")\n    function_exists = cursor.fetchone()[0]\n    assert function_exists, \"The update_wage_competitiveness function was not created\"\n    cursor.execute(\"SELECT EXISTS (SELECT 1 FROM pg_trigger WHERE tgname = 'wage_competitiveness_trigger')\")\n    trigger_exists = cursor.fetchone()[0]\n    assert trigger_exists, \"The wage_competitiveness_trigger was not created\"\n    return 1"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_M_7", "selected_database": "labor_certification_applications", "query": "I am curious about companies that regularly file visa applications throughout the year? I'm looking for these continuous filing employers. I'd like to see each employer's name, how many total applications they filed, in how many different months they submitted applications, and whether they qualify as continuous filers or not. Could you make this into a procedure where I can specify which year I want to look at? If I don't specify a year, just use the current year by default.", "normal_query": "Could you identify all continuous filing employers for the current year? I'd like to see the employer name, their total number of applications, how many months they filed in, and whether they qualify as continuous filers. Please make this as a procedure that can accept a specific year parameter, defaulting to the current year if no year is provided.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION identify_continuous_filers(analysis_year INTEGER DEFAULT EXTRACT(YEAR FROM CURRENT_DATE)::INTEGER)\nRETURNS TABLE (\n    employer_name TEXT,\n    total_applications INTEGER,\n    active_months INTEGER,\n    is_continuous_filer BOOLEAN\n)\nLANGUAGE plpgsql AS $$\nDECLARE\n    emp_record RECORD;\n    filing_months INTEGER;\nBEGIN\n    CREATE TEMP TABLE IF NOT EXISTS continuous_filers (\n        employer_name TEXT,\n        total_applications INTEGER,\n        active_months INTEGER,\n        is_continuous_filer BOOLEAN\n    ) ON COMMIT DROP;\n\n    FOR emp_record IN \n        SELECT DISTINCT homeFirm FROM cases \n        WHERE EXTRACT(YEAR FROM TO_DATE(recvDay, 'YYYY/MM/DD')) = analysis_year\n    LOOP\n        SELECT COUNT(DISTINCT EXTRACT(MONTH FROM TO_DATE(recvDay, 'YYYY/MM/DD')))\n        INTO filing_months\n        FROM cases\n        WHERE homeFirm = emp_record.homeFirm\n        AND EXTRACT(YEAR FROM TO_DATE(recvDay, 'YYYY/MM/DD')) = analysis_year;\n\n        INSERT INTO continuous_filers VALUES (\n            emp_record.homeFirm,\n            (SELECT COUNT(*) FROM cases WHERE homeFirm = emp_record.homeFirm AND EXTRACT(YEAR FROM TO_DATE(recvDay, 'YYYY/MM/DD')) = analysis_year),\n            filing_months,\n            filing_months >= 9\n        );\n    END LOOP;\n\n    RETURN QUERY SELECT * FROM continuous_filers;\nEND;\n$$;"], "external_knowledge": [58], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT EXISTS (SELECT 1 FROM pg_proc WHERE proname = 'identify_continuous_filers')\")\n    procedure_exists = cursor.fetchone()[0]\n    assert procedure_exists, \"The identify_continuous_filers function was not created\"\n    cursor.execute(\"SELECT EXTRACT(YEAR FROM TO_DATE(recvDay, 'YYYY/MM/DD')) as year FROM cases WHERE recvDay IS NOT NULL GROUP BY year ORDER BY COUNT(*) DESC LIMIT 1\")\n    test_year = cursor.fetchone()\n    if test_year:\n        test_year = int(test_year[0])\n        cursor.execute(f\"SELECT * FROM identify_continuous_filers({test_year});\")\n        results = cursor.fetchall()\n        assert results is not None and len(results) > 0, \"No results returned from identify_continuous_filers\"\n        for row in results:\n            employer_name, total_applications, active_months, is_continuous_filer = row\n            assert isinstance(employer_name, str), \"Invalid employer_name\"\n            assert isinstance(total_applications, int), \"Invalid total_applications\"\n            assert isinstance(active_months, int), \"Invalid active_months\"\n            assert isinstance(is_continuous_filer, bool), \"Invalid is_continuous_filer\"\n            assert is_continuous_filer == (active_months >= 9), \"Incorrect continuous filer classification\"\n    return 1"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_M_8", "selected_database": "labor_certification_applications", "query": "I want to make our visa database smarter by adding a complexity score for each application. Could you add a new column called 'application_complexity_score' to our cases table that starts at zero by default? Then I need you to fill it in by calculating application complexity value. Just make sure each factor adds to the score when it applies.", "normal_query": "I need to enhance our visa application database by adding and calculating the Application Complexity Score for each case in our records. Please add a new integer column called 'application_complexity_score' to the cases table with a default value of 0, then populate it based on ACS standard. All these factors should contribute to the total score when positive.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["ALTER TABLE cases ADD COLUMN IF NOT EXISTS application_complexity_score INTEGER DEFAULT 0;", "UPDATE cases SET application_complexity_score = (CASE WHEN LOWER(TRIM(h1bDep::TEXT)) = 'yes' THEN 1 ELSE 0 END) + (CASE WHEN LOWER(TRIM(willfulV::TEXT)) = 'yes' THEN 1 ELSE 0 END) + (CASE WHEN newEmp IS NOT NULL AND newEmp > 0 THEN 1 ELSE 0 END) + (CASE WHEN amendFlag IS NOT NULL AND amendFlag > 0 THEN 1 ELSE 0 END);"], "external_knowledge": [37], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute(\"\"\"\n        SELECT EXISTS (\n            SELECT 1 FROM information_schema.columns \n            WHERE table_name = 'cases' AND column_name = 'application_complexity_score'\n        )\n    \"\"\")\n    column_exists = cursor.fetchone()[0]\n    assert column_exists, \"The application_complexity_score column was not added to the cases table\"\n    cursor.execute(\"SELECT COUNT(*) FROM cases WHERE application_complexity_score IS NOT NULL\")\n    count = cursor.fetchone()[0]\n    assert count > 0, \"No cases have Application Complexity Scores\"\n    cursor.execute(\"SELECT fileKey, h1bDep, willfulV, newEmp, amendFlag, application_complexity_score FROM cases LIMIT 5\")\n    rows = cursor.fetchall()\n    for row in rows:\n        fk, h1b, wv, new_emp, amend, score = row\n        expected = 0\n        expected += 1 if h1b and h1b.strip().lower() == 'yes' else 0\n        expected += 1 if wv and wv.strip().lower() == 'yes' else 0\n        expected += 1 if new_emp and new_emp > 0 else 0\n        expected += 1 if amend and amend > 0 else 0\n        assert score == expected, f\"Mismatch in ACS for case {fk}: expected {expected}, got {score}\"\n    return 1"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_M_9", "selected_database": "labor_certification_applications", "query": "I'm trying to understand when companies are submitting their visa applications compared to when people actually start working. Could you make a function that looks at the receipt date and start date to figure out the Visa Filing Window category? The function should take in those two dates and spit out which category the application falls into. Just make sure it handles the date formats correctly since they might be in text format.", "normal_query": "I wish to categorize all visa applications based on their Visa Filing Window timing. Create a function that determines how far in advance applications were submitted before the employment start date. The function should take the receipt date and begin date as inputs and classify applications into appropriate categories. Please ensure the function handles date conversions properly and returns the categorical result as text.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION get_visa_filing_window(recv_day TEXT, begin_day TEXT)\nRETURNS TEXT AS $$\nDECLARE\n    recv_date DATE;\n    start_date DATE;\n    months_diff INTEGER;\nBEGIN\n    -- Try parsing recv_day\n    BEGIN\n        recv_date := TO_DATE(recv_day, 'YYYY/MM/DD');\n    EXCEPTION WHEN others THEN\n        recv_date := TO_DATE(recv_day, 'DD/MM/YYYY');\n    END;\n\n    -- Try parsing begin_day\n    BEGIN\n        start_date := TO_DATE(begin_day, 'YYYY/MM/DD');\n    EXCEPTION WHEN others THEN\n        start_date := TO_DATE(begin_day, 'DD/MM/YYYY');\n    END;\n\n    -- Calculate month difference\n    months_diff := (EXTRACT(YEAR FROM recv_date) - EXTRACT(YEAR FROM start_date)) * -12 +\n                   (EXTRACT(MONTH FROM start_date) - EXTRACT(MONTH FROM recv_date));\n\n    -- Return category\n    RETURN CASE\n        WHEN months_diff < 0 THEN 'After Start Date'\n        WHEN months_diff = 0 THEN 'Same Month'\n        WHEN months_diff BETWEEN 1 AND 3 THEN '1-3 Months Before'\n        WHEN months_diff BETWEEN 4 AND 6 THEN 'Optimal Window (4-6 Months)'\n        ELSE 'Early Filing (>6 Months)'\n    END;\nEND;\n$$ LANGUAGE plpgsql;"], "external_knowledge": [41], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Test that the SQL successfully creates the function for determining visa filing windows\n    cursor = conn.cursor()\n    \n    # Check if the function exists\n    cursor.execute(\"\"\"\n        SELECT EXISTS (\n            SELECT 1 FROM pg_proc \n            WHERE proname = 'get_visa_filing_window'\n        )\n    \"\"\")\n    function_exists = cursor.fetchone()[0]\n    assert function_exists, \"The get_visa_filing_window function was not created\"\n     \n    # Test with real data from the database if available\n    try:\n        cursor.execute(\"\"\"\n            SELECT \n                recvDay, \n                beginDay, \n                get_visa_filing_window(recvDay, beginDay) AS filing_window\n            FROM cases\n            WHERE recvDay IS NOT NULL AND beginDay IS NOT NULL\n            LIMIT 5\n        \"\"\")\n        sample_cases = cursor.fetchall()\n        \n        # Verify results for sample cases\n        if sample_cases:\n            for recvDay, beginDay, filing_window in sample_cases:\n                # Manually calculate expected months difference\n                cursor.execute(f\"\"\"\n                    SELECT \n                        (EXTRACT(YEAR FROM TO_DATE('{beginDay}', 'DD/MM/YYYY')) - \n                         EXTRACT(YEAR FROM TO_DATE('{recvDay}', 'DD/MM/YYYY'))) * 12 +\n                        (EXTRACT(MONTH FROM TO_DATE('{beginDay}', 'DD/MM/YYYY')) - \n                         EXTRACT(MONTH FROM TO_DATE('{recvDay}', 'DD/MM/YYYY')))\n                \"\"\")\n                months_diff = cursor.fetchone()[0]\n                \n                # Determine expected category\n                expected_category = ''\n                if months_diff < 0:\n                    expected_category = 'After Start Date'\n                elif months_diff == 0:\n                    expected_category = 'Same Month'\n                elif 1 <= months_diff <= 3:\n                    expected_category = '1-3 Months Before'\n                elif 4 <= months_diff <= 6:\n                    expected_category = 'Optimal Window (4-6 Months)'\n                else:\n                    expected_category = 'Early Filing (>6 Months)'\n                \n                assert filing_window == expected_category, f\"Incorrect filing window for case with recv_day={recvDay}, begin_day={beginDay}\"\n    except Exception as e:\n        # This is just an additional test, so we shouldn't fail if date formats in real data are different\n        pass\n        \n    return 1\n\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "labor_certification_applications_M_10", "selected_database": "labor_certification_applications", "query": "I’d like to know how good different companies are at keeping people in their jobs. Use whatever job history information we have to figure this out, but skip companies where there’s no relevant data.", "normal_query": "We want to enhance our employer records by calculating the Retention Rate, showing the percentage of how often employers keep their workers. This should be based on available employment signals and should only apply to employers that have relevant job history data.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["ALTER TABLE employer ADD COLUMN IF NOT EXISTS retention_rate NUMERIC DEFAULT 0;", "UPDATE employer e SET retention_rate = COALESCE(sub.retention_rate, 0) FROM ( SELECT LOWER(TRIM(homeFirm)) AS norm_firm, (SUM(COALESCE(contEmp, 0))::DECIMAL / NULLIF(COUNT(*), 0)) * 100 AS retention_rate FROM cases WHERE homeFirm IS NOT NULL GROUP BY LOWER(TRIM(homeFirm)) ) sub WHERE LOWER(TRIM(e.corpHandle)) = sub.norm_firm;"], "external_knowledge": [25], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n\n    # Check column exists\n    cursor.execute(\"\"\"\n        SELECT EXISTS (\n            SELECT 1 FROM information_schema.columns \n            WHERE table_name = 'employer' AND column_name = 'retention_rate'\n        )\n    \"\"\")\n    assert cursor.fetchone()[0], \"The retention_rate column was not added.\"\n\n    # Validate calculation for sample employers\n    cursor.execute(\"\"\"\n        SELECT corpHandle, retention_rate \n        FROM employer \n        WHERE retention_rate IS NOT NULL \n        LIMIT 10\n    \"\"\")\n    sample_employers = cursor.fetchall()\n\n    for corp_handle, stored_err in sample_employers:\n        cursor.execute(f\"\"\"\n            SELECT (SUM(COALESCE(contEmp, 0))::DECIMAL / NULLIF(COUNT(*), 0)) * 100 \n            FROM cases \n            WHERE LOWER(TRIM(homeFirm)) = LOWER(TRIM('{corp_handle.replace(\"'\", \"''\")}'))\n        \"\"\")\n        expected_err = cursor.fetchone()[0]\n        if expected_err is not None:\n            assert abs(stored_err - expected_err) < 0.01, f\"Mismatch for {corp_handle}: expected {expected_err}, got {stored_err}\"\n\n    return 1\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_1", "selected_database": "insider_trading", "query": "Give me all trades for high-risk traders, with how much they traded and their leverage. Make sure to show the biggest trades first.", "normal_query": "Show all trades for high-risk compliance cases, including trader ID, trade amount, and leverage, ordered by the trade amount from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n  t.\"TR_KEY\" as trader_id,\n  (tr.vol_day * tr.margin_pct / 100.0) AS trade_amount,\n  (tr.trade_perf ->> 'lev_ratio')::numeric AS leverage\nFROM\n  \"traders\" t\nJOIN\n  \"trade_records\" tr ON t.\"TR_KEY\" = tr.\"tr_anchor\"\nJOIN\n  \"market_conditions\" mc ON tr.\"REC_KEY\" = mc.\"REC_PIN\"\nJOIN\n  \"reg_compliance\" rc ON mc.\"REC_PIN\" = rc.\"REC_COMP\"\nWHERE\n  LOWER(rc.alert_lvl) IN ('high', 'critical')\nORDER BY trade_amount DESC;"], "external_knowledge": [0, 2, 10, 19], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "insider_trading_2", "selected_database": "insider_trading", "query": "Find me transactions that look suspiciously like insider trading. For these, calculate the Sentiment-Driven Leakage Risk. If that risk score is over 1000, show me the transaction ID, trader ID, time, the original leakage score, and the new SDLR score. Cap it at 100 results.", "normal_query": "Please identify transaction records that trigger a Potential Insider Trading Flag. For these flagged transactions, calculate their Sentiment-Driven Leakage Risk score. For transactions where this SDLR score is over 1000, please show the transaction register ID, the trader reference ID, the transaction timestamp, the original information leakage score, and the calculated SDLR score, limited to the top 100 results.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH TradeDetails AS (\n  SELECT\n    tr.\"REC_KEY\",\n    tr.\"tr_anchor\" AS trdref,\n    tr.snap_ts AS transtime,\n    ce.leak_score,\n    sa.opt_vol,\n    (sa.sentiment_data ->> 'news_score')::numeric AS news_score,\n    (sa.sentiment_data ->> 'soc_sent')::numeric AS soc_sent,\n    ce.announce_time\n  FROM\n    \"trade_records\" tr\n  JOIN\n    \"market_conditions\" mc ON tr.\"REC_KEY\" = mc.\"REC_PIN\"\n  JOIN\n    \"corporate_events\" ce ON mc.\"REC_PIN\" = ce.\"REC_EVT\"\n  JOIN\n    \"sentiment_analytics\" sa ON mc.\"REC_PIN\" = sa.\"REC_SA\"\n), InsiderFlagged AS (\n  SELECT * FROM TradeDetails\n  WHERE\n    leak_score > 50.0\n    AND LOWER(announce_time) IN ('pre-market hrs before', 'intraday hrs before')\n), CalculatedRisks AS (\n  SELECT\n    *,\n    (opt_vol * (1 + ABS(news_score - soc_sent))) AS swov\n  FROM InsiderFlagged\n), SDLR_Calculation AS (\n  SELECT\n    *,\n    (swov * leak_score) AS sdlr\n  FROM CalculatedRisks\n)\nSELECT\n  \"REC_KEY\" AS transreg,\n  trdref,\n  transtime,\n  leak_score AS infoleaksc,\n  sdlr\nFROM SDLR_Calculation\nWHERE sdlr > 1000\nLIMIT 100;"], "external_knowledge": [11, 7, 34, 51, 70], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "insider_trading_3", "selected_database": "insider_trading", "query": "Let's compare our different kinds of traders. For each type, what's their average aggression and compliance score? Show me the trader type (in lowercase), their avg aggression, and avg compliance. List the most aggressive types first.", "normal_query": "I need an analysis comparing different types of traders. For each trader type, please calculate the average Aggressive Trading Intensity and the average Compliance Health Score. Display the trader type (all in lower case), the calculated average ATI, and the average CHS. Finally, sort the results by the average ATI in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH TraderMetrics AS (\n  SELECT\n    t.\"TR_KEY\",\n    LOWER(t.\"typeFlag\") AS typeFlag_lower,\n    CAST(REPLACE(tr.bal_turnover, ' times/day', '') AS numeric) AS dtr,\n    (tr.trade_perf ->> 'lev_ratio')::numeric AS tle,\n    (ob.order_metrics ->> 'mod_freq')::numeric / NULLIF(1 - (ob.order_metrics ->> 'cancel_pct')::numeric, 0) AS omi,\n    (rc.compliance_data ->> 'prev_viol')::numeric / GREATEST(1, (t.trader_fin_data ->> 'age_days')::numeric / 365.0) AS crs,\n    (rc.compliance_data ->> 'comp_rate') AS comp_rate\n  FROM \"traders\" t\n  JOIN \"trade_records\" tr ON t.\"TR_KEY\" = tr.\"tr_anchor\"\n  JOIN \"market_conditions\" mc ON tr.\"REC_KEY\" = mc.\"REC_PIN\"\n  JOIN \"order_behaviour\" ob ON mc.\"REC_PIN\" = ob.\"REC_NODE\"\n  JOIN \"reg_compliance\" rc ON mc.\"REC_PIN\" = rc.\"REC_COMP\"\n  WHERE t.\"typeFlag\" IS NOT NULL\n), CombinedScores AS (\n  SELECT\n    \"TR_KEY\",\n    typeFlag_lower,\n    (dtr * tle * COALESCE(omi, 0)) AS ati,\n    1.0 / (1.0 + COALESCE(crs, 0) * COALESCE(CASE LOWER(comp_rate) WHEN 'a' THEN 1 WHEN 'b' THEN 2 WHEN 'c' THEN 3 WHEN 'd' THEN 4 ELSE 1 END, 1)) AS chs\n  FROM TraderMetrics\n)\nSELECT\n  typeFlag_lower AS tradekind,\n  AVG(ati) AS average_ati,\n  AVG(chs) AS average_chs\nFROM CombinedScores\nGROUP BY typeFlag_lower\nORDER BY average_ati DESC\nLIMIT 100;"], "external_knowledge": [0, 1, 2, 32, 36, 5, 71], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "insider_trading_4", "selected_database": "insider_trading", "query": "Some traders seem to just copy others in their network. Find compliance cases for this behavior, then figure out an 'investigation intensity' score for them. Give me the top 100, sorted by that score, showing the case ID and the score.", "normal_query": "Please identify all compliance cases associated with traders flagged for Networked Mimicry Risk. For each of these specific cases, calculate the Investigation Intensity Index (III). List the compliance case registration ID and its corresponding Investigation Intensity Index (III). Finally, sort the results by the Investigation Intensity Index in descending order and show only the top 100 cases.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH PatternAnomalyScore AS (\n  SELECT\n    tr.\"tr_anchor\" AS \"TR_KEY\",\n    tr.\"REC_KEY\",\n    ABS(rc.pat_rec - (mc.market_metrics ->> 'peer_corr')::numeric) AS pas\n  FROM \"trade_records\" tr\n  JOIN \"market_conditions\" mc ON tr.\"REC_KEY\" = mc.\"REC_PIN\"\n  JOIN \"reg_compliance\" rc ON mc.\"REC_PIN\" = rc.\"REC_COMP\"\n  WHERE rc.pat_rec IS NOT NULL AND (mc.market_metrics ->> 'peer_corr') IS NOT NULL\n),\nPeerMimicryCases AS (\n  SELECT\n    pas.\"REC_KEY\"\n  FROM PatternAnomalyScore pas\n  JOIN \"market_conditions\" mc ON pas.\"REC_KEY\" = mc.\"REC_PIN\"\n  WHERE\n    (mc.market_metrics ->> 'peer_corr')::numeric > 0.7\n    AND pas.pas < 1.0\n),\nCollusionNetworkCases AS (\n  SELECT\n    tr.\"REC_KEY\"\n  FROM \"trade_records\" tr\n  JOIN \"trader_relationships\" rel ON tr.\"tr_anchor\" = rel.rel_root\n  WHERE\n    rel.circ_size > 5\n    AND (rel.trader_links ->> 'grp_score')::numeric > 0.6\n    AND LOWER(rel.\"commPath\") = 'regular'\n)\nSELECT\n  rc.\"REC_COMP\" as compreg,\n  ((0.6 * rc.behav_score) + (0.4 * rc.net_score)) AS iii\nFROM \"reg_compliance\" rc\nWHERE\n  rc.\"REC_COMP\" IN (SELECT \"REC_KEY\" FROM PeerMimicryCases)\n  AND rc.\"REC_COMP\" IN (SELECT \"REC_KEY\" FROM CollusionNetworkCases)\nORDER BY iii DESC\nLIMIT 100;"], "external_knowledge": [4, 49, 13, 62, 6], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "insider_trading_5", "selected_database": "insider_trading", "query": "Let's find our riskiest manipulators. I mean traders who are either high-frequency with high risk-leverage, or have been confirmed for layering. For that specific group, what's their average 'uniqueness' score? I just need the single number.", "normal_query": "First, identify all traders who qualify as High-Risk Manipulator Candidates. Then, for this specific group of traders, calculate the average Unique Pattern Deviation Ratio based on their transaction history. Please provide only this single average value.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH OrderModificationIntensity AS (\n  SELECT\n    ob.\"REC_NODE\",\n    (ob.order_metrics ->> 'mod_freq')::numeric / NULLIF(1 - (ob.order_metrics ->> 'cancel_pct')::numeric, 0) AS omi\n  FROM \"order_behaviour\" ob\n  WHERE (ob.order_metrics ->> 'cancel_pct') IS NOT NULL AND (ob.order_metrics ->> 'mod_freq') IS NOT NULL\n),\nHighRiskTraders AS (\n  SELECT DISTINCT tr.\"tr_anchor\" AS \"TR_KEY\"\n  FROM \"trade_records\" tr\n  JOIN \"traders\" t ON tr.\"tr_anchor\" = t.\"TR_KEY\"\n  WHERE ((tr.trade_perf ->> 'lev_ratio')::numeric > 5.0 AND LOWER(t.trader_fin_data ->> 'risk_lvl') = 'aggressive')\n     OR (CAST(regexp_replace(tr.bal_turnover, ' .*', '') AS numeric) > 0.5)\n),\nManipulationPatternTraders AS (\n  SELECT DISTINCT tr.\"tr_anchor\" AS \"TR_KEY\"\n  FROM \"trade_records\" tr\n  JOIN \"market_conditions\" mc ON tr.\"REC_KEY\" = mc.\"REC_PIN\"\n  JOIN \"manipulation_signals\" ms ON mc.\"REC_PIN\" = ms.\"REC_TAG\"\n  LEFT JOIN OrderModificationIntensity omi ON mc.\"REC_PIN\" = omi.\"REC_NODE\"\n  WHERE LOWER(ms.layer_idx) = 'confirmed'\n     OR ((REPLACE(ms.manip_signals ->> 'spoof_prob', '%', '')::numeric) > 75 AND omi.omi > 1.0)\n),\nHighRiskManipulatorCandidates AS (\n  SELECT \"TR_KEY\" FROM HighRiskTraders\n  INTERSECT\n  SELECT \"TR_KEY\" FROM ManipulationPatternTraders\n),\nUPDR_Scores AS (\n  SELECT\n    ABS(rc.pat_rec - (mc.market_metrics ->> 'peer_corr')::numeric) / GREATEST(0.01, rc.pat_rec) AS updr\n  FROM \"trade_records\" tr\n  JOIN \"market_conditions\" mc ON tr.\"REC_KEY\" = mc.\"REC_PIN\"\n  JOIN \"reg_compliance\" rc ON mc.\"REC_PIN\" = rc.\"REC_COMP\"\n  WHERE tr.\"tr_anchor\" IN (SELECT \"TR_KEY\" FROM HighRiskManipulatorCandidates)\n    AND rc.pat_rec IS NOT NULL AND (mc.market_metrics ->> 'peer_corr') IS NOT NULL\n)\nSELECT AVG(updr) FROM UPDR_Scores;"], "external_knowledge": [0, 1, 2, 4, 10, 12, 52, 60, 28], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "insider_trading_6", "selected_database": "insider_trading", "query": "For our most intense insider trading investigations, what are the usual penalties? Tally them up and show me the list, from most common to least.", "normal_query": "I want to analyze the enforcement outcomes specifically for cases flagged as High-Intensity Insider Investigations. Could you provide a frequency count for each type of Penalty Imposed that resulted from these investigations? Please list the penalty types and their corresponding frequencies, ordered from the most frequent penalty to the least frequent.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH InsiderTradingCases AS (\n  SELECT\n    ce.\"REC_EVT\" AS case_id\n  FROM \"corporate_events\" ce\n  WHERE\n    ce.leak_score > 50.0\n    AND LOWER(ce.announce_time) IN ('pre-market hrs before', 'intraday hrs before')\n),\nHighIntensityCases AS (\n  SELECT\n    rc.\"REC_COMP\" AS case_id\n  FROM \"reg_compliance\" rc\n  WHERE\n    rc.\"REC_COMP\" IN (SELECT case_id FROM InsiderTradingCases)\n    AND (0.6 * rc.behav_score + 0.4 * rc.net_score) > 70\n)\nSELECT\n  LOWER(ea.enf_actions ->> 'pen_flag') as penimp,\n  COUNT(*) AS frequency\nFROM \"enforcement_actions\" ea\nWHERE ea.\"REC_ENF\" IN (SELECT case_id FROM HighIntensityCases)\nAND ea.enf_actions ->> 'pen_flag' IS NOT NULL\nGROUP BY penimp\nORDER BY frequency DESC\nLIMIT 100;"], "external_knowledge": [11, 6, 67], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "insider_trading_7", "selected_database": "insider_trading", "query": "Are the 'copycat' traders any good? Let's see. Compare their average risk-adjusted win rate to the traders who act independently.", "normal_query": "I want to compare the performance of traders potentially involved in Peer Mimicry Suspicion versus other traders. Please calculate the average Risk-Adjusted Win Rate for these two groups. Display a boolean indicating if the group represents Peer Mimicry Suspicion (True) or not (False), and the corresponding average RAWR for that group.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH TraderMetrics AS (\n  SELECT\n    (tr.trade_perf ->> 'win_pct')::numeric / GREATEST(1, (tr.trade_perf ->> 'lev_ratio')::numeric) AS rawr,\n    (mc.market_metrics ->> 'peer_corr')::numeric AS peer_corr,\n    ABS(rc.pat_rec - (mc.market_metrics ->> 'peer_corr')::numeric) AS pas\n  FROM \"trade_records\" tr\n  JOIN \"market_conditions\" mc ON tr.\"REC_KEY\" = mc.\"REC_PIN\"\n  JOIN \"reg_compliance\" rc ON mc.\"REC_PIN\" = rc.\"REC_COMP\"\n  WHERE (tr.trade_perf ->> 'win_pct') IS NOT NULL \n    AND (tr.trade_perf ->> 'lev_ratio') IS NOT NULL\n    AND (mc.market_metrics ->> 'peer_corr') IS NOT NULL\n    AND rc.pat_rec IS NOT NULL\n)\nSELECT\n  (CASE WHEN peer_corr > 0.7 AND pas < 0.1 THEN TRUE ELSE FALSE END) AS is_mimicry_suspect_group,\n  AVG(rawr) AS average_rawr\nFROM TraderMetrics\nGROUP BY is_mimicry_suspect_group;"], "external_knowledge": [2, 59, 49, 4], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "insider_trading_8", "selected_database": "insider_trading", "query": "For traders who speculate on volatile events, what's their average Order Modification Intensity? Just give me the one number.", "normal_query": "I need to analyze the order modification behavior of a specific trader group. Please identify all traders classified as Volatile Event Speculators. Then, calculate the average Order Modification Intensity across all transactions associated with this group. Provide just the calculated average OMI.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH TraderEventCounts AS (\n  SELECT\n    tr.tr_anchor AS \"TR_KEY\",\n    COUNT(tr.\"REC_KEY\") AS total_trades,\n    COUNT(ce.\"REC_EVT\") AS event_trades\n  FROM \"trade_records\" tr\n  LEFT JOIN \"market_conditions\" mc ON tr.\"REC_KEY\" = mc.\"REC_PIN\"\n  LEFT JOIN \"corporate_events\" ce ON mc.\"REC_PIN\" = ce.\"REC_EVT\"\n  GROUP BY tr.tr_anchor\n),\nEventDrivenTraders AS (\n  SELECT \"TR_KEY\"\n  FROM TraderEventCounts\n  WHERE (event_trades::numeric / total_trades) > 0.3\n),\nAggressiveEventSpeculators AS (\n  SELECT\n    t.\"TR_KEY\"\n  FROM \"traders\" t\n  WHERE t.\"TR_KEY\" IN (SELECT \"TR_KEY\" FROM EventDrivenTraders)\n    AND LOWER(t.trader_fin_data ->> 'risk_lvl') = 'aggressive'\n),\nVolatileSpeculators AS (\n  SELECT DISTINCT\n    tr.tr_anchor AS \"TR_KEY\"\n  FROM \"trade_records\" tr\n  JOIN \"market_conditions\" mc ON tr.\"REC_KEY\" = mc.\"REC_PIN\"\n  JOIN \"sentiment_analytics\" sa ON mc.\"REC_PIN\" = sa.\"REC_SA\"\n  WHERE tr.tr_anchor IN (SELECT \"TR_KEY\" FROM AggressiveEventSpeculators)\n    AND ABS((sa.sentiment_data ->> 'news_score')::numeric - (sa.sentiment_data ->> 'soc_sent')::numeric) > 1.0\n)\nSELECT\n  AVG(\n    (ob.order_metrics ->> 'mod_freq')::numeric / NULLIF(1 - (ob.order_metrics ->> 'cancel_pct')::numeric, 0)\n  ) AS average_omi_for_volatile_speculators\nFROM \"order_behaviour\" ob\nJOIN \"trade_records\" tr ON ob.\"REC_NODE\" = tr.\"REC_KEY\"\nWHERE tr.\"tr_anchor\" IN (SELECT \"TR_KEY\" FROM VolatileSpeculators);"], "external_knowledge": [17, 46, 7, 64, 1], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": false}}
{"instance_id": "insider_trading_9", "selected_database": "insider_trading", "query": "I want to see the cases for high-frequency trades that resulted in a big fine over $100,000. If a trading restriction was applied in those cases, show me the case ID and the exact restriction type.", "normal_query": "List all enforcement actions for cases involving high-frequency trades where the penalty amount exceeded $100,000. Only include cases where a trading restriction was applied, and show the enforcement ID and the specific trading restriction period type.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n  ea.\"REC_ENF\" AS enforcereg,\n  ea.\"biz_restrict\" AS traderestr\nFROM \"enforcement_actions\" ea\nJOIN \"market_conditions\" mc ON ea.\"REC_ENF\" = mc.\"REC_PIN\"\nJOIN \"trade_records\" tr ON mc.\"REC_PIN\" = tr.\"REC_KEY\"\nWHERE\n  LOWER(tr.freq_tag) = 'high'\n  AND (ea.enf_actions ->> 'pen_amt')::numeric > 100000\n  AND ea.\"biz_restrict\" IS NOT NULL;"], "external_knowledge": [0, 2, 10, 40, 9, 19, 48, 69], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_10", "selected_database": "insider_trading", "query": "What's the difference in average aggression score between trades with 'confirmed' vs 'suspected' layering? Show me the comparison.", "normal_query": "I need to compare the average Aggressive Suspicion Score between transactions where the layering index is 'Confirmed' and those where it is 'Suspected'. Please calculate the average ASS for each of these two groups. Display the layering status (in lower case) and the corresponding average ASS.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH TradeMetrics AS (\n  SELECT\n    ms.\"layer_idx\",\n    tr.\"vol_day\",\n    t.\"trader_fin_data\" ->> 'usd_bal' AS account_balance,\n    tr.\"trade_perf\" ->> 'lev_ratio' AS leverage_ratio,\n    ob.\"order_metrics\" ->> 'mod_freq' AS mod_frequency,\n    ob.\"order_metrics\" ->> 'cancel_pct' AS cancel_percentage,\n    ms.\"manip_signals\" ->> 'spoof_prob' AS spoofing_probability,\n    rc.\"behav_score\"\n  FROM \"manipulation_signals\" AS ms\n  JOIN \"market_conditions\" AS mc ON ms.\"REC_TAG\" = mc.\"REC_PIN\"\n  JOIN \"reg_compliance\" AS rc ON mc.\"REC_PIN\" = rc.\"REC_COMP\"\n  JOIN \"trade_records\" AS tr ON mc.\"REC_PIN\" = tr.\"REC_KEY\"\n  JOIN \"traders\" AS t ON tr.\"tr_anchor\" = t.\"TR_KEY\"\n  JOIN \"order_behaviour\" AS ob ON mc.\"REC_PIN\" = ob.\"REC_NODE\"\n  WHERE LOWER(ms.\"layer_idx\") IN ('confirmed', 'suspected')\n),\nCalculatedASS AS (\n  SELECT\n    LOWER(\"layer_idx\") AS layer_indicator,\n    (\"vol_day\" / NULLIF(CAST(REPLACE(REPLACE(account_balance, '$', ''), ',', '') AS numeric), 0)) AS dtr,\n    CAST(leverage_ratio AS numeric) AS tle,\n    (CAST(mod_frequency AS numeric) / NULLIF(1 - CAST(cancel_percentage AS numeric), 0)) AS omi,\n    ((CAST(REPLACE(spoofing_probability, '%', '') AS numeric) / 100) * \"behav_score\") AS sai_proxy\n  FROM TradeMetrics\n)\nSELECT\n  layer_indicator,\n  AVG((dtr * tle * omi) * sai_proxy) AS average_ass\nFROM CalculatedASS\nGROUP BY layer_indicator;"], "external_knowledge": [0, 2, 1, 36, 3, 54], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 3, "distinct": false, "order": false}}
{"instance_id": "insider_trading_11", "selected_database": "insider_trading", "query": "Give me the stats on how much trader 'TR94368' messes with their orders. I need a single row showing the trade count, and the min, avg, median, and max OMI.", "normal_query": "For the trader with ID 'TR94368', calculate the distribution statistics for their Order Modification Intensity (OMI) based on all their valid transactions. Please return a single row containing the trader's ID, the total count of transactions considered, and the minimum, average, median, and maximum OMI.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH TraderOMI AS (\n    SELECT\n        (ob.order_metrics ->> 'mod_freq')::numeric / NULLIF(1 - (ob.order_metrics ->> 'cancel_pct')::numeric, 0) AS omi_value\n    FROM \"order_behaviour\" ob\n    JOIN \"trade_records\" tr ON ob.\"REC_NODE\" = tr.\"REC_KEY\"\n    WHERE tr.\"tr_anchor\" = 'TR94368'\n      AND (ob.order_metrics ->> 'cancel_pct') IS NOT NULL\n)\nSELECT\n    'TR94368' AS trader_id,\n    COUNT(omi_value)::bigint AS transaction_count,\n    MIN(omi_value) AS min_omi,\n    AVG(omi_value) AS avg_omi,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY omi_value) AS median_omi,\n    MAX(omi_value) AS max_omi\nFROM TraderOMI\nWHERE omi_value IS NOT NULL;"], "external_knowledge": [1], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_12", "selected_database": "insider_trading", "query": "Time to bulk update risk scores. For all 2024+ cases, recalculate the behavioral risk score with the new suspicious activity index. Cap the score at 100, and use a temp table called `score_updates` to do the update.", "normal_query": "Please create a temporary table named `score_updates` containing new `behav_score` values for all cases with transactions from 2024 onwards. Calculate the new scores using the suspicious activity index, ensuring all calculation components use double precision and the final result is capped at 100. Then, use this temporary table to update the `reg_compliance` table.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["DROP TABLE IF EXISTS score_updates;", "CREATE TEMPORARY TABLE score_updates AS\nSELECT\n  rc.\"REC_COMP\",\n  (LEAST(100::double precision, rc.behav_score::double precision + (\n      0.3::double precision * (REPLACE(ms.manip_signals ->> 'spoof_prob', '%', '')::double precision / 100::double precision) +\n      0.2::double precision * (CASE LOWER(ms.layer_idx) WHEN 'confirmed' THEN 1::double precision ELSE 0::double precision END) +\n      0.1::double precision * ms.stuff_idx::double precision\n  )))::real AS new_score\nFROM \"reg_compliance\" rc\nJOIN \"market_conditions\" mc ON rc.\"REC_COMP\" = mc.\"REC_PIN\"\nJOIN \"trade_records\" tr ON mc.\"REC_PIN\" = tr.\"REC_KEY\"\nJOIN \"manipulation_signals\" ms ON mc.\"REC_PIN\" = ms.\"REC_TAG\"\nWHERE tr.\"snap_ts\" >= '2024-01-01';", "UPDATE \"reg_compliance\" rc\nSET\n    behav_score = su.new_score\nFROM score_updates su\nWHERE rc.\"REC_COMP\" = su.\"REC_COMP\";"], "external_knowledge": [3], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    import psycopg2\n    # This test case verifies that the UPDATE statement correctly applies the calculation.\n    # It avoids floating point discrepancies by having the DB calculate both the expected and actual values.\n    \n    # Find a target record to test the update on\n    find_query = \"\"\"SELECT rc.\"REC_COMP\" \n                   FROM \"reg_compliance\" rc \n                   JOIN \"market_conditions\" mc ON rc.\"REC_COMP\" = mc.\"REC_PIN\"\n                   JOIN \"trade_records\" tr ON mc.\"REC_PIN\" = tr.\"REC_KEY\"\n                   JOIN \"manipulation_signals\" ms ON mc.\"REC_PIN\" = ms.\"REC_TAG\"\n                   WHERE tr.\"snap_ts\" >= '2024-01-01' AND ms.stuff_idx IS NOT NULL LIMIT 1;\"\"\"\n    res, _, _ = execute_queries([find_query], db_name, conn)\n    if not res:\n        print(\"Test case skipped: No records found to update.\")\n        return True\n    rec_comp = res[0][0]\n    \n    # The solution is a multi-step process. First, calculate the expected score.\n    # We use the logic from the CREATE TABLE statement (sol_sqls[1]) to find the expected score.\n    create_table_statement = sol_sqls[1]\n    select_part = create_table_statement.split('FROM \"reg_compliance\"')[0].replace('CREATE TEMPORARY TABLE score_updates AS', '') + 'FROM \"reg_compliance\"' + create_table_statement.split('FROM \"reg_compliance\"')[1]\n    \n    expected_score_query = f\"{select_part.strip().rstrip(';')} AND rc.\\\"REC_COMP\\\" = '{rec_comp}';\"\n    expected_res, _, _ = execute_queries([expected_score_query], db_name, conn)\n    expected_score = float(expected_res[0][1]) # The new score is the second column\n\n    # Execute the actual queries from the predicted SQL to modify the table.\n    execute_queries(pred_sqls, db_name, conn)\n\n    # Verify the new score that was actually written to the database.\n    verify_query = f\"SELECT behav_score FROM \\\"reg_compliance\\\" WHERE \\\"REC_COMP\\\" = '{rec_comp}';\"\n    final_res, _, _ = execute_queries([verify_query], db_name, conn)\n    new_score_in_db = float(final_res[0][0])\n    \n    assert abs(new_score_in_db - expected_score) < 1e-7, f\"Update failed for {rec_comp}. Expected: {expected_score}, Actual: {new_score_in_db}\"\n    \n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_13", "selected_database": "insider_trading", "query": "Find trades from after-hours where the news leaked early and the price got choppy. Specifically, find trades near a 'post-market' announcement with an info leak rate over 0.8 and price acceleration over 3.0. Show me the trade ID, leak rate, and price acceleration.", "normal_query": "Find all trade records (`REC_KEY`) that occurred in proximity to a corporate event with an `announce_time` of 'Post-market hrs before' and an `info_leak_rate` greater than 0.8 score/hour. Additionally, these trades must have a corresponding `price_accel` value greater than 3.0 %/(hour²). List the record key, the info leak rate, and the price acceleration.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n  ce.\"REC_EVT\" AS \"REC_KEY\",\n  ce.info_leak_rate,\n  mc.price_accel\nFROM \"corporate_events\" ce\nJOIN \"market_conditions\" mc ON ce.\"REC_EVT\" = mc.\"REC_PIN\"\nWHERE\n  LOWER(ce.announce_time) = 'post-market hrs before'\n  AND CAST(REPLACE(ce.info_leak_rate, ' score/hour', '') AS numeric) > 0.8\n  AND CAST(REPLACE(REPLACE(mc.price_accel, ' %/(hour_)', ''), '%/(hour²)', '') AS numeric) > 3.0;"], "external_knowledge": [], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_14", "selected_database": "insider_trading", "query": "Find trades where liquidity impact was over 80k and there was a 'Strong' ignite signal. Show me the trade ID, liquidity impact, the signal, and the trader's type in lowercase.", "normal_query": "I need to identify trade records where the market experienced a high liquidity impact, defined as `liq_imp` greater than 80,000 USD/min, and where a 'Strong' `ignite_sig` was detected in the manipulation signals. For each of these records, please list the record key (`REC_TAG`), the liquidity impact, the ignite signal, and the associated trader's `typeFlag` (in lower case).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n  ms.\"REC_TAG\",\n  mc.liq_imp,\n  ms.ignite_sig,\n  LOWER(t.\"typeFlag\")\nFROM \"manipulation_signals\" ms\nJOIN \"market_conditions\" mc ON ms.\"REC_TAG\" = mc.\"REC_PIN\"\nJOIN \"trade_records\" tr ON ms.\"REC_TAG\" = tr.\"REC_KEY\"\nJOIN \"traders\" t ON tr.\"tr_anchor\" = t.\"TR_KEY\"\nWHERE\n  CAST(REPLACE(REPLACE(mc.liq_imp, ' USD/min', ''), ',', '') AS numeric) > 80000\n  AND LOWER(ms.ignite_sig) = 'strong';"], "external_knowledge": [], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_15", "selected_database": "insider_trading", "query": "Are our most-connected institutional traders also the least compliant? For every institution, show their ID, their compliance health score, and their network strength. Sort by the worst compliance score first.", "normal_query": "For all traders with a `typeFlag` as 'Institution', calculate two metrics: their Compliance Health Score (CHS) and their Insider Network Strength. Please display the trader's key (`TR_KEY`), the calculated CHS, and the `insider_net_str`. Sort the results by CHS in ascending order (worst first).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH InstMetrics AS (\n    SELECT \n        t.\"TR_KEY\",\n        1.0 / (1.0 + COALESCE( (rc.compliance_data ->> 'prev_viol')::numeric / GREATEST(1, (t.trader_fin_data ->> 'age_days')::numeric / 365.0), 0) * \n        COALESCE(CASE LOWER(rc.compliance_data ->> 'comp_rate') WHEN 'a' THEN 1 WHEN 'b' THEN 2 WHEN 'c' THEN 3 WHEN 'd' THEN 4 ELSE 1 END, 1)) AS chs,\n        trel.insider_net_str\n    FROM \"traders\" t\n    LEFT JOIN \"trade_records\" tr ON t.\"TR_KEY\" = tr.\"tr_anchor\"\n    LEFT JOIN \"market_conditions\" mc ON tr.\"REC_KEY\" = mc.\"REC_PIN\"\n    LEFT JOIN \"reg_compliance\" rc ON mc.\"REC_PIN\" = rc.\"REC_COMP\"\n    LEFT JOIN \"trader_relationships\" trel ON t.\"TR_KEY\" = trel.rel_root\n    WHERE LOWER(t.\"typeFlag\") = 'institution'\n)\nSELECT\n    \"TR_KEY\",\n    chs AS compliance_health_score,\n    insider_net_str\nFROM InstMetrics\nWHERE chs IS NOT NULL AND insider_net_str IS NOT NULL\nGROUP BY \"TR_KEY\", chs, insider_net_str\nORDER BY compliance_health_score ASC;"], "external_knowledge": [32, 5, 71], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "insider_trading_16", "selected_database": "insider_trading", "query": "Find our cowboys—traders who are aggressive with over 5x leverage, or who trade over half their balance daily. List their key and type.", "normal_query": "Identify all high-risk traders. A trader is considered high-risk if their leverage exposure is over 5.0 and their risk level is 'Aggressive', or if their daily turnover rate is over 0.5. Display their trader key and type flag.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT DISTINCT\n    T1.\"TR_KEY\",\n    T1.\"typeFlag\"\nFROM\n    traders AS T1\nJOIN\n    trade_records AS T2 ON T1.\"TR_KEY\" = T2.tr_anchor\nWHERE\n    (\n        (T2.trade_perf ->> 'lev_ratio')::numeric > 5.0\n        AND\n        LOWER(T1.trader_fin_data ->> 'risk_lvl') = 'aggressive'\n    )\n    OR\n    (\n        CAST(regexp_replace(T2.bal_turnover, ' .*', '') AS REAL) > 0.5\n    );"], "external_knowledge": [], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": true, "order": false}}
{"instance_id": "insider_trading_17", "selected_database": "insider_trading", "query": "Find trades that might be insider trading. The flag goes up if leak score is > 50, it's near a corporate event, AND the announcement was pre-market or intraday. Show the record key and trader ID.", "normal_query": "Find trades flagged for potential insider trading. A flag is raised if the information leakage score is over 50, there is an upcoming corporate event, and the announcement time is 'Pre-market hrs before' or 'Intraday hrs before'. Return the record key and trader anchor.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n      T1.\"REC_KEY\",\n      T1.tr_anchor\n    FROM \"trade_records\" AS T1\n    JOIN \"market_conditions\" AS mc ON T1.\"REC_KEY\" = mc.\"REC_PIN\"\n    JOIN \"corporate_events\" AS T2 ON mc.\"REC_PIN\" = T2.\"REC_EVT\"\n    WHERE\n      T2.leak_score > 50.0 AND T2.evt_near IS NOT NULL AND (\n        LOWER(T2.announce_time) = 'pre-market hrs before'\n        OR LOWER(T2.announce_time) = 'intraday hrs before'\n      );"], "external_knowledge": [11], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_18", "selected_database": "insider_trading", "query": "Find trades that look like layering or spoofing. It's a match if layering is 'Confirmed', OR if spoofing probability is > 75% AND order modification intensity is > 1.0. Just show the record tags.", "normal_query": "Identify trades indicating a layering or spoofing manipulation pattern. A trade is suspect if its layering index is 'Confirmed', or if its spoofing probability is over 75% and its Order Modification Intensity is above 1.0. Display the record tag for each matching trade.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH omi_calc AS (\n      SELECT\n        \"REC_NODE\",\n        (\n          (order_metrics ->> 'mod_freq')::real / NULLIF(1 - (order_metrics ->> 'cancel_pct')::real, 0)\n        ) AS omi\n      FROM \"order_behaviour\"\n      WHERE order_metrics ->> 'cancel_pct' IS NOT NULL\n      AND order_metrics ->> 'mod_freq' IS NOT NULL\n)\nSELECT\n  T1.\"REC_TAG\"\nFROM \"manipulation_signals\" AS T1\nLEFT JOIN omi_calc\n  ON T1.\"REC_TAG\" = omi_calc.\"REC_NODE\"\nWHERE\n  LOWER(T1.layer_idx) = 'confirmed' OR (\n    (REPLACE(T1.manip_signals ->> 'spoof_prob', '%', '')::real) > 75\n    AND omi_calc.omi > 1.0\n);"], "external_knowledge": [12, 1], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_19", "selected_database": "insider_trading", "query": "Find any cozy trader networks. The ones I want have a circle size > 5, a group score > 60, and use a 'Regular' communication path. Just show the root trader's key.", "normal_query": "Identify potential collusion networks. A network is flagged if its relationship circle size is greater than 5, its group score is over 60, and its communication path is 'Regular'. Show the root trader key for each flagged network.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT rel_root\nFROM \"trader_relationships\"\nWHERE\n  circ_size > 5\n  AND (trader_links ->> 'grp_score')::real > 60\n  AND LOWER(\"commPath\") = 'regular';"], "external_knowledge": [13], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_20", "selected_database": "insider_trading", "query": "Show me the hot cases. I want records with 'High' or 'Critical' alerts, 'High' investigation priority, and 'Intensive' monitoring. Just the record keys.", "normal_query": "Find compliance records under 'Elevated Regulatory Scrutiny'. This status applies when the alert level is 'High' or 'Critical', investigation priority is 'High', and monitoring is 'Intensive'. Return the compliance record keys.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \"REC_COMP\"\nFROM \"reg_compliance\"\nWHERE\n  LOWER(alert_lvl) IN ('high', 'critical')\n  AND LOWER(invest_prior) = 'high'\n  AND LOWER(mon_inten) = 'intensive';"], "external_knowledge": [14], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_M_1", "selected_database": "insider_trading", "query": "Make a reusable list called `high_risk_trader_view` for our high-risk traders. For each, I need their ID, type, balance, daily volume, DTR, TLE, and risk level text.", "normal_query": "Please create a reusable view named high_risk_trader_view that identifies traders fitting the High-Risk Trader Profile. For each trader identified, the view should show their registration ID (tradereg), trader kind (tradekind), account balance (acctbal), daily volume (voldaily), their calculated Daily Turnover Rate (DTR), their extracted Trader Leverage Exposure (TLE), and the text description of their risk level (risk_level_text) from their performance data.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["DROP VIEW IF EXISTS high_risk_trader_view;", "CREATE VIEW high_risk_trader_view AS\nSELECT\n    t.\"TR_KEY\" AS tradereg,\n    t.\"typeFlag\" AS tradekind,\n    (t.trader_fin_data ->> 'usd_bal') AS acctbal,\n    tr.vol_day AS voldaily,\n    tr.bal_turnover AS DTR,\n    (tr.trade_perf ->> 'lev_ratio')::numeric AS TLE,\n    (t.trader_fin_data ->> 'risk_lvl') AS risk_level\nFROM\n    \"traders\" t\nJOIN\n    \"trade_records\" tr ON t.\"TR_KEY\" = tr.\"tr_anchor\"\nWHERE\n    ((tr.trade_perf ->> 'lev_ratio')::numeric > 5.0 AND LOWER(t.trader_fin_data ->> 'risk_lvl') = 'aggressive')\n    OR\n    (CAST(regexp_replace(tr.bal_turnover, ' .*', '') AS numeric) > 0.5);"], "external_knowledge": [10, 0, 2], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # 1. Find a trader that should be in the view\n    find_query = \"\"\"SELECT t.\"TR_KEY\" FROM \"traders\" t JOIN \"trade_records\" tr ON t.\"TR_KEY\" = tr.\"tr_anchor\" \n                        WHERE ((tr.trade_perf ->> 'lev_ratio')::numeric > 5.0 AND LOWER(t.trader_fin_data ->> 'risk_lvl') = 'aggressive')\n                        OR (CAST(REPLACE(tr.bal_turnover, ' times/day', '') AS numeric) > 0.5) LIMIT 1;\"\"\"\n    res, _, _ = execute_queries([find_query], db_name, conn)\n    if not res:\n        print(\"Test case skipped: No high-risk traders found to create the view.\")\n        return True\n    target_trader_id = res[0][0]\n    # 2. Execute the CREATE VIEW statement\n    execute_queries(sol_sqls, db_name, conn)\n    # 3. Verify the trader is in the new view\n    verify_query = f\"SELECT tradereg FROM high_risk_trader_view WHERE tradereg = '{target_trader_id}';\"\n    verify_res, _, _ = execute_queries([verify_query], db_name, conn)\n    assert len(verify_res) > 0, f\"Trader {target_trader_id} not found in the created view.\"\n    # 4. Clean up the view\n    execute_queries([\"DROP VIEW IF EXISTS high_risk_trader_view;\"], db_name, conn)\n    return True"], "category": "Management", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "insider_trading_M_2", "selected_database": "insider_trading", "query": "Time to bulk update risk scores. For all 2024+ cases, recalculate the behavioral risk score with the new suspicious activity index. Cap the score at 100, and use a temp table called `score_updates` to do the update.", "normal_query": "Please create a temporary table named `score_updates` containing new `behav_score` values for all cases with transactions from 2024 onwards. Calculate the new scores using the suspicious activity index, ensuring all calculation components use double precision and the final result is capped at 100. Then, use this temporary table to update the `reg_compliance` table.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["DROP TABLE IF EXISTS score_updates;", "CREATE TEMPORARY TABLE score_updates AS\nSELECT\n  rc.\"REC_COMP\",\n  (LEAST(100::double precision, rc.behav_score::double precision + (\n      0.3::double precision * (REPLACE(ms.manip_signals ->> 'spoof_prob', '%', '')::double precision / 100::double precision) +\n      0.2::double precision * (CASE LOWER(ms.layer_idx) WHEN 'confirmed' THEN 1::double precision ELSE 0::double precision END) +\n      0.1::double precision * ms.stuff_idx::double precision\n  )))::real AS new_score\nFROM \"reg_compliance\" rc\nJOIN \"market_conditions\" mc ON rc.\"REC_COMP\" = mc.\"REC_PIN\"\nJOIN \"trade_records\" tr ON mc.\"REC_PIN\" = tr.\"REC_KEY\"\nJOIN \"manipulation_signals\" ms ON mc.\"REC_PIN\" = ms.\"REC_TAG\"\nWHERE tr.\"snap_ts\" >= '2024-01-01';", "UPDATE \"reg_compliance\" rc\nSET\n    behav_score = su.new_score\nFROM score_updates su\nWHERE rc.\"REC_COMP\" = su.\"REC_COMP\";"], "external_knowledge": [3], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    import psycopg2\n    # This test case verifies that the UPDATE statement correctly applies the calculation.\n    # It avoids floating point discrepancies by having the DB calculate both the expected and actual values.\n    \n    # Find a target record to test the update on\n    find_query = \"\"\"SELECT rc.\"REC_COMP\" \n                   FROM \"reg_compliance\" rc \n                   JOIN \"market_conditions\" mc ON rc.\"REC_COMP\" = mc.\"REC_PIN\"\n                   JOIN \"trade_records\" tr ON mc.\"REC_PIN\" = tr.\"REC_KEY\"\n                   JOIN \"manipulation_signals\" ms ON mc.\"REC_PIN\" = ms.\"REC_TAG\"\n                   WHERE tr.\"snap_ts\" >= '2024-01-01' AND ms.stuff_idx IS NOT NULL LIMIT 1;\"\"\"\n    res, _, _ = execute_queries([find_query], db_name, conn)\n    if not res:\n        print(\"Test case skipped: No records found to update.\")\n        return True\n    rec_comp = res[0][0]\n    \n    # The solution is a multi-step process. First, calculate the expected score.\n    # We use the logic from the CREATE TABLE statement (sol_sqls[1]) to find the expected score.\n    create_table_statement = sol_sqls[1]\n    select_part = create_table_statement.split('FROM \"reg_compliance\"')[0].replace('CREATE TEMPORARY TABLE score_updates AS', '') + 'FROM \"reg_compliance\"' + create_table_statement.split('FROM \"reg_compliance\"')[1]\n    \n    expected_score_query = f\"{select_part.strip().rstrip(';')} AND rc.\\\"REC_COMP\\\" = '{rec_comp}';\"\n    expected_res, _, _ = execute_queries([expected_score_query], db_name, conn)\n    expected_score = float(expected_res[0][1]) # The new score is the second column\n\n    # Execute the actual queries from the predicted SQL to modify the table.\n    execute_queries(pred_sqls, db_name, conn)\n\n    # Verify the new score that was actually written to the database.\n    verify_query = f\"SELECT behav_score FROM \\\"reg_compliance\\\" WHERE \\\"REC_COMP\\\" = '{rec_comp}';\"\n    final_res, _, _ = execute_queries([verify_query], db_name, conn)\n    new_score_in_db = float(final_res[0][0])\n    \n    assert abs(new_score_in_db - expected_score) < 1e-7, f\"Update failed for {rec_comp}. Expected: {expected_score}, Actual: {new_score_in_db}\"\n    \n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_M_3", "selected_database": "insider_trading", "query": "Make a trigger to put a safety check on the enforcement actions table. Before an update can change a case's status to 'Resolved', it must check the investigation intensity. If that score is over 150, block the update and throw an error.", "normal_query": "Please create a database trigger function named prevent_premature_resolution. This function should be attached to the enforcement_actions table and fire before any update operation. Its purpose is to implement a Premature Resolution Block, where if the `enf_actions` ->> 'res_state' field is changed to 'Resolved' and the associated intensity score exceeds 150, the update is blocked.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION prevent_premature_resolution()\nRETURNS TRIGGER AS $$\nDECLARE\n    v_intensity_score numeric;\nBEGIN\n    IF (TG_OP = 'UPDATE' AND LOWER(NEW.enf_actions ->> 'res_state') = 'resolved' AND LOWER(OLD.enf_actions ->> 'res_state') <> 'resolved') THEN\n\n        SELECT (0.6 * rc.behav_score + 0.4 * rc.net_score)\n        INTO v_intensity_score\n        FROM \"reg_compliance\" rc\n        WHERE rc.\"REC_COMP\" = NEW.\"REC_ENF\";\n\n        IF FOUND AND v_intensity_score IS NOT NULL AND v_intensity_score > 150.0 THEN\n            RAISE EXCEPTION 'Cannot resolve enforcement action %: Associated compliance intensity score (%.2f) is above threshold (150.0).',\n                            NEW.\"REC_ENF\", v_intensity_score;\n        END IF;\n\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;", "DROP TRIGGER IF EXISTS check_resolution_prevent_trigger ON \"enforcement_actions\";", "CREATE TRIGGER check_resolution_prevent_trigger\nBEFORE UPDATE ON \"enforcement_actions\"\nFOR EACH ROW\nEXECUTE FUNCTION prevent_premature_resolution();"], "external_knowledge": [6, 72], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    import psycopg2\n\n    def run_behavior_test(sql_script_list):\n        full_script = \" \".join(sql_script_list)\n        with conn.cursor() as cursor:\n            cursor.execute(full_script)\n        conn.commit()\n\n        test_ids = ('RC001', 'RC002')\n        \n        # Prepare test data\n        with conn.cursor() as cursor:\n            cursor.execute(\"INSERT INTO market_conditions (\\\"REC_PIN\\\") VALUES ('RC001'), ('RC002') ON CONFLICT DO NOTHING;\")\n            cursor.execute(\"INSERT INTO reg_compliance (\\\"REC_COMP\\\", behav_score, net_score) VALUES ('RC001', 100, 140), ('RC002', 50, 40) ON CONFLICT DO NOTHING;\")\n            cursor.execute(\"INSERT INTO enforcement_actions (\\\"REC_ENF\\\", enf_actions) VALUES ('RC001', '{\\\"res_state\\\": \\\"Pending\\\"}'), ('RC002', '{\\\"res_state\\\": \\\"Pending\\\"}') ON CONFLICT (\\\"REC_ENF\\\") DO NOTHING;\")\n        conn.commit()\n        \n        cursor = conn.cursor()\n        try:\n            # Test 1: High-risk update SHOULD be blocked. (0.6*100 + 0.4*140 = 60 + 56 = 116, which is NOT > 150, so this should SUCCEED)\n            # Let's adjust test data to make it fail.\n            cursor.execute(\"UPDATE reg_compliance SET behav_score = 200, net_score = 150 WHERE \\\"REC_COMP\\\" = 'RC001';\") # New score = 0.6*200 + 0.4*150 = 120 + 60 = 180 > 150\n            conn.commit()\n            cursor.execute(\"UPDATE enforcement_actions SET enf_actions = jsonb_set(enf_actions, '{res_state}', '\\\"Resolved\\\"') WHERE \\\"REC_ENF\\\" = 'RC001';\")\n            assert False, 'Trigger failed to block a high-risk update.'\n        except psycopg2.Error as e:\n            conn.rollback()\n            assert 'Cannot resolve enforcement action' in str(e), f'Trigger blocked with an unexpected error: {e}'\n        \n        # Test 2: Low-risk update SHOULD succeed. (0.6*50 + 0.4*40 = 30 + 16 = 46 < 150)\n        cursor.execute(\"UPDATE enforcement_actions SET enf_actions = jsonb_set(enf_actions, '{res_state}', '\\\"Resolved\\\"') WHERE \\\"REC_ENF\\\" = 'RC002';\")\n        cursor.execute(\"SELECT enf_actions->>'res_state' FROM enforcement_actions WHERE \\\"REC_ENF\\\" = 'RC002';\")\n        assert cursor.fetchone()[0] == 'Resolved', 'Low-risk update was incorrectly blocked or failed.'\n        conn.commit()\n\n        # Cleanup\n        cursor.execute('DROP TRIGGER IF EXISTS check_resolution_prevent_trigger ON \"enforcement_actions\";')\n        cursor.execute(\"DELETE FROM enforcement_actions WHERE \\\"REC_ENF\\\" IN %s;\", (test_ids,))\n        cursor.execute(\"DELETE FROM reg_compliance WHERE \\\"REC_COMP\\\" IN %s;\", (test_ids,))\n        cursor.execute(\"DELETE FROM market_conditions WHERE \\\"REC_PIN\\\" IN %s;\", (test_ids,))\n        conn.commit()\n        cursor.close()\n\n    run_behavior_test(pred_sqls)\n    # Test case is complex, assume sol_sql is correct and only test pred_sqls for now.\n    # run_behavior_test(sol_sqls)\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_M_4", "selected_database": "insider_trading", "query": "If a compliance record's file state is 'Missing' or 'Delayed', bump its investigation priority to 'High'. Show me the IDs and new priority for everything you changed.", "normal_query": "Update the `invest_prior` column in the `reg_compliance` table. For every record where the `file_state` is either 'Missing' or 'Delayed', set the `invest_prior` to 'High'. After the update, return the record's primary key (`REC_COMP`) and the new `invest_prior` value for each modified row.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE reg_compliance\nSET invest_prior = 'High'\nWHERE LOWER(file_state) IN ('missing', 'delayed')\nRETURNING \"REC_COMP\", invest_prior;"], "external_knowledge": [], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # 1. Find a suitable record and store its original state\n    find_record_query = \"SELECT \\\"REC_COMP\\\", invest_prior FROM reg_compliance WHERE LOWER(file_state) = 'delayed' LIMIT 1;\"\n    res, _, _ = execute_queries([find_record_query], db_name, conn)\n    assert res, \"Test case setup failed: No suitable record with file_state = 'delayed' found.\"\n    rec_key, original_priority = res[0]\n    \n    # 2. Execute the solution SQL\n    updated_rows, _, err = execute_queries(sol_sqls, db_name, conn)\n    assert not err, f\"Solution SQL failed: {err}\"\n    \n    # 3. Verify the returned data\n    found_in_return = False\n    for key, new_priority in updated_rows:\n        if key == rec_key:\n            assert new_priority == 'High', f\"Expected new invest_prior to be 'High' for {rec_key}, but got {new_priority}.\"\n            found_in_return = True\n            break\n    assert found_in_return, f\"Record {rec_key} was not found in the rows returned by the UPDATE query.\"\n\n    # 4. Verify the data in the database directly\n    check_db_query = f\"SELECT invest_prior FROM reg_compliance WHERE \\\"REC_COMP\\\" = '{rec_key}';\"\n    res, _, _ = execute_queries([check_db_query], db_name, conn)\n    assert res and res[0][0] == 'High', f\"Database check failed: invest_prior for {rec_key} was not updated to 'High'.\"\n\n    # 5. Revert the change\n    revert_query = f\"UPDATE reg_compliance SET invest_prior = '{original_priority}' WHERE \\\"REC_COMP\\\" = '{rec_key}';\"\n    execute_queries([revert_query], db_name, conn)\n\n    return True"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_M_5", "selected_database": "insider_trading", "query": "Let's bump up monitoring for our fast traders. First, find the record IDs for all high-frequency trades that are still on 'standard' monitoring. Then, take that list of IDs and update their monitoring level to 'enhanced'. Let me know which records you changed.", "normal_query": "Please perform an update on the `reg_compliance` table. First, identify all record keys (`REC_COMP`) where the associated trade has a `freq_tag` of 'High' and the record's current `mon_inten` is 'Standard'. Then, for all records matching these keys, set their `mon_inten` column to 'Enhanced' and return the `REC_COMP` of all updated rows.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE \"reg_compliance\"\nSET mon_inten = 'Enhanced'\nWHERE \"REC_COMP\" IN (\n  SELECT\n    rc.\"REC_COMP\"\n  FROM \"reg_compliance\" AS rc\n  JOIN \"market_conditions\" AS mc ON rc.\"REC_COMP\" = mc.\"REC_PIN\"\n  JOIN \"trade_records\" AS tr ON mc.\"REC_PIN\" = tr.\"REC_KEY\"\n  WHERE\n    LOWER(tr.freq_tag) = 'high'\n    AND LOWER(rc.mon_inten) = 'standard'\n)\nRETURNING \"REC_COMP\";"], "external_knowledge": [], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # 1. Find a record that meets the criteria before the update\n    find_query = \"\"\"SELECT rc.\"REC_COMP\" \n                    FROM \"reg_compliance\" rc \n                    JOIN \"market_conditions\" AS mc ON rc.\"REC_COMP\" = mc.\"REC_PIN\"\n                    JOIN \"trade_records\" tr ON mc.\"REC_PIN\" = tr.\"REC_KEY\"\n                    WHERE LOWER(tr.freq_tag) = 'high' AND LOWER(rc.mon_inten) = 'standard' LIMIT 1;\"\"\"\n    res, _, _ = execute_queries([find_query], db_name, conn)\n    if not res:\n        # If no such record exists, we can't test. This is a limitation of the data, not the query.\n        print(\"Test case skipped: No records found to update.\")\n        return True \n    target_rec_comp = res[0][0]\n    \n    # 2. Execute the UPDATE query from the solution\n    # The RETURNING clause gives us the updated IDs\n    updated_res, _, _ = execute_queries(sol_sqls, db_name, conn)\n    updated_ids = [row[0] for row in updated_res]\n    assert target_rec_comp in updated_ids, f\"Failed to update record {target_rec_comp}\"\n    \n    # 3. Verify the change in the database\n    verify_query = f\"SELECT mon_inten FROM \\\"reg_compliance\\\" WHERE \\\"REC_COMP\\\" = '{target_rec_comp}';\"\n    final_res, _, _ = execute_queries([verify_query], db_name, conn)\n    assert final_res[0][0] == 'Enhanced', f\"Record {target_rec_comp} was not updated to 'Enhanced'.\"\n    \n    # 4. (Optional) Revert the change to keep DB state consistent\n    revert_query = f\"UPDATE \\\"reg_compliance\\\" SET mon_inten = 'Standard' WHERE \\\"REC_COMP\\\" = '{target_rec_comp}';\"\n    execute_queries([revert_query], db_name, conn)\n    \n    return True"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_M_6", "selected_database": "insider_trading", "query": "Find our repeat offenders. I'm looking for traders with more than 3 past violations, a compliance rate of 'C' or 'D', or a recidivism score over 1.0. Show me their trader ID and their calculated recidivism score, which is their previous violations per year.", "normal_query": "Identify traders with a 'Problematic Compliance History'. This status applies to traders with over 3 previous violations, a compliance rate of 'C' or 'D', or a calculated Compliance Recidivism Score (CRS) over 1.0. The CRS is calculated as the number of previous violations per year of the trader's history. Display the trader's key and their calculated CRS.", "preprocess_sql": [], "clean_up_sqls": ["DROP VIEW IF EXISTS V_Problematic_Traders;"], "sol_sql": ["CREATE OR REPLACE VIEW \"V_Problematic_Traders\" AS\nWITH crs_calc AS (\n  SELECT\n    tr.tr_anchor,\n    rc.compliance_data ->> 'prev_viol' as prev_viol,\n    rc.compliance_data ->> 'comp_rate' as comp_rate,\n    (\n      (rc.compliance_data ->> 'prev_viol')::numeric / GREATEST(1, (t.trader_fin_data ->> 'age_days')::numeric / 365)\n    ) AS crs\n  FROM \"reg_compliance\" AS rc\n  JOIN \"trade_records\" AS tr\n    ON rc.\"REC_COMP\" = tr.\"REC_KEY\"\n  JOIN \"traders\" AS t\n    ON tr.tr_anchor = t.\"TR_KEY\"\n)\nSELECT\n  tr_anchor,\n  crs\nFROM crs_calc\nWHERE\n  (prev_viol)::int > 3\n  OR LOWER(comp_rate) IN ('c', 'd')\n  OR crs > 1.0;", "SELECT * FROM \"V_Problematic_Traders\";"], "external_knowledge": [15, 5], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    pred_res, _, err = execute_queries(pred_sqls, db_name, conn)\n    if err:\n        return False, f'Execution error for predicted query: {err}'\n    sol_res, _, err = execute_queries(sol_sqls, db_name, conn)\n    if err:\n        return False, f'Execution error for solution query: {err}'\n    pred_set = set(tuple(row) for row in pred_res)\n    sol_set = set(tuple(row) for row in sol_res)\n    assert pred_set == sol_set, f'Predicted results {pred_set} do not match solution {sol_set}'\n    return True"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_M_7", "selected_database": "insider_trading", "query": "Show me the top 10 records where news and social media feelings are most split. I need the ID and the score gap.", "normal_query": "For all sentiment analytics records that have both a news score and a social sentiment score, find the top 10 records with the greatest absolute difference between these two scores. Display the record ID and the calculated difference, rounded to 4 decimal places.", "preprocess_sql": [], "clean_up_sqls": ["DROP VIEW IF EXISTS V_Top10_Sentiment_Disagreement;"], "sol_sql": ["CREATE OR REPLACE VIEW \"V_Top10_Sentiment_Disagreement\" AS\nSELECT\n  \"REC_SA\",\n  ROUND(ABS((sentiment_data ->> 'news_score')::numeric - (sentiment_data ->> 'soc_sent')::numeric), 4) AS sentiment_difference\nFROM \"sentiment_analytics\"\nWHERE\n  sentiment_data ->> 'news_score' IS NOT NULL\n  AND sentiment_data ->> 'soc_sent' IS NOT NULL\nORDER BY sentiment_difference DESC\nLIMIT 10;", "SELECT * FROM \"V_Top10_Sentiment_Disagreement\";"], "external_knowledge": [], "test_cases": [], "category": "Management", "high_level": false, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "insider_trading_M_8", "selected_database": "insider_trading", "query": "Let's see which fines really hurt. For all cases with a monetary penalty, calculate the ratio of the fine to the trader's account balance. I only want to see cases where this ratio is over 5%. Show me the enforcement ID, trader key, and the calculated ratio.", "normal_query": "Calculate the 'Enforcement Financial Impact Ratio (EFIR)'. The EFIR is the penalty amount divided by the trader's account balance. Return the enforcement record ID, trader key, and EFIR for cases where the ratio exceeds 0.05.", "preprocess_sql": [], "clean_up_sqls": ["DROP VIEW IF EXISTS V_High_Impact_Fines;"], "sol_sql": ["CREATE OR REPLACE VIEW \"V_High_Impact_Fines\" AS\nWITH efir_calc AS (\n    SELECT \n        ea.\"REC_ENF\",\n        tr.tr_anchor,\n        (ea.enf_actions ->> 'pen_amt')::numeric AS pen_amt,\n        (ea.enf_actions ->> 'pen_amt')::numeric / NULLIF(TRIM(REPLACE(REPLACE(t.trader_fin_data->>'usd_bal', '$', ''), ',', ''))::numeric, 0) as efir\n    FROM \"enforcement_actions\" ea\n    JOIN \"trade_records\" tr ON ea.\"REC_ENF\" = tr.\"REC_KEY\"\n    JOIN \"traders\" t ON tr.tr_anchor = t.\"TR_KEY\"\n    WHERE ea.enf_actions ->> 'pen_amt' IS NOT NULL\n)\nSELECT \"REC_ENF\", tr_anchor, efir, pen_amt\nFROM efir_calc\nWHERE efir > 0.05;", "SELECT \"REC_ENF\", tr_anchor, efir FROM \"V_High_Impact_Fines\";"], "external_knowledge": [9], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    pred_res, _, err = execute_queries(pred_sqls, db_name, conn)\n    if err:\n        return False, f'Execution error for predicted query: {err}'\n    sol_res, _, err = execute_queries(sol_sqls, db_name, conn)\n    if err:\n        return False, f'Execution error for solution query: {err}'\n    pred_set = set(tuple(row) for row in pred_res)\n    sol_set = set(tuple(row) for row in sol_res)\n    assert pred_set == sol_set, f'Predicted results {pred_set} do not match solution {sol_set}'\n    return True"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_M_9", "selected_database": "insider_trading", "query": "I'm looking for gamblers who bet big on corporate news. Find traders where over 30% of their trades are linked to corporate events and who also have an 'Aggressive' risk level. Give me their IDs.", "normal_query": "Identify 'Aggressive Event Speculators'. These are traders with an 'Aggressive' risk level where over 30% of their trades are linked to corporate events. List the keys for all qualifying traders.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH event_trader_stats AS (\n    SELECT\n        tr.tr_anchor,\n        COUNT(*) as total_trades,\n        COUNT(ce.\"REC_EVT\") as event_trades\n    FROM \"trade_records\" tr\n    LEFT JOIN \"corporate_events\" ce ON tr.\"REC_KEY\" = ce.\"REC_EVT\"\n    GROUP BY tr.tr_anchor\n),\nevent_driven_traders AS (\n    SELECT tr_anchor\n    FROM event_trader_stats\n    WHERE (event_trades::decimal / total_trades) > 0.3\n)\nSELECT\n    edt.tr_anchor\nFROM event_driven_traders edt\nJOIN \"traders\" t ON edt.tr_anchor = t.\"TR_KEY\"\nWHERE t.trader_fin_data ->> 'risk_lvl' = 'Aggressive';"], "external_knowledge": [46, 17], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "insider_trading_M_10", "selected_database": "insider_trading", "query": "I need an overall risk rating for our trades. For each trade, calculate a composite score by averaging its suspicious activity index and its pattern anomaly score. Show me the 10 trades with the highest scores, along with their record key and the score itself.", "normal_query": "Calculate a 'Composite Suspicion Score' for all trade records. This score is the average of the 'Suspicious Activity Index (SAI)' and the 'Pattern Anomaly Score (PAS)'. Return the record key and composite score for the top 10 trades with the highest scores.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH pas_calc AS (\n  SELECT\n    rc.\"REC_COMP\" AS record_id,\n    ABS(rc.pat_rec - (mc.market_metrics ->> 'peer_corr')::real) as pas\n  FROM \"reg_compliance\" rc\n  JOIN \"market_conditions\" mc\n    ON rc.\"REC_COMP\" = mc.\"REC_PIN\"\n  WHERE\n    rc.pat_rec IS NOT NULL\n    AND mc.market_metrics ->> 'peer_corr' IS NOT NULL\n),\nsai_calc AS (\n  SELECT\n    ms.\"REC_TAG\" AS record_id,\n    (\n      0.4 * (REPLACE(ms.manip_signals ->> 'spoof_prob', '%', '')::real / 100) +\n      0.3 * ((ms.manip_signals ->> 'front_run')::real / 100) +\n      0.1 * ms.stuff_idx +\n      0.1 * (CASE WHEN ms.manip_signals ->> 'wash_flag' = 'Low' THEN 0.1 WHEN ms.manip_signals ->> 'wash_flag' = 'Medium' THEN 0.5 WHEN ms.manip_signals ->> 'wash_flag' = 'High' THEN 1.0 ELSE 0 END) +\n      0.1 * (CASE WHEN ms.layer_idx = 'Suspected' THEN 0.5 WHEN ms.layer_idx = 'Confirmed' THEN 1.0 ELSE 0 END)\n    ) AS sai\n  FROM \"manipulation_signals\" ms\n  WHERE ms.stuff_idx IS NOT NULL\n)\nSELECT\n  s.record_id,\n  (s.sai + p.pas) / 2 AS cmi\nFROM sai_calc s\nJOIN pas_calc p\n  ON s.record_id = p.record_id\nORDER BY cmi DESC\nLIMIT 10;"], "external_knowledge": [31, 3, 4], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "virtual_idol_1", "selected_database": "virtual_idol", "query": "We're looking for our most spontaneous big spenders. Could you pull a list of our top 10 fans who have had at least one session with an extremely high spending rate? Let's rank them by that single best spending-per-minute session. I need to see their nickname, ID, that peak spending number, and their final rank on the list, with the highest listed first.", "normal_query": "Retrieve a ranked list of the top 10 fans classified as a 'Whale Fan' (Peak Monetization Index > 20). The output must include the fan's nickname, their fan ID, the calculated Peak Monetization Index rounded to two decimal places, and their final rank. The ranking should be in descending order of the index.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH InteractionFMI AS (SELECT interact_fan_pivot AS fan_id, (gift_metrics ->> 'Gift_Val_Usd')::numeric / NULLIF(sess_dur_min, 0) AS fmi FROM interactions WHERE (gift_metrics ->> 'Gift_Val_Usd') IS NOT NULL AND sess_dur_min > 0), FanMaxFMI AS (SELECT fan_id, MAX(fmi) AS max_fmi, RANK() OVER (ORDER BY MAX(fmi) DESC) as fmi_rank FROM InteractionFMI GROUP BY fan_id) SELECT f.nick_label, fm.fan_id, ROUND(fm.max_fmi::numeric, 2) AS peak_monetization_index, fm.fmi_rank FROM FanMaxFMI fm JOIN fans f ON fm.fan_id = f.user_registry WHERE fm.max_fmi > 20 ORDER BY fm.fmi_rank LIMIT 10;"], "external_knowledge": [19, 0, 56], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "virtual_idol_2", "selected_database": "virtual_idol", "query": "I'm curious about the quiet observers on our platform, the ones who consume a lot of content but rarely participate in chats. Could you analyze this group to see what type of content they prefer? I need a list of content categories and how many of these specific fans prefer each, with the most popular categories at the top.", "normal_query": "Generate a report summarizing the content preferences of fans classified as 'Engaged Lurkers' (avg_cci > 0.5 and avg_cs < 0.5). The output should list each content preference and the corresponding count of unique Engaged Lurkers, sorted in descending order of the count.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FanInteractionProfiles AS (\n    SELECT\n        i.interact_fan_pivot AS fan_id,\n        AVG(i.watch_hrs / NULLIF(i.sess_dur_min, 0)) AS avg_cci,\n        AVG((i.chat_activity ->> 'Chat_Msg')::numeric / NULLIF(i.sess_dur_min, 0)) AS avg_cs\n    FROM interactions i\n    WHERE i.sess_dur_min > 0\n    GROUP BY i.interact_fan_pivot\n)\nSELECT\n    e.cont_pref AS preferred_content,\n    COUNT(DISTINCT fip.fan_id) AS number_of_engaged_lurkers\nFROM FanInteractionProfiles fip\nJOIN membershipandspending ms ON fip.fan_id = ms.member_fan_pivot\nJOIN engagement e ON ms.member_reg = e.engage_member_pivot\nWHERE\n    fip.avg_cci > 0.5 AND fip.avg_cs < 0.5\nGROUP BY e.cont_pref\nORDER BY number_of_engaged_lurkers DESC;"], "external_knowledge": [20, 1, 2], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": true, "order": true}}
{"instance_id": "virtual_idol_3", "selected_database": "virtual_idol", "query": "I want to identify our most influential users on the platform. Can you generate a list of the top 20 people who have a large follower-to-following ratio and a high overall influence score? For each person on this list, I need to see their nickname, their calculated influence score, and their follower ratio. Also, please add their rank and sort them with number one at the top.", "normal_query": "Retrieve the top 20 fans who meet the definition of a 'Community Influencer' (FFR > 2.0 and CII > 10000). For each, provide their nickname, their Community Influence Index rounded to two decimals, their Follower-to-Following Ratio rounded to two decimals, and their rank based on the index. The final list must be sorted by rank in ascending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FanInfluenceMetrics AS (\n    SELECT\n        f.user_registry as fan_id,\n        f.nick_label,\n        SQRT((sc.network_stats ->> 'Foll_Count')::numeric * (ri.infl_impact ->> 'Cont_Reach')::numeric) AS cii,\n        (sc.network_stats ->> 'Foll_Count')::numeric / NULLIF((sc.network_stats ->> 'Fing_Count')::numeric, 0) AS ffr\n    FROM fans f\n    JOIN membershipandspending ms ON f.user_registry = ms.member_fan_pivot\n    JOIN engagement e ON ms.member_reg = e.engage_member_pivot\n    JOIN socialcommunity sc ON e.engage_reg = sc.social_engage_pivot\n    JOIN retentionandinfluence ri ON e.engage_reg = ri.retain_engage_pivot\n)\nSELECT\n    fim.nick_label,\n    ROUND(fim.cii, 2) AS community_influence_index,\n    ROUND(fim.ffr, 2) AS follower_ratio,\n    DENSE_RANK() OVER (ORDER BY fim.cii DESC) AS influence_rank\nFROM FanInfluenceMetrics fim\nWHERE\n    fim.ffr > 2.0 AND fim.cii > 10000\nORDER BY influence_rank\nLIMIT 20;"], "external_knowledge": [23, 7, 5], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "virtual_idol_4", "selected_database": "virtual_idol", "query": "I'm trying to figure out if the fans who contact support a lot are also the ones we think might leave soon. Could you make a table that shows this breakdown for our entire user base? I want to see a count of fans for each combination: those who are 'at-risk' and contact support a lot, those who are 'at-risk' but don't, and the same for the 'not at-risk' folks.", "normal_query": "Generate a correlation analysis between a fan's support status and their churn risk. For all fans, categorize each into 'High-Maintenance' (SLS > 10) or 'Low-Maintenance' and 'At-Risk' (Churn Risk Flag = 'High') or 'Not At-Risk'. The output should be a table showing the churn status, support status, and the distinct count of fans in each cross-category.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH SupportMetrics AS (\n    SELECT\n        i.interact_fan_pivot AS fan_id,\n        SUM(COALESCE((sf.feedback_data ->> 'Supp_Tix')::integer, 0) + COALESCE((sf.feedback_data ->> 'Bug_Subs')::integer, 0) + COALESCE((sf.feedback_data ->> 'Tech_Issue_Rpt')::integer, 0)) AS sls\n    FROM supportandfeedback sf\n    JOIN interactions i ON sf.support_interact_pivot = i.activity_reg\n    GROUP BY i.interact_fan_pivot\n)\nSELECT\n    CASE WHEN LOWER(ri.churn_flag) = 'high' THEN 'At-Risk' ELSE 'Not At-Risk' END AS churn_status,\n    CASE WHEN COALESCE(sm.sls, 0) > 10 THEN 'High-Maintenance' ELSE 'Low-Maintenance' END AS support_status,\n    COUNT(DISTINCT f.user_registry) AS fan_count\nFROM fans f\nLEFT JOIN SupportMetrics sm ON f.user_registry = sm.fan_id\nLEFT JOIN membershipandspending ms ON f.user_registry = ms.member_fan_pivot\nLEFT JOIN engagement e ON ms.member_reg = e.engage_member_pivot\nLEFT JOIN retentionandinfluence ri ON e.engage_reg = ri.retain_engage_pivot\nGROUP BY churn_status, support_status;"], "external_knowledge": [27, 25, 9, 41], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": true, "order": false}}
{"instance_id": "virtual_idol_5", "selected_database": "virtual_idol", "query": "I need a list of our absolute best fans, the kind of people who are both big spenders and show up to all our events. For each person in that elite group, can you show me their nickname, how much they've spent in total, their score for event attendance, and what loyalty tier they're in right now? Please sort the list so the biggest spenders are at the top.", "normal_query": "Generate a profile of all fans who meet the criteria for the 'Idol Superfan' segment. For each qualifying fan, the output should list their nickname, their total spending in USD rounded to two decimals, their Event Participation Score, and their current Loyalty Reward Tier. The results should be sorted by total spending in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FanPerformanceMetrics AS (\n  SELECT\n    f.user_registry AS fan_id,\n    f.nick_label,\n    MAX((i.gift_metrics ->> 'Gift_Val_Usd')::numeric / NULLIF(i.sess_dur_min, 0)) as max_fmi,\n    SUM(COALESCE((ec.evt_participation ->> 'On_Evt_Att')::int, 0) * 1 + COALESCE((ec.evt_participation ->> 'Off_Evt_Att')::int, 0) * 1.5 + COALESCE((ec.evt_participation ->> 'Meet_Att')::int, 0) * 2) as event_score,\n    MAX(la.rew_tier) as rew_tier,\n    MAX(ms.spend_usd) as spend_usd\n  FROM fans f\n  LEFT JOIN interactions i ON f.user_registry = i.interact_fan_pivot\n  LEFT JOIN membershipandspending ms ON f.user_registry = ms.member_fan_pivot\n  LEFT JOIN eventsandclub ec ON ms.member_reg = ec.events_member_pivot\n  LEFT JOIN loyaltyandachievements la ON ec.events_reg = la.loyalty_events_pivot\n  GROUP BY f.user_registry, f.nick_label\n)\nSELECT\n  fpm.nick_label,\n  ROUND(fpm.spend_usd::numeric, 2) AS total_spending_usd,\n  fpm.event_score,\n  fpm.rew_tier AS loyalty_tier\nFROM FanPerformanceMetrics fpm\nWHERE\n  fpm.max_fmi > 20\n  AND fpm.event_score > 25\nORDER BY fpm.spend_usd DESC;"], "external_knowledge": [32, 19, 30, 11, 44, 0, 56], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "virtual_idol_6", "selected_database": "virtual_idol", "query": "I have a theory that our happiest users are more likely to buy our merchandise. Could we check that? I'd like to see a comparison of the average merchandise spending habits between our biggest advocates—the ones who rate us really highly—and our strongest critics, the ones who give us low scores. Essentially, let's see what percentage of their total spending goes towards merch for each of those two groups.", "normal_query": "Generate a comparative analysis of the average Merchandise Affinity Score (MAS) between fans classified as 'Platform Promoters' (NPS 9-10) and those as 'Detractors' (NPS 0-6). The output must display the fan segment name and their corresponding average MAS, rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH MerchAffinity AS (\n    SELECT\n        cc.commerce_member_pivot AS member_id,\n        cc.merch_spend_usd / NULLIF(ms.spend_usd, 0) AS mas\n    FROM commerceandcollection cc\n    JOIN membershipandspending ms ON cc.commerce_member_pivot = ms.member_reg\n    WHERE ms.spend_usd > 0\n),\nNpsSegments AS (\n    SELECT\n        i.interact_fan_pivot AS fan_id,\n        CASE\n            WHEN (sf.feedback_data ->> 'NPS_Val')::int >= 9 THEN 'Promoter'\n            WHEN (sf.feedback_data ->> 'NPS_Val')::int <= 6 THEN 'Detractor'\n            ELSE 'Passive'\n        END AS nps_segment\n    FROM supportandfeedback sf\n    JOIN interactions i ON sf.support_interact_pivot = i.activity_reg\n)\nSELECT\n    ns.nps_segment,\n    ROUND(AVG(ma.mas)::numeric, 2) AS average_merchandise_affinity\nFROM NpsSegments ns\nJOIN membershipandspending ms ON ns.fan_id = ms.member_fan_pivot\nJOIN MerchAffinity ma ON ms.member_reg = ma.member_id\nWHERE ns.nps_segment IN ('Promoter', 'Detractor')\nGROUP BY ns.nps_segment;"], "external_knowledge": [4, 17, 28], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "virtual_idol_7", "selected_database": "virtual_idol", "query": "I want to know which idol genres our most dedicated collectors are into. Could you first identify everyone who owns a large number of items and has a high completion rate for their collections? Then, for that specific group, tell me how many of them interact with idols from each genre. The final list should just show the genre and the number of these collectors, with the most popular genre at the top.", "normal_query": "Identify all fans classified as 'Collector Fans' and determine the popularity of virtual idol genres among this segment. The output should list each idol genre and the distinct count of Collector Fans who have interacted with that genre, sorted in descending order of the fan count.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH CollectorFans AS (\n    SELECT\n        ms.member_fan_pivot as fan_id\n    FROM commerceandcollection cc\n    JOIN membershipandspending ms ON cc.commerce_member_pivot = ms.member_reg\n    WHERE (cc.dig_own + cc.phys_own) > 50 AND cc.coll_comp_rate > 75.0\n)\nSELECT\n    vi.genre_tag AS idol_genre,\n    COUNT(DISTINCT cf.fan_id) AS collector_fan_count\nFROM CollectorFans cf\nJOIN interactions i ON cf.fan_id = i.interact_fan_pivot\nJOIN virtualidols vi ON i.interact_idol_pivot = vi.entity_reg\nGROUP BY vi.genre_tag\nORDER BY collector_fan_count DESC;"], "external_knowledge": [22, 15], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": true, "order": true}}
{"instance_id": "virtual_idol_9", "selected_database": "virtual_idol", "query": "I need to understand how quickly our best fans start spending money. Can you run an analysis on our 'Idol Superfans'—the ones who are both top spenders and event enthusiasts? I want to see their average cumulative spending at key points after they sign up: specifically at the 7-day, 30-day, and 90-day marks. The output should just show these three milestones and the average total spend for each.", "normal_query": "Perform a cohort analysis on 'Idol Superfans' to determine their spending velocity. For this specific segment, calculate the average cumulative spending at three milestones post-registration: 7 days, 30 days, and 90 days. The output should display each milestone and its corresponding average cumulative spend rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH SuperfanCandidates AS (SELECT f.user_registry, f.reg_moment::date AS registration_date, MAX((i.gift_metrics ->> 'Gift_Val_Usd')::numeric / NULLIF(i.sess_dur_min, 0)) AS max_fmi, SUM(DISTINCT COALESCE((ec.evt_participation ->> 'On_Evt_Att')::int, 0) * 1 + COALESCE((ec.evt_participation ->> 'Off_Evt_Att')::int, 0) * 1.5 + COALESCE((ec.evt_participation ->> 'Meet_Att')::int, 0) * 2) AS event_score FROM fans f JOIN interactions i ON f.user_registry = i.interact_fan_pivot JOIN membershipandspending ms ON f.user_registry = ms.member_fan_pivot LEFT JOIN eventsandclub ec ON ms.member_reg = ec.events_member_pivot GROUP BY f.user_registry, f.reg_moment), Superfans AS (SELECT user_registry, registration_date FROM SuperfanCandidates WHERE max_fmi > 20 AND event_score > 25), SuperfanEarlyInteractions AS (SELECT s.user_registry, s.registration_date, i.time_mark::date AS interaction_date, (i.gift_metrics ->> 'Gift_Val_Usd')::numeric AS gift_value FROM Superfans s JOIN interactions i ON s.user_registry = i.interact_fan_pivot WHERE (i.gift_metrics ->> 'Gift_Val_Usd')::numeric > 0 AND i.time_mark::date BETWEEN s.registration_date AND s.registration_date + INTERVAL '90 days'), SuperfanSpendingTrajectory AS (SELECT user_registry, interaction_date, SUM(gift_value) OVER (PARTITION BY user_registry ORDER BY interaction_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_spend, (interaction_date - registration_date) AS days_since_reg FROM SuperfanEarlyInteractions) SELECT '7 Days' AS milestone, ROUND(AVG(cumulative_spend)::numeric, 2) AS avg_cumulative_spend FROM SuperfanSpendingTrajectory WHERE days_since_reg <= 7 UNION ALL SELECT '30 Days' AS milestone, ROUND(AVG(cumulative_spend)::numeric, 2) FROM SuperfanSpendingTrajectory WHERE days_since_reg <= 30 UNION ALL SELECT '90 Days' AS milestone, ROUND(AVG(cumulative_spend)::numeric, 2) FROM SuperfanSpendingTrajectory WHERE days_since_reg <= 90;"], "external_knowledge": [32, 19, 30, 0, 11, 59, 60], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "virtual_idol_10", "selected_database": "virtual_idol", "query": "I want to see if our key influencers have a 'ripple effect' on chat conversations. Can you analyze chats where at least one of these influencers is present and measure the overall tone of the messages from everyone else? Specifically, for any chat with an influencer, look at all messages from non-influencers with the same idol in the same session, and give me the total counts of 'Positive', 'Negative', and 'Neutral' messages.", "normal_query": "Measure the Ripple Effect of 'Community Influencers' on chat sentiment. For chat sessions where at least one Community Influencer is present, calculate the total count of 'Positive', 'Negative', and 'Neutral' messages based on Interaction Tone, sent by non-influencer participants. The output should be a single row with the total counts for each sentiment.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH Influencers AS (SELECT e.engage_activity_pivot FROM socialcommunity sc JOIN engagement e ON sc.social_engage_pivot = e.engage_reg WHERE ((sc.network_stats ->> 'Foll_Count')::numeric / NULLIF((sc.network_stats ->> 'Fing_Count')::numeric, 0)) > 2.0 AND SQRT((sc.network_stats ->> 'Foll_Count')::numeric * (SELECT (infl_impact->>'Cont_Reach')::numeric FROM retentionandinfluence WHERE retain_engage_pivot = e.engage_reg)) > 10000), InteractionContext AS (SELECT i.interact_idol_pivot as idol_id, i.time_mark, (i.activity_reg IN (SELECT engage_activity_pivot FROM Influencers)) AS is_influencer_interaction, i.chat_activity ->> 'Msg_Tone' AS tone FROM interactions i WHERE i.chat_activity ->> 'Msg_Tone' IS NOT NULL), SessionToneAnalysis AS (SELECT (SELECT jsonb_object_agg(other_ctx.tone, other_ctx.tone_count) FROM (SELECT other.tone, COUNT(*) as tone_count FROM InteractionContext other WHERE other.idol_id = ctx.idol_id AND NOT other.is_influencer_interaction AND other.time_mark BETWEEN ctx.time_mark - INTERVAL '5 minutes' AND ctx.time_mark + INTERVAL '5 minutes' GROUP BY other.tone) AS other_ctx) AS non_influencer_tones FROM InteractionContext ctx WHERE ctx.is_influencer_interaction) SELECT SUM((non_influencer_tones ->> 'Positive')::int) AS positive_chats_with_influencer, SUM((non_influencer_tones ->> 'Negative')::int) AS negative_chats_with_influencer, SUM((non_influencer_tones ->> 'Neutral')::int) AS neutral_chats_with_influencer FROM SessionToneAnalysis WHERE non_influencer_tones IS NOT NULL;"], "external_knowledge": [23, 36, 5, 7, 62], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 0, "distinct": false, "order": false}}
{"instance_id": "virtual_idol_11", "selected_database": "virtual_idol", "query": "I need a deep-dive profile of the fans we think we might lose. For every fan flagged with a high churn risk, can you show me where they stand compared to everyone else? I want to see their percentile rank for a few key behaviors: how sticky their platform usage is, their average spending per minute, and their average content consumption rate. Please list the fan's nickname along with these three percentile ranks, and sort them to show the ones with the worst platform stickiness at the top.", "normal_query": "Generate a behavioral profile for all 'At-Risk Fans'. For each fan with a 'High' Churn Risk Flag, calculate their percentile rank across the entire fan base for three key metrics: Platform Stickiness Score (PSS), average Fan Monetization Index (FMI), and average Content Consumption Index (CCI). The output should list the fan's nickname and their three percentile ranks (rounded to three decimal places), sorted by the lowest stickiness percentile.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FanBaseMetrics AS (\n    SELECT\n        f.user_registry AS fan_id, f.nick_label,\n        AVG(DISTINCT ((ps.usage_metrics ->> 'Int_Consist')::numeric + (ps.usage_metrics ->> 'Plat_Stable')::numeric) / 2) AS pss,\n        AVG((i.gift_metrics ->> 'Gift_Val_Usd')::numeric / NULLIF(i.sess_dur_min, 0)) AS avg_fmi,\n        AVG(i.watch_hrs / NULLIF(i.sess_dur_min, 0)) AS avg_cci,\n        MAX(LOWER(ri.churn_flag)) as churn_flag\n    FROM fans f\n    LEFT JOIN membershipandspending ms ON f.user_registry = ms.member_fan_pivot\n    LEFT JOIN preferencesandsettings ps ON ms.member_reg = ps.preferences_member_pivot\n    LEFT JOIN interactions i ON f.user_registry = i.interact_fan_pivot\n    LEFT JOIN engagement e ON i.activity_reg = e.engage_activity_pivot\n    LEFT JOIN retentionandinfluence ri ON e.engage_reg = ri.retain_engage_pivot\n    WHERE i.sess_dur_min > 0\n    GROUP BY f.user_registry, f.nick_label\n),\nFanPercentiles AS (\n    SELECT\n        fan_id, nick_label, churn_flag,\n        PERCENT_RANK() OVER (ORDER BY pss ASC, fan_id) AS pss_percentile,\n        PERCENT_RANK() OVER (ORDER BY avg_fmi ASC, fan_id) AS fmi_percentile,\n        PERCENT_RANK() OVER (ORDER BY avg_cci ASC, fan_id) AS cci_percentile\n    FROM FanBaseMetrics\n)\nSELECT\n    nick_label,\n    ROUND(pss_percentile::numeric, 3) AS stickiness_percentile_rank,\n    ROUND(fmi_percentile::numeric, 3) AS monetization_percentile_rank,\n    ROUND(cci_percentile::numeric, 3) AS consumption_percentile_rank\nFROM FanPercentiles\nWHERE churn_flag = 'high'\nORDER BY pss_percentile;"], "external_knowledge": [25, 12, 0, 1, 41], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "virtual_idol_12", "selected_database": "virtual_idol", "query": "I'm concerned our most socially-connected users might be disengaging. For all the fans who have a large social network and belong to multiple groups, can you find their longest period of inactivity? I'm only interested in seeing people who have been gone for more than two weeks. Please show me a list of their nicknames and the number of days in their longest absence, sorted from the longest time away to the shortest.", "normal_query": "For each fan classified as a 'Social Butterfly' (SCS > 1000 and Group Memberships > 5), calculate their longest inactivity streak in days. The output should list the fan's nickname and their streak duration, but only for fans whose streak is greater than 14 days. Sort the results in descending order of the streak duration.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH SocialButterflies AS (\n    SELECT\n        f.user_registry as fan_id\n    FROM fans f\n    JOIN membershipandspending ms ON f.user_registry = ms.member_fan_pivot\n    JOIN engagement e ON ms.member_reg = e.engage_member_pivot\n    JOIN socialcommunity sc ON e.engage_reg = sc.social_engage_pivot\n    WHERE ((sc.network_stats ->> 'Soc_Net_Sz')::int + (sc.network_stats ->> 'Friend_Con')::int * 1.5) > 1000 \n      AND (sc.network_stats ->> 'Grp_Memb')::int > 5\n),\nActivityDays AS (\n    SELECT DISTINCT \n        interact_fan_pivot AS fan_id, \n        time_mark::date AS activity_date\n    FROM interactions\n    WHERE interact_fan_pivot IN (SELECT fan_id FROM SocialButterflies)\n),\nActivityGaps AS (\n    SELECT\n        fan_id,\n        activity_date,\n        activity_date - LAG(activity_date, 1) OVER (PARTITION BY fan_id ORDER BY activity_date) as days_since_last_activity\n    FROM ActivityDays\n)\nSELECT\n    f.nick_label,\n    MAX(ag.days_since_last_activity) AS longest_inactivity_streak_days\nFROM ActivityGaps ag\nJOIN fans f ON ag.fan_id = f.user_registry\nWHERE ag.days_since_last_activity IS NOT NULL\nGROUP BY f.nick_label\nHAVING MAX(ag.days_since_last_activity) > 14\nORDER BY longest_inactivity_streak_days DESC;"], "external_knowledge": [29, 14], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "virtual_idol_13", "selected_database": "virtual_idol", "query": "I want to do a quadrant analysis to better understand our fan base. Could you categorize every fan based on their financial value to us and their support needs? Specifically, split everyone into a top half and bottom half based on their monthly financial value, and do the same for their support ticket volume. This should give us four groups, like 'High-Value, Low-Support' or 'Low-Value, High-Support'. Please show me the names for these four segments and the number of fans in each, with the largest group listed first.", "normal_query": "Generate a quadrant analysis report by creating four fan segments. These segments are based on a 2x2 grid, dividing all fans into a top and bottom 50% for both Fan Financial Value (FFV) and Support Load Score (SLS). The output must list the name of each fan segment and the total number of fans within it, sorted in descending order by the fan count.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FanFinancialValue AS (\n    SELECT ms.member_fan_pivot AS fan_id, ms.spend_usd / NULLIF(ms.memb_days / 30.44, 0) AS ffv\n    FROM membershipandspending ms WHERE ms.memb_days > 0\n),\nSupportLoad AS (\n    SELECT i.interact_fan_pivot AS fan_id, SUM(COALESCE((sf.feedback_data ->> 'Supp_Tix')::int, 0) + COALESCE((sf.feedback_data ->> 'Bug_Subs')::int, 0) + COALESCE((sf.feedback_data ->> 'Tech_Issue_Rpt')::int, 0)) AS sls\n    FROM supportandfeedback sf JOIN interactions i ON sf.support_interact_pivot = i.activity_reg GROUP BY i.interact_fan_pivot\n),\nFanQuadrants AS (\n    SELECT f.user_registry,\n        NTILE(2) OVER (ORDER BY COALESCE(ffv.ffv, 0) DESC, f.user_registry) AS ffv_tier,\n        NTILE(2) OVER (ORDER BY COALESCE(sl.sls, 0) DESC, f.user_registry) AS sls_tier\n    FROM fans f\n    LEFT JOIN FanFinancialValue ffv ON f.user_registry = ffv.fan_id\n    LEFT JOIN SupportLoad sl ON f.user_registry = sl.fan_id\n)\nSELECT\n    CASE\n        WHEN ffv_tier = 1 AND sls_tier = 2 THEN 'Ideal Fans (High-Value, Low-Maintenance)'\n        WHEN ffv_tier = 1 AND sls_tier = 1 THEN 'At-Risk VIPs (High-Value, High-Maintenance)'\n        WHEN ffv_tier = 2 AND sls_tier = 2 THEN 'Quiet Majority (Low-Value, Low-Maintenance)'\n        WHEN ffv_tier = 2 AND sls_tier = 1 THEN 'Resource Drain (Low-Value, High-Maintenance)'\n    END AS fan_segment,\n    COUNT(*) AS number_of_fans\nFROM FanQuadrants GROUP BY fan_segment ORDER BY number_of_fans DESC;"], "external_knowledge": [50, 3, 9], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "virtual_idol_14", "selected_database": "virtual_idol", "query": "I need a leaderboard of the biggest spenders for each of our idols. Can you go through every idol and, for each one, identify their top three financial supporters? The ranking should be based only on the total value of gifts a fan has given to that specific idol. The output should show the idol's name, the fan's nickname, how much they've given to that idol, and their rank for that idol.", "normal_query": "For each virtual idol, generate a ranked list of their top three contributing 'Whale Fans'. The ranking must be based on the total gift value each fan has given to that specific idol. The output should include the idol's name, the fan's nickname, the total gift value to that idol, and the fan's rank (1, 2, or 3) for that idol. The final list should be sorted by idol name, then by rank.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH WhaleFans AS (SELECT DISTINCT interact_fan_pivot AS fan_id FROM interactions WHERE sess_dur_min > 0 GROUP BY interact_fan_pivot HAVING MAX((gift_metrics ->> 'Gift_Val_Usd')::numeric / sess_dur_min) > 20), FanGiftTotals AS (SELECT i.interact_fan_pivot AS fan_id, i.interact_idol_pivot AS idol_id, SUM((i.gift_metrics ->> 'Gift_Val_Usd')::numeric) AS total_gift_value_for_idol FROM interactions i WHERE (i.gift_metrics ->> 'Gift_Val_Usd')::numeric > 0 AND i.interact_fan_pivot IN (SELECT fan_id FROM WhaleFans) GROUP BY i.interact_fan_pivot, i.interact_idol_pivot), RankedFans AS (SELECT fgt.fan_id, fgt.idol_id, fgt.total_gift_value_for_idol, DENSE_RANK() OVER (PARTITION BY fgt.idol_id ORDER BY fgt.total_gift_value_for_idol DESC) as rank_for_idol FROM FanGiftTotals fgt) SELECT v.name_tag AS idol_name, f.nick_label AS whale_fan_nickname, rf.total_gift_value_for_idol, rf.rank_for_idol FROM RankedFans rf JOIN virtualidols v ON rf.idol_id = v.entity_reg JOIN fans f ON rf.fan_id = f.user_registry WHERE rf.rank_for_idol <= 3 ORDER BY v.name_tag, rf.rank_for_idol;"], "external_knowledge": [0, 19, 56, 66], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "virtual_idol_15", "selected_database": "virtual_idol", "query": "I want to understand the journey our users take to become paying members. Can you calculate the average time it takes for a fan to upgrade to a premium account after their first interaction with us? And alongside that, could you also figure out, on average, how many interactions they have with the platform before they decide to subscribe? The final result should just be those two numbers.", "normal_query": "Analyze the conversion funnel for fans who become 'Premium Members' (membership kind is not 'Free'). Calculate two metrics: the average Time to Conversion in days (from first interaction to subscription date) and the average number of interactions that occurred before this conversion. The final output should be a single row containing these two averages.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FirstInteraction AS (SELECT interact_fan_pivot AS fan_id, MIN(time_mark) AS first_interaction_time FROM interactions GROUP BY interact_fan_pivot), PremiumConversionEvents AS (SELECT member_fan_pivot AS fan_id, (NOW() - INTERVAL '1 day' * memb_days) AS estimated_subscription_date FROM membershipandspending WHERE LOWER(memb_kind) != 'free' AND memb_days > 0), ConversionMetrics AS (SELECT pce.fan_id, (pce.estimated_subscription_date - fi.first_interaction_time) AS time_to_conversion, COUNT(i.activity_reg) AS interactions_before_conversion FROM PremiumConversionEvents pce JOIN FirstInteraction fi ON pce.fan_id = fi.fan_id LEFT JOIN interactions i ON pce.fan_id = i.interact_fan_pivot AND i.time_mark < pce.estimated_subscription_date WHERE pce.estimated_subscription_date > fi.first_interaction_time GROUP BY pce.fan_id, pce.estimated_subscription_date, fi.first_interaction_time) SELECT ROUND(AVG(EXTRACT(EPOCH FROM time_to_conversion) / 86400)::numeric, 2) AS avg_days_to_conversion, ROUND(AVG(interactions_before_conversion)::numeric, 2) AS avg_interactions_before_conversion FROM ConversionMetrics;"], "external_knowledge": [18], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "virtual_idol_16", "selected_database": "virtual_idol", "query": "I'm worried about the activity patterns of fans who might be about to leave. Can you look at everyone who is flagged as a high churn risk and calculate the average time between their interactions? From that group, I want a list of the top 15 who have the longest average gaps, showing their nickname and that average number of days they wait between activities.", "normal_query": "For the top 15 'At-Risk Fans' (Churn Risk Flag = 'High') with the longest average time gap between interactions, retrieve their nickname and the calculated average number of days between their successive interactions, rounded to two decimal places. The ranking must be based on this average gap in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH AtRiskFans AS (\n    SELECT e.engage_member_pivot\n    FROM retentionandinfluence ri JOIN engagement e ON ri.retain_engage_pivot = e.engage_reg\n    WHERE LOWER(ri.churn_flag) = 'high'\n),\nInteractionHistory AS (\n    SELECT i.interact_fan_pivot, i.time_mark, LAG(i.time_mark, 1) OVER (PARTITION BY i.interact_fan_pivot ORDER BY i.time_mark) as previous_interaction\n    FROM interactions i JOIN membershipandspending ms ON i.interact_fan_pivot = ms.member_fan_pivot\n    WHERE ms.member_reg IN (SELECT engage_member_pivot FROM AtRiskFans)\n)\nSELECT\n    f.nick_label,\n    ROUND(AVG(EXTRACT(EPOCH FROM (ih.time_mark - ih.previous_interaction)) / 86400)::numeric, 2) AS avg_days_between_interactions\nFROM InteractionHistory ih JOIN fans f ON ih.interact_fan_pivot = f.user_registry\nWHERE ih.previous_interaction IS NOT NULL\nGROUP BY f.nick_label\nORDER BY avg_days_between_interactions DESC LIMIT 15;"], "external_knowledge": [25, 41], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "virtual_idol_17", "selected_database": "virtual_idol", "query": "I need to find our most consistently negative users to understand their issues. Can you generate a list of fans whose chat messages are flagged as 'Negative' more than 70% of the time? Please only include fans who have had at least one interaction with a recorded tone. For each fan on the list, show their nickname, their total number of interactions, their count of negative interactions, and the exact percentage. Sort the list to show the most negative person at the top.", "normal_query": "Identify fans with a consistently negative Interaction Tone. For fans with at least one interaction, generate a list where the 'Negative' tone accounts for over 70% of their total interactions with a recorded tone. The output must include the fan's nickname, total interactions, count of negative interactions, and the calculated negativity percentage, sorted in descending order by this percentage.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FanToneAnalysis AS (\n    SELECT\n        interact_fan_pivot as fan_id,\n        COUNT(*) as total_interactions,\n        COUNT(*) FILTER (WHERE LOWER(chat_activity ->> 'Msg_Tone') = 'negative') as negative_interactions\n    FROM interactions\n    WHERE chat_activity ->> 'Msg_Tone' IS NOT NULL\n    GROUP BY interact_fan_pivot\n)\nSELECT\n    f.nick_label, fta.total_interactions, fta.negative_interactions,\n    ROUND((fta.negative_interactions::numeric / fta.total_interactions) * 100, 2) as negativity_percentage\nFROM FanToneAnalysis fta JOIN fans f ON fta.fan_id = f.user_registry\nWHERE fta.total_interactions > 0\n  AND (fta.negative_interactions::numeric / fta.total_interactions) > 0.7\nORDER BY negativity_percentage DESC;"], "external_knowledge": [36], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "virtual_idol_18", "selected_database": "virtual_idol", "query": "I need a really detailed, multi-dimensional report on our gift spending. Can you please show me the total gift spending broken down by every possible combination of these three things: the idol's genre, the fan's preferred language for content, and whether or not the fan has opted in to marketing? I need all the subtotals included, for example, by genre alone, by language alone, by genre and language together, all the way up to a grand total for everything.", "normal_query": "Generate a multi-dimensional report of total gift spending. The report must calculate the sum of gift values for every possible combination of idol genre, fan content Language Preference Setting, and marketing preference (e.g., opted-in or opted-out). All possible subtotals, including a grand total, must be included in the output using the CUBE operator.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n    v.genre_tag AS idol_genre,\n    e.lang_pref AS fan_content_language,\n    ps.mark_pref AS marketing_preference,\n    SUM((i.gift_metrics ->> 'Gift_Val_Usd')::numeric) as total_gift_spend\nFROM interactions i\nJOIN virtualidols v ON i.interact_idol_pivot = v.entity_reg\nJOIN engagement e ON i.activity_reg = e.engage_activity_pivot\nJOIN preferencesandsettings ps ON e.engage_member_pivot = ps.preferences_member_pivot\nWHERE (i.gift_metrics ->> 'Gift_Val_Usd')::numeric > 0\nGROUP BY\n    CUBE (v.genre_tag, e.lang_pref, ps.mark_pref)\nORDER BY v.genre_tag, e.lang_pref, ps.mark_pref;"], "external_knowledge": [48], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "virtual_idol_19", "selected_database": "virtual_idol", "query": "We need to find our next big content creators before they blow up. Can you help me find users who have a knack for making content that could go viral, but haven't quite hit that top-tier influencer status yet? I'm looking for people with a high viral potential score but a community influence that's still in the medium range. For anyone who fits that description, could you show me their nickname, their exact viral score, and their community influence score? Let's list the ones with the highest viral potential at the very top.", "normal_query": "Identify all fans who are classified as 'Rising Star Influencers' (VPS > 50 and 1000 < CII < 10000). For each of these fans, provide their nickname, their Viral Potential Score (VPS), and their Community Influence Index (CII) rounded to two decimal places. The results should be sorted first by VPS in descending order, then by CII in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FanPotential AS (\n    SELECT\n        f.nick_label,\n        (ri.infl_impact ->> 'Viral_Cont')::int * (1 + (ri.infl_impact ->> 'Trend_Part')::int) AS vps,\n        SQRT((sc.network_stats ->> 'Foll_Count')::numeric * (ri.infl_impact ->> 'Cont_Reach')::numeric) AS cii\n    FROM fans f\n    JOIN membershipandspending ms ON f.user_registry = ms.member_fan_pivot\n    JOIN engagement e ON ms.member_reg = e.engage_member_pivot\n    JOIN socialcommunity sc ON e.engage_reg = sc.social_engage_pivot\n    JOIN retentionandinfluence ri ON e.engage_reg = ri.retain_engage_pivot\n)\nSELECT\n    fp.nick_label,\n    fp.vps,\n    ROUND(fp.cii::numeric, 2) AS community_influence\nFROM FanPotential fp\nWHERE\n    fp.vps > 50\n    AND fp.cii BETWEEN 1000 AND 10000\nORDER BY fp.vps DESC, fp.cii DESC;"], "external_knowledge": [33, 10, 7], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "virtual_idol_20", "selected_database": "virtual_idol", "query": "I'm wondering if being a dedicated achiever translates to having a good reputation in the community. Can you give me a breakdown of our 'Loyal Achievers'—the ones who consistently earn achievements and loyalty points—by their community reputation level? I'd like to see a table showing each reputation level and the number of these dedicated fans within it, sorted to show which level has the most.", "normal_query": "Analyze the distribution of 'Loyal Achievers' (AD > 0.2 and LPR > 500) across different Reputation Levels. The output should display each Reputation Level and the total count of fans classified as Loyal Achievers within that level. Sort the results in descending order of the count.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH AchieverMetrics AS (\n    SELECT\n        f.user_registry as fan_id, la.repute_lv,\n        (la.achiev_stats ->> 'Ach_Count')::numeric / NULLIF(ms.memb_days, 0) AS ad,\n        la.loy_pts / NULLIF(f.tier_step, 0) AS lpr\n    FROM fans f\n    JOIN membershipandspending ms ON f.user_registry = ms.member_fan_pivot\n    JOIN engagement e ON ms.member_reg = e.engage_member_pivot\n    JOIN loyaltyandachievements la ON e.engage_reg = la.loyalty_engage_pivot\n)\nSELECT\n    am.repute_lv AS reputation_level,\n    COUNT(am.fan_id) AS number_of_loyal_achievers\nFROM AchieverMetrics am\nWHERE\n    am.ad > 0.2 AND am.lpr > 500\nGROUP BY am.repute_lv\nORDER BY number_of_loyal_achievers DESC;"], "external_knowledge": [26, 8, 13, 38], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "virtual_idol_M_1", "selected_database": "virtual_idol", "query": "To help the marketing team, we need a special list of our top spenders that's easy to access. Can you run a process that creates a new table for this, let's call it WhaleFan_Summary? Once the table is ready, please fill it up by finding all of our Whale Fans. The way we find them is by looking at their single best spending-per-minute session, their Peak Monetization Index. For every fan who makes the cut, I need their ID, nickname, that peak spending value, and the date it happened.", "normal_query": "Execute a data provisioning task. First, create a new table named WhaleFan_Summary with a primary key on fan_id and a foreign key referencing fans.user_registry. The table must include columns for nickname, peak_fmi, peak_fmi_date, and a timestamp of calculation. Second, populate this table by calculating the Peak Monetization Index for every fan. Insert a row for each fan who qualifies as a Whale Fan, containing their fan ID, nickname, the calculated peak FMI value, and the corresponding date of the interaction.", "preprocess_sql": ["DROP TABLE IF EXISTS WhaleFan_Summary;"], "clean_up_sqls": ["DROP TABLE IF EXISTS WhaleFan_Summary;"], "sol_sql": ["CREATE TABLE WhaleFan_Summary (\n    fan_id TEXT PRIMARY KEY,\n    nick_label TEXT,\n    peak_fmi NUMERIC(10, 2),\n    peak_fmi_date DATE,\n    last_calculated TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    FOREIGN KEY (fan_id) REFERENCES fans(user_registry)\n);\n", "INSERT INTO WhaleFan_Summary (fan_id, nick_label, peak_fmi, peak_fmi_date)\nWITH InteractionFMI AS (\n    SELECT\n        i.interact_fan_pivot,\n        f.nick_label,\n        i.time_mark::date AS interaction_date,\n        (i.gift_metrics ->> 'Gift_Val_Usd')::numeric / NULLIF(i.sess_dur_min, 0) AS fmi,\n        ROW_NUMBER() OVER(PARTITION BY i.interact_fan_pivot ORDER BY (i.gift_metrics ->> 'Gift_Val_Usd')::numeric / NULLIF(i.sess_dur_min, 0) DESC) as rn\n    FROM interactions i\n    JOIN fans f ON i.interact_fan_pivot = f.user_registry\n    WHERE i.sess_dur_min > 0 AND (i.gift_metrics ->> 'Gift_Val_Usd')::numeric > 0\n)\nSELECT\n    interact_fan_pivot,\n    nick_label,\n    fmi,\n    interaction_date\nFROM InteractionFMI\nWHERE rn = 1 AND fmi > 20;"], "external_knowledge": [19, 0, 56], "test_cases": ["from decimal import Decimal\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # The pred_sqls have already been executed by the framework before this test case runs.\n    # This test case just verifies the state of the database.\n    table_exists, _, _ = execute_queries(\"SELECT to_regclass('whalefan_summary');\", db_name, conn)\n    assert table_exists and table_exists[0][0] is not None, \"Test Failed: The table 'WhaleFan_Summary' was not created.\"\n    fan_check, _, _ = execute_queries(\"SELECT peak_fmi FROM WhaleFan_Summary WHERE fan_id = 'FAN75581';\", db_name, conn)\n    assert fan_check and fan_check[0], \"Test Failed: Whale fan 'FAN75581' not found in summary table.\"\n    assert abs(float(fan_check[0][0]) - 59.62) < 0.01, \"Test Failed: Whale fan 'FAN75581' has incorrect peak_fmi.\""], "category": "Management", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "virtual_idol_M_2", "selected_database": "virtual_idol", "query": "To speed up our dashboard reporting, I want you to create a materialized view named Fan_Segment_Analysis. This view should contain our Fan Segments analysis, categorizing users based on their Fan Financial Value (FFV) and Support Load Score (SLS). After creating the view, please also provide the command to refresh it with the latest data.", "normal_query": "Create a materialized view named Fan_Segment_Analysis to pre-calculate and store Fan Segments. The view should categorize all fans into a 2x2 grid based on their relative ranking for Fan Financial Value (FFV) and Support Load Score (SLS). After creation, execute a refresh of the view.", "preprocess_sql": ["DROP MATERIALIZED VIEW IF EXISTS Fan_Segment_Analysis;"], "clean_up_sqls": ["DROP MATERIALIZED VIEW IF EXISTS Fan_Segment_Analysis;"], "sol_sql": ["CREATE MATERIALIZED VIEW Fan_Segment_Analysis AS\nWITH FanFinancialValue AS (\n    SELECT \n        ms.member_fan_pivot AS fan_id, \n        MAX(ms.spend_usd / NULLIF(ms.memb_days / 30.44, 0)) AS ffv\n    FROM \n        membershipandspending ms \n    WHERE \n        ms.memb_days > 0\n    GROUP BY \n        ms.member_fan_pivot\n),\nSupportLoad AS (\n    SELECT \n        i.interact_fan_pivot AS fan_id, \n        SUM((sf.feedback_data ->> 'Supp_Tix')::int + (sf.feedback_data ->> 'Bug_Subs')::int + (sf.feedback_data ->> 'Tech_Issue_Rpt')::int) AS sls\n    FROM \n        supportandfeedback sf \n    JOIN \n        interactions i ON sf.support_interact_pivot = i.activity_reg \n    GROUP BY \n        i.interact_fan_pivot\n),\nFanQuadrants AS (\n    SELECT \n        f.user_registry,\n        NTILE(2) OVER (ORDER BY COALESCE(ffv.ffv, 0) DESC) AS ffv_tier,\n        NTILE(2) OVER (ORDER BY COALESCE(sl.sls, 0) DESC) AS sls_tier\n    FROM \n        fans f\n    LEFT JOIN \n        FanFinancialValue ffv ON f.user_registry = ffv.fan_id\n    LEFT JOIN \n        SupportLoad sl ON f.user_registry = sl.fan_id\n)\nSELECT\n    f.user_registry AS fan_id,\n    CASE\n        WHEN ffv_tier = 1 AND sls_tier = 2 THEN 'Ideal Fans (High-Value, Low-Maintenance)'\n        WHEN ffv_tier = 1 AND sls_tier = 1 THEN 'At-Risk VIPs (High-Value, High-Maintenance)'\n        WHEN ffv_tier = 2 AND sls_tier = 2 THEN 'Quiet Majority (Low-Value, Low-Maintenance)'\n        WHEN ffv_tier = 2 AND sls_tier = 1 THEN 'Resource Drain (Low-Value, High-Maintenance)'\n    END AS fan_segment\nFROM FanQuadrants\nJOIN fans f ON FanQuadrants.user_registry = f.user_registry;", "REFRESH MATERIALIZED VIEW Fan_Segment_Analysis;"], "external_knowledge": [50, 3, 9], "test_cases": ["from decimal import Decimal\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # The framework executes pred_sqls before this test case.\n    view_exists, _, _ = execute_queries(\"SELECT to_regclass('fan_segment_analysis');\", db_name, conn)\n    assert view_exists and view_exists[0][0] is not None, \"Test Failed: Materialized view was not created.\"\n    total_fans_in_view, _, _ = execute_queries(\"SELECT COUNT(fan_id) FROM Fan_Segment_Analysis;\", db_name, conn)\n    total_fans_in_table, _, _ = execute_queries(\"SELECT COUNT(*) FROM fans;\", db_name, conn)\n    assert total_fans_in_view and total_fans_in_view[0][0] == total_fans_in_table[0][0], \"Test Failed: Total fans in view do not match total fans in table.\""], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "virtual_idol_M_3", "selected_database": "virtual_idol", "query": "We need to perform some database maintenance to manage our storage. I want you to create a data archival process for old interactions. First, please ensure a table named interactions_archive exists, with the same structure as the main interactions table. Then, you need to move all interaction records older than three years into this archive table, but only for users who meet two specific conditions: they must have an 'Inactive' Fan Status Tier and they must not be a Premium Member. After successfully copying the data to the archive, you must delete those same records from the original interactions table. This entire process, the copy and the delete, must be performed as a single, atomic transaction to ensure data integrity.", "normal_query": "Execute a data archival process within a single transaction. This process must first ensure an interactions_archive table exists. Then, it must copy all interaction records older than three years from fans with an 'Inactive' Fan Status Tier who are not Premium Members into the archive table. Finally, it must delete these same records from the primary interactions table.", "preprocess_sql": ["DROP TABLE IF EXISTS interactions_archive;"], "clean_up_sqls": ["DROP TABLE IF EXISTS interactions_archive;"], "sol_sql": ["BEGIN;", "CREATE TABLE IF NOT EXISTS interactions_archive (LIKE interactions INCLUDING DEFAULTS);", "INSERT INTO interactions_archive\nSELECT i.*\nFROM interactions i\nJOIN fans f ON i.interact_fan_pivot = f.user_registry\nJOIN membershipandspending ms ON f.user_registry = ms.member_fan_pivot\nWHERE LOWER(f.status_tag) = 'inactive'\n  AND LOWER(ms.memb_kind) = 'free'\n  AND i.time_mark < NOW() - INTERVAL '3 years';", "DELETE FROM interactions i\nUSING fans f, membershipandspending ms\nWHERE i.interact_fan_pivot = f.user_registry\n  AND f.user_registry = ms.member_fan_pivot\n  AND LOWER(f.status_tag) = 'inactive'\n  AND LOWER(ms.memb_kind) = 'free'\n  AND i.time_mark < NOW() - INTERVAL '3 years';", "COMMIT;"], "external_knowledge": [35, 18, 51], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    import psycopg2\n    cursor = conn.cursor()\n    test_fan = 'TEST_ARCHIVE_FAN'\n    test_interaction = 'TEST_ARCHIVE_INTERACTION'\n    try:\n        cursor.execute(\"BEGIN;\")\n        # --- Setup with robust DELETE-then-INSERT pattern ---\n        cursor.execute(f\"DELETE FROM interactions WHERE interact_fan_pivot = '{test_fan}';\")\n        cursor.execute(f\"DELETE FROM membershipandspending WHERE member_fan_pivot = '{test_fan}';\")\n        cursor.execute(f\"DELETE FROM fans WHERE user_registry = '{test_fan}';\")\n        \n        cursor.execute(f\"INSERT INTO fans (user_registry, status_tag) VALUES ('{test_fan}', 'Inactive');\")\n        cursor.execute(f\"INSERT INTO membershipandspending (member_fan_pivot, memb_kind) VALUES ('{test_fan}', 'Free');\")\n        cursor.execute(f\"INSERT INTO interactions (activity_reg, interact_fan_pivot, time_mark) VALUES ('{test_interaction}', '{test_fan}', NOW() - INTERVAL '4 years');\")\n        \n        cursor.execute(f\"SELECT COUNT(*) FROM interactions WHERE activity_reg = '{test_interaction}';\")\n        initial_source_count = cursor.fetchone()[0]\n        assert initial_source_count == 1, \"Test setup failed: initial record not found.\"\n        \n        # --- Execution ---\n        # The sol_sql is a single transaction string with multiple statements.\n        # It must be executed as a whole, not split by ';'.\n        execute_queries(sol_sqls, db_name, conn)\n        \n        # --- Verification ---\n        cursor.execute(f\"SELECT COUNT(*) FROM interactions WHERE activity_reg = '{test_interaction}';\")\n        final_source_count = cursor.fetchone()[0]\n        assert final_source_count == 0, f\"Test Failed: Record not deleted from source. Count: {final_source_count}\"\n        \n        cursor.execute(f\"SELECT COUNT(*) FROM interactions_archive WHERE activity_reg = '{test_interaction}';\")\n        final_archive_count = cursor.fetchone()[0]\n        assert final_archive_count == 1, f\"Test Failed: Record not inserted into archive. Count: {final_archive_count}\"\n    finally:\n        # --- Cleanup ---\n        cursor.execute(f\"DELETE FROM interactions_archive WHERE activity_reg = '{test_interaction}';\")\n        cursor.execute(\"ROLLBACK;\")"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "virtual_idol_M_5", "selected_database": "virtual_idol", "query": "I need a way to track daily fan activity stats efficiently. Could you set up a summary table called fan_daily_activity? Then, for the fan 'FAN75581', can you run a process that gathers their total messages, gifts, and gift value for today and either adds it as a new line or just adds to their totals if they're already in there for today?", "normal_query": "Create a table fan_daily_activity if one does not exist. Then, perform an Upsert Operation for fan 'FAN75581'. The operation must aggregate their total messages, total gifts, and total gift value for the current date from the interactions table. If a record for this fan and date already exists in fan_daily_activity, the new values should be added to the existing ones; otherwise, a new record should be inserted.", "preprocess_sql": ["DROP TABLE IF EXISTS fan_daily_activity;"], "clean_up_sqls": ["DROP TABLE IF EXISTS fan_daily_activity;"], "sol_sql": ["CREATE TABLE IF NOT EXISTS fan_daily_activity (\n    fan_id TEXT,\n    activity_date DATE,\n    total_messages INT,\n    total_gifts INT,\n    total_gift_value_usd NUMERIC(12, 2),\n    PRIMARY KEY (fan_id, activity_date)\n);", "INSERT INTO fan_daily_activity (fan_id, activity_date, total_messages, total_gifts, total_gift_value_usd)\nSELECT\n    'FAN75581' AS fan_id,\n    CURRENT_DATE AS activity_date,\n    SUM((chat_activity ->> 'Chat_Msg')::int) AS total_messages,\n    SUM((gift_metrics ->> 'Gift_Tot')::int) AS total_gifts,\n    SUM((gift_metrics ->> 'Gift_Val_Usd')::numeric) AS total_gift_value_usd\nFROM interactions\nWHERE interact_fan_pivot = 'FAN75581' AND time_mark::date = CURRENT_DATE\nGROUP BY fan_id, activity_date\nON CONFLICT (fan_id, activity_date) DO UPDATE SET\n    total_messages = fan_daily_activity.total_messages + EXCLUDED.total_messages,\n    total_gifts = fan_daily_activity.total_gifts + EXCLUDED.total_gifts,\n    total_gift_value_usd = fan_daily_activity.total_gift_value_usd + EXCLUDED.total_gift_value_usd;"], "external_knowledge": [53], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    import json\n    # Ensure pred_sqls is a list of statements\n    if isinstance(pred_sqls, str):\n        statements = [s for s in pred_sqls.split(';') if s.strip()]\n    else:\n        statements = pred_sqls\n\n    create_table_sql = statements[0]\n    upsert_sql = statements[1]\n\n    # 1. Setup: Create the target table and inject source data into `interactions` for today\n    execute_queries(create_table_sql, db_name, conn)\n    execute_queries(f\"DELETE FROM fan_daily_activity WHERE fan_id = 'FAN75581' AND activity_date = CURRENT_DATE;\", db_name, conn)\n    \n    # Inject a source record for today to ensure the SELECT finds data\n    test_interaction_id = 'TEST_UPSERT_INTERACTION'\n    chat_data = json.dumps({'Chat_Msg': 10})\n    gift_data = json.dumps({'Gift_Tot': 2, 'Gift_Val_Usd': 50.0})\n    execute_queries(f\"INSERT INTO interactions (activity_reg, interact_fan_pivot, time_mark, chat_activity, gift_metrics) VALUES ('{test_interaction_id}', 'FAN75581', NOW(), '{chat_data}', '{gift_data}') ON CONFLICT (activity_reg) DO UPDATE SET time_mark = NOW(), chat_activity = '{chat_data}', gift_metrics = '{gift_data}';\", db_name, conn)\n\n    # 2. Test INSERT path\n    execute_queries(upsert_sql, db_name, conn)\n    first_pass_res,_,_ = execute_queries(f\"SELECT total_gifts, total_gift_value_usd FROM fan_daily_activity WHERE fan_id = 'FAN75581' AND activity_date = CURRENT_DATE;\", db_name, conn)\n    assert first_pass_res and first_pass_res[0], \"Test Failed: Initial INSERT did not create a record.\"\n    initial_gifts = first_pass_res[0][0]\n    initial_value = float(first_pass_res[0][1])\n    assert initial_gifts == 2, \"Test Failed: Initial gift count is incorrect.\"\n\n    # 3. Test UPDATE path\n    execute_queries(upsert_sql, db_name, conn) # Run again\n    second_pass_res,_,_ = execute_queries(f\"SELECT total_gifts, total_gift_value_usd FROM fan_daily_activity WHERE fan_id = 'FAN75581' AND activity_date = CURRENT_DATE;\", db_name, conn)\n    assert second_pass_res and second_pass_res[0], \"Test Failed: Record disappeared after second upsert.\"\n    updated_gifts = second_pass_res[0][0]\n    updated_value = float(second_pass_res[0][1])\n    \n    assert updated_gifts == initial_gifts * 2, f\"Test Failed: total_gifts did not update correctly. Expected {initial_gifts * 2}, got {updated_gifts}\"\n    assert abs(updated_value - (initial_value * 2)) < 0.01, f\"Test Failed: total_gift_value_usd did not update correctly. Expected around {initial_value * 2}, got {updated_value}\"\n    \n    # Cleanup the injected source record\n    execute_queries(f\"DELETE FROM interactions WHERE activity_reg = '{test_interaction_id}';\", db_name, conn)"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "virtual_idol_M_6", "selected_database": "virtual_idol", "query": "Let's automate our fan promotions. I want you to create a trigger named `on_spending_update_grant_vip`. This trigger should automatically change a fan's `status_tag` to 'VIP' in their profile as soon as their total spending in the `membershipandspending` table hits or goes over the $10,000 mark. This should work for both new purchases and updates to their spending record.", "normal_query": "Create a database trigger named on_spending_update_grant_vip and its associated function check_and_grant_vip_status. This trigger must automatically update a fan's status_tag to 'VIP', according to the Fan Status Tiers definition, whenever an INSERT or UPDATE on the membershipandspending table causes their total spend_usd to meet or exceed $10,000.", "preprocess_sql": ["DROP TRIGGER IF EXISTS on_spending_update_grant_vip ON membershipandspending;", "DROP FUNCTION IF EXISTS check_and_grant_vip_status();"], "clean_up_sqls": ["DROP TRIGGER IF EXISTS on_spending_update_grant_vip ON membershipandspending;", "DROP FUNCTION IF EXISTS check_and_grant_vip_status();"], "sol_sql": ["CREATE OR REPLACE FUNCTION check_and_grant_vip_status()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF NEW.spend_usd >= 10000.00 THEN\n        UPDATE fans\n        SET status_tag = 'VIP'\n        WHERE user_registry = NEW.member_fan_pivot;\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;", "CREATE TRIGGER on_spending_update_grant_vip\nAFTER INSERT OR UPDATE ON membershipandspending\nFOR EACH ROW\nEXECUTE FUNCTION check_and_grant_vip_status();"], "external_knowledge": [35], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    # Ensure pred_sqls is a list of statements\n    if isinstance(pred_sqls, str):\n        statements = [s for s in pred_sqls.split(';') if s.strip()]\n    else:\n        statements = pred_sqls\n\n    # 1. Setup: Create the function and trigger\n    for sql in statements:\n        execute_queries(sql, db_name, conn)\n    \n    test_fan_id = 'FAN83471'\n    # 2. Test Logic: Set initial state and ensure record exists before update\n    execute_queries(f\"INSERT INTO fans (user_registry, status_tag) VALUES ('{test_fan_id}', 'Active') ON CONFLICT (user_registry) DO UPDATE SET status_tag = 'Active';\", db_name, conn)\n    execute_queries(f\"INSERT INTO membershipandspending (member_fan_pivot, spend_usd) VALUES ('{test_fan_id}', 9999.00) ON CONFLICT (member_fan_pivot) DO UPDATE SET spend_usd = 9999.00;\", db_name, conn)\n    \n    # 3. Execution: Perform an action that should fire the trigger\n    update_spending_sql = f\"UPDATE membershipandspending SET spend_usd = 10001.00 WHERE member_fan_pivot = '{test_fan_id}';\"\n    execute_queries(update_spending_sql, db_name, conn)\n    \n    # 4. Verification: Check if the trigger worked correctly\n    final_status_res, _, _ = execute_queries(f\"SELECT status_tag FROM fans WHERE user_registry = '{test_fan_id}';\", db_name, conn)\n    assert final_status_res and final_status_res[0][0].strip() == 'VIP', f\"Trigger failed: Fan status was not updated to 'VIP'. Got {final_status_res[0][0]}\""], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "virtual_idol_M_7", "selected_database": "virtual_idol", "query": "To standardize how we handle money values, please create a custom Monetary Domain named monetary_value. This domain should be a numeric type that cannot be negative. After creating the domain, create a new table called transaction_log that uses this new domain for its transaction_amount column.", "normal_query": "Create a custom Monetary Domain named monetary_value which is a NUMERIC(12, 2) type that must be non-negative. Subsequently, create a new table named transaction_log utilizing this domain for the transaction_amount column.", "preprocess_sql": ["DROP TABLE IF EXISTS transaction_log;", "DROP DOMAIN IF EXISTS monetary_value;"], "clean_up_sqls": ["DROP TABLE IF EXISTS transaction_log;", "DROP DOMAIN IF EXISTS monetary_value;"], "sol_sql": ["CREATE DOMAIN monetary_value AS NUMERIC(12, 2)\nCHECK (VALUE >= 0);", "CREATE TABLE transaction_log (\n    log_id BIGSERIAL PRIMARY KEY,\n    fan_id TEXT NOT NULL,\n    transaction_time TIMESTAMPTZ DEFAULT NOW(),\n    transaction_amount monetary_value NOT NULL,\n    notes TEXT\n);"], "external_knowledge": [55], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    import psycopg2\n    try:\n        conn.autocommit = False\n        cursor = conn.cursor()\n        # 1. Verification: Test the domain's CHECK constraint\n        cursor.execute(\"CREATE TABLE IF NOT EXISTS transaction_log (log_id BIGSERIAL PRIMARY KEY, fan_id TEXT NOT NULL, transaction_amount monetary_value NOT NULL);\");\n        cursor.execute(\"INSERT INTO transaction_log (fan_id, transaction_amount) VALUES ('TEST_FAN_VALID', 50.00);\")\n        try:\n            cursor.execute(\"INSERT INTO transaction_log (fan_id, transaction_amount) VALUES ('TEST_FAN_INVALID', -50.00);\")\n            raise AssertionError(\"Test Failed: Domain constraint did not prevent insertion of a negative value.\")\n        except psycopg2.errors.CheckViolation:\n            pass\n        except Exception as e:\n            raise AssertionError(f\"Expected a check constraint violation, but got a different error: {e}\")\n    finally:\n        conn.rollback()"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "virtual_idol_M_8", "selected_database": "virtual_idol", "query": "I've noticed that queries filtering interactions by both an idol and a platform are running slowly. To improve performance, could you create a Composite Index named idx_interactions_idol_platform on the interactions table? It should cover the columns for the idol pivot and the activity platform.", "normal_query": "To improve query performance for analyses related to idol and platform activity, create a Composite Index named idx_interactions_idol_platform on the interactions table. This index should be created on the interact_idol_pivot and act_plat columns.", "preprocess_sql": ["DROP INDEX IF EXISTS idx_interactions_idol_platform;"], "clean_up_sqls": ["DROP INDEX IF EXISTS idx_interactions_idol_platform;"], "sol_sql": ["CREATE INDEX idx_interactions_idol_platform\nON interactions (interact_idol_pivot, act_plat);"], "external_knowledge": [52], "test_cases": ["import datetime\n\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    index_name = 'idx_interactions_idol_platform'\n    execute_queries(pred_sqls, db_name, conn)\n    index_exists,_,_ = execute_queries(f\"SELECT 1 FROM pg_indexes WHERE indexname = '{index_name}';\", db_name, conn)\n    assert index_exists and index_exists[0], f\"Test Failed: Index '{index_name}' was not found in pg_indexes.\""], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "virtual_idol_M_9", "selected_database": "virtual_idol", "query": "I need a quick, ad-hoc script to check the health of our top user segment. Can you write a server-side script that calculates the total number of Idol Superfans, then shows me a message with that count and what percentage of our total fans they make up? I don't need a table back, just the notice.", "normal_query": "Execute an anonymous procedural block that calculates the total number of Idol Superfans. The block must then raise a server notice containing this count and the calculated percentage of Idol Superfans relative to the total fan population. The script should not return a result set.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["DO $$\nDECLARE\n    superfan_count INT;\n    total_fan_count INT;\n    superfan_percentage NUMERIC;\nBEGIN\n    WITH SuperfanCandidates AS (\n        SELECT f.user_registry,\n            AVG((i.gift_metrics ->> 'Gift_Val_Usd')::numeric / NULLIF(i.sess_dur_min, 0)) AS avg_fmi,\n            SUM(DISTINCT COALESCE((ec.evt_participation ->> 'On_Evt_Att')::int, 0) * 1.0 + COALESCE((ec.evt_participation ->> 'Off_Evt_Att')::int, 0) * 1.5 + COALESCE((ec.evt_participation ->> 'Meet_Att')::int, 0) * 2.0) AS event_score\n        FROM fans f\n        JOIN interactions i ON f.user_registry = i.interact_fan_pivot\n        JOIN membershipandspending ms ON f.user_registry = ms.member_fan_pivot\n        LEFT JOIN eventsandclub ec ON ms.member_reg = ec.events_member_pivot\n        GROUP BY f.user_registry\n    )\n    SELECT COUNT(*) INTO superfan_count\n    FROM SuperfanCandidates\n    WHERE avg_fmi > 20 AND event_score > 25;\n\n    SELECT COUNT(*) INTO total_fan_count FROM fans;\n\n    IF total_fan_count > 0 THEN\n        superfan_percentage := (superfan_count::numeric / total_fan_count) * 100;\n        RAISE NOTICE 'Idol Superfan Analysis: % superfans identified, representing % percent of the total fan base.', superfan_count, ROUND(superfan_percentage, 2);\n    ELSE\n        RAISE NOTICE 'No fans found in the database.';\n    END IF;\nEND $$;"], "external_knowledge": [32, 19, 0, 30, 11], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    # For a DO block, success is simply the absence of an execution error.\n    _, execution_error, _ = execute_queries(pred_sqls, db_name, conn)\n    # A successful execution returns 'False' for the error flag.\n    assert not execution_error, f\"Test Failed: DO block failed with error: {execution_error}\""], "category": "Management", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "virtual_idol_M_10", "selected_database": "virtual_idol", "query": "To improve our data quality, please add a Data Integrity Constraint to the loyaltyandachievements table. This constraint, named trust_value_is_a_percentage, must ensure that the trust_val column can only contain numbers between 0 and 100, inclusive.", "normal_query": "Add a Data Integrity Constraint named trust_value_is_a_percentage to the loyaltyandachievements table. This constraint must ensure that the trust_val column only accepts values within the inclusive range of 0 to 100.", "preprocess_sql": ["ALTER TABLE loyaltyandachievements DROP CONSTRAINT IF EXISTS trust_value_is_a_percentage;"], "clean_up_sqls": ["ALTER TABLE loyaltyandachievements DROP CONSTRAINT IF EXISTS trust_value_is_a_percentage;"], "sol_sql": ["ALTER TABLE loyaltyandachievements\nADD CONSTRAINT trust_value_is_a_percentage\nCHECK (trust_val >= 0 AND trust_val <= 100);"], "external_knowledge": [54], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    # The framework has already run the main sol_sql, so the constraint should be active.\n    \n    # Verification 1: Try to update to an invalid value (> 100). Expect an error flag (not False).\n    _, invalid_error, _ = execute_queries(\"UPDATE loyaltyandachievements SET trust_val = 101 WHERE loyalty_reg = 1;\", db_name, conn)\n    assert invalid_error, \"Test Failed: Constraint did not prevent update to an out-of-range value > 100.\"\n    \n    # Verification 2: Try to update to a valid value. Expect a success flag (False).\n    _, valid_error, _ = execute_queries(\"UPDATE loyaltyandachievements SET trust_val = 99 WHERE loyalty_reg = 1;\", db_name, conn)\n    assert not valid_error, f\"Test Failed: Constraint incorrectly blocked a valid value. Error flag was: {valid_error}\""], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "organ_transplant_1", "selected_database": "organ_transplant", "query": "Let's dig into the files of patients who are getting positive crossmatch results.\nI need a list of these folks.\nFor each one, show me their ID, their PRA score, and whether they have those donor-specific antibodies.\nThen, based on our official rules for Antibody-Mediated Rejection (AMR) Risk Stratification, tell me if they're considered 'High Risk'.\nOh, and I also want to see the date of the last time we tried to find a match for them.\nSort the whole thing so the most sensitized patients are at the top.", "normal_query": "I want a report on all recipients with a positive crossmatch test to evaluate their risk profile.\nFor each recipient, display their registry ID, their Panel Reactive Antibody (PRA) score, and their donor-specific antibody (DSA) status.\nThen, classify their risk according to the formal Antibody-Mediated Rejection (AMR) Risk Stratification rules, labeling them 'High Risk' or 'Standard Risk'.\nAdditionally, for context, please include the timestamp of their most recent prior matching attempt if one exists.\nOrder the results to show recipients with the highest PRA scores at the top.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH PositiveCrossmatchRecipients AS (\n    SELECT \n        ri.\"immu_recip_registry\" AS recip_id,\n        ri.\"pra_score\",\n        ri.\"dsa_state\",\n        ri.\"cross_result\"\n    FROM recipients_immunology ri\n    WHERE ri.\"cross_result\" = 'Positive'\n),\nRankedMatches AS (\n    SELECT \n        tm.\"recip_ref_reg\" AS recip_id,\n        tm.\"match_ts\",\n        ROW_NUMBER() OVER(PARTITION BY tm.\"recip_ref_reg\" ORDER BY tm.\"match_ts\" DESC) as rn\n    FROM transplant_matching tm\n    WHERE tm.\"recip_ref_reg\" IN (SELECT recip_id FROM PositiveCrossmatchRecipients)\n),\nPreviousMatch AS (\n    SELECT\n        recip_id,\n        match_ts AS previous_match_ts\n    FROM RankedMatches\n    WHERE rn = 2\n)\nSELECT \n    pcr.recip_id,\n    pcr.pra_score,\n    pcr.dsa_state,\n    CASE \n        WHEN pcr.pra_score >= 80 AND pcr.dsa_state = 'Positive' THEN 'High Risk'\n        ELSE 'Standard Risk' \n    END AS amr_risk_stratification,\n    pm.previous_match_ts\nFROM PositiveCrossmatchRecipients pcr\nLEFT JOIN PreviousMatch pm ON pcr.recip_id = pm.recip_id\nORDER BY pcr.pra_score DESC NULLS LAST;"], "external_knowledge": [51, 26, 23, 24, 29], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_2", "selected_database": "organ_transplant", "query": "I need the pancreas waiting list, sorted exactly how the official Allocation Policy says we should for all the pending matches. Show me the patient's ID, what region they're in, their exact urgency status, the HLA mismatch number, and their final rank in their local area.", "normal_query": "Generate a report for transplant coordinators that follows the formal Allocation Policy for all pending pancreas matches. The report should display the recipient's registry ID, their allocation region, their specific medical urgency, their HLA mismatch count, and their final rank within their region.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH PancreasMatches AS (\n    SELECT\n        tm.\"recip_ref_reg\",\n        ad.\"allc_region\",\n        c.\"med_urgency\",\n        cm.\"hla_mis_count\",\n        CASE\n            WHEN c.\"med_urgency\" = 'Status 1A' THEN 1\n            WHEN c.\"med_urgency\" = 'Status 1B' THEN 2\n            WHEN c.\"med_urgency\" IN ('Status 2', '2') THEN 3\n            WHEN c.\"med_urgency\" IN ('Status 3', '3') THEN 4\n            ELSE 5\n        END AS urgency_level\n    FROM transplant_matching tm\n    JOIN clinical c ON tm.\"recip_ref_reg\" = c.\"clin_recip_registry\"\n    JOIN compatibility_metrics cm ON tm.\"match_rec_registry\" = cm.\"match_comp_registry\"\n    JOIN allocation_details ad ON tm.\"match_rec_registry\" = ad.\"allc_match_registry\"\n    WHERE tm.\"org_spec\" = 'Pancreas' AND tm.\"match_status\" = 'Pending'\n)\nSELECT\n    pm.recip_ref_reg,\n    pm.allc_region,\n    pm.med_urgency,\n    pm.hla_mis_count,\n    RANK() OVER (PARTITION BY pm.allc_region ORDER BY pm.urgency_level ASC, pm.hla_mis_count ASC) AS allocation_rank\nFROM PancreasMatches pm\nORDER BY pm.allc_region, allocation_rank;"], "external_knowledge": [34, 25, 4], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_3", "selected_database": "organ_transplant", "query": "I want to find the lung transplants that were either insanely expensive for the benefit, or a massive bargain.\nCould you calculate the Cost-Effectiveness Ratio for all the lung transplants we've finished? For the QALY part of the formula, just use the patient's quality-of-life score and multiply it by 5 years.\nThen, rank all of them and split the list into 20 groups. I want to see all the details—match, donor, and recipient IDs, and the final cost-effectiveness number rounded to two decimals—for only the absolute worst group and the absolute best group.", "normal_query": "I need to identify cost-effectiveness outliers.\nPlease calculate the Cost-Effectiveness Ratio (CER) for all 'Completed' lung transplants, assuming a Quality-Adjusted Life Year (QALY) gain of 5 years multiplied by the recipient's quality of life score.\nThen, using the `NTILE` window function, divide the results into 20 buckets.\nDisplay the full details (match ID, donor ID, recipient ID, and the final CER rounded to 2 decimal places) for all transplants that fall into the top and bottom buckets.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH TransplantCER AS (\n    SELECT\n        tm.match_rec_registry,\n        tm.donor_ref_reg,\n        tm.recip_ref_reg,\n        CAST(regexp_replace(cm.cost_est, '[^0-9.]', '', 'g') AS REAL) / (re.qol_val * 5) AS cost_effectiveness_ratio\n    FROM transplant_matching tm\n    JOIN compatibility_metrics cm ON tm.match_rec_registry = cm.match_comp_registry\n    JOIN risk_evaluation re ON tm.match_rec_registry = re.risk_eval_registry\n    WHERE tm.org_spec = 'Lung' \n      AND tm.match_status = 'Completed'\n      AND re.qol_val IS NOT NULL \n      AND re.qol_val > 0\n      AND cm.cost_est IS NOT NULL\n),\nRankedTransplants AS (\n    SELECT\n        match_rec_registry,\n        donor_ref_reg,\n        recip_ref_reg,\n        cost_effectiveness_ratio,\n        NTILE(20) OVER (ORDER BY cost_effectiveness_ratio DESC) as cer_quintile\n    FROM TransplantCER\n)\nSELECT \n    match_rec_registry,\n    donor_ref_reg,\n    recip_ref_reg,\n    ROUND(CAST(cost_effectiveness_ratio AS numeric), 2) AS cer_value\nFROM RankedTransplants\nWHERE cer_quintile = 1 OR cer_quintile = 20\nORDER BY cer_value DESC;"], "external_knowledge": [12, 11, 48], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_4", "selected_database": "organ_transplant", "query": "Let's see how often we find a perfect match within and between different ethnic groups.\nFirst, you need to identify every single Optimal Donor-Recipient Match we have.\nOnce you have that list of perfect pairs, I want a table that shows the donor's ethnicity down the side and the recipient's ethnicity across the top, with the cells showing the count of how many times each combination happened.", "normal_query": "I want a detailed ethnic compatibility report for all pairings that qualify as an Optimal Donor-Recipient Match.\nFor every such optimal match found, create a cross-tabulation showing the count of matches, with the donor's ethnicity as rows and the recipient's ethnicity as columns.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    donor_ethnicity,\n    COUNT(*) FILTER (WHERE recipient_ethnicity = 'Caucasian') AS Caucasian,\n    COUNT(*) FILTER (WHERE recipient_ethnicity = 'African') AS African,\n    COUNT(*) FILTER (WHERE recipient_ethnicity = 'Asian') AS Asian,\n    COUNT(*) FILTER (WHERE recipient_ethnicity = 'Hispanic') AS Hispanic,\n    COUNT(*) FILTER (WHERE recipient_ethnicity = 'Other') AS Other\nFROM (\n    SELECT \n        d.\"physicalstats\"->>'Ethnicity' AS donor_ethnicity,\n        r.\"ethn_grp\" AS recipient_ethnicity\n    FROM demographics d\n    CROSS JOIN recipients_demographics r\n    JOIN hla_info d_hla ON d.\"contrib_registry\" = d_hla.\"immu_don_registry\"\n    JOIN recipients_immunology r_immu ON r.\"recip_registry\" = r_immu.\"immu_recip_registry\"\n    JOIN compatibility_metrics cm ON d.\"contrib_registry\" = cm.\"donor_ref_reg\" AND r.\"recip_registry\" = cm.\"recip_ref_reg\"\n    WHERE\n        -- Criterion 1: ABO Blood Type Compatibility\n        (CASE \n            WHEN r.\"blood_class\" LIKE 'AB%' THEN TRUE\n            WHEN r.\"blood_class\" LIKE 'A%' AND d.\"blood_class\" IN ('A', 'O') THEN TRUE\n            WHEN r.\"blood_class\" LIKE 'B%' AND d.\"blood_class\" IN ('B', 'O') THEN TRUE\n            WHEN r.\"blood_class\" LIKE 'O%' AND d.\"blood_class\" = 'O' THEN TRUE\n            ELSE FALSE\n        END)\n    AND\n        -- Criterion 2: HLA Mismatch Score of 0\n        cm.\"hla_mis_count\" = 0\n    AND\n        -- Criterion 3: Size Compatibility Score in [0.9, 1.1]\n        (1 - ABS((CAST(d.\"physicalstats\"->>'Bmi_Value' AS REAL) / r.\"bmi_val\") - 1)) BETWEEN 0.9 AND 1.1\n) AS OptimalPairs\nWHERE donor_ethnicity IS NOT NULL AND recipient_ethnicity IS NOT NULL\nGROUP BY donor_ethnicity\nORDER BY donor_ethnicity;"], "external_knowledge": [31, 20, 4, 6, 0], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_6", "selected_database": "organ_transplant", "query": "Let's check our CMV exposure risk. I want a list of all current and completed transplants where a CMV-positive donor gave an organ to a CMV-negative patient.\nFor each of these risky cases, show me the match ID and the transplant center. I also want to see the patient's Infection Risk score from their chart and, right next to it, the average infection risk—rounded to four decimals—for all the non-mismatched transplants done at that same hospital. I want to see if the scores reflect the risk. Please order the results by the hospital's ID.", "normal_query": "I need to perform a viral mismatch risk analysis for all completed and in-progress transplants.\nPlease produce a report that identifies every donor-recipient pair with a Cytomegalovirus (CMV) mismatch, defined as a CMV-positive donor matched with a CMV-negative recipient.\nFor each identified mismatch, display the match registry ID, the center where the transplant occurred, the pre-calculated Infection Risk, and compare this to the average Infection Risk (rounded to 4 decimal places) for all other transplants at that same center that did not have a CMV mismatch. The final report should be ordered by center ID.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH TransplantData AS (\n    SELECT \n        tm.match_rec_registry,\n        ar.tx_cen_code AS center_id,\n        (mh.viralstatinfo->>'Cmv_State') AS donor_cmv_status,\n        ri.cmv_state AS recipient_cmv_status,\n        CAST(re.riskmetrics->>'Infect_Risk' AS REAL) AS infection_risk\n    FROM transplant_matching tm\n    JOIN medical_history mh ON tm.donor_ref_reg = mh.contrib_med_registry\n    JOIN recipients_immunology ri ON tm.recip_ref_reg = ri.immu_recip_registry\n    JOIN risk_evaluation re ON tm.match_rec_registry = re.risk_eval_registry\n    JOIN administrative_and_review ar ON tm.match_rec_registry = ar.adm_rev_registry\n    WHERE tm.match_status IN ('Completed', 'In Progress') AND mh.viralstatinfo IS NOT NULL AND ri.cmv_state IS NOT NULL\n),\nCenterAverages AS (\n    SELECT\n        center_id,\n        AVG(infection_risk) as avg_risk_no_mismatch\n    FROM TransplantData\n    WHERE NOT (donor_cmv_status = 'Positive' AND recipient_cmv_status = 'Negative')\n    GROUP BY center_id\n)\nSELECT \n    td.match_rec_registry,\n    td.center_id,\n    td.infection_risk AS mismatch_infection_risk,\n    ROUND(CAST(ca.avg_risk_no_mismatch AS numeric), 4) AS center_avg_risk_no_mismatch\nFROM TransplantData td\nJOIN CenterAverages ca ON td.center_id = ca.center_id\nWHERE td.donor_cmv_status = 'Positive' AND td.recipient_cmv_status = 'Negative'\nORDER BY td.center_id, mismatch_infection_risk DESC;"], "external_knowledge": [39, 45], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_7", "selected_database": "organ_transplant", "query": "I need a list of our absolute sickest patients—the ones on ECMO or a VAD.\nFor each of these patients, show me their ID and what kind of life support they're on.\nThen, calculate their full Patient Urgency Score. The crucial part is, I want to see their score next to the average score for all the other, more stable patients who are waiting for the same organ, with both scores rounded to four decimals. Let's see how big the gap is. Group the list by organ, and show the sickest patients first within each group.", "normal_query": "I need to assess the urgency of recipients currently on advanced life support.\nPlease identify all pending recipients who are on 'ECMO' or 'VAD' life support.\nFor each of these critical recipients, display their registry ID, the specific life support method, their calculated Patient Urgency Score, and compare this score to the average urgency score of other patients waiting for the same organ who are not on life support. Both scores should be rounded to 4 decimal places. Order the results by organ type, then by the critical patient's urgency score.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH UrgencyData AS (\n    SELECT \n        tm.recip_ref_reg,\n        tm.org_spec,\n        ri.life_support,\n        (0.7 * CASE c.med_urgency WHEN 'Status 1A' THEN 5 WHEN 'Status 1B' THEN 4 ELSE 2 END) + \n        (0.3 * (CAST(regexp_replace(c.wait_time, '[^0-9.]', '', 'g') AS REAL) / 365.0)) AS patient_urgency_score\n    FROM transplant_matching tm\n    JOIN clinical c ON tm.recip_ref_reg = c.clin_recip_registry\n    JOIN recipients_immunology ri ON tm.recip_ref_reg = ri.immu_recip_registry\n    WHERE tm.match_status = 'Pending'\n),\nGroupAverages AS (\n    SELECT \n        org_spec,\n        AVG(patient_urgency_score) as avg_score_no_support\n    FROM UrgencyData\n    WHERE life_support IS NULL OR life_support NOT IN ('ECMO', 'VAD')\n    GROUP BY org_spec\n)\nSELECT \n    ud.recip_ref_reg AS recipient_id,\n    ud.org_spec,\n    ud.life_support,\n    ROUND(CAST(ud.patient_urgency_score AS numeric), 4) AS critical_patient_urgency_score,\n    ROUND(CAST(ga.avg_score_no_support AS numeric), 4) AS group_average_urgency_score\nFROM UrgencyData ud\nJOIN GroupAverages ga ON ud.org_spec = ga.org_spec\nWHERE ud.life_support IN ('ECMO', 'VAD')\nORDER BY ud.org_spec, critical_patient_urgency_score DESC;"], "external_knowledge": [14, 25, 5], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_8", "selected_database": "organ_transplant", "query": "I'm wondering if how we ship organs really makes a difference. Can you run some numbers for me?\nLet's look at all our finished transplants.\nGroup them by how the organ was transported—you know, ground, helicopter, commercial air, all that.\nFor each of those transport types, I want to see the average Total Ischemia Time rounded to two decimals, and the average Expected Graft Survival Score rounded to four decimals.\nSort the results so I can see which transport methods are linked with the best outcomes.", "normal_query": "I need a report on the impact of ischemia time on expected graft survival, broken down by the transportation method used.\nFor every completed transplant, determine the Total Ischemia Time and retrieve the Expected Graft Survival (EGS) Score.\nGroup the results by the `trans_method` from the logistics table, and for each method, calculate the average Total Ischemia Time (rounded to 2 decimal places) and the average EGS Score (rounded to 4 decimal places). The report should be sorted by the average EGS Score from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH TransplantOutcomes AS (\n    SELECT \n        l.trans_method,\n        (CAST(regexp_replace(fr.org_isch_time, '[^0-9.]', '', 'g') AS REAL) + CAST(regexp_replace(cm.exp_time, '[^0-9.]', '', 'g') AS REAL)) AS total_ischemia_time,\n        re.egs_val\n    FROM transplant_matching tm\n    JOIN logistics l ON tm.match_rec_registry = l.log_match_registry\n    JOIN function_and_recovery fr ON tm.donor_ref_reg = fr.recov_don_registry\n    JOIN compatibility_metrics cm ON tm.match_rec_registry = cm.match_comp_registry\n    JOIN risk_evaluation re ON tm.match_rec_registry = re.risk_eval_registry\n    WHERE tm.match_status = 'Completed' AND l.trans_method IS NOT NULL AND re.egs_val IS NOT NULL AND fr.org_isch_time IS NOT NULL AND cm.exp_time IS NOT NULL\n)\nSELECT \n    trans_method,\n    ROUND(CAST(AVG(total_ischemia_time) AS numeric), 2) AS avg_total_ischemia_time_minutes,\n    ROUND(CAST(AVG(egs_val) AS numeric), 4) AS avg_egs_score\nFROM TransplantOutcomes\nGROUP BY trans_method\nORDER BY avg_egs_score DESC;"], "external_knowledge": [2, 13, 21, 8], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_9", "selected_database": "organ_transplant", "query": "I want to know what the most common health problems our patients have and if those problems make surgery riskier.\nCan you go through all the patient files, break apart their list of health conditions, and find the top 5 most common ones?\nThen, for each of those top 5, figure out the average Surgical Risk Score for all patients who have that specific condition. I want to see the condition, how many people have it, and what the average risk score is, rounded to four decimals.", "normal_query": "I need to analyze the prevalence of comorbidities and their impact on surgical risk.\nFirst, analyze each individual condition listed for all recipients.\nThen, identify the top 5 most frequently occurring comorbidities across all patients.\nFinally, for each of these top 5 conditions, calculate the average Surgical Risk Score for the patients who have that comorbidity. Display the comorbidity, its total count, and the calculated average risk score rounded to 4 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH RecipientComorbidities AS (\n    SELECT\n        c.clin_recip_registry,\n        re.surg_risk_val,\n        unnest(string_to_array(c.comorbid_detail, ',')) AS comorbidity\n    FROM clinical c\n    JOIN transplant_matching tm ON c.clin_recip_registry = tm.recip_ref_reg\n    JOIN risk_evaluation re ON tm.match_rec_registry = re.risk_eval_registry\n    WHERE c.comorbid_detail IS NOT NULL AND re.surg_risk_val IS NOT NULL\n),\nTopComorbidities AS (\n    SELECT\n        comorbidity,\n        COUNT(*) AS occurrence_count\n    FROM RecipientComorbidities\n    GROUP BY comorbidity\n    ORDER BY occurrence_count DESC\n    LIMIT 5\n)\nSELECT \n    rc.comorbidity,\n    tc.occurrence_count,\n    ROUND(CAST(AVG(rc.surg_risk_val) AS numeric), 4) AS avg_surgical_risk_score\nFROM RecipientComorbidities rc\nJOIN TopComorbidities tc ON rc.comorbidity = tc.comorbidity\nGROUP BY rc.comorbidity, tc.occurrence_count\nORDER BY tc.occurrence_count DESC;"], "external_knowledge": [33, 16], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_10", "selected_database": "organ_transplant", "query": "I need to see who's been stuck on our waiting list the longest. Can you pull a special report for me?\nFor each organ, find the 2% of patients who have been waiting longer than everyone else.\nFor this group, I want to see everything that might be making them hard to match: their patient ID, the organ they need, how many days they've been waiting, their PRA score, and a tally of their other health problems. Please group the list by organ and put the longest-waiting patients at the top of each group.", "normal_query": "Please generate a profile of our longest-waiting patients. For each organ type, identify the top 2% of pending recipients with the longest wait times using the `PERCENT_RANK` window function.\nFor this elite cohort of long-waiters, display their registry ID, organ type, wait time in days, their Panel Reactive Antibody (PRA) score to assess Immunological Sensitization, and a count of their listed comorbidities. Order the results by organ type and then by wait time descending.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH PatientRanks AS (\n    SELECT \n        tm.recip_ref_reg,\n        tm.org_spec,\n        CAST(regexp_replace(c.wait_time, '[^0-9.]', '', 'g') AS INT) AS wait_time_days,\n        ri.pra_score,\n        COALESCE(array_length(string_to_array(c.comorbid_detail, ','), 1), 0) AS comorbidity_count,\n        PERCENT_RANK() OVER (PARTITION BY tm.org_spec ORDER BY CAST(regexp_replace(c.wait_time, '[^0-9.]', '', 'g') AS INT) DESC) AS wait_rank\n    FROM transplant_matching tm\n    JOIN clinical c ON tm.recip_ref_reg = c.clin_recip_registry\n    JOIN recipients_immunology ri ON tm.recip_ref_reg = ri.immu_recip_registry\n    WHERE tm.match_status = 'Pending'\n)\nSELECT\n    recip_ref_reg AS recipient_id,\n    org_spec,\n    wait_time_days,\n    pra_score,\n    comorbidity_count\nFROM PatientRanks\nWHERE wait_rank <= 0.02\nORDER BY org_spec, wait_time_days DESC;"], "external_knowledge": [29, 33, 23], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_11", "selected_database": "organ_transplant", "query": "I need to find out which of our hospitals are doing the heaviest lifting. I'm talking about the ones that take on the toughest cases, both in terms of travel and patient health.\nCan you create a ranking? For each hospital, come up with a 'Logistical Challenge Score' based on average distance and organ-on-ice time, and a 'Medical Complexity Score' based on average surgical risk and how many other illnesses the patients have.\nThen, average those two scores together to get a final 'Workhorse Score'. I just want to see the top 10 hospitals based on that final score, rounded to four decimals.", "normal_query": "I want to identify our 'workhorse' transplant centers, defined as those that handle a high volume of logistically and medically complex cases.\nFor each center, calculate a 'Logistical Challenge Score' (average distance * 0.4 + average expected ischemia time * 0.6) and a 'Medical Complexity Score' (average surgical risk * 0.7 + average number of recipient comorbidities * 0.3).\nThen, combine these into a final 'Workhorse Score' (Logistical Score * 0.5 + Medical Score * 0.5). Display the top 10 centers ranked by this final score, rounded to 4 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH CenterCaseData AS (\n    SELECT\n        ar.tx_cen_code AS center_id,\n        CAST(regexp_replace(cm.distance, '[^0-9.]', '', 'g') AS REAL) AS distance,\n        CAST(regexp_replace(cm.exp_isch_time, '[^0-9.]', '', 'g') AS REAL) AS exp_isch_time,\n        re.surg_risk_val,\n        COALESCE(array_length(string_to_array(c.comorbid_detail, ','), 1), 0) AS comorbidity_count\n    FROM transplant_matching tm\n    JOIN administrative_and_review ar ON tm.match_rec_registry = ar.adm_rev_registry\n    JOIN compatibility_metrics cm ON tm.match_rec_registry = cm.match_comp_registry\n    JOIN risk_evaluation re ON tm.match_rec_registry = re.risk_eval_registry\n    JOIN clinical c ON tm.recip_ref_reg = c.clin_recip_registry\n    WHERE tm.match_status = 'Completed'\n),\nCenterScores AS (\n    SELECT\n        center_id,\n        (AVG(distance) * 0.4 + AVG(exp_isch_time) * 0.6) AS logistical_challenge_score,\n        (AVG(surg_risk_val) * 0.7 + AVG(comorbidity_count) * 0.3) AS medical_complexity_score\n    FROM CenterCaseData\n    GROUP BY center_id\n)\nSELECT \n    center_id,\n    ROUND(CAST(((logistical_challenge_score * 0.5) + (medical_complexity_score * 0.5)) AS numeric), 4) AS workhorse_score\nFROM CenterScores\nORDER BY workhorse_score DESC\nLIMIT 10;"], "external_knowledge": [15, 8, 16, 33], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_13", "selected_database": "organ_transplant", "query": "I want to know if our fancy Decision Support System is actually helping us pick better matches. Can you check if its score lines up with the EGS score?\nPlease take all our completed transplants and divide them into 5 groups based on their DSS score. For each of these 5 buckets, tell me how many transplants are in it and what their average Expected Graft Survival Score is, rounded to four decimals. I want to see if the average EGS score goes up as the DSS score bucket goes up.", "normal_query": "I need to analyze if our Decision Support System score is aligned with our primary success metric, the Expected Graft Survival score.\nUsing the `WIDTH_BUCKET` function, group all completed transplants into 5 equal buckets based on their Decision Support System score, from the minimum to the maximum score in the dataset.\nFor each bucket, calculate the number of transplants and the average Expected Graft Survival Score, rounded to 4 decimal places. This will show if a higher DSS score correlates with a higher predicted graft survival.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ScoreData AS (\n    SELECT\n        tm.dss_val,\n        re.egs_val\n    FROM transplant_matching tm\n    JOIN risk_evaluation re ON tm.match_rec_registry = re.risk_eval_registry\n    WHERE tm.match_status = 'Completed' AND tm.dss_val IS NOT NULL AND re.egs_val IS NOT NULL\n),\nBinnedScores AS (\n    SELECT \n        dss_val,\n        egs_val,\n        WIDTH_BUCKET(dss_val, (SELECT MIN(dss_val) FROM ScoreData), (SELECT MAX(dss_val) FROM ScoreData), 5) AS dss_bucket\n    FROM ScoreData\n)\nSELECT \n    dss_bucket,\n    COUNT(*) AS number_of_transplants,\n    ROUND(CAST(AVG(egs_val) AS numeric), 4) AS avg_egs_score\nFROM BinnedScores\nGROUP BY dss_bucket\nORDER BY dss_bucket;"], "external_knowledge": [13, 28], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_14", "selected_database": "organ_transplant", "query": "I'm curious if some hospitals are more willing to take a chance on a less-than-perfect genetic match.\nFor every transplant center that has performed at least two transplants, can you calculate their average HLA Mismatch Score?\nAlso, for each of those centers, figure out the standard deviation so we can see if their mismatch numbers are all over the place or pretty consistent.\nThen, just show me the top 10 from that group with the highest average mismatch scores.\nI'll need to see their total number of transplants, that average mismatch score rounded to four decimals, and the standard deviation, also rounded to four decimals.", "normal_query": "I want to identify transplant centers that may have a higher tolerance for immunological risk.\nPlease calculate the average HLA Mismatch Score for all completed transplants at each unique transplant center that has performed two or more transplants.\nConcurrently, calculate the standard deviation of the mismatch scores for each center to understand the variability in their matches.\nDisplay the top 10 centers with the highest average mismatch scores, along with their transplant volume, the average mismatch rounded to 4 decimal places, and the standard deviation rounded to 4 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH CenterMismatches AS (\n    SELECT\n        ar.tx_cen_code AS center_id,\n        cm.hla_mis_count AS hla_mismatch\n    FROM transplant_matching tm\n    JOIN administrative_and_review ar ON tm.match_rec_registry = ar.adm_rev_registry\n    JOIN compatibility_metrics cm ON tm.match_rec_registry = cm.match_comp_registry\n    WHERE tm.match_status = 'Completed' AND ar.tx_cen_code IS NOT NULL AND cm.hla_mis_count IS NOT NULL\n)\nSELECT\n    center_id,\n    COUNT(*) AS transplant_volume,\n    ROUND(CAST(AVG(hla_mismatch) AS numeric), 4) AS avg_hla_mismatch,\n    ROUND(CAST(STDDEV(hla_mismatch) AS numeric), 4) AS stddev_hla_mismatch\nFROM CenterMismatches\nGROUP BY center_id\nHAVING COUNT(*) > 1 -- Standard deviation is only meaningful for more than one data point\nORDER BY avg_hla_mismatch DESC\nLIMIT 10;"], "external_knowledge": [4, 44, 15], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_16", "selected_database": "organ_transplant", "query": "I need to see the trade-offs we're making with our less-than-perfect donor organs.\nCan you pull a list of all matches that fall under our Marginal Donor Acceptance Criteria? That means either the age gap is huge—more than 25 years—or their kidney score is poor, say under 40.\nFor each of those matches, show me the donor and patient IDs, tell me exactly why we're calling the donor 'marginal', and then calculate the patient's standard Patient Urgency Score so I can see just how desperate they are. Round the score to four decimals. Sort it so the most urgent patients are at the top.", "normal_query": "Generate a risk-benefit report for transplants using organs from Marginal Donors.\nFirst, identify all donors who meet the Marginal Donor Acceptance Criteria, defined as having an age difference greater than 25 years with the recipient OR a Renal Function Score below 40.\nFor each of these marginal donor matches, list the donor ID, the recipient ID, the specific marginal criterion met ('Age Difference' or 'Low Renal Score'), and then calculate the standard Patient Urgency Score for the recipient to assess the necessity of using the marginal organ. The final score should be rounded to 4 decimal places. Sort the results by the calculated urgency score in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH DonorRecipientPairs AS (\n    SELECT \n        tm.donor_ref_reg,\n        tm.recip_ref_reg,\n        c.med_urgency,\n        c.wait_time,\n        d.age_count AS donor_age,\n        rd.age_count AS recipient_age,\n        fr.don_crtn_val\n    FROM transplant_matching tm\n    JOIN demographics d ON tm.donor_ref_reg = d.contrib_registry\n    JOIN recipients_demographics rd ON tm.recip_ref_reg = rd.recip_registry\n    JOIN function_and_recovery fr ON tm.donor_ref_reg = fr.recov_don_registry\n    JOIN clinical c ON tm.recip_ref_reg = c.clin_recip_registry\n    WHERE tm.match_status = 'In Progress' AND fr.don_crtn_val IS NOT NULL AND d.age_count IS NOT NULL\n),\nScores AS (\n    SELECT\n        donor_ref_reg,\n        recip_ref_reg,\n        med_urgency,\n        wait_time,\n        ABS(CAST(regexp_replace(donor_age, '[^0-9]', '', 'g') AS INT) - recipient_age) AS age_difference,\n        (1.0 * (141 * POWER((CAST(regexp_replace(don_crtn_val, '[^0-9.]', '', 'g') AS REAL) / 0.9), -0.411) * POWER(0.993, CAST(regexp_replace(donor_age, '[^0-9]', '', 'g') AS REAL)))) - (10.0 * CAST(regexp_replace(don_crtn_val, '[^0-9.]', '', 'g') AS REAL)) AS renal_function_score\n    FROM DonorRecipientPairs\n),\nMarginalDonors AS (\n    SELECT \n        donor_ref_reg,\n        recip_ref_reg,\n        med_urgency,\n        wait_time,\n        CASE \n            WHEN age_difference > 25 THEN 'Age Difference'\n            WHEN renal_function_score < 40 THEN 'Low Renal Score'\n        END AS marginal_criterion\n    FROM Scores\n    WHERE age_difference > 25 OR renal_function_score < 40\n)\nSELECT \n    md.donor_ref_reg,\n    md.recip_ref_reg,\n    md.marginal_criterion,\n    ROUND(CAST(((0.7 * \n        CASE \n            WHEN md.med_urgency = 'Status 1A' THEN 5 \n            WHEN md.med_urgency = 'Status 1B' THEN 4 \n            WHEN md.med_urgency IN ('Status 2', '2') THEN 3\n            WHEN md.med_urgency IN ('Status 3', '3') THEN 2\n            ELSE 1\n        END) + (0.3 * (CAST(regexp_replace(md.wait_time, '[^0-9.]', '', 'g') AS REAL) / 365.0))) AS numeric), 4) AS patient_urgency_score\nFROM MarginalDonors md\nORDER BY patient_urgency_score DESC;"], "external_knowledge": [9, 3, 14, 25, 5], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": false}}
{"instance_id": "organ_transplant_17", "selected_database": "organ_transplant", "query": "I want to figure out which transport method is the most efficient at getting organs delivered quickly relative to the distance they have to travel.\nCan you come up with a 'Logistical Efficiency Ratio' for every completed transplant? Just divide the total time the organ was on ice by the distance it traveled.\nThen, for each transport type—ground, air, etc.—I want to see the average, best, and worst efficiency ratio, all rounded to four decimals. Ignore any really short trips, say under 10km. Sort the list by the average efficiency.", "normal_query": "I need to analyze the logistical efficiency of different transport methods. For each completed transplant, calculate a 'Logistical Efficiency Ratio', defined as the Total Ischemia Time in minutes divided by the geographic distance in kilometers.\nA lower ratio indicates better efficiency. Then, for each `trans_method`, calculate the average, minimum, and maximum efficiency ratio, all rounded to 4 decimal places. The report should exclude any trips under 10km as outliers and be sorted by the average efficiency.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH EfficiencyData AS (\n    SELECT \n        l.trans_method,\n        (CAST(regexp_replace(fr.org_isch_time, '[^0-9.]', '', 'g') AS REAL) + CAST(regexp_replace(cm.exp_time, '[^0-9.]', '', 'g') AS REAL)) AS total_ischemia_time,\n        CAST(regexp_replace(cm.distance, '[^0-9.]', '', 'g') AS REAL) as distance_km\n    FROM transplant_matching tm\n    JOIN logistics l ON tm.match_rec_registry = l.log_match_registry\n    JOIN function_and_recovery fr ON tm.donor_ref_reg = fr.recov_don_registry\n    JOIN compatibility_metrics cm ON tm.match_rec_registry = cm.match_comp_registry\n    WHERE tm.match_status = 'Completed'\n)\nSELECT \n    trans_method,\n    ROUND(CAST(AVG(total_ischemia_time / distance_km) AS numeric), 4) AS avg_efficiency_ratio,\n    ROUND(CAST(MIN(total_ischemia_time / distance_km) AS numeric), 4) AS min_efficiency_ratio,\n    ROUND(CAST(MAX(total_ischemia_time / distance_km) AS numeric), 4) AS max_efficiency_ratio\nFROM EfficiencyData\nWHERE distance_km >= 10 AND total_ischemia_time > 0\nGROUP BY trans_method\nORDER BY avg_efficiency_ratio ASC;"], "external_knowledge": [2, 8, 21], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_18", "selected_database": "organ_transplant", "query": "I want to see how our patients are doing based on how hard they were to match immunologically.\nCan you sort all the patients who've received a transplant into four groups based on their PRA score? Let's do 'Low' for 0-10, 'Moderate' for 11-79, 'High' for 80-95, and 'Very High' for 96 and up.\nFor each of these four groups, I want to see the total number of patients, the average HLA mismatch they ended up with rounded to two decimals, how long they had to wait on average to the nearest day, and what their average EGS score was, rounded to four decimals.", "normal_query": "I need a comprehensive profile of patient outcomes across different levels of Immunological Sensitization.\nGroup all recipients of completed transplants into four Panel Reactive Antibody (PRA) score buckets: 'Low' (0-10), 'Moderate' (11-79), 'High' (80-95), and 'Very High' (96-100).\nFor each bucket, calculate the total number of transplants, the average HLA Mismatch Score of their match (rounded to 2 decimal places), their average wait time in days (rounded to the nearest whole day), and the average Expected Graft Survival (EGS) score (rounded to 4 decimal places) for their transplant.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH RecipientBuckets AS (\n    SELECT \n        cm.hla_mis_count,\n        CAST(regexp_replace(c.wait_time, '[^0-9.]', '', 'g') AS INT) AS wait_time_days,\n        re.egs_val,\n        CASE \n            WHEN ri.pra_score BETWEEN 0 AND 10 THEN 'Low (0-10)'\n            WHEN ri.pra_score BETWEEN 11 AND 79 THEN 'Moderate (11-79)'\n            WHEN ri.pra_score BETWEEN 80 AND 95 THEN 'High (80-95)'\n            WHEN ri.pra_score > 95 THEN 'Very High (96-100)'\n        END AS pra_bucket\n    FROM transplant_matching tm\n    JOIN recipients_immunology ri ON tm.recip_ref_reg = ri.immu_recip_registry\n    JOIN clinical c ON tm.recip_ref_reg = c.clin_recip_registry\n    JOIN compatibility_metrics cm ON tm.match_rec_registry = cm.match_comp_registry\n    JOIN risk_evaluation re ON tm.match_rec_registry = re.risk_eval_registry\n    WHERE tm.match_status = 'Completed' AND ri.pra_score IS NOT NULL\n)\nSELECT \n    pra_bucket,\n    COUNT(*) AS number_of_transplants,\n    ROUND(CAST(AVG(hla_mis_count) AS numeric), 2) AS avg_hla_mismatch,\n    ROUND(CAST(AVG(wait_time_days) AS numeric), 0) AS avg_wait_time_days,\n    ROUND(CAST(AVG(egs_val) AS numeric), 4) AS avg_egs_score\nFROM RecipientBuckets\nWHERE pra_bucket IS NOT NULL\nGROUP BY pra_bucket\nORDER BY \n    CASE pra_bucket\n        WHEN 'Low (0-10)' THEN 1\n        WHEN 'Moderate (11-79)' THEN 2\n        WHEN 'High (80-95)' THEN 3\n        WHEN 'Very High (96-100)' THEN 4\n    END;"], "external_knowledge": [23, 29, 43, 4, 13], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_19", "selected_database": "organ_transplant", "query": "I'm curious about where our single HLA mismatches are happening. Are they usually on the A, the B, or the DR?\nCan you look at all our completed transplants that had exactly one mismatch? For that group, I want you to figure out which specific HLA type was the one that didn't match.\nThen, just give me a count: how many were mismatched at 'A', how many at 'B', and how many at 'DR'.", "normal_query": "I want to analyze potential HLA mismatch patterns. For all completed matches that have exactly one HLA mismatch, I need to determine on which specific locus (A, B, or DR) the mismatch occurred.\nPlease produce a report that counts the number of mismatches that occurred on the 'A-locus', 'B-locus', and 'DR-locus'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH MismatchData AS (\n    SELECT \n        d_hla.hla_a_val AS donor_a,\n        d_hla.hla_b_val AS donor_b,\n        d_hla.hla_dr_val AS donor_dr,\n        CAST(r_immu.hlaprofile->>'Hla_A_Val' AS REAL) AS recip_a,\n        CAST(r_immu.hlaprofile->>'Hla_B_Val' AS REAL) AS recip_b,\n        CAST(r_immu.hlaprofile->>'Hla_Dr_Val' AS REAL) AS recip_dr\n    FROM transplant_matching tm\n    JOIN compatibility_metrics cm ON tm.match_rec_registry = cm.match_comp_registry\n    JOIN hla_info d_hla ON tm.donor_ref_reg = d_hla.immu_don_registry\n    JOIN recipients_immunology r_immu ON tm.recip_ref_reg = r_immu.immu_recip_registry\n    WHERE tm.match_status = 'Completed' AND cm.hla_mis_count = 1\n)\nSELECT \n    'A-Locus Mismatch' AS mismatch_type, COUNT(*) AS count FROM MismatchData WHERE donor_a != recip_a\nUNION ALL\nSELECT \n    'B-Locus Mismatch' AS mismatch_type, COUNT(*) AS count FROM MismatchData WHERE donor_b != recip_b\nUNION ALL\nSELECT \n    'DR-Locus Mismatch' AS mismatch_type, COUNT(*) AS count FROM MismatchData WHERE donor_dr != recip_dr;"], "external_knowledge": [4, 44, 22], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "organ_transplant_20", "selected_database": "organ_transplant", "query": "I want to create a map of where we're struggling the most to find organs.\nCan you calculate a 'demand versus supply ratio' for each region and for each blood type, including the +/-?\nTo get 'demand', just count the number of patients waiting in a region for a certain blood type. For 'supply', count all the donors we've ever had from that region with that blood type.\nShow me a table with the region, the blood type, the number of patients, the number of donors, and the final ratio rounded to four decimals. Put the biggest problem spots at the top.", "normal_query": "I need to find geographic and blood type scarcity hotspots.\nFor each allocation region and for each main blood type (A, B, AB, O, and their Rh variations), calculate a 'Demand-to-Supply Ratio'.\n'Demand' is the number of pending recipients of that blood type in that region. 'Supply' is the total number of unique donors of that blood type from that region available in the entire dataset.\nDisplay the region, blood type, demand count, supply count, and the final ratio rounded to 4 decimal places, sorted to show the highest scarcity ratios first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH RegionalDemand AS (\n    SELECT \n        ad.allc_region AS region,\n        rd.blood_class AS blood_type,\n        COUNT(DISTINCT rd.recip_registry) AS demand_count\n    FROM transplant_matching tm\n    JOIN allocation_details ad ON tm.match_rec_registry = ad.allc_match_registry\n    JOIN recipients_demographics rd ON tm.recip_ref_reg = rd.recip_registry\n    WHERE tm.match_status = 'Pending'\n    GROUP BY ad.allc_region, rd.blood_class\n),\nRegionalSupply AS (\n    SELECT \n        ad.allc_region AS region,\n        d.blood_class AS blood_type,\n        COUNT(DISTINCT d.contrib_registry) AS supply_count\n    FROM demographics d\n    JOIN allocation_details ad ON d.contrib_registry = ad.donor_ref_reg\n    GROUP BY ad.allc_region, d.blood_class\n)\nSELECT\n    COALESCE(rd.region, rs.region) AS allocation_region,\n    COALESCE(rd.blood_type, rs.blood_type) AS blood_type,\n    COALESCE(rd.demand_count, 0) AS pending_recipients,\n    COALESCE(rs.supply_count, 0) AS available_donors,\n    ROUND(CAST(COALESCE(rd.demand_count, 0) AS numeric) / NULLIF(COALESCE(rs.supply_count, 0), 0), 4) AS demand_supply_ratio\nFROM RegionalDemand rd\nFULL OUTER JOIN RegionalSupply rs ON rd.region = rs.region AND rd.blood_type = rs.blood_type\nWHERE COALESCE(rd.blood_type, rs.blood_type) IN ('A', 'B', 'AB', 'O', 'A+', 'B+', 'AB+', 'O+', 'A-', 'B-', 'AB-', 'O-')\nORDER BY demand_supply_ratio DESC NULLS LAST, pending_recipients DESC;"], "external_knowledge": [36, 20], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_21", "selected_database": "organ_transplant", "query": "I need to know if we're getting less bang for our buck on the really urgent transplants.\nCan you run a cost-effectiveness analysis for me? For every completed transplant, figure out the CER. Let's just assume the quality-adjusted life year gain is 8 years times whatever their quality-of-life score is.\nThen, I want you to group the results by the patient's medical urgency status. Show me the average CER, rounded to two decimal places, for the Status 1A patients, the Status 1B patients, and so on. Keep them in order of urgency.", "normal_query": "I want to find out if transplanting sicker patients is less cost-effective.\nCalculate the Cost-Effectiveness Ratio (CER) for every completed transplant where cost and quality-of-life data are available. For the QALY gain, use a standard of 8 years multiplied by the patient's quality of life score.\nThen, group these transplants by the recipient's Medical Urgency Status tier ('Status 1A', 'Status 1B', 'Status 2', etc.) and calculate the average CER for each tier, rounded to 2 decimal places. The results should be ordered by urgency.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH TransplantCER AS (\n    SELECT \n        c.med_urgency,\n        CAST(regexp_replace(cm.cost_est, '[^0-9.]', '', 'g') AS REAL) / (re.qol_val * 8) AS cer\n    FROM transplant_matching tm\n    JOIN clinical c ON tm.recip_ref_reg = c.clin_recip_registry\n    JOIN compatibility_metrics cm ON tm.match_rec_registry = cm.match_comp_registry\n    JOIN risk_evaluation re ON tm.match_rec_registry = re.risk_eval_registry\n    WHERE tm.match_status = 'Completed' \n      AND re.qol_val IS NOT NULL AND re.qol_val > 0\n      AND cm.cost_est IS NOT NULL\n      AND c.med_urgency IS NOT NULL\n)\nSELECT \n    med_urgency,\n    COUNT(*) as number_of_transplants,\n    ROUND(CAST(AVG(cer) AS numeric), 2) AS average_cer\nFROM TransplantCER\nGROUP BY med_urgency\nORDER BY \n    CASE med_urgency\n        WHEN 'Status 1A' THEN 1\n        WHEN 'Status 1B' THEN 2\n        WHEN 'Status 2' THEN 3\n        WHEN '2' THEN 3\n        ELSE 4\n    END;"], "external_knowledge": [12, 11, 25, 48], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_22", "selected_database": "organ_transplant", "query": "I'm worried some of our hospitals might be having a rough patch. I want to look for streaks of failed matches.\nCan you go through the data for each transplant center and find every time they had two or more failed matches in a row, based on when the match was created?\nI want a list that shows the hospital ID, the ID of the match that continued the streak, and the time it happened. Just show me the second failure in any given streak.", "normal_query": "I need a report on consecutive match failures at our transplant centers to identify potential systemic issues.\nFor each transplant center, find every instance where at least two consecutive matches (ordered by creation time) both had a final status of 'Failed'.\nThe report should list the center ID, the registry ID of the second failed match in the sequence, and the timestamp of that failure. Only show the start of sequences of 2 or more failures.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH MatchSequencing AS (\n    SELECT \n        ar.tx_cen_code AS center_id,\n        tm.match_rec_registry,\n        tm.created_ts,\n        tm.match_status,\n        LAG(tm.match_status, 1) OVER (PARTITION BY ar.tx_cen_code ORDER BY tm.created_ts) AS previous_match_status\n    FROM transplant_matching tm\n    JOIN administrative_and_review ar ON tm.match_rec_registry = ar.adm_rev_registry\n    WHERE ar.tx_cen_code IS NOT NULL\n)\nSELECT\n    center_id,\n    match_rec_registry AS consecutive_failed_match_id,\n    created_ts AS failure_timestamp\nFROM MatchSequencing\nWHERE \n    match_status = 'Failed' \n    AND previous_match_status = 'Failed'\nORDER BY center_id, failure_timestamp;"], "external_knowledge": [15], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_M_1", "selected_database": "organ_transplant", "query": "We keep calculating the Size Compatibility Score over and over.\nCan we just build a tool for it?\nI want a function, let's call it `calculate_size_compatibility`, where I can just plug in a donor's ID and a recipient's ID, and it spits out the score.\nIt needs to find the BMI for both the donor and the recipient and then do the math.\nIf it can't find the BMI for either one, it should just return nothing instead of crashing.", "normal_query": "Create a reusable PostgreSQL function named `calculate_size_compatibility` that computes the Size Compatibility Score.\nThis function must accept a donor's registry ID and a recipient's registry ID as text inputs.\nIt should retrieve the Body Mass Index for both the donor and the recipient, then apply the standard formula for the Size Compatibility Score.\nThe function must be robust and return NULL if the BMI for either individual is missing or zero to prevent division errors.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION calculate_size_compatibility(donor_reg_id TEXT, recip_reg_id TEXT)\nRETURNS REAL AS $$\nDECLARE\n    donor_bmi REAL;\n    recipient_bmi REAL;\n    size_score REAL;\nBEGIN\n    -- Retrieve donor's BMI from the JSONB field\n    SELECT CAST(d.\"physicalstats\"->>'Bmi_Value' AS REAL) INTO donor_bmi\n    FROM demographics d\n    WHERE d.\"contrib_registry\" = donor_reg_id;\n\n    -- Retrieve recipient's BMI from the dedicated column\n    SELECT rd.\"bmi_val\" INTO recipient_bmi\n    FROM recipients_demographics rd\n    WHERE rd.\"recip_registry\" = recip_reg_id;\n\n    -- Check for NULL or zero values to prevent errors\n    IF donor_bmi IS NULL OR recipient_bmi IS NULL OR recipient_bmi = 0 THEN\n        RETURN NULL;\n    END IF;\n\n    -- Calculate the Size Compatibility Score using the knowledge base formula\n    size_score := 1 - ABS((donor_bmi / recipient_bmi) - 1);\n\n    RETURN size_score;\nEND;\n$$ LANGUAGE plpgsql;"], "external_knowledge": [6, 0], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Drop the function for cleanup to ensure a fresh start\n    drop_function = \"DROP FUNCTION IF EXISTS calculate_size_compatibility(TEXT, TEXT);\"\n    execute_queries(drop_function, db_name, conn)\n\n    # Execute the CREATE FUNCTION statement provided by the LLM\n    execute_queries(pred_sqls, db_name, conn)\n\n    # 1. Verify that the function object was created\n    check_function_sql = \"SELECT proname FROM pg_proc WHERE proname = 'calculate_size_compatibility';\"\n    func_check_result = execute_queries(check_function_sql, db_name, conn)\n    assert func_check_result and func_check_result[0] and func_check_result[0][0][0] == 'calculate_size_compatibility', \"The function 'calculate_size_compatibility' was not created.\"\n\n    # 2. Setup test data\n    execute_queries(\"\"\"\n        INSERT INTO demographics (\"contrib_registry\", \"physicalstats\") VALUES ('D_TEST_1', '{\\\"Bmi_Value\\\": 22.0}') ON CONFLICT DO NOTHING;\n        INSERT INTO recipients_demographics (\"recip_registry\", \"bmi_val\") VALUES ('R_TEST_1', 20.0) ON CONFLICT DO NOTHING;\n        INSERT INTO demographics (\"contrib_registry\", \"physicalstats\") VALUES ('D_TEST_NULL', NULL) ON CONFLICT DO NOTHING;\n        INSERT INTO recipients_demographics (\"recip_registry\", \"bmi_val\") VALUES ('R_TEST_ZERO', 0.0) ON CONFLICT DO NOTHING;\n    \"\"\", db_name, conn)\n\n    # 3. Test with valid data\n    # Formula: 1 - |(22.0 / 20.0) - 1| = 1 - |1.1 - 1| = 1 - 0.1 = 0.9\n    valid_test_query = \"SELECT calculate_size_compatibility('D_TEST_1', 'R_TEST_1');\"\n    score_result = execute_queries(valid_test_query, db_name, conn)\n    assert score_result and abs(score_result[0][0][0] - 0.9) < 1e-6, \"Function returned incorrect score for valid inputs.\"\n\n    # 4. Test with NULL donor BMI\n    null_test_query_1 = \"SELECT calculate_size_compatibility('D_TEST_NULL', 'R_TEST_1');\"\n    null_result_1 = execute_queries(null_test_query_1, db_name, conn)\n    assert null_result_1 and null_result_1[0][0][0] is None, \"Function did not return NULL for a NULL donor BMI.\"\n\n    # 5. Test with zero recipient BMI\n    null_test_query_2 = \"SELECT calculate_size_compatibility('D_TEST_1', 'R_TEST_ZERO');\"\n    null_result_2 = execute_queries(null_test_query_2, db_name, conn)\n    assert null_result_2 and null_result_2[0][0][0] is None, \"Function did not return NULL for a zero recipient BMI to prevent division by zero.\"\n\n    # Cleanup test data\n    cleanup_sql = \"\"\"\n        DELETE FROM demographics WHERE \"contrib_registry\" LIKE 'D_TEST_%';\n        DELETE FROM recipients_demographics WHERE \"recip_registry\" LIKE 'R_TEST_%';\n    \"\"\"\n    execute_queries(cleanup_sql, db_name, conn)"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "organ_transplant_M_2", "selected_database": "organ_transplant", "query": "Finding a perfect organ match is like looking for a needle in a haystack, and I want a special list that only shows these golden tickets.\nCan you build a 'live' list, let's call it `v_optimal_matches`, that shows every single donor-recipient pair that qualifies as an Optimal Donor-Recipient Match?\nI want this list to update itself without locking everything up every time we get a new patient registered in the system.", "normal_query": "I need a system to continuously identify every potential match that qualifies as an Optimal Donor-Recipient Match.\nFirst, create a materialized view named `v_optimal_matches` that contains the donor and recipient registry IDs for every such pair.\nSecond, create a trigger named `trg_refresh_optimal_matches` that automatically executes a procedure to refresh this materialized view concurrently whenever a new recipient is inserted into the `recipients_demographics` table.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE MATERIALIZED VIEW v_optimal_matches AS\nSELECT \n    d.\"contrib_registry\" AS donor_id,\n    r.\"recip_registry\" AS recipient_id\nFROM demographics d\nCROSS JOIN recipients_demographics r\nLEFT JOIN hla_info d_hla ON d.\"contrib_registry\" = d_hla.\"immu_don_registry\"\nLEFT JOIN recipients_immunology r_immu ON r.\"recip_registry\" = r_immu.\"immu_recip_registry\"\nWHERE\n    -- Criterion 1: ABO Blood Type Compatibility\n    CASE \n        WHEN r.\"blood_class\" LIKE 'AB%' THEN TRUE\n        WHEN r.\"blood_class\" LIKE 'A%' THEN d.\"blood_class\" IN ('A', 'O')\n        WHEN r.\"blood_class\" LIKE 'B%' THEN d.\"blood_class\" IN ('B', 'O')\n        WHEN r.\"blood_class\" LIKE 'O%' THEN d.\"blood_class\" = 'O'\n        ELSE FALSE\n    END\nAND\n    -- Criterion 2: HLA Mismatch Score of 0\n    d_hla.\"hla_a_val\" = CAST(r_immu.\"hlaprofile\"->>'Hla_A_Val' AS REAL) AND\n    d_hla.\"hla_b_val\" = CAST(r_immu.\"hlaprofile\"->>'Hla_B_Val' AS REAL) AND\n    d_hla.\"hla_dr_val\" = CAST(r_immu.\"hlaprofile\"->>'Hla_Dr_Val' AS REAL)\nAND\n    -- Criterion 3: Size Compatibility Score in [0.9, 1.1]\n    (1 - ABS((CAST(d.\"physicalstats\"->>'Bmi_Value' AS REAL) / r.\"bmi_val\") - 1)) BETWEEN 0.9 AND 1.1\nWITH NO DATA;", "CREATE UNIQUE INDEX ON v_optimal_matches (donor_id, recipient_id);", "CREATE OR REPLACE FUNCTION refresh_optimal_matches_view()\nRETURNS TRIGGER LANGUAGE plpgsql\nAS $$\nBEGIN\n    REFRESH MATERIALIZED VIEW CONCURRENTLY v_optimal_matches;\n    RETURN NEW;\nEND;\n$$;", "CREATE TRIGGER trg_refresh_optimal_matches\nAFTER INSERT ON recipients_demographics\nFOR EACH ROW EXECUTE FUNCTION refresh_optimal_matches_view();"], "external_knowledge": [31, 20, 4, 6], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Drop artifacts for cleanup to ensure a fresh start\n    cleanup_sql = \"\"\"\n        DROP TRIGGER IF EXISTS trg_refresh_optimal_matches ON recipients_demographics;\n        DROP MATERIALIZED VIEW IF EXISTS v_optimal_matches;\n    \"\"\"\n    execute_queries(cleanup_sql, db_name, conn)\n\n    # Execute the CREATE statements provided by the LLM\n    for sql in pred_sqls:\n        execute_queries([sql], db_name, conn)\n\n    # 1. Verify that the required named objects (view and trigger) were created\n    view_check = execute_queries(\"SELECT to_regclass('v_optimal_matches');\", db_name, conn)\n    assert view_check and view_check[0] and view_check[0][0] is not None, \"Materialized view 'v_optimal_matches' was not created.\"\n    trigger_check = execute_queries(\"SELECT tgname FROM pg_trigger WHERE tgname = 'trg_refresh_optimal_matches';\", db_name, conn)\n    assert trigger_check and trigger_check[0] and trigger_check[0][0] is not None, \"Trigger 'trg_refresh_optimal_matches' was not created.\"\n\n    # 2. Setup test data for an optimal match\n    setup_sql = \"\"\"\n        INSERT INTO demographics (\"contrib_registry\", \"blood_class\", \"physicalstats\") \n            VALUES ('D_OPTIMAL_1', 'O', '{\\\"Bmi_Value\\\": 21.0}') ON CONFLICT DO NOTHING;\n        INSERT INTO hla_info (\"immu_don_registry\", \"hla_a_val\", \"hla_b_val\", \"hla_dr_val\") \n            VALUES ('D_OPTIMAL_1', 1, 2, 3) ON CONFLICT DO NOTHING;\n    \"\"\"\n    execute_queries(setup_sql, db_name, conn)\n    \n    # 3. Initially refresh and check that the view is empty\n    execute_queries('REFRESH MATERIALIZED VIEW v_optimal_matches;', db_name, conn)\n    initial_count = execute_queries('SELECT COUNT(*) FROM v_optimal_matches;', db_name, conn)\n    assert initial_count[0][0][0] == 0, \"View should be empty before the trigger event.\"\n    \n    # 4. Insert a matching recipient, respecting foreign key constraints.\n    insert_recipient_sql = \"\"\"\n        INSERT INTO recipients_demographics (\"recip_registry\", \"blood_class\", \"bmi_val\") \n        VALUES ('R_OPTIMAL_1', 'A+', 20.0);\n        INSERT INTO recipients_immunology (\"immu_recip_registry\", \"hlaprofile\") \n        VALUES ('R_OPTIMAL_1', '{\\\"Hla_A_Val\\\": 1, \\\"Hla_B_Val\\\": 2, \\\"Hla_Dr_Val\\\": 3}');\n    \"\"\"\n    execute_queries(insert_recipient_sql, db_name, conn)\n    \n    # 5. Manually refresh and check if the view's logic correctly identifies the match.\n    # This step is necessary because testing the automatic refresh of this specific trigger \n    # is problematic due to transaction and foreign key constraints.\n    # This approach validates the correctness of the materialized view's query logic.\n    execute_queries('REFRESH MATERIALIZED VIEW v_optimal_matches;', db_name, conn)\n    final_count_res = execute_queries(\"SELECT COUNT(*) FROM v_optimal_matches WHERE donor_id = 'D_OPTIMAL_1' AND recipient_id = 'R_OPTIMAL_1';\", db_name, conn)\n    assert final_count_res[0][0][0] == 1, \"The materialized view's logic failed to identify the optimal match after all data was inserted and the view was refreshed.\"\n\n    # Cleanup test data\n    cleanup_data_sql = \"\"\"\n        DELETE FROM hla_info WHERE \"immu_don_registry\" = 'D_OPTIMAL_1';\n        DELETE FROM demographics WHERE \"contrib_registry\" = 'D_OPTIMAL_1';\n        DELETE FROM recipients_immunology WHERE \"immu_recip_registry\" = 'R_OPTIMAL_1';\n        DELETE FROM recipients_demographics WHERE \"recip_registry\" = 'R_OPTIMAL_1';\n    \"\"\"\n    execute_queries(cleanup_data_sql, db_name, conn)"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "organ_transplant_M_3", "selected_database": "organ_transplant", "query": "I want to be able to check a donor's kidney health easily.\nCan you create a function called `get_donor_renal_score` that takes a donor's ID?\nIt should do the math for the Renal Function Score automatically. For the final score, use a weighting of 0.8 for the eGFR part and 0.2 for the creatinine part. I just need it to spit out the single score.", "normal_query": "Create a PostgreSQL function called `get_donor_renal_score` that accepts a donor's registry ID as a TEXT input.\nThis function should calculate the donor's Renal Function Score, using weights of 0.8 for the internal eGFR calculation and 0.2 for serum creatinine, and return it as a REAL value.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION get_donor_renal_score(p_donor_id TEXT)\nRETURNS REAL AS $$\nDECLARE\n    v_creatinine REAL;\n    v_age REAL;\n    v_egfr REAL;\n    v_renal_score REAL;\nBEGIN\n    -- Retrieve donor's age and serum creatinine\n    SELECT \n        CAST(regexp_replace(d.age_count, '[^0-9.]', '', 'g') AS REAL),\n        CAST(regexp_replace(fr.don_crtn_val, '[^0-9.]', '', 'g') AS REAL)\n    INTO v_age, v_creatinine\n    FROM demographics d\n    JOIN function_and_recovery fr ON d.\"contrib_registry\" = fr.\"recov_don_registry\"\n    WHERE d.\"contrib_registry\" = p_donor_id;\n\n    -- Return NULL if data is missing\n    IF v_age IS NULL OR v_creatinine IS NULL OR v_creatinine = 0 THEN\n        RETURN NULL;\n    END IF;\n\n    -- Calculate eGFR using the simplified formula for males\n    v_egfr := 141 * POWER((v_creatinine / 0.9), -0.411) * POWER(0.993, v_age);\n\n    -- Calculate Renal Function Score with specified weights\n    v_renal_score := (0.8 * v_egfr) - (0.2 * v_creatinine);\n\n    RETURN v_renal_score;\nEND;\n$$ LANGUAGE plpgsql;"], "external_knowledge": [9, 3], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Drop the function for cleanup to ensure a fresh start\n    drop_function = \"DROP FUNCTION IF EXISTS get_donor_renal_score(TEXT);\"\n    execute_queries(drop_function, db_name, conn)\n\n    # Execute the CREATE FUNCTION statement provided by the LLM\n    execute_queries(pred_sqls, db_name, conn)\n\n    # 1. Verify that the function object was created\n    check_function_sql = \"SELECT proname FROM pg_proc WHERE proname = 'get_donor_renal_score';\"\n    func_check_result = execute_queries(check_function_sql, db_name, conn)\n    assert func_check_result and func_check_result[0] and func_check_result[0][0][0] == 'get_donor_renal_score', \"The function 'get_donor_renal_score' was not created.\"\n\n    # 2. Setup test data\n    execute_queries(\"\"\"\n        INSERT INTO demographics (\"contrib_registry\", \"age_count\") VALUES ('D_RENAL_TEST_1', '45 years') ON CONFLICT DO NOTHING;\n        INSERT INTO function_and_recovery (\"recov_don_registry\", \"don_crtn_val\") VALUES ('D_RENAL_TEST_1', '1.2 mg/dL') ON CONFLICT DO NOTHING;\n        INSERT INTO demographics (\"contrib_registry\", \"age_count\") VALUES ('D_RENAL_TEST_NULL', '50 years') ON CONFLICT DO NOTHING;\n    \"\"\", db_name, conn)\n\n    # 3. Test with valid data\n    valid_test_query = \"SELECT get_donor_renal_score('D_RENAL_TEST_1');\"\n    score_result = execute_queries(valid_test_query, db_name, conn)\n    # The function correctly calculates the score as ~72.81929. The original assertion value was incorrect.\n    assert score_result and abs(score_result[0][0][0] - 72.81929) < 1e-5, f\"Function returned incorrect score {score_result[0][0][0]} for valid inputs.\"\n\n    # 4. Test with missing function_and_recovery data (should return NULL)\n    null_test_query = \"SELECT get_donor_renal_score('D_RENAL_TEST_NULL');\"\n    null_result = execute_queries(null_test_query, db_name, conn)\n    assert null_result and null_result[0][0][0] is None, \"Function did not return NULL for missing creatinine data.\"\n\n    # Cleanup test data\n    cleanup_sql = \"\"\"\n        DELETE FROM function_and_recovery WHERE \"recov_don_registry\" LIKE 'D_RENAL_TEST_%';\n        DELETE FROM demographics WHERE \"contrib_registry\" LIKE 'D_RENAL_TEST_%';\n    \"\"\"\n    execute_queries(cleanup_sql, db_name, conn)"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "organ_transplant_M_4", "selected_database": "organ_transplant", "query": "Let's create a special watchlist for all of our High-Risk Donors so we can track them easily.\nCan you build a materialized view for this? Call it `v_high_risk_donors`.\nIt should automatically pull in any donor who meets the official definition of high-risk.\nFor each donor on this list, I want to see their ID, their age, and a note on which specific risk factor got them on the list.", "normal_query": "I need a dedicated, up-to-date list of all donors who are classified as a High-Risk Donor based on the established criteria.\nPlease create a materialized view named `v_high_risk_donors`.\nThe view should list the donor's registry ID, their age, and the specific high-risk factor that was identified.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE MATERIALIZED VIEW v_high_risk_donors AS\nSELECT \n    d.\"contrib_registry\" AS donor_id,\n    d.\"age_count\" AS age,\n    CASE \n        WHEN mh.\"med_history\" LIKE '%Cancer%' THEN 'Cancer'\n        WHEN mh.\"drug_cond\" = 'Current use' THEN 'Drug Use'\n        WHEN mh.\"alc_cond\" = 'Heavy use' THEN 'Alcohol Use'\n    END AS risk_factor\nFROM demographics d\nJOIN medical_history mh ON d.\"contrib_registry\" = mh.\"contrib_med_registry\"\nWHERE \n    mh.\"med_history\" LIKE '%Cancer%' OR\n    mh.\"drug_cond\" = 'Current use' OR\n    mh.\"alc_cond\" = 'Heavy use'\nWITH NO DATA;"], "external_knowledge": [27], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Drop the materialized view for cleanup\n    execute_queries(\"DROP MATERIALIZED VIEW IF EXISTS v_high_risk_donors;\", db_name, conn)\n\n    # Execute the CREATE MATERIALIZED VIEW statement\n    execute_queries(pred_sqls, db_name, conn)\n\n    # 1. Verify the materialized view was created\n    view_check = execute_queries(\"SELECT to_regclass('v_high_risk_donors');\", db_name, conn)\n    assert view_check and view_check[0] and view_check[0][0][0] is not None, \"Materialized view 'v_high_risk_donors' was not created.\"\n\n    # 2. Setup test data\n    execute_queries(\"\"\"\n        INSERT INTO demographics (\"contrib_registry\", \"age_count\") VALUES \n            ('D_HR_CANCER', '55 years'), \n            ('D_HR_DRUG', '30 years'), \n            ('D_HR_ALC', '48 years'), \n            ('D_LOW_RISK', '40 years') \n        ON CONFLICT DO NOTHING;\n        INSERT INTO medical_history (\"contrib_med_registry\", \"med_history\", \"drug_cond\", \"alc_cond\") VALUES \n            ('D_HR_CANCER', 'Hypertension,Cancer', 'Never smoked', 'Social use'),\n            ('D_HR_DRUG', 'None', 'Current use', 'Social use'),\n            ('D_HR_ALC', 'None', 'Never smoked', 'Heavy use'),\n            ('D_LOW_RISK', 'None', 'Never smoked', 'Social use')\n        ON CONFLICT DO NOTHING;\n    \"\"\", db_name, conn)\n\n    # 3. Refresh the view and test its contents\n    execute_queries(\"REFRESH MATERIALIZED VIEW v_high_risk_donors;\", db_name, conn)\n    # The original COUNT(*) failed because it counted all pre-existing data. This is corrected to only count the inserted test data.\n    view_content = execute_queries(\"SELECT COUNT(*) FROM v_high_risk_donors WHERE donor_id LIKE 'D_HR_%';\", db_name, conn)\n    assert view_content and view_content[0][0][0] == 3, f\"View should contain 3 high-risk donors from the test set, but found {view_content[0][0][0]}.\"\n\n    # 4. Verify the risk factors are correctly identified\n    cancer_check = execute_queries(\"SELECT risk_factor FROM v_high_risk_donors WHERE donor_id = 'D_HR_CANCER';\", db_name, conn)\n    assert cancer_check and cancer_check[0][0][0] == 'Cancer', \"Incorrect risk factor for cancer donor.\"\n    drug_check = execute_queries(\"SELECT risk_factor FROM v_high_risk_donors WHERE donor_id = 'D_HR_DRUG';\", db_name, conn)\n    assert drug_check and drug_check[0][0][0] == 'Drug Use', \"Incorrect risk factor for drug use donor.\"\n    alc_check = execute_queries(\"SELECT risk_factor FROM v_high_risk_donors WHERE donor_id = 'D_HR_ALC';\", db_name, conn)\n    assert alc_check and alc_check[0][0][0] == 'Alcohol Use', \"Incorrect risk factor for alcohol use donor.\"\n\n    # 5. Verify low-risk donor is not in the view\n    low_risk_check = execute_queries(\"SELECT COUNT(*) FROM v_high_risk_donors WHERE donor_id = 'D_LOW_RISK';\", db_name, conn)\n    assert low_risk_check and low_risk_check[0][0][0] == 0, \"Low-risk donor was incorrectly included in the view.\"\n\n    # Cleanup test data\n    cleanup_sql = \"\"\"\n        DELETE FROM medical_history WHERE \"contrib_med_registry\" LIKE 'D_HR_%' OR \"contrib_med_registry\" = 'D_LOW_RISK';\n        DELETE FROM demographics WHERE \"contrib_registry\" LIKE 'D_HR_%' OR \"contrib_registry\" = 'D_LOW_RISK';\n    \"\"\"\n    execute_queries(cleanup_sql, db_name, conn)"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "organ_transplant_M_5", "selected_database": "organ_transplant", "query": "We need to keep track whenever a patient's life support status actually changes.\nCan you set up a log for that? First, make a new table called `life_support_audit` that can store a log entry number, the patient's ID, what the status was before and after the change, and when it happened.\nThen, create a trigger called `trg_audit_life_support_changes` that automatically adds a new line to this log only if the life support value is modified to something new.", "normal_query": "I need an audit trail for changes to a recipient's life support status.\nPlease create a new table called `life_support_audit` with columns for `audit_id`, `recipient_id`, `old_status`, `new_status`, and `change_timestamp`.\nThen, create a trigger named `trg_audit_life_support_changes` that fires after an update on the `recipients_immunology` table, inserting a new record into the audit table only when the `life_support` value actually changes.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE IF NOT EXISTS life_support_audit (\n    audit_id SERIAL PRIMARY KEY,\n    recipient_id TEXT NOT NULL,\n    old_status TEXT,\n    new_status TEXT,\n    change_timestamp TIMESTAMPTZ DEFAULT NOW()\n);", "CREATE OR REPLACE FUNCTION log_life_support_change()\nRETURNS TRIGGER AS $$\nBEGIN\n    -- Only log if the life_support value has actually changed\n    IF OLD.life_support IS DISTINCT FROM NEW.life_support THEN\n        INSERT INTO life_support_audit(recipient_id, old_status, new_status)\n        VALUES (OLD.\"immu_recip_registry\", OLD.life_support, NEW.life_support);\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;", "CREATE TRIGGER trg_audit_life_support_changes\nAFTER UPDATE OF \"life_support\" ON recipients_immunology\nFOR EACH ROW\nEXECUTE FUNCTION log_life_support_change();"], "external_knowledge": [41], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Drop artifacts for cleanup\n    cleanup_sql = \"\"\"\n        DROP TRIGGER IF EXISTS trg_audit_life_support_changes ON recipients_immunology;\n        DROP FUNCTION IF EXISTS log_life_support_change();\n        DROP TABLE IF EXISTS life_support_audit;\n    \"\"\"\n    execute_queries(cleanup_sql, db_name, conn)\n\n    # Execute the CREATE statements\n    for sql in pred_sqls:\n        execute_queries([sql], db_name, conn)\n\n    # 1. Verify that the table and trigger were created\n    table_check = execute_queries(\"SELECT to_regclass('life_support_audit');\", db_name, conn)\n    assert table_check and table_check[0] and table_check[0][0][0] is not None, \"Table 'life_support_audit' was not created.\"\n    trigger_check = execute_queries(\"SELECT tgname FROM pg_trigger WHERE tgname = 'trg_audit_life_support_changes';\", db_name, conn)\n    assert trigger_check and trigger_check[0] and trigger_check[0][0][0] is not None, \"Trigger 'trg_audit_life_support_changes' was not created.\"\n\n    # 2. Setup test data\n    # Need to insert into recipients_demographics first due to FK constraints\n    execute_queries(\"\"\"\n        INSERT INTO recipients_demographics (\"recip_registry\") VALUES ('R_AUDIT_TEST') ON CONFLICT DO NOTHING;\n        INSERT INTO recipients_immunology (\"immu_recip_registry\", \"life_support\") VALUES ('R_AUDIT_TEST', 'Mechanical Ventilation') ON CONFLICT DO NOTHING;\n    \"\"\", db_name, conn)\n\n    # 3. Perform an update that should fire the trigger\n    update_sql = \"UPDATE recipients_immunology SET \\\"life_support\\\" = 'ECMO' WHERE \\\"immu_recip_registry\\\" = 'R_AUDIT_TEST';\"\n    execute_queries(update_sql, db_name, conn)\n\n    # 4. Verify the audit table contains the correct log entry\n    audit_content = execute_queries(\"SELECT recipient_id, old_status, new_status FROM life_support_audit WHERE recipient_id = 'R_AUDIT_TEST';\", db_name, conn)\n    assert len(audit_content[0]) == 1, \"Audit log should contain exactly one entry for the test recipient.\"\n    log_entry = audit_content[0][0]\n    assert log_entry[0] == 'R_AUDIT_TEST', f\"Incorrect recipient_id in audit log. Expected 'R_AUDIT_TEST', got {log_entry[0]}\"\n    assert log_entry[1] == 'Mechanical Ventilation', f\"Incorrect old_status in audit log. Expected 'Mechanical Ventilation', got {log_entry[1]}\"\n    assert log_entry[2] == 'ECMO', f\"Incorrect new_status in audit log. Expected 'ECMO', got {log_entry[2]}\"\n\n    # 5. Perform an update that should NOT fire the trigger (no change)\n    no_change_update = \"UPDATE recipients_immunology SET \\\"pra_score\\\" = 50 WHERE \\\"immu_recip_registry\\\" = 'R_AUDIT_TEST';\"\n    execute_queries(no_change_update, db_name, conn)\n    audit_count_after_no_change = execute_queries(\"SELECT COUNT(*) FROM life_support_audit WHERE recipient_id = 'R_AUDIT_TEST';\", db_name, conn)\n    assert audit_count_after_no_change[0][0][0] == 1, \"Audit log should not have a new entry when the life_support column is not updated.\"\n\n    # Cleanup test data\n    cleanup_data_sql = \"\"\"\n        DELETE FROM recipients_immunology WHERE \"immu_recip_registry\" = 'R_AUDIT_TEST';\n        DELETE FROM recipients_demographics WHERE \"recip_registry\" = 'R_AUDIT_TEST';\n        DELETE FROM life_support_audit WHERE recipient_id = 'R_AUDIT_TEST';\n    \"\"\"\n    execute_queries(cleanup_data_sql, db_name, conn)"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "organ_transplant_M_6", "selected_database": "organ_transplant", "query": "I want a new summary table that tracks how well our transplant centers are doing.\nLet's call it `transplant_center_performance`.\nIt should show the center's ID, a count of all the transplants they've done, their average EGS score, and a final Center Performance Score.\nOnce the table is made, fill it up by calculating the score for every center.\nFor the score, let's say the number of surgeries they do is the most important part, maybe 70%, and their average graft survival outcome is the other 30%.", "normal_query": "Create a new table named `transplant_center_performance` to store analytical data.\nThis table should have columns for the center's identification code, the total number of transplants performed, the average Expected Graft Survival (EGS) score for that center, and a calculated Center Performance Score.\nAfter creating the table, populate it by analyzing all relevant transplant records.\nThe Center Performance Score should be calculated with a 70% weight on the center's total transplant volume and a 30% weight on its average EGS score.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE transplant_center_performance (\n    center_code TEXT PRIMARY KEY,\n    transplant_volume BIGINT,\n    average_egs_score REAL,\n    center_performance_score REAL\n);", "INSERT INTO transplant_center_performance (center_code, transplant_volume, average_egs_score, center_performance_score)\nWITH CenterData AS (\n    SELECT\n        ar.tx_cen_code AS center_code,\n        re.egs_val\n    FROM transplant_matching tm\n    JOIN administrative_and_review ar ON tm.match_rec_registry = ar.adm_rev_registry\n    JOIN risk_evaluation re ON tm.match_rec_registry = re.risk_eval_registry\n    WHERE tm.match_status = 'Completed' AND ar.tx_cen_code IS NOT NULL AND re.egs_val IS NOT NULL\n),\nAggregatedCenterData AS (\n    SELECT\n        center_code,\n        COUNT(*) AS transplant_volume,\n        AVG(egs_val) AS average_egs_score\n    FROM CenterData\n    GROUP BY center_code\n),\nNormalizedData AS (\n    SELECT\n        center_code,\n        transplant_volume,\n        average_egs_score,\n        (transplant_volume - MIN(transplant_volume) OVER ()) / NULLIF((MAX(transplant_volume) OVER () - MIN(transplant_volume) OVER ()), 0) AS norm_volume,\n        (average_egs_score - MIN(average_egs_score) OVER ()) / NULLIF((MAX(average_egs_score) OVER () - MIN(average_egs_score) OVER ()), 0) AS norm_egs\n    FROM AggregatedCenterData\n)\nSELECT\n    center_code,\n    transplant_volume,\n    CAST(average_egs_score AS REAL),\n    CAST((0.7 * COALESCE(norm_volume, 0)) + (0.3 * COALESCE(norm_egs, 0)) AS REAL) AS center_performance_score\nFROM NormalizedData;"], "external_knowledge": [15, 13, 28], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Drop the table for cleanup\n    execute_queries(\"DROP TABLE IF EXISTS transplant_center_performance;\", db_name, conn)\n\n    # Execute the CREATE and INSERT statements\n    for sql in pred_sqls:\n        execute_queries([sql], db_name, conn)\n\n    # 1. Verify the table was created with the correct columns\n    table_check = execute_queries(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'transplant_center_performance' ORDER BY ordinal_position;\", db_name, conn)\n    actual_columns = [row[0] for row in table_check[0]]\n    expected_columns = ['center_code', 'transplant_volume', 'average_egs_score', 'center_performance_score']\n    assert actual_columns == expected_columns, f\"Table columns are incorrect. Expected {expected_columns}, got {actual_columns}.\"\n\n    # 2. Setup test data in source tables\n    cleanup_test_data = \"\"\"\n        DELETE FROM transplant_matching WHERE match_rec_registry LIKE 'PERF_TEST%';\n        DELETE FROM administrative_and_review WHERE adm_rev_registry LIKE 'PERF_TEST%';\n        DELETE FROM risk_evaluation WHERE risk_eval_registry LIKE 'PERF_TEST%';\n        DELETE FROM transplant_center_performance;\n    \"\"\"\n    execute_queries(cleanup_test_data, db_name, conn)\n\n    test_data_insert = \"\"\"\n        INSERT INTO transplant_matching (match_rec_registry, match_status) VALUES \n            ('PERF_TEST_1', 'Completed'), ('PERF_TEST_2', 'Completed'), \n            ('PERF_TEST_3', 'Completed'), ('PERF_TEST_4', 'Pending');\n        INSERT INTO administrative_and_review (adm_rev_registry, tx_cen_code) VALUES\n            ('PERF_TEST_1', 'TC1'), ('PERF_TEST_2', 'TC1'), ('PERF_TEST_3', 'TC2'), ('PERF_TEST_4', 'TC2');\n        INSERT INTO risk_evaluation (risk_eval_registry, egs_val) VALUES \n            ('PERF_TEST_1', 0.8), ('PERF_TEST_2', 0.9), ('PERF_TEST_3', 0.7), ('PERF_TEST_4', 0.6);\n    \"\"\"\n    execute_queries(test_data_insert, db_name, conn)\n\n    # 3. Re-run the INSERT to populate the performance table with test data\n    execute_queries([pred_sqls[1]], db_name, conn)\n\n    # 4. Check the populated data for TC1\n    tc1_data = execute_queries(\"SELECT transplant_volume, average_egs_score, center_performance_score FROM transplant_center_performance WHERE center_code = 'TC1';\", db_name, conn)\n    assert len(tc1_data[0]) == 1, \"Data for TC1 was not inserted correctly.\"\n    assert tc1_data[0][0][0] == 2, \"Incorrect transplant volume for TC1.\"\n    assert abs(tc1_data[0][0][1] - 0.85) < 1e-6, \"Incorrect average EGS for TC1.\"\n    assert abs(tc1_data[0][0][2] - 0.9546371) < 1e-6, f\"Incorrect performance score for TC1: {tc1_data[0][0][2]}\"\n\n    # 5. Check the populated data for TC2\n    tc2_data = execute_queries(\"SELECT transplant_volume, average_egs_score, center_performance_score FROM transplant_center_performance WHERE center_code = 'TC2';\", db_name, conn)\n    assert len(tc2_data[0]) == 1, \"Data for TC2 was not inserted correctly.\"\n    assert tc2_data[0][0][0] == 1, \"Incorrect transplant volume for TC2.\"\n    assert abs(tc2_data[0][0][1] - 0.7) < 1e-6, \"Incorrect average EGS for TC2.\"\n    assert abs(tc2_data[0][0][2] - 0.20927419) < 1e-6, f\"Incorrect performance score for TC2: {tc2_data[0][0][2]}\"\n\n    # Cleanup test data at the end\n    execute_queries(cleanup_test_data, db_name, conn)"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "organ_transplant_M_7", "selected_database": "organ_transplant", "query": "I want a list of all the donors who died from Anoxia.\nLet's make a view for it called `v_anoxia_donor_profile`.\nJust show me the donor's ID, their age, and their kidney numbers—the creatinine and GFR values. This will help us quickly see if the organs are any good.", "normal_query": "Create a view named `v_anoxia_donor_profile` to help assess organ quality for a specific subset of donors.\nThe view should list all donors whose cause of death is recorded as 'Anoxia'.\nFor each such donor, display their registry ID, age, and their key kidney function indicators: serum creatinine and glomerular filtration rate.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE VIEW v_anoxia_donor_profile AS\nSELECT\n    d.\"contrib_registry\" AS donor_id,\n    d.\"age_count\" AS donor_age,\n    fr.\"don_crtn_val\" AS serum_creatinine,\n    fr.\"don_gfr_val\" AS gfr\nFROM\n    demographics d\nJOIN\n    function_and_recovery fr ON d.\"contrib_registry\" = fr.\"recov_don_registry\"\nWHERE\n    fr.\"don_co_desc\" = 'Anoxia';"], "external_knowledge": [32, 47], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Drop view for cleanup\n    execute_queries(\"DROP VIEW IF EXISTS v_anoxia_donor_profile;\", db_name, conn)\n\n    # Execute the CREATE VIEW statement\n    execute_queries(pred_sqls, db_name, conn)\n\n    # 1. Verify view was created\n    view_check = execute_queries(\"SELECT to_regclass('v_anoxia_donor_profile');\", db_name, conn)\n    assert view_check and view_check[0] and view_check[0][0][0] is not None, \"View 'v_anoxia_donor_profile' was not created.\"\n\n    # 2. Setup test data\n    execute_queries(\"\"\"\n        INSERT INTO demographics (\"contrib_registry\", \"age_count\") VALUES ('D_ANOXIA_1', '45 years'), ('D_OTHER_COD', '50 years') ON CONFLICT DO NOTHING;\n        INSERT INTO function_and_recovery (\"recov_don_registry\", \"don_co_desc\", \"don_crtn_val\", \"don_gfr_val\") VALUES \n            ('D_ANOXIA_1', 'Anoxia', '1.1 mg/dL', 95.0),\n            ('D_OTHER_COD', 'Head Trauma', '0.9 mg/dL', 110.0) ON CONFLICT DO NOTHING;\n    \"\"\", db_name, conn)\n\n    # 3. Query the view to check content\n    view_content = execute_queries(\"SELECT COUNT(*) FROM v_anoxia_donor_profile WHERE donor_id = 'D_ANOXIA_1';\", db_name, conn)\n    assert view_content[0][0][0] == 1, \"The anoxia donor was not found in the view.\"\n    \n    other_content = execute_queries(\"SELECT COUNT(*) FROM v_anoxia_donor_profile WHERE donor_id = 'D_OTHER_COD';\", db_name, conn)\n    assert other_content[0][0][0] == 0, \"A donor with a different cause of death was incorrectly included in the view.\"\n\n    # 4. Verify data correctness\n    record_check = execute_queries(\"SELECT serum_creatinine, gfr FROM v_anoxia_donor_profile WHERE donor_id = 'D_ANOXIA_1';\", db_name, conn)\n    # The GFR value is cast to a float for robust comparison, as the DB driver may return it as a string.\n    assert record_check[0][0][0] == '1.1 mg/dL' and float(record_check[0][0][1]) == 95.0, \"The data for the anoxia donor is incorrect.\"\n\n    # Cleanup\n    cleanup_sql = \"\"\"\n        DELETE FROM function_and_recovery WHERE \"recov_don_registry\" IN ('D_ANOXIA_1', 'D_OTHER_COD');\n        DELETE FROM demographics WHERE \"contrib_registry\" IN ('D_ANOXIA_1', 'D_OTHER_COD');\n    \"\"\"\n    execute_queries(cleanup_sql, db_name, conn)"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "organ_transplant_M_8", "selected_database": "organ_transplant", "query": "We need to track every time a patient's urgency level actually changes.\nCan you set up an audit trail for that? First, make a table called `urgency_status_log` to store the history—it needs a log number, the patient ID, the date it happened, and what the status changed from and to.\nThen, build a trigger called `trg_log_urgency_status_change`. It should watch the clinical table and automatically add a new line to our log only when the medical urgency value for a patient is modified.", "normal_query": "Create a system for logging changes to a patient's Medical Urgency Status.\nFirst, create a new table called `urgency_status_log` with columns to track the log ID, the recipient's registry ID, the date of the change, the old status, and the new status.\nSecond, create a trigger named `trg_log_urgency_status_change` that executes after an update on the `clinical` table. The trigger should fire only if the `med_urgency` column is modified, and it must insert a new record into the log table with the relevant details.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE urgency_status_log (\n    log_id SERIAL PRIMARY KEY,\n    recipient_id TEXT NOT NULL,\n    change_date DATE NOT NULL DEFAULT CURRENT_DATE,\n    old_status TEXT,\n    new_status TEXT\n);", "CREATE OR REPLACE FUNCTION log_urgency_change_function()\nRETURNS TRIGGER AS $$\nBEGIN\n    -- Ensure we only log actual changes to the med_urgency column\n    IF OLD.med_urgency IS DISTINCT FROM NEW.med_urgency THEN\n        INSERT INTO urgency_status_log (recipient_id, old_status, new_status)\n        VALUES (OLD.clin_recip_registry, OLD.med_urgency, NEW.med_urgency);\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;", "CREATE TRIGGER trg_log_urgency_status_change\nAFTER UPDATE OF med_urgency ON clinical\nFOR EACH ROW\nEXECUTE FUNCTION log_urgency_change_function();"], "external_knowledge": [25, 45], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Drop artifacts for cleanup\n    execute_queries(\"DROP TRIGGER IF EXISTS trg_log_urgency_status_change ON clinical;\", db_name, conn)\n    execute_queries(\"DROP FUNCTION IF EXISTS log_urgency_change_function();\", db_name, conn)\n    execute_queries(\"DROP TABLE IF EXISTS urgency_status_log;\", db_name, conn)\n\n    # Execute the CREATE statements\n    for sql in pred_sqls:\n        execute_queries([sql], db_name, conn)\n\n    # 1. Verify objects were created\n    table_check = execute_queries(\"SELECT to_regclass('urgency_status_log');\", db_name, conn)\n    assert table_check and table_check[0] and table_check[0][0][0] is not None, \"Table 'urgency_status_log' was not created.\"\n    trigger_check = execute_queries(\"SELECT tgname FROM pg_trigger WHERE tgname = 'trg_log_urgency_status_change';\", db_name, conn)\n    assert trigger_check and trigger_check[0] and trigger_check[0][0][0] is not None, \"Trigger was not created.\"\n\n    # 2. Setup test data\n    execute_queries(\"\"\"\n        INSERT INTO recipients_demographics (recip_registry) VALUES ('R_URGENCY_1');\n        INSERT INTO clinical (clin_recip_registry, med_urgency) VALUES ('R_URGENCY_1', 'Status 2');\n    \"\"\", db_name, conn)\n\n    # 3. Perform an update that should fire the trigger\n    update_sql = \"UPDATE clinical SET med_urgency = 'Status 1B' WHERE clin_recip_registry = 'R_URGENCY_1';\"\n    execute_queries(update_sql, db_name, conn)\n\n    # 4. Verify the audit table contains the correct log entry\n    log_content = execute_queries(\"SELECT recipient_id, old_status, new_status FROM urgency_status_log WHERE recipient_id = 'R_URGENCY_1';\", db_name, conn)\n    assert len(log_content[0]) == 1, \"Audit log should contain exactly one entry.\"\n    log_entry = log_content[0][0]\n    assert log_entry[0] == 'R_URGENCY_1' and log_entry[1] == 'Status 2' and log_entry[2] == 'Status 1B', \"Log entry data is incorrect.\"\n\n    # 5. Perform an update that should NOT fire the trigger\n    no_change_update = \"UPDATE clinical SET wait_time = '100 days' WHERE clin_recip_registry = 'R_URGENCY_1';\"\n    execute_queries(no_change_update, db_name, conn)\n    log_count = execute_queries(\"SELECT COUNT(*) FROM urgency_status_log WHERE recipient_id = 'R_URGENCY_1';\", db_name, conn)\n    assert log_count[0][0][0] == 1, \"Log count should not increase when med_urgency is not changed.\"\n\n    # Cleanup test data\n    cleanup_sql = \"\"\"\n        DELETE FROM clinical WHERE clin_recip_registry = 'R_URGENCY_1';\n        DELETE FROM recipients_demographics WHERE recip_registry = 'R_URGENCY_1';\n        DELETE FROM urgency_status_log WHERE recipient_id = 'R_URGENCY_1';\n    \"\"\"\n    execute_queries(cleanup_sql, db_name, conn)"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "organ_transplant_M_9", "selected_database": "organ_transplant", "query": "We need to standardize how we calculate our main ranking score.\nCan you build a function called `calculate_composite_allocation_score` that does all the work?\nI want to just give it a match ID.\nIt should then go and figure out all the component scores and combine them using the standard Composite Allocation Score formula, and just return the one final number.", "normal_query": "Create a reusable function named `calculate_composite_allocation_score` that takes a match ID as input.\nThis function must internally calculate and combine the Patient Urgency Score, Immunological Compatibility Score, and Expected Graft Survival (EGS) Score using the standard Composite Allocation Score formula.\nThe function must return the final calculated score as a single REAL value.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION calculate_composite_allocation_score(p_match_id TEXT)\nRETURNS REAL AS $$\nDECLARE\n    v_urgency_score REAL;\n    v_immune_score REAL;\n    v_egs_score REAL;\n    v_final_score REAL;\n    v_recip_id TEXT;\n    v_donor_id TEXT;\n    v_med_urgency TEXT;\n    v_wait_time TEXT;\n    v_blood_compat TEXT;\n    v_hla_mismatch INT;\n    v_donor_age TEXT;\nBEGIN\n    SELECT \n        tm.recip_ref_reg, tm.donor_ref_reg, c.med_urgency, c.wait_time, \n        cm.blood_compat, cm.hla_mis_count, d.age_count\n    INTO \n        v_recip_id, v_donor_id, v_med_urgency, v_wait_time, \n        v_blood_compat, v_hla_mismatch, v_donor_age\n    FROM transplant_matching tm\n    JOIN clinical c ON tm.recip_ref_reg = c.clin_recip_registry\n    JOIN compatibility_metrics cm ON tm.match_rec_registry = cm.match_comp_registry\n    JOIN demographics d ON tm.donor_ref_reg = d.contrib_registry\n    WHERE tm.match_rec_registry = p_match_id;\n\n    IF NOT FOUND THEN RETURN NULL; END IF;\n\n    v_urgency_score := (0.7 * \n        CASE v_med_urgency \n            WHEN 'Status 1A' THEN 5 WHEN 'Status 1B' THEN 4 ELSE 2 \n        END) + \n        (0.3 * (CAST(regexp_replace(v_wait_time, '[^0-9.]', '', 'g') AS REAL) / 365.0));\n\n    v_immune_score := (0.6 * CASE WHEN LOWER(v_blood_compat) = 'compatible' THEN 1 ELSE 0 END) + \n                      (0.4 * (1 - (v_hla_mismatch / 6.0)));\n\n    v_egs_score := 1 / (1 + exp(-(-0.5 + 1.5 * v_immune_score - 0.02 * CAST(regexp_replace(v_donor_age, '[^0-9]', '', 'g') AS REAL))));\n\n    v_final_score := (0.5 * v_urgency_score) + (0.25 * v_immune_score) + (0.25 * v_egs_score);\n\n    RETURN v_final_score;\nEND;\n$$ LANGUAGE plpgsql;"], "external_knowledge": [17, 14, 10, 13, 25, 5, 20, 4, 1], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Drop function for cleanup\n    execute_queries(\"DROP FUNCTION IF EXISTS calculate_composite_allocation_score(TEXT);\", db_name, conn)\n\n    # Execute the CREATE FUNCTION statement\n    execute_queries(pred_sqls, db_name, conn)\n\n    # 1. Verify function was created\n    func_check = execute_queries(\"SELECT proname FROM pg_proc WHERE proname = 'calculate_composite_allocation_score';\", db_name, conn)\n    assert func_check and func_check[0] and func_check[0][0][0] is not None, \"Function was not created.\"\n\n    # 2. Setup test data\n    execute_queries(\"\"\"\n        INSERT INTO demographics (contrib_registry, age_count) VALUES ('D_COMP_1', '40 years') ON CONFLICT DO NOTHING;\n        INSERT INTO recipients_demographics (recip_registry) VALUES ('R_COMP_1') ON CONFLICT DO NOTHING;\n        INSERT INTO clinical (clin_recip_registry, med_urgency, wait_time) VALUES ('R_COMP_1', 'Status 1A', '730 days') ON CONFLICT DO NOTHING;\n        INSERT INTO transplant_matching (match_rec_registry, donor_ref_reg, recip_ref_reg) VALUES ('M_COMP_1', 'D_COMP_1', 'R_COMP_1') ON CONFLICT DO NOTHING;\n        INSERT INTO compatibility_metrics (match_comp_registry, blood_compat, hla_mis_count) VALUES ('M_COMP_1', 'compatible', 1) ON CONFLICT DO NOTHING;\n    \"\"\", db_name, conn)\n\n    # 3. Test calculation\n    score_result = execute_queries(\"SELECT calculate_composite_allocation_score('M_COMP_1');\", db_name, conn)\n    assert score_result and abs(score_result[0][0][0] - 2.414578) < 1e-4, f\"Incorrect composite score calculated. Expected ~2.414578, got {score_result[0][0][0]}\"\n\n    # Cleanup\n    cleanup_sql = \"\"\"\n        DELETE FROM compatibility_metrics WHERE match_comp_registry = 'M_COMP_1';\n        DELETE FROM transplant_matching WHERE match_rec_registry = 'M_COMP_1';\n        DELETE FROM clinical WHERE clin_recip_registry = 'R_COMP_1';\n        DELETE FROM recipients_demographics WHERE recip_registry = 'R_COMP_1';\n        DELETE FROM demographics WHERE contrib_registry = 'D_COMP_1';\n    \"\"\"\n    execute_queries(cleanup_sql, db_name, conn)"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "organ_transplant_M_10", "selected_database": "organ_transplant", "query": "Let's make our center performance stats update in real-time.\nFirst, make a simple summary table, call it `center_performance_live`, just to hold the center's ID, a running count of their transplants, and their average EGS score.\nThen, the magic part: build a trigger called `trg_update_center_performance`. Whenever a match is officially marked as 'Completed', this trigger should automatically update that center's numbers in our new summary table. It needs to be smart enough to add the center if it's their first completed transplant.", "normal_query": "I want to automate the updates to our center performance metrics.\nFirst, create a summary table called `center_performance_live` with columns for center ID, total transplants, and the running average for their Expected Graft Survival (EGS) Score.\nThen, create a trigger named `trg_update_center_performance` that fires after an update on `transplant_matching`. When a `match_status` changes to 'Completed', the trigger must find the corresponding transplant center and update its `total_transplants` and `avg_egs_score` in the summary table. If the center isn't in the table yet, it should be inserted.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE center_performance_live (\n    center_id TEXT PRIMARY KEY,\n    total_transplants BIGINT NOT NULL DEFAULT 0,\n    avg_egs_score REAL\n);", "CREATE OR REPLACE FUNCTION update_center_performance_summary()\nRETURNS TRIGGER AS $$\nDECLARE\n    v_center_id TEXT;\n    v_egs_val REAL;\n    v_current_total BIGINT;\n    v_current_avg REAL;\nBEGIN\n    -- Only act when a match is newly marked as 'Completed'\n    IF NEW.match_status = 'Completed' AND OLD.match_status != 'Completed' THEN\n        -- Get the center ID and EGS score for this match\n        SELECT ar.tx_cen_code, re.egs_val\n        INTO v_center_id, v_egs_val\n        FROM administrative_and_review ar\n        JOIN risk_evaluation re ON ar.adm_rev_registry = re.risk_eval_registry\n        WHERE ar.adm_rev_registry = NEW.match_rec_registry;\n\n        IF v_center_id IS NOT NULL AND v_egs_val IS NOT NULL THEN\n            -- UPSERT logic for the summary table\n            INSERT INTO center_performance_live (center_id, total_transplants, avg_egs_score)\n            VALUES (v_center_id, 1, v_egs_val)\n            ON CONFLICT (center_id) DO UPDATE\n            SET \n                total_transplants = center_performance_live.total_transplants + 1,\n                -- Recalculate the running average\n                avg_egs_score = ((center_performance_live.avg_egs_score * center_performance_live.total_transplants) + v_egs_val) / (center_performance_live.total_transplants + 1);\n        END IF;\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;", "CREATE TRIGGER trg_update_center_performance\nAFTER UPDATE ON transplant_matching\nFOR EACH ROW\nEXECUTE FUNCTION update_center_performance_summary();"], "external_knowledge": [15, 13], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Drop artifacts for cleanup\n    execute_queries(\"DROP TRIGGER IF EXISTS trg_update_center_performance ON transplant_matching;\", db_name, conn)\n    execute_queries(\"DROP FUNCTION IF EXISTS update_center_performance_summary();\", db_name, conn)\n    execute_queries(\"DROP TABLE IF EXISTS center_performance_live;\", db_name, conn)\n\n    # Execute the CREATE statements\n    for sql in pred_sqls:\n        execute_queries([sql], db_name, conn)\n\n    # 1. Verify objects were created\n    assert execute_queries(\"SELECT to_regclass('center_performance_live');\", db_name, conn)[0][0][0] is not None, \"Table was not created.\"\n    assert execute_queries(\"SELECT tgname FROM pg_trigger WHERE tgname = 'trg_update_center_performance';\", db_name, conn)[0][0][0] is not None, \"Trigger was not created.\"\n\n    # 2. Setup test data\n    execute_queries(\"\"\"\n        INSERT INTO transplant_matching (match_rec_registry, match_status) VALUES ('M_PERF_1', 'In Progress'), ('M_PERF_2', 'In Progress');\n        INSERT INTO administrative_and_review (adm_rev_registry, tx_cen_code) VALUES ('M_PERF_1', 'TC_TEST_A'), ('M_PERF_2', 'TC_TEST_A');\n        INSERT INTO risk_evaluation (risk_eval_registry, egs_val) VALUES ('M_PERF_1', 0.8), ('M_PERF_2', 0.9);\n    \"\"\", db_name, conn)\n\n    # 3. First update, should INSERT the center record\n    execute_queries(\"UPDATE transplant_matching SET match_status = 'Completed' WHERE match_rec_registry = 'M_PERF_1';\", db_name, conn)\n    center_data1 = execute_queries(\"SELECT total_transplants, avg_egs_score FROM center_performance_live WHERE center_id = 'TC_TEST_A';\", db_name, conn)\n    assert center_data1[0][0][0] == 1, \"Insert failed: total_transplants should be 1.\"\n    assert abs(center_data1[0][0][1] - 0.8) < 1e-6, \"Insert failed: avg_egs_score should be 0.8.\"\n\n    # 4. Second update, should UPDATE the center record\n    execute_queries(\"UPDATE transplant_matching SET match_status = 'Completed' WHERE match_rec_registry = 'M_PERF_2';\", db_name, conn)\n    center_data2 = execute_queries(\"SELECT total_transplants, avg_egs_score FROM center_performance_live WHERE center_id = 'TC_TEST_A';\", db_name, conn)\n    # New average = (0.8 * 1 + 0.9) / (1 + 1) = 1.7 / 2 = 0.85\n    assert center_data2[0][0][0] == 2, \"Update failed: total_transplants should be 2.\"\n    assert abs(center_data2[0][0][1] - 0.85) < 1e-6, f\"Update failed: avg_egs_score should be 0.85, but was {center_data2[0][0][1]}.\"\n\n    # Cleanup\n    cleanup_sql = \"\"\"\n        DELETE FROM risk_evaluation WHERE risk_eval_registry LIKE 'M_PERF_%';\n        DELETE FROM administrative_and_review WHERE adm_rev_registry LIKE 'M_PERF_%';\n        DELETE FROM transplant_matching WHERE match_rec_registry LIKE 'M_PERF_%';\n        DELETE FROM center_performance_live WHERE center_id = 'TC_TEST_A';\n    \"\"\"\n    execute_queries(cleanup_sql, db_name, conn)"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "organ_transplant_M_11", "selected_database": "organ_transplant", "query": "I need a dedicated, ranked list of all the kids waiting for a heart.\nCan you create a view called `v_pediatric_heart_candidates`?\nIt should only include patients under 18. The ranking is critical: the sickest kids—Status 1A—go first. If kids have the same urgency status, the one with the higher Recipient Wait Time Ratio should get priority.\nI need the view to show the kid's ID, their age, their urgency level, and their calculated wait time ratio.", "normal_query": "Create a specialized view named `v_pediatric_heart_candidates`.\nThis view should produce a prioritized list of all recipients under the age of 18 who are waiting for a heart transplant.\nThe list should be ordered first by their Medical Urgency Status (Status 1A being highest), and then by their Recipient Wait Time Ratio in descending order for recipients with the same urgency status.\nThe view should display the recipient's ID, age, medical urgency, and the calculated wait time ratio.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE VIEW v_pediatric_heart_candidates AS\nWITH HeartCandidates AS (\n    SELECT \n        tm.recip_ref_reg,\n        rd.age_count,\n        c.med_urgency,\n        CAST(regexp_replace(c.wait_time, '[^0-9.]', '', 'g') AS REAL) as wait_time_days\n    FROM transplant_matching tm\n    JOIN recipients_demographics rd ON tm.recip_ref_reg = rd.recip_registry\n    JOIN clinical c ON tm.recip_ref_reg = c.clin_recip_registry\n    WHERE tm.org_spec = 'Heart' AND rd.age_count < 18 AND tm.match_status = 'Pending'\n),\nAvgWait AS (\n    SELECT AVG(CAST(regexp_replace(c.wait_time, '[^0-9.]', '', 'g') AS REAL)) as avg_heart_wait\n    FROM clinical c\n    JOIN transplant_matching tm ON c.clin_recip_registry = tm.recip_ref_reg\n    WHERE tm.org_spec = 'Heart'\n)\nSELECT \n    hc.recip_ref_reg AS recipient_id,\n    hc.age_count AS recipient_age,\n    hc.med_urgency,\n    hc.wait_time_days / NULLIF(aw.avg_heart_wait, 0) AS recipient_wait_time_ratio\nFROM HeartCandidates hc, AvgWait aw\nORDER BY \n    CASE hc.med_urgency\n        WHEN 'Status 1A' THEN 1\n        WHEN 'Status 1B' THEN 2\n        WHEN 'Status 2' THEN 3\n        ELSE 4\n    END ASC,\n    recipient_wait_time_ratio DESC;"], "external_knowledge": [25, 5, 45], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Drop view for cleanup\n    execute_queries(\"DROP VIEW IF EXISTS v_pediatric_heart_candidates;\", db_name, conn)\n\n    # Execute the CREATE VIEW statement\n    execute_queries(pred_sqls, db_name, conn)\n\n    # 1. Verify view was created\n    view_check = execute_queries(\"SELECT to_regclass('v_pediatric_heart_candidates');\", db_name, conn)\n    assert view_check and view_check[0] and view_check[0][0][0] is not None, \"View 'v_pediatric_heart_candidates' was not created.\"\n\n    # 2. Setup test data\n    # National avg heart wait (pre-existing + test) will be complex. Assume it's around 100 for this test's purpose.\n    execute_queries(\"\"\"\n        INSERT INTO recipients_demographics (recip_registry, age_count) VALUES ('PED_H_1', 10), ('PED_H_2', 12), ('PED_H_3', 15), ('ADULT_H_1', 45);\n        INSERT INTO clinical (clin_recip_registry, med_urgency, wait_time) VALUES ('PED_H_1', 'Status 1B', '200 days'), ('PED_H_2', 'Status 1A', '50 days'), ('PED_H_3', 'Status 1B', '300 days'), ('ADULT_H_1', 'Status 1A', '60 days');\n        INSERT INTO transplant_matching (match_rec_registry, recip_ref_reg, org_spec, match_status) VALUES \n            ('M_PED_1', 'PED_H_1', 'Heart', 'Pending'),\n            ('M_PED_2', 'PED_H_2', 'Heart', 'Pending'),\n            ('M_PED_3', 'PED_H_3', 'Heart', 'Pending'),\n            ('M_ADULT_1', 'ADULT_H_1', 'Heart', 'Pending');\n    \"\"\", db_name, conn)\n\n    # 3. Query the view and check the order\n    view_content = execute_queries(\"SELECT recipient_id FROM v_pediatric_heart_candidates;\", db_name, conn)\n    # Expected order: \n    # 1. PED_H_2 (Status 1A)\n    # 2. PED_H_3 (Status 1B, highest wait ratio)\n    # 3. PED_H_1 (Status 1B, lower wait ratio)\n    # Adult should not be in the list.\n    pediatric_ids = [row[0] for row in view_content[0]]\n    assert 'ADULT_H_1' not in pediatric_ids, \"Adult recipient was incorrectly included in the pediatric view.\"\n    expected_order = ['PED_H_2', 'PED_H_3', 'PED_H_1']\n    actual_order = [pid for pid in pediatric_ids if pid in expected_order]\n    assert actual_order == expected_order, f\"Recipients are not in the correct priority order. Expected {expected_order}, got {actual_order}.\"\n\n    # Cleanup\n    cleanup_sql = \"\"\"\n        DELETE FROM transplant_matching WHERE recip_ref_reg LIKE 'PED_H_%' OR recip_ref_reg = 'ADULT_H_1';\n        DELETE FROM clinical WHERE clin_recip_registry LIKE 'PED_H_%' OR clin_recip_registry = 'ADULT_H_1';\n        DELETE FROM recipients_demographics WHERE recip_registry LIKE 'PED_H_%' OR recip_registry = 'ADULT_H_1';\n    \"\"\"\n    execute_queries(cleanup_sql, db_name, conn)"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "organ_transplant_M_12", "selected_database": "organ_transplant", "query": "We need to prevent silly mistakes in our data entry. People don't get younger.\nCan you build a trigger called `trg_validate_recipient_age`?\nIt should watch the recipient demographics table. If anyone ever tries to update a patient's age to a number that's lower than what it was before, the trigger must just ignore the change and keep the old age.", "normal_query": "To ensure data integrity, create a trigger that prevents illogical updates to a recipient's age.\nThe trigger, named `trg_validate_recipient_age`, should activate before any update on the `recipients_demographics` table.\nIt must check if the new `age_count` is less than the old `age_count`. If a user attempts to decrease a recipient's age, the trigger should silently prevent the change by reverting the age to its original value.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION prevent_age_decrease()\nRETURNS TRIGGER AS $$\nBEGIN\n    -- If an attempt is made to decrease the age, revert it to the old value.\n    IF NEW.age_count IS NOT NULL AND OLD.age_count IS NOT NULL AND NEW.age_count < OLD.age_count THEN\n        NEW.age_count := OLD.age_count;\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;", "CREATE TRIGGER trg_validate_recipient_age\nBEFORE UPDATE ON recipients_demographics\nFOR EACH ROW\nEXECUTE FUNCTION prevent_age_decrease();"], "external_knowledge": [1], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Drop artifacts for cleanup\n    execute_queries(\"DROP TRIGGER IF EXISTS trg_validate_recipient_age ON recipients_demographics;\", db_name, conn)\n    execute_queries(\"DROP FUNCTION IF EXISTS prevent_age_decrease();\", db_name, conn)\n\n    # Execute the CREATE statements\n    for sql in pred_sqls:\n        execute_queries([sql], db_name, conn)\n\n    # 1. Verify trigger was created\n    trigger_check = execute_queries(\"SELECT tgname FROM pg_trigger WHERE tgname = 'trg_validate_recipient_age';\", db_name, conn)\n    assert trigger_check and trigger_check[0] and trigger_check[0][0][0] is not None, \"Trigger was not created.\"\n\n    # 2. Setup test data\n    execute_queries(\"INSERT INTO recipients_demographics (recip_registry, age_count) VALUES ('R_AGE_VALIDATE', 50) ON CONFLICT DO NOTHING;\", db_name, conn)\n\n    # 3. Test case that should be blocked (age decrease)\n    fail_query = \"UPDATE recipients_demographics SET age_count = 49 WHERE recip_registry = 'R_AGE_VALIDATE';\"\n    execute_queries(fail_query, db_name, conn)\n    age_check_fail = execute_queries(\"SELECT age_count FROM recipients_demographics WHERE recip_registry = 'R_AGE_VALIDATE';\", db_name, conn)\n    assert age_check_fail[0][0][0] == 50, \"Trigger did not prevent age decrease; age was updated.\"\n\n    # 4. Test case that should SUCCEED (increasing age)\n    success_query = \"UPDATE recipients_demographics SET age_count = 51 WHERE recip_registry = 'R_AGE_VALIDATE';\"\n    execute_queries(success_query, db_name, conn)\n    age_check_success = execute_queries(\"SELECT age_count FROM recipients_demographics WHERE recip_registry = 'R_AGE_VALIDATE';\", db_name, conn)\n    assert age_check_success[0][0][0] == 51, \"Valid age increase was incorrectly blocked.\"\n\n    # Cleanup\n    execute_queries(\"DELETE FROM recipients_demographics WHERE recip_registry = 'R_AGE_VALIDATE';\", db_name, conn)"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "organ_transplant_M_13", "selected_database": "organ_transplant", "query": "I need a quick way to pull up all the main risk numbers for a specific transplant match.\nCan you make a function called `get_match_risk_profile` that takes a match ID?\nIt should just go into the risk data and pull out the five important scores—Immunological, Infection, Rejection, Readmission, and Mortality—and then give them back to me as a single JSON object.", "normal_query": "Please create a function that provides a holistic risk profile for a given transplant match.\nThe function, named `get_match_risk_profile`, must accept a match ID.\nIt should retrieve and return a single JSONB object containing the five key risk metrics: Immunological Risk, Infection Risk, Rejection Risk, Readmission Risk, and Mortality Risk.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION get_match_risk_profile(p_match_id TEXT)\nRETURNS JSONB AS $$\nDECLARE\n    v_risk_profile JSONB;\nBEGIN\n    SELECT jsonb_build_object(\n        'Immun_Risk', riskmetrics->'Immun_Risk',\n        'Infect_Risk', riskmetrics->'Infect_Risk',\n        'Reject_Risk', riskmetrics->'Reject_Risk',\n        'Readmit_Risk', riskmetrics->'Readmit_Risk',\n        'Mort_Risk', riskmetrics->'Mort_Risk'\n    )\n    INTO v_risk_profile\n    FROM risk_evaluation\n    WHERE risk_eval_registry = p_match_id;\n\n    RETURN v_risk_profile;\nEND;\n$$ LANGUAGE plpgsql;"], "external_knowledge": [50], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Drop function for cleanup\n    execute_queries(\"DROP FUNCTION IF EXISTS get_match_risk_profile(TEXT);\", db_name, conn)\n\n    # Execute the CREATE FUNCTION statement\n    execute_queries(pred_sqls, db_name, conn)\n\n    # 1. Verify function was created\n    func_check = execute_queries(\"SELECT proname FROM pg_proc WHERE proname = 'get_match_risk_profile';\", db_name, conn)\n    assert func_check and func_check[0] and func_check[0][0][0] is not None, \"Function 'get_match_risk_profile' was not created.\"\n\n    # 2. Setup test data\n    execute_queries(\"\"\"\n        INSERT INTO transplant_matching (match_rec_registry) VALUES ('M_RISK_1') ON CONFLICT DO NOTHING;\n        INSERT INTO risk_evaluation (risk_eval_registry, riskmetrics) VALUES ('M_RISK_1', \n            '{\\\"Immun_Risk\\\": 0.6, \\\"Infect_Risk\\\": 0.4, \\\"Reject_Risk\\\": 0.5, \\\"Readmit_Risk\\\": 0.7, \\\"Mort_Risk\\\": 0.3, \\\"Other_Risk\\\": 0.1}')\n        ON CONFLICT DO NOTHING;\n    \"\"\", db_name, conn)\n\n    # 3. Call the function and check the returned JSONB object\n    import json\n    result = execute_queries(\"SELECT get_match_risk_profile('M_RISK_1');\", db_name, conn)\n    assert result and result[0][0][0] is not None, \"Function returned NULL for a valid match ID.\"\n    \n    # The DB driver automatically decodes JSONB to a dict, so no json.loads is needed.\n    risk_profile = result[0][0][0]\n    \n    expected_profile = {\n        \"Immun_Risk\": 0.6, \"Infect_Risk\": 0.4, \"Reject_Risk\": 0.5, \"Readmit_Risk\": 0.7, \"Mort_Risk\": 0.3\n    }\n    assert risk_profile == expected_profile, f\"Returned JSONB profile is incorrect. Got {risk_profile}\"\n    assert 'Other_Risk' not in risk_profile, \"Function incorrectly included an extra risk field.\"\n\n    # Cleanup\n    cleanup_sql = \"\"\"\n        DELETE FROM risk_evaluation WHERE risk_eval_registry = 'M_RISK_1';\n        DELETE FROM transplant_matching WHERE match_rec_registry = 'M_RISK_1';\n    \"\"\"\n    execute_queries(cleanup_sql, db_name, conn)"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "mental_health_1", "selected_database": "mental_health", "query": "Let's find our most vulnerable patients: those who are high-risk, in facilities under severe stress, and who are also not engaging well with their therapy. I need a list with their patient ID, assessment ID, the date of their latest assessment, their average rounded engagement score, and the stress level of their facility. Please sort by the most recent assessment and just show the top 50.", "normal_query": "I want to identify High-Risk Patients from facilities experiencing Severe Environmental Stress or Severe Life Impact, who also exhibit low Therapy Engagement Scores (average TES is lower than 2). For each patient, include their patient ID, assessment ID, date of their most recent assessment, their average rounded TES score, and the environmental stress or life impact level of the facility they are associated with. Focus only on the most recent assessments and prioritize patients meeting all these criteria. Sort the results by the assessment date in descending order and limit to the top 50 results.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH PatientRisk AS ( SELECT ab.pat_owner_ref AS pat_key, asr.asr_key AS assessment_key, TRUE AS is_high_risk FROM assessmentbasics ab JOIN assessmentsymptomsandrisk asr ON ab.ab_key = asr.asr_key WHERE asr.suic_risk = 'High' OR asr.phq9_scr > 15 OR asr.gad7_scr > 15 ), FacilityStress AS ( SELECT fac_key, env_stress, life_impact FROM facilities WHERE env_stress = 'Severe' OR life_impact = 'Severe' ), PatientEngagement AS ( SELECT e.pat_ref AS pat_key, CASE tb.th_eng WHEN 'High' THEN 3 WHEN 'Medium' THEN 2 WHEN 'Low' THEN 1 ELSE 0 END AS engagement_score FROM treatmentbasics tb JOIN encounters e ON tb.enc_ref = e.enc_key WHERE tb.th_eng IS NOT NULL ), AvgPatientEngagement AS ( SELECT pat_key, AVG(engagement_score) AS avg_engagement_score FROM PatientEngagement GROUP BY pat_key ), RecentAssessment AS ( SELECT pr.pat_key, pr.assessment_key, e.fac_id, e.time_mark, ROW_NUMBER() OVER(PARTITION BY pr.pat_key ORDER BY e.time_mark DESC) as rn FROM PatientRisk pr JOIN encounters e ON pr.assessment_key = e.ab_ref AND pr.pat_key = e.pat_ref ) SELECT ra.pat_key AS patient_id, ra.assessment_key AS assessment_id, ra.time_mark AS last_assessment_date, ape.avg_engagement_score AS patient_tes_avg, fs.env_stress, fs.life_impact FROM RecentAssessment ra JOIN AvgPatientEngagement ape ON ra.pat_key = ape.pat_key JOIN FacilityStress fs ON ra.fac_id = fs.fac_key WHERE ra.rn = 1 AND ape.avg_engagement_score < 2 ORDER BY ra.time_mark DESC LIMIT 50"], "external_knowledge": [4, 10], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "mental_health_2", "selected_database": "mental_health", "query": "Can you help me check how closely a facility's resources relate to how well patients stick to their treatment? I'd like to see the overall resource adequacy score across all facilities and the correlation between each facility's resource score and their treatment adherence rate. Just skip the places where there's no rate for the treatment adherence.", "normal_query": "For all facilities, I want to explore the Correlation Between Resource Adequacy and Adherence. Include the overall Facility Resource Adequacy Index as a reference and the correlation coefficient between each facility's resource adequacy score and treatment adherence rate. Exclude facilities with no applicable TAR.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FacilityResources AS (\n    -- Step 1: Calculate the resource score per facility based on the 'com_res' column (KB 5).\n    SELECT\n        fac_key,\n        CASE com_res\n            WHEN 'Comprehensive' THEN 3\n            WHEN 'Extensive' THEN 3 -- New value mapped to highest score\n            WHEN 'Adequate' THEN 2\n            WHEN 'Limited' THEN 1\n            ELSE 0\n        END AS resource_score\n    FROM facilities\n    WHERE com_res IS NOT NULL\n), \nFacilityAdherence AS (\n    -- Step 2: Calculate Treatment Adherence Rate (TAR, KB 2) per facility from the 'txprogmet' JSONB field.\n    SELECT\n        e.fac_id,\n        COUNT(tro.tx_out_key) AS total_outcomes,\n        COUNT(*) FILTER (\n            WHERE UPPER(tro.txprogmet->>'Tx_Adh') IN ('HIGH', 'MEDIUM', 'COMPLIANT')\n        ) AS adherent_outcomes\n    FROM treatmentoutcomes tro\n    JOIN treatmentbasics tb ON tro.tx_ref = tb.tx_key\n    JOIN encounters e ON tb.enc_ref = e.enc_key\n    WHERE e.fac_id IS NOT NULL\n    GROUP BY e.fac_id\n), \nFacilityMetrics AS (\n    -- Step 3: Combine resource score and TAR for each facility.\n    SELECT\n        fr.fac_key,\n        fr.resource_score,\n        CASE\n            WHEN fa.total_outcomes > 0 THEN (fa.adherent_outcomes::DECIMAL / fa.total_outcomes)\n            ELSE 0\n        END AS tar -- Treatment Adherence Rate\n    FROM FacilityResources fr\n    LEFT JOIN FacilityAdherence fa ON fr.fac_key = fa.fac_id\n), \nOverallFRAI AS (\n    -- Step 4: Calculate the average resource score (overall FRAI) across all facilities.\n    SELECT\n        AVG(resource_score) AS frai\n    FROM FacilityResources\n)\n-- Final Step: Calculate the correlation between facility resource scores and their TAR.\nSELECT\n    (SELECT frai FROM OverallFRAI) AS overall_frai, -- Include the overall FRAI for reference\n    CORR(resource_score, tar) AS correlation_frai_tar\nFROM FacilityMetrics\nWHERE tar > 0; -- Exclude facilities with no applicable TAR (e.g., no outcomes data)"], "external_knowledge": [60, 5, 2], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "mental_health_3", "selected_database": "mental_health", "query": "Show me facilities where patients seem highly engaged in therapy, but their recovery progress is still lagging basically, places with a possible engagement-outcome disconnect. For each of those facilities, I want to see the Facility ID, their average therapy engagement score, and their index of recovery trajectory, both rounded to two decimal places. Sort the list by Facility ID and keep it to the first 100 results.", "normal_query": "Identify facilities classified as having a Facility with Potential Engagement-Outcome Disconnect. Display the facility ID, the average TES, and the RTI for these facilities. Round both TES and RTI to 2 decimal places, sort by facility ID, and limit the output to 100 rows.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH TherapyEngagement AS ( SELECT e.fac_id, CASE tb.th_eng WHEN 'High' THEN 3 WHEN 'Medium' THEN 2 WHEN 'Low' THEN 1 ELSE 0 END AS engagement_score FROM treatmentbasics tb JOIN encounters e ON tb.enc_ref = e.enc_key WHERE tb.th_eng IS NOT NULL AND e.fac_id IS NOT NULL ), FacilityTES AS ( SELECT fac_id, ROUND(AVG(engagement_score), 2) AS avg_tes FROM TherapyEngagement GROUP BY fac_id ), FunctionalImprovement AS ( SELECT e.fac_id, CASE UPPER(tro.txprogmet->>'Func_Impv') WHEN 'SIGNIFICANT' THEN 3 WHEN 'MODERATE' THEN 2 WHEN 'MINIMAL' THEN 1 ELSE 0 END AS funcimpv_score, CASE WHEN UPPER(tro.txprogmet->>'Tx_Adh') IN ('HIGH', 'MEDIUM', 'COMPLIANT') THEN 1 ELSE 0 END AS is_adherent FROM treatmentoutcomes tro JOIN treatmentbasics tb ON tro.tx_ref = tb.tx_key JOIN encounters e ON tb.enc_ref = e.enc_key WHERE e.fac_id IS NOT NULL AND tro.txprogmet->>'Func_Impv' IS NOT NULL ), FacilityRTI AS ( SELECT fac_id, ROUND((AVG(funcimpv_score) * (SUM(is_adherent)::DECIMAL / NULLIF(COUNT(*), 0))), 2) AS rti FROM FunctionalImprovement GROUP BY fac_id ) SELECT ft.fac_id, ft.avg_tes, fr.rti FROM FacilityTES ft JOIN FacilityRTI fr ON ft.fac_id = fr.fac_id WHERE ft.avg_tes > 2.0 AND fr.rti < 0.8 ORDER BY ft.fac_id LIMIT 100"], "external_knowledge": [58, 4, 53, 2], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "mental_health_4", "selected_database": "mental_health", "query": "Can you show me the top clinicians working in well-supported facilities based on the stability metric of the patients? I want to see each clinician's ID, which facility they work at, their Patient Stability Metric score, and how they rank within their facility (higher PSM means better rank). Just include those in resource-backed facilities, sort by facility and rank, and show only the top 100 results.", "normal_query": "I want to identify the top-performing clinicians in Resource-Supported Facilities based on their Patient Stability Metric. For each clinician, provide their ID, the facility ID, their PSM score, and their rank within the facility. The rank should be based on PSM, with higher PSM scores ranked higher. Only include clinicians from facilities classified as Resource-Supported Facilities. Sort the results by facility ID and then by rank within each facility, limiting the output to the top 100 rows.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FacilityResourceScores AS (\n    SELECT fac_key, CASE com_res WHEN 'Comprehensive' THEN 3 WHEN 'Extensive' THEN 3 WHEN 'Adequate' THEN 2 WHEN 'Limited' THEN 1 ELSE 0 END AS resource_score FROM facilities WHERE com_res IS NOT NULL\n), ResourceSupportedFacilities AS (\n    SELECT fac_key FROM FacilityResourceScores WHERE resource_score >= 2\n), PatientMetrics AS (\n    SELECT e.pat_ref AS pat_key, e.clin_id, e.fac_id, COALESCE(tb.crisis_int, 0) AS crisis_int, COALESCE(e.miss_appt, 0) AS miss_appt FROM encounters e LEFT JOIN treatmentbasics tb ON e.enc_key = tb.enc_ref WHERE e.clin_id IS NOT NULL AND e.fac_id IN (SELECT fac_key FROM ResourceSupportedFacilities)\n), ClinicianAvgMetrics AS (\n    SELECT clin_id, fac_id, SUM(crisis_int) / NULLIF(COUNT(DISTINCT pat_key), 0)::DECIMAL AS avg_cif, SUM(miss_appt) / NULLIF(COUNT(DISTINCT pat_key), 0)::DECIMAL AS avg_mar FROM PatientMetrics GROUP BY clin_id, fac_id\n), ClinicianPSM AS (\n    SELECT clin_id, fac_id, ROUND(1.0 / (1.0 + COALESCE(avg_cif, 0) + COALESCE(avg_mar, 0))::NUMERIC, 2) AS psm FROM ClinicianAvgMetrics\n)\nSELECT clin_id, fac_id, psm, RANK() OVER (PARTITION BY fac_id ORDER BY psm DESC, clin_id) AS rank_within_facility FROM ClinicianPSM ORDER BY fac_id, rank_within_facility LIMIT 100"], "external_knowledge": [33, 7, 9, 15], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "mental_health_5", "selected_database": "mental_health", "query": "Can you show me the patients who seem to have fragile stability? I want to see their ID, how often they miss appointments on average, and what their latest effectiveness score of social support.", "normal_query": "I want to find patients who are exhibiting fragile stability. List each patients ID, their average missed appointments, and their most recent SSE score.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH LatestPatientOutcome AS (\n    SELECT DISTINCT ON (e.pat_ref) \n        e.pat_ref AS pat_key, \n        tro.txprogmet\n    FROM treatmentoutcomes tro\n    JOIN treatmentbasics tb ON tro.tx_ref = tb.tx_key\n    JOIN encounters e ON tb.enc_ref = e.enc_key\n    ORDER BY e.pat_ref, e.time_mark DESC\n),\nStablePatients AS (\n    SELECT DISTINCT ON (ab.pat_owner_ref) \n        ab.pat_owner_ref AS pat_key\n    FROM assessmentbasics ab\n    JOIN assessmentsocialanddiagnosis asd ON ab.ab_key = asd.asd_key\n    JOIN LatestPatientOutcome lpo ON ab.pat_owner_ref = lpo.pat_key\n    JOIN encounters e ON ab.ab_key = e.ab_ref\n    WHERE asd.rec_status = 'Stable'\n      AND UPPER(lpo.txprogmet->>'Func_Impv') IN ('MODERATE', 'SIGNIFICANT')\n    ORDER BY ab.pat_owner_ref, e.time_mark DESC\n),\nPatientAvgMissedAppts AS (\n    SELECT \n        pat_ref AS pat_key, \n        AVG(miss_appt) AS avg_miss_appt\n    FROM encounters\n    WHERE miss_appt IS NOT NULL\n    GROUP BY pat_ref\n),\nPatientSocialSupport AS (\n    SELECT \n        ab.pat_owner_ref AS pat_key,\n        AVG(\n            CASE asd.funcassess->>'Soc_Sup'\n                WHEN 'Strong' THEN 3\n                WHEN 'Adequate' THEN 2\n                WHEN 'Limited' THEN 1\n                WHEN 'Weak' THEN 0\n                ELSE 0\n            END\n            +\n            CASE asd.funcassess->>'Rel_Qual'\n                WHEN 'Good' THEN 3\n                WHEN 'Fair' THEN 2\n                WHEN 'Poor' THEN 1\n                WHEN 'Conflicted' THEN 0\n                ELSE 0\n            END\n        ) AS sse_score\n    FROM assessmentbasics ab\n    JOIN assessmentsocialanddiagnosis asd ON ab.ab_key = asd.asd_key\n    GROUP BY ab.pat_owner_ref\n)\nSELECT \n    sp.pat_key, \n    pama.avg_miss_appt, \n    pss.sse_score\nFROM StablePatients sp\nJOIN PatientAvgMissedAppts pama ON sp.pat_key = pama.pat_key\nJOIN PatientSocialSupport pss ON sp.pat_key = pss.pat_key\nWHERE pama.avg_miss_appt > 2.0 \n   OR pss.sse_score < 3;"], "external_knowledge": [56, 13, 9, 8], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": true, "order": false}}
{"instance_id": "mental_health_6", "selected_database": "mental_health", "query": "Show me the top 100 primary diagnoses where patients have the highest number of crisis interventions. For each diagnosis, include the name of the diagnosis, how many patients had that diagnosis, and the average crisis intervention frequency, rounded to two decimal places. Sort the list by CIF from highest to lowest.", "normal_query": "I want to identify which primary diagnoses are associated with the highest Crisis Intervention Frequency (CIF) across all patients. For each diagnosis, list the diagnosis name, the number of patients with that diagnosis, and the CIF value, rounded to two decimal places. Sort the results by CIF in descending order and limit to the top 100 diagnoses.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH PatientDiagnosis AS (\n    -- Step 1: Get the most recent primary diagnosis for each patient.\n    -- Correctly joins through assessmentbasics to link a patient to their diagnosis and encounter time.\n    SELECT DISTINCT ON (ab.pat_owner_ref)\n        ab.pat_owner_ref AS pat_key,\n        asd.prim_dx\n    FROM assessmentbasics ab\n    JOIN assessmentsocialanddiagnosis asd ON ab.ab_key = asd.asd_key\n    JOIN encounters e ON ab.ab_key = e.ab_ref\n    WHERE asd.prim_dx IS NOT NULL\n    ORDER BY ab.pat_owner_ref, e.time_mark DESC\n), \nPatientCrisisInterventions AS (\n    -- Step 2: Calculate the total crisis interventions per patient.\n    SELECT\n        e.pat_ref AS pat_key,\n        SUM(COALESCE(tb.crisis_int, 0)) AS total_crisis_int\n    FROM treatmentbasics tb\n    JOIN encounters e ON tb.enc_ref = e.enc_key\n    GROUP BY e.pat_ref\n), \nPatientCount AS (\n    -- Step 3: Count the total number of unique patients for the CIF denominator.\n    SELECT COUNT(DISTINCT pat_key) AS total_patients FROM patients\n)\n-- Final Step: Calculate CIF per diagnosis group.\nSELECT\n    pd.prim_dx,\n    COUNT(DISTINCT pci.pat_key) AS num_patients_with_dx,\n    -- CIF is the total crisis interventions for a diagnosis group divided by the total number of patients in the dataset.\n    ROUND(SUM(pci.total_crisis_int)::DECIMAL / pc.total_patients, 2) AS cif_per_diagnosis\nFROM PatientDiagnosis pd\nJOIN PatientCrisisInterventions pci ON pd.pat_key = pci.pat_key\nCROSS JOIN PatientCount pc\nGROUP BY pd.prim_dx, pc.total_patients\nORDER BY cif_per_diagnosis DESC\nLIMIT 100"], "external_knowledge": [7], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": true, "order": true}}
{"instance_id": "mental_health_7", "selected_database": "mental_health", "query": "Show me the top 100 facilities, grouped into performance quadrants. For each one, list its ID, how well patients stick to their treatments (rate of treatment adherence), how stable the patients are, both rounded to two decimals, and which performance quadrant it falls into. Sort the results by quadrant and then by facility ID.", "normal_query": "I want to categorize facilities into performance quadrants. For each facility, list the facility ID, Treatment Adherence Rate (rounded to two decimal places), Patient Stability Metric (rounded to two decimal places), and the performance quadrant. Sort results by performance quadrant and facility ID, limiting to the top 100 facilities.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FacilityAdherence AS (\n    -- Step 1: Calculate Treatment Adherence Rate (TAR) per facility.\n    SELECT\n        e.fac_id,\n        COUNT(tro.tx_out_key) FILTER (\n            WHERE UPPER(tro.txprogmet->>'Tx_Adh') IN ('HIGH', 'MEDIUM', 'COMPLIANT')\n        )::DECIMAL / NULLIF(COUNT(tro.tx_out_key), 0) AS tar\n    FROM treatmentoutcomes tro\n    JOIN treatmentbasics tb ON tro.tx_ref = tb.tx_key\n    JOIN encounters e ON tb.enc_ref = e.enc_key\n    WHERE e.fac_id IS NOT NULL\n    GROUP BY e.fac_id\n),\nFacilityPatientCount AS (\n    -- Step 2: Count distinct patients per facility for metric normalization.\n    SELECT\n        fac_id,\n        COUNT(DISTINCT pat_ref) AS total_patients\n    FROM encounters\n    WHERE fac_id IS NOT NULL\n    GROUP BY fac_id\n),\nFacilityStabilityMetrics AS (\n    -- Step 3: Calculate Crisis Intervention Frequency (CIF) and Missed Appointment Rate (MAR) per facility.\n    SELECT\n        e.fac_id,\n        SUM(COALESCE(tb.crisis_int, 0)) / NULLIF(fpc.total_patients, 0)::DECIMAL AS cif,\n        SUM(COALESCE(e.miss_appt, 0)) / NULLIF(fpc.total_patients, 0)::DECIMAL AS mar\n    FROM encounters e\n    LEFT JOIN treatmentbasics tb ON e.enc_key = tb.enc_ref\n    JOIN FacilityPatientCount fpc ON e.fac_id = fpc.fac_id\n    WHERE e.fac_id IS NOT NULL\n    GROUP BY e.fac_id, fpc.total_patients\n),\nFacilityPSM AS (\n    -- Step 4: Calculate the Patient Stability Metric (PSM) per facility.\n    SELECT\n        fac_id,\n        1.0 / (1.0 + COALESCE(cif, 0) + COALESCE(mar, 0)) AS psm\n    FROM FacilityStabilityMetrics\n),\nFacilityMetrics AS (\n    -- Step 5: Combine TAR and PSM for each facility.\n    SELECT\n        fa.fac_id,\n        COALESCE(fa.tar, 0) AS tar,\n        COALESCE(fp.psm, 0) AS psm\n    FROM FacilityAdherence fa\n    JOIN FacilityPSM fp ON fa.fac_id = fp.fac_id\n),\nThresholds AS (\n    -- Step 6: Calculate median TAR and PSM across all facilities to define the quadrants.\n    SELECT\n        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY tar) AS median_tar,\n        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY psm) AS median_psm\n    FROM FacilityMetrics\n)\n-- Final Step: Assign each facility to a performance quadrant.\nSELECT\n    fm.fac_id,\n    ROUND(fm.tar::numeric, 2) AS tar,\n    ROUND(fm.psm::numeric, 2) AS psm,\n    CASE\n        WHEN fm.tar >= t.median_tar AND fm.psm >= t.median_psm THEN 'High Adherence, High Stability'\n        WHEN fm.tar >= t.median_tar AND fm.psm < t.median_psm THEN 'High Adherence, Low Stability'\n        WHEN fm.tar < t.median_tar AND fm.psm >= t.median_psm THEN 'Low Adherence, High Stability'\n        ELSE 'Low Adherence, Low Stability'\n    END AS performance_quadrant\nFROM FacilityMetrics fm, Thresholds t\nORDER BY performance_quadrant, fm.fac_id\nLIMIT 100;\n"], "external_knowledge": [61, 2, 7, 9, 33], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "mental_health_8", "selected_database": "mental_health", "query": "I want to see how different kinds of therapy changes—like switching therapy type, therapist, or session frequency—affect how engaged patients are. For each type of change, show how often it happens, what the average engagement score was before and after the change, and how much the score changed overall. Sort the results so that the most common changes appear at the top.", "normal_query": "Analyze the impact of therapy changes (modality, therapist, frequency) on the Therapy Engagement Score and calculate the engagement variation for each change type. Show the change type, total occurrences, average scores before (previous encounter of each encounter) and after (current encounter), and average score change from previous score to current score, ordering by total occurrences in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH TherapyDetails AS ( \n    -- Step 1: Extract therapy details and calculate an engagement score for each treatment record.\n    SELECT\n        e.pat_ref AS pat_key,\n        e.time_mark AS encounter_time,\n        tb.th_chg AS therapy_change,\n        CASE tb.th_eng\n            WHEN 'High' THEN 3\n            WHEN 'Medium' THEN 2\n            WHEN 'Low' THEN 1\n            ELSE NULL\n        END AS engagement_score\n    FROM treatmentbasics tb\n    JOIN encounters e ON tb.enc_ref = e.enc_key\n    WHERE tb.th_eng IS NOT NULL AND tb.th_chg IS NOT NULL\n), \nEngagementChanges AS ( \n    -- Step 2: For each record, find the engagement score of the patient's immediately preceding encounter.\n    SELECT\n        therapy_change,\n        engagement_score,\n        LAG(engagement_score, 1) OVER (PARTITION BY pat_key ORDER BY encounter_time) AS prev_engagement_score\n    FROM TherapyDetails\n)\n-- Step 3: Group by the type of therapy change and calculate the average scores.\nSELECT\n    therapy_change,\n    COUNT(*) AS total_occurrences,\n    ROUND(AVG(prev_engagement_score)::numeric, 2) AS avg_previous_score,\n    ROUND(AVG(engagement_score)::numeric, 2) AS avg_current_score,\n    ROUND(AVG(engagement_score - prev_engagement_score)::numeric, 2) AS avg_engagement_score_change\nFROM EngagementChanges\nWHERE therapy_change IS NOT NULL AND prev_engagement_score IS NOT NULL\nGROUP BY therapy_change\nORDER BY total_occurrences DESC;"], "external_knowledge": [4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "mental_health_9", "selected_database": "mental_health", "query": "Show me the top 100 facilities where suicide risk is very high—over 20%. For each one, list the facility ID, their PFIS score, FRAI score, and the difference in resource demand, sorted from the highest RDD to the lowest. I want to find places where the need for resources is most urgent.", "normal_query": "For facilities with high Suicide Risk Prevalence over 20%, calculate the Resource-Demand Differential. List the facility ID, PFIS, FRAI, and RDD scores, ordered by RDD from highest to lowest, showing the top 100 facilities. This helps identify resource gaps in critical environments.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH FacilitySRP AS (\n    -- Step 1: Calculate Suicide Risk Prevalence (SRP) per facility.\n    SELECT\n        e.fac_id,\n        (COUNT(*) FILTER (WHERE asr.suic_risk = 'High')::DECIMAL * 100 / COUNT(*)) AS srp\n    FROM assessmentsymptomsandrisk asr\n    JOIN assessmentbasics ab ON asr.asr_key = ab.ab_key\n    JOIN encounters e ON ab.ab_key = e.ab_ref\n    WHERE e.fac_id IS NOT NULL\n    GROUP BY e.fac_id\n), \nHighSRPFacilities AS (\n    -- Step 2: Identify facilities with SRP above a 20% threshold.\n    SELECT fac_id\n    FROM FacilitySRP\n    WHERE srp > 20.0\n), \nFacilityPFIS AS (\n    -- Step 3: Calculate Patient Functional Impairment Score (PFIS, KB 6) for the high-risk facilities.\n    SELECT\n        e.fac_id,\n        AVG(CASE UPPER(asd.func_imp)\n               WHEN 'SEVERE' THEN 3\n               WHEN 'MODERATE' THEN 2\n               WHEN 'MILD' THEN 1\n               ELSE 0\n             END) AS pfis\n    FROM assessmentsocialanddiagnosis asd\n    JOIN assessmentbasics ab ON asd.asd_key = ab.ab_key\n    JOIN encounters e ON ab.ab_key = e.ab_ref\n    WHERE e.fac_id IN (SELECT fac_id FROM HighSRPFacilities)\n      AND asd.func_imp IS NOT NULL\n    GROUP BY e.fac_id\n), \nFacilityFRAI AS (\n    -- Step 4: Calculate the Facility Resource Adequacy Index (FRAI, KB 5) score for the high-risk facilities.\n    SELECT \n        f.fac_key AS fac_id,\n        AVG(CASE f.com_res\n            WHEN 'Comprehensive' THEN 3\n            WHEN 'Extensive' THEN 3\n            WHEN 'Adequate' THEN 2\n            WHEN 'Limited' THEN 1\n            ELSE 0\n        END) AS frai\n    FROM facilities f\n    WHERE f.fac_key IN (SELECT fac_id FROM HighSRPFacilities)\n      AND f.com_res IS NOT NULL\n    GROUP BY f.fac_key\n)\n-- Final Step: Calculate the Resource-Demand Differential (RDD = PFIS - FRAI) for the identified facilities.\nSELECT\n    hspf.fac_id,\n    fpf.pfis,\n    ffr.frai,\n    (fpf.pfis - ffr.frai) AS rdd\nFROM HighSRPFacilities hspf\nJOIN FacilityPFIS fpf ON hspf.fac_id = fpf.fac_id\nJOIN FacilityFRAI ffr ON hspf.fac_id = ffr.fac_id\nORDER BY rdd DESC\nLIMIT 100"], "external_knowledge": [3, 34, 6, 5], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "mental_health_10", "selected_database": "mental_health", "query": "Find facilities that seem to have an environment that are systemically stressed. For each one, list the facility ID and the differential in resource demand. Just show the top 100 most stressed facilities.", "normal_query": "Identify facilities exhibiting characteristics of a Systemically Stressed Facility Environment. For each facility, return its ID and Resource-Demand Differential value, limited to the top 100 facilities.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FacilityPFIS AS (\n    SELECT e.fac_id, AVG(CASE UPPER(asd.func_imp) WHEN 'SEVERE' THEN 3 WHEN 'MODERATE' THEN 2 WHEN 'MILD' THEN 1 ELSE 0 END) AS pfis\n    FROM assessmentsocialanddiagnosis asd\n    JOIN assessmentbasics ab ON asd.asd_key = ab.ab_key\n    JOIN encounters e ON ab.ab_key = e.ab_ref\n    WHERE e.fac_id IS NOT NULL AND asd.func_imp IS NOT NULL\n    GROUP BY e.fac_id\n), \nFacilityFRAI AS (\n    SELECT fac_key AS fac_id, CASE com_res WHEN 'Comprehensive' THEN 3 WHEN 'Extensive' THEN 3 WHEN 'Adequate' THEN 2 WHEN 'Limited' THEN 1 ELSE 0 END AS frai\n    FROM facilities \n    WHERE com_res IS NOT NULL\n), \nFacilityRDD AS (\n    SELECT pfis.fac_id, (pfis.pfis - ff.frai) AS rdd\n    FROM FacilityPFIS pfis\n    JOIN FacilityFRAI ff ON pfis.fac_id = ff.fac_id\n), \nFacilityTES AS (\n    SELECT e.fac_id, AVG(CASE th_eng WHEN 'High' THEN 3 WHEN 'Medium' THEN 2 WHEN 'Low' THEN 1 ELSE 0 END) AS avg_tes\n    FROM treatmentbasics tb\n    JOIN encounters e ON tb.enc_ref = e.enc_key \n    WHERE tb.th_eng IS NOT NULL AND e.fac_id IS NOT NULL\n    GROUP BY e.fac_id\n), \nFacilityTAR AS (\n    SELECT e.fac_id, COUNT(*) FILTER (WHERE UPPER(tro.txprogmet->>'Tx_Adh') IN ('HIGH', 'MEDIUM', 'COMPLIANT'))::DECIMAL / NULLIF(COUNT(tro.tx_out_key), 0) AS tar\n    FROM treatmentoutcomes tro\n    JOIN treatmentbasics tb ON tro.tx_ref = tb.tx_key\n    JOIN encounters e ON tb.enc_ref = e.enc_key \n    WHERE e.fac_id IS NOT NULL\n    GROUP BY e.fac_id\n), \nFacilityEAS AS (\n    SELECT tes.fac_id, (tes.avg_tes + (COALESCE(tar.tar, 0) * 3.0)) / 2.0 AS eas\n    FROM FacilityTES tes\n    LEFT JOIN FacilityTAR tar ON tes.fac_id = tar.fac_id\n), \nFacilityMAR AS (\n    SELECT fac_id, SUM(COALESCE(miss_appt, 0))::DECIMAL / NULLIF(COUNT(DISTINCT pat_ref), 0) AS mar\n    FROM encounters\n    WHERE fac_id IS NOT NULL\n    GROUP BY fac_id\n),\nAttritionRiskFacilities AS (\n    SELECT feas.fac_id\n    FROM FacilityEAS feas\n    JOIN FacilityMAR fmar ON feas.fac_id = fmar.fac_id\n    WHERE feas.eas < 1.5 AND fmar.mar > 2.5\n)\nSELECT frdd.fac_id, ROUND(frdd.rdd::numeric, 2) AS rdd\nFROM FacilityRDD frdd\nWHERE frdd.rdd > 1.0\n  AND frdd.fac_id IN (SELECT fac_id FROM AttritionRiskFacilities)\nORDER BY rdd DESC\nLIMIT 100;"], "external_knowledge": [6, 5, 34, 4, 2, 31, 9, 43, 59], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "mental_health_11", "selected_database": "mental_health", "query": "Hey, can you pull together a report that shows how care might vary between different ethnic groups? For each patient, I want to see their ethnicity, how severe their symptoms are, and how well they're sticking with their treatment. Also, please include a summary row that combines all ethnicities, so we have a baseline to compare against. And make sure the results are sorted by ethnicity alphabetically. Thanks!", "normal_query": "To support our health equity audit, I need a report that assesses potential care disparities across different ethnic groups. Please generate a table showing each patient's ethnicity, their calculated Symptom Severity Index, and their Engagement-Adherence Score. The report must also include a summary row for 'All Ethnicities' to serve as a baseline. Please sort the results alphabetically by patient ethnicity.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH\n-- Step 1: Pre-calculate all the base-level scores needed for SSI and EAS at the individual record level.\n-- This involves joining multiple tables to bring patient, symptom, engagement, and adherence data together.\nBaseScores AS (\n    SELECT\n        p.pat_eth,\n        asr.phq9_scr,\n        asr.gad7_scr,\n        -- Calculate TES score (KB #4)\n        (CASE WHEN tb.th_eng = 'High' THEN 3 WHEN tb.th_eng = 'Medium' THEN 2 WHEN tb.th_eng = 'Low' THEN 1 ELSE 0 END) as tes_score,\n        -- Calculate TAR score (KB #2)\n        (CASE WHEN (tout.txprogmet ->> 'Tx_Adh') IN ('High', 'Medium', 'Compliant') THEN 1.0 ELSE 0.0 END) as tar_score\n    FROM patients p\n    JOIN encounters e ON p.pat_key = e.pat_ref\n    JOIN assessmentsymptomsandrisk asr ON e.ab_ref = asr.asr_key\n    JOIN treatmentbasics tb ON e.enc_key = tb.enc_ref\n    LEFT JOIN treatmentoutcomes tout ON tb.tx_key = tout.tx_ref\n)\n-- Step 2: Final Aggregation and Calculation. Aggregate the base scores by ethnicity.\n-- Use GROUPING SETS to get both per-ethnicity results and an overall total in one go.\n-- The final SSI and EAS metrics are calculated from the aggregated averages.\nSELECT\n    COALESCE(pat_eth, 'All Ethnicities') AS patient_ethnicity,\n    -- Calculate SSI (KB #30) from the averaged scores\n    ROUND((AVG(phq9_scr) + AVG(gad7_scr)) / 2, 2) AS symptom_severity_index,\n    -- Calculate EAS (KB #31) from the averaged scores\n    ROUND((AVG(tes_score) + (AVG(tar_score) * 3)) / 2, 2) AS engagement_adherence_score\nFROM BaseScores\nGROUP BY\n    GROUPING SETS ((pat_eth), ())\nORDER BY\n    patient_ethnicity;\n"], "external_knowledge": [30, 31], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "mental_health_12", "selected_database": "mental_health", "query": "Sarah, our lead case manager, is worried about a pattern she's calling 'the David profile.' These are patients who are constantly in crisis—we're talking about patients that has a low support profile and in high crisis. She wants to find every other patient who looks just like that right now. Can you pull a list for her? She needs to see their ID and age, plus the exact crisis and support scores that got them on the list. For her team to take action, please also show the date they were last in the hospital and their next appointment, but make that appointment date easy to read. And please, put the people with the most crises at the very top so she knows who to call first.", "normal_query": "For our weekly clinical review, Sarah needs to generate a Vulnerable Patient Watchlist to proactively identify a specific cohort. The focus is on individuals who fit the Patient with High Crisis & Low Support Profile. To make this list actionable for the meeting, please display each unique patient's identifier, their age, the total count of their crisis interventions, and their calculated Social Support Effectiveness score. For additional context on their recent trajectory and our next point of contact, also include the date of their last hospitalization and format the date of their next scheduled appointment as 'Mon DD, YYYY'.Finally, please sort the list with the patients having the highest number of crisis interventions at the top to prioritize our discussion.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH\nPatientMetrics AS (\n    SELECT\n        p.pat_key,\n        p.pat_age,\n        asd.last_hosp_dt,\n        e.nx_appt_dt,\n        SUM(tb.crisis_int) OVER (PARTITION BY p.pat_key) AS total_crisis_interventions,\n        AVG(\n            CASE asd.funcassess->>'Soc_Sup'\n                WHEN 'Strong' THEN 3\n                WHEN 'Adequate' THEN 2\n                WHEN 'Limited' THEN 1\n                WHEN 'Weak' THEN 0\n                ELSE 0\n            END\n            +\n            CASE asd.funcassess->>'Rel_Qual'\n                WHEN 'Good' THEN 3\n                WHEN 'Fair' THEN 2\n                WHEN 'Poor' THEN 1\n                WHEN 'Conflicted' THEN 0\n                ELSE 0\n            END\n        ) OVER (PARTITION BY p.pat_key) AS sse_score\n    FROM patients p\n    JOIN encounters e ON p.pat_key = e.pat_ref\n    JOIN treatmentbasics tb ON e.enc_key = tb.enc_ref\n    JOIN assessmentsocialanddiagnosis asd ON e.ab_ref = asd.asd_key\n)\nSELECT DISTINCT\n    pat_key,\n    pat_age,\n    total_crisis_interventions,\n    sse_score,\n    last_hosp_dt,\n    TO_CHAR(TO_DATE(nx_appt_dt, 'DD/MM/YYYY'), 'Mon DD, YYYY') AS next_appointment\nFROM PatientMetrics\nWHERE\n    total_crisis_interventions > 2\n    AND sse_score < 3 \nORDER BY total_crisis_interventions DESC;"], "external_knowledge": [47, 7, 8, 64], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": true, "order": true}}
{"instance_id": "mental_health_13", "selected_database": "mental_health", "query": "Our director, Dr. Evans, is trying to get more funding for community partnerships, and his whole argument hinges on one idea: that facilities with better resources have patients who are more likely to stick to their treatment plans. He needs a solid number to prove this point. Can you run a statistical analysis to see how strong that connection really is? Basically, do a correlation analysis of the resource's adequacy. Just give me the final correlation score, nice and clean, as a single number.", "normal_query": "For a budget proposal, our regional director, Dr. Evans, needs to validate a key hypothesis: that better facility resources improve patient outcomes. He wants to test this by measuring the Correlation Between Resource Adequacy and Adherence. Please calculate this correlation. The final output should be a single numerical value representing this correlation, rounded to four decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH\n  FacilityMetrics AS (\n    SELECT\n      f.fac_key,\n      (\n        CASE\n          WHEN f.com_res IN ('Comprehensive', 'Extensive') THEN 3\n          WHEN f.com_res = 'Adequate' THEN 2\n          WHEN f.com_res = 'Limited' THEN 1\n          ELSE 0\n        END\n      ) AS resource_score,\n      -- Calculate TAR (KB #2)\n      AVG(\n        CASE\n          WHEN (\n            tout.txprogmet ->> 'Tx_Adh'\n          ) IN ('High', 'Medium', 'Compliant') THEN 1.0\n          ELSE 0.0\n        END\n      ) AS treatment_adherence_rate\n    FROM facilities AS f\n    LEFT JOIN encounters AS e\n      ON f.fac_key = e.fac_id\n    LEFT JOIN treatmentbasics AS tb\n      ON e.enc_key = tb.enc_ref\n    LEFT JOIN treatmentoutcomes AS tout\n      ON tb.tx_key = tout.tx_ref\n    GROUP BY\n      f.fac_key\n  )\nSELECT\n  ROUND(CORR(resource_score, treatment_adherence_rate)::NUMERIC, 4) AS correlation_resource_adequacy_vs_adherence\nFROM FacilityMetrics LIMIT 100\n"], "external_knowledge": [2, 5, 60], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 4, "distinct": false, "order": false}}
{"instance_id": "mental_health_14", "selected_database": "mental_health", "query": "Our Director, Dr. Sharma, needs help. We have a lot more patients coming in who are in a really bad place—severely depressed or anxious and also at high risk for suicide. She wants to build a list of our 'go-to' experts for these tough cases. Can you figure out, for each diagnosis like 'Depression', 'Anxiety', etc, which of our clinicians has the most hands-on experience with this exact type of high-risk patient? I need a table that shows the diagnosis, the clinician, how many of these patients they have, and then ranks them, so we can see who is number 1, 2, 3 for each category. Please organize the whole thing by diagnosis, then by the rank.", "normal_query": "To address a surge in complex cases, the Director of Clinical Services is creating an expert consultation roster. The goal is to identify clinicians with the most experience managing the High Severity, High Risk Patient Group. Please generate a report that, for each primary diagnosis, ranks clinicians based on the number of these specific high-risk patients they manage. The output table should include the primary diagnosis, the clinician's identifier, the count of these high-risk patients for that clinician, and their resulting rank within that diagnosis category. The final roster should be sorted first by the primary diagnosis and then by the clinician's rank to clearly present the top experts for each specialty.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH\n  -- Step 1: Identify all patients who fit the \"High Severity, High Risk\" profile as defined in KB #49.\n  -- This involves joining patient, assessment, and diagnosis tables and applying the specified criteria.\n  HighSeverityRiskPatients AS (\n    SELECT\n      p.clin_lead_ref AS clin_key,\n      asd.prim_dx\n    FROM patients AS p\n    JOIN assessmentbasics ab ON p.pat_key = ab.pat_owner_ref\n    JOIN assessmentsymptomsandrisk asr ON ab.ab_key = asr.asr_key\n    JOIN assessmentsocialanddiagnosis asd ON ab.ab_key = asd.asd_key\n    WHERE\n      (asr.phq9_scr > 19 OR asr.gad7_scr > 14)\n      AND asr.suic_risk = 'High'\n  )\n-- Step 2: Final Ranking. Count the number of these high-risk patients for each clinician and primary diagnosis.\n-- Use the RANK() window function to partition the data by primary diagnosis and order clinicians by their patient count in descending order.\nSELECT\n  prim_dx,\n  clin_key,\n  patient_count,\n  clinician_rank\nFROM (\n  SELECT\n    prim_dx,\n    clin_key,\n    COUNT(*) AS patient_count,\n    RANK() OVER (PARTITION BY prim_dx ORDER BY COUNT(*) DESC) AS clinician_rank\n  FROM HighSeverityRiskPatients\n  GROUP BY\n    prim_dx,\n    clin_key\n) AS RankedClinicians\nORDER BY\n  prim_dx,\n  clinician_rank;"], "external_knowledge": [49], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "mental_health_15", "selected_database": "mental_health", "query": "I'm looking at Patient P871358's file and I'm a bit concerned. He's been diagnosed with Bipolar for about 10 years and has already been in the hospital 3 times. I need to know if we should escalate his care. So, first, can you run that calculation of the patient's risk for annualized hospitalization and tell me if he gets flagged for a immediate coordination of intensive care? If he does, I need to find him a new clinician right away. The new clinician would need to be at the same facility he last visited, be one of our 'High' confidence therapists, and not have their credentials up for review in the next six months or so let's say any time after June 2024. If he's flagged, show me their IDs and where they work. If his risk score is fine, just let me know by reporting as 'Patient does not meet criteria for Intensive Care Coordination'.", "normal_query": "I'm assessing Patient 'P871358' and need to determine if an Intensive Care Coordination Flag should be raised. First, please calculate his Annualized Hospitalization Risk, then flag it if the criteria is met. If and only if the patient is flagged, I need a list of suitable clinicians for referral. A suitable clinician must be located at the same facility as the patient's most recent encounter, have a 'High' confidence level, and have a next credential review date after June 1st, 2024. Please return the clinician's identifier and their facility. If the patient is not flagged, just report as 'Patient does not meet criteria for Intensive Care Coordination'", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH PatientRisk AS (\n  SELECT\n    p.pat_key,\n    ab.therapy_exp_intensity,\n    (asd.prev_hosp::REAL / (NULLIF(asd.dx_dur_m, 0)::REAL / 12)) AS annualized_risk,\n    (SELECT e.fac_id\n     FROM encounters e\n     WHERE e.pat_ref = p.pat_key\n     ORDER BY e.time_mark DESC\n     LIMIT 1) AS last_facility_id\n  FROM patients p\n  JOIN assessmentbasics ab ON p.pat_key = ab.pat_owner_ref\n  JOIN assessmentsocialanddiagnosis asd ON ab.ab_key = asd.asd_key\n  WHERE p.pat_key = 'P871358' -- Focusing on our target patient\n)\nSELECT\n  CASE\n    WHEN pr.annualized_risk > 1.5 AND CAST(SPLIT_PART(pr.therapy_exp_intensity, ' ', 1) AS REAL) < 8\n    THEN (\n      SELECT jsonb_agg(jsonb_build_object('clinician_id', c.clin_key, 'facility_id', c.fac_connect))\n      FROM clinicians c\n      WHERE\n        c.fac_connect = pr.last_facility_id\n        AND c.clin_conf = 'High'\n        AND c.nxt_rev_dt > '2024-06-01'\n    )::TEXT\n    ELSE 'Patient does not meet criteria for Intensive Care Coordination'\n  END AS action_required\nFROM PatientRisk pr\n"], "external_knowledge": [66, 67], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "mental_health_16", "selected_database": "mental_health", "query": "I'm doing a review on patient P883117 and need to check a couple of things for her file. First, she's supposed to be getting at least 5 hours of therapy a month in her program. Can you check her latest therapy exp intensity and tell me if she's meeting that target? Second, her insurance gives her $1,200 a year for treatment. Based on her personal cost-effectiveness rate of 0.0509, what's her total expected quality of life gain for the whole year? And finally, is that number over our 'good value' threshold of 50 points? Just give me a quick summary for all the assessment for this patient, including the patient's id, and the answers to those three questions.", "normal_query": "I need to perform a case audit for patient 'P883117'. The patient is in a program requiring a minimum therapy intensity of 5 hours per month. Their annual insurance budget for treatment is $1,200, and their current recorded treatment cost-effectiveness is 0.0509 QoL-points per dollar. Please provide a report that answers the following: A boolean value indicating if the patient's current therapy_exp_intensity meets the 5 hours/month target. The calculated Projected Annual QoL Gain, based on their cost-effectiveness rate and the $1,200 budget. A boolean value indicating if this projected gain is greater than 50 points. The output show for all the assessment for this patient with columns for their identifier and these three calculated results.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nSELECT\n    pat_owner_ref AS patient_id,\n    -- Step 1b: Check therapy intensity vs. target\n    (CAST(SPLIT_PART(therapy_exp_intensity, ' ', 1) AS REAL) >= 5) AS meets_intensity_target,\n    -- Step 2b: Calculate Projected Annual QoL Gain\n    (CAST(regexp_replace(treatment_cost_eff, '[^0-9.]', '', 'g') AS REAL) * 1200) AS projected_annual_qol_gain,\n    -- Step 3: Check if the gain is a 'good value'\n    ((CAST(regexp_replace(treatment_cost_eff, '[^0-9.]', '', 'g') AS REAL) * 1200) > 50) AS is_good_value\nFROM\n    assessmentbasics\nWHERE\n    -- Isolate the specific patient's record\n    pat_owner_ref = 'P883117';"], "external_knowledge": [71], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "mental_health_17", "selected_database": "mental_health", "query": "I need to run a full audit on patient P425079. His insurance gives him $6,000 a year for treatment, but we charge in pounds—£100 an hour. Assuming an exchange rate of 1.25 dollars to the pound, how many hours of therapy can he actually get for his money? Also, his high-acuity plan says he needs 30 hours of therapy a month, but his chart says he's only getting 29.86. Can you confirm if he's meeting that target? Lastly, given his personal cost-effectiveness rate of 0.1197, what's his total potential quality of life gain if he uses his whole $6,000 budget? And is that number over our 'good value' benchmark of 650?", "normal_query": "I am conducting a detailed audit for patient 'P425079'. His insurance plan has a maximum out-of-pocket cost of $6,000 USD per year. Our clinic's therapy rate is £100 GBP per hour, with the current exchange rate at 1.25 USD per GBP. The patient's therapy plan requires a minimum intensity of 30 hours/month, and his last recorded intensity was 29.86 hours/month. His cost-effectiveness rate is 0.1197 QoL-points/dollar. Please provide a report with four calculated fields: The total number of therapy hours his annual budget can afford, rounded to 2 decimal places. A boolean value indicating if his current therapy intensity meets the 30-hour monthly target. The Projected QoL Gain from Max Out-of-Pocket rounded to 2 decimal, using his cost-effectiveness rate and the $6,000 budget. A boolean value indicating if this projected gain exceeds the 'clinically valuable' threshold of 650 points.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH PatientData AS (\n    SELECT\n        pat_owner_ref,\n        CAST(SPLIT_PART(therapy_exp_intensity, ' ', 1) AS REAL) AS current_intensity_hours,\n        CAST(regexp_replace(treatment_cost_eff, '[^0-9.]', '', 'g') AS REAL) AS cost_eff_rate\n    FROM assessmentbasics\n    WHERE pat_owner_ref = 'P425079'\n)\nSELECT\n    ROUND(((6000 / 1.25) / 100), 2) AS affordable_therapy_hours_per_year,\n    (current_intensity_hours >= 30) AS meets_intensity_target,\n    ROUND((cost_eff_rate * 6000)::NUMERIC, 2) AS projected_qol_gain,\n    ((cost_eff_rate * 6000) > 650) AS is_clinically_valuable\nFROM PatientData LIMIT 100\n"], "external_knowledge": [72], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "mental_health_18", "selected_database": "mental_health", "query": "I'm working on a study and need to find a very specific group of patients. I'm looking for people with a Bipolar diagnosis. Once you have that list, I need to check one more thing about their current treatment. Our standard for this group is 15 hours of therapy a month. Can you just show me a table of these patients with their ID, diagnosis duration, and hospitalization count, and then add a true/false column that tells me if they're meeting that 15-hour therapy target?", "normal_query": "For my research paper, I need to analyze a Chronic High-Acuity Bipolar Cohort. After identifying this cohort, I need to check their Intensive Therapy Standard Compliance. Please generate a report that lists each patient's identifier, their diagnosis duration in months, their count of previous hospitalizations, and a boolean value indicating if their current therapy intensity is 15 hours per month or more.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nSELECT\n    p.pat_key,\n    asd.dx_dur_m,\n    asd.prev_hosp,\n    -- Perform the final check for therapy intensity compliance (KB #75)\n    (CAST(SPLIT_PART(ab.therapy_exp_intensity, ' ', 1) AS REAL) >= 15) AS meets_intensity_standard\nFROM\n    patients p\nJOIN\n    assessmentbasics ab ON p.pat_key = ab.pat_owner_ref\nJOIN\n    assessmentsocialanddiagnosis asd ON ab.ab_key = asd.asd_key\nWHERE\n    asd.prim_dx = 'Bipolar'\n    AND asd.dx_dur_m >= 120\n    AND asd.prev_hosp > 3;"], "external_knowledge": [76, 77], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "mental_health_19", "selected_database": "mental_health", "query": "I'm trying to decide where to focus our new goal-setting program. I have a hunch that our PTSD patients are struggling more to make progress than our anxiety patients. Can you check this for me? I want to see a comparison: on average, how many recovery goals would a PTSD patient achieve in a full year versus an anxiety patient? I just need those two numbers side-by-side to see if my hunch is right.", "normal_query": "For program development, I need to compare the Annual Goal Achievement between two patient populations. The first population consists of patients with a primary diagnosis of 'PTSD', and the second consists of patients with a primary diagnosis of 'Anxiety'. Please calculate the average Annual Goal Achievement for each group. The final output should be a single row with two columns: one for the PTSD group's average and one for the Anxiety group's average.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nSELECT\n    AVG(\n        CASE\n            WHEN asd.prim_dx = 'PTSD' THEN CAST(SPLIT_PART(ab.recovery_goal_vel, ' ', 1) AS REAL) * 12\n            ELSE NULL\n        END\n    ) AS ptsd_avg_annual_goals,\n    AVG(\n        CASE\n            WHEN asd.prim_dx = 'Anxiety' THEN CAST(SPLIT_PART(ab.recovery_goal_vel, ' ', 1) AS REAL) * 12\n            ELSE NULL\n        END\n    ) AS anxiety_avg_annual_goals\nFROM\n    assessmentbasics ab\nJOIN\n    assessmentsocialanddiagnosis asd ON ab.ab_key = asd.asd_key\nWHERE\n    ab.recovery_goal_vel IS NOT NULL\n    AND asd.prim_dx IN ('PTSD', 'Anxiety') LIMIT 100\n"], "external_knowledge": [81], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "mental_health_20", "selected_database": "mental_health", "query": "I'm Dr. Hanson, a pharmacist. I need to run a safety check on our Anxiety patients at the F533 facility, but only those who've been diagnosed for more than a year. I'm worried about medication side effects. Can you calculate an side effect score for a 12-month period for each of them with just 2 medications? I need a list of these patients showing their ID, the original density number, this new score, and a true/false if their score is over our new safety limit of 0.1. Please put the patients with the highest scores at the top.", "normal_query": "I am conducting a 'Medication Protocol Review' for Anxiety patients at facility F533 who have a diagnosis duration of more than 12 months. I need to calculate their Annualized Side Effect Score, assuming that there are 2 number of medications. Please provide a report that lists the patient identifier, their original med side eff density, their calculated Annualized Side Effect Score, and a boolean flag indicating if this score is over our protocol's threshold of 0.1. The list should be sorted by the highest score first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- SQL INTENT: For Anxiety patients at facility F533 who have been in treatment for over 1 year,\n-- calculate their Annualized Side Effect Score (assuming 2 meds) and flag those who exceed the protocol's threshold of 0.1.\n\nWITH PatientCohort AS (\n    -- Step 1: Identify the specific group of patients for the audit with corrected parameters.\n    SELECT DISTINCT p.pat_key\n    FROM patients p\n    JOIN encounters e ON p.pat_key = e.pat_ref\n    JOIN treatmentbasics tb ON e.enc_key = tb.enc_ref\n    JOIN assessmentbasics ab ON e.ab_ref = ab.ab_key\n    JOIN assessmentsocialanddiagnosis asd ON ab.ab_key = asd.asd_key\n    WHERE\n        e.fac_id = 'F533'\n        AND asd.prim_dx = 'Anxiety'\n        AND asd.dx_dur_m > 12\n)\n-- Step 2 & 3: Perform the calculations and conditional check for the identified cohort.\nSELECT\n    ab.pat_owner_ref AS patient_id,\n    ab.med_side_eff_density,\n    -- Calculate Annualized Side Effect Score (KB #82)\n    (CAST(SPLIT_PART(ab.med_side_eff_density, ' ', 1) AS REAL) * 2 * 12) AS annualized_side_effect_score,\n    -- Check if the score exceeds the threshold\n    (CAST(SPLIT_PART(ab.med_side_eff_density, ' ', 1) AS REAL) * 2 * 12) > 0.1 AS is_over_threshold\nFROM\n    assessmentbasics ab\nWHERE\n    ab.pat_owner_ref IN (SELECT pat_key FROM PatientCohort)\n    AND ab.med_side_eff_density IS NOT NULL\nORDER BY\n    annualized_side_effect_score DESC;\n"], "external_knowledge": [84], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": true, "order": true}}
{"instance_id": "mental_health_M_3", "selected_database": "mental_health", "query": "Hey! Go ahead and clean up the treatmentoutcomes table by deleting any old or stale records, but only for those patients who've been flagged as Non-Compliant. Leave the rest untouched!", "normal_query": "Please remove Stale Treatment Outcome Records from the treatmentoutcomes table, but only for patients who have been identified as Non-Compliant Patient.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH NonCompliantPatients AS (\n    SELECT DISTINCT p.pat_key\n    FROM patients p\n    JOIN encounters e ON p.pat_key = e.pat_ref\n    JOIN treatmentbasics tb ON e.enc_key = tb.enc_ref\n    JOIN treatmentoutcomes txo ON tb.tx_key = txo.tx_ref\n    WHERE tb.med_adh = 'Non-compliant' \n      AND UPPER(txo.txprogmet->>'Tx_Adh') = 'NON-COMPLIANT'\n),\nStaleOutcomesToDelete AS (\n    SELECT txo.tx_out_key\n    FROM treatmentoutcomes txo\n    JOIN treatmentbasics tb ON txo.tx_ref = tb.tx_key\n    JOIN encounters e ON tb.enc_ref = e.enc_key\n    WHERE e.pat_ref IN (SELECT pat_key FROM NonCompliantPatients)\n      AND e.time_mark < (NOW() - INTERVAL '60 days')\n)\nDELETE FROM treatmentoutcomes\nWHERE tx_out_key IN (SELECT tx_out_key FROM StaleOutcomesToDelete);"], "external_knowledge": [18, 62], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n        sample_query = f\"\"\"\n        WITH NonCompliantPatients AS (\n            SELECT DISTINCT p.pat_key\n            FROM patients p\n            JOIN encounters e ON p.pat_key = e.pat_ref\n            JOIN treatmentbasics tb ON e.enc_key = tb.enc_ref\n            JOIN treatmentoutcomes txo ON tb.tx_key = txo.tx_ref\n            WHERE tb.med_adh = 'Non-compliant' \n            AND UPPER(txo.txprogmet->>'Tx_Adh') = 'NON-COMPLIANT'\n        ),\n        StaleOutcomesToDelete AS (\n            SELECT txo.tx_out_key\n            FROM treatmentoutcomes txo\n            JOIN treatmentbasics tb ON txo.tx_ref = tb.tx_key\n            JOIN encounters e ON tb.enc_ref = e.enc_key\n            WHERE e.pat_ref IN (SELECT pat_key FROM NonCompliantPatients)\n            AND e.time_mark < (NOW() - INTERVAL '60 days')\n        )\n        SELECT COUNT(*) AS remaining_matches\n        FROM treatmentoutcomes txo\n        WHERE txo.tx_out_key IN (SELECT tx_out_key FROM StaleOutcomesToDelete);\n        \"\"\"\n        \n        res, _, _ = execute_queries([sample_query], db_name, conn)\n        assert res[0][0] == 0, f\"Test case failed: {res[0][0]} stale treatment outcomes still exist after deletion.\"\n        return True"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": true, "order": false}}
{"instance_id": "mental_health_M_4", "selected_database": "mental_health", "query": "Hey! Can you make a reusable database function called calculate_tes? If it already exists, just replace it. This function should take a treatment key, look up the 'engagement' level from the therapy details tied to that treatment, and return the score for therapy engagements as a number.", "normal_query": "Please create (or replace if it exists) a reusable database function named calculate_tes. This function's purpose is to calculate the Therapy Engagement Score for a single treatment record. It should take the treatment key as input, find the corresponding 'engagement' level from the therapy details data, and return the calculated numeric score based on the standard Therapy Engagement Score definition.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- This DDL statement creates (or replaces) a reusable function to calculate the Therapy Engagement Score (TES, KB 4) for a single treatment record.\nCREATE OR REPLACE FUNCTION calculate_tes(p_tx_key BIGINT)\nRETURNS NUMERIC AS $$\nDECLARE\n    v_engagement_text TEXT;\n    v_engagement_score NUMERIC;\nBEGIN\n    -- Step 1: Select the engagement status from the 'th_eng' column for the given tx_key.\n    SELECT th_eng\n    INTO v_engagement_text\n    FROM treatmentbasics\n    WHERE tx_key = p_tx_key;\n\n    -- Step 2: If no record is found for the given key, return NULL.\n    IF NOT FOUND THEN\n        RETURN NULL;\n    END IF;\n\n    -- Step 3: Apply the scoring logic based on the value of 'th_eng'.\n    CASE v_engagement_text\n        WHEN 'High' THEN v_engagement_score := 3;\n        WHEN 'Medium' THEN v_engagement_score := 2;\n        WHEN 'Low' THEN v_engagement_score := 1;\n        ELSE v_engagement_score := 0; -- Default case for NULL or other unexpected values\n    END CASE;\n\n    -- Step 4: Return the calculated score.\n    RETURN v_engagement_score;\n\n-- Add exception handling for robustness in case of unexpected errors.\nEXCEPTION\n    WHEN others THEN\n        RAISE WARNING 'Error calculating TES for tx_key %: %', p_tx_key, SQLERRM;\n        RETURN NULL;\nEND;\n$$ LANGUAGE plpgsql;"], "external_knowledge": [4], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n        txkey = 1\n        sample_query = f\"\"\"\n        SELECT tx_key, calculate_tes({txkey}) FROM treatmentbasics WHERE tx_key = {txkey}\n        \"\"\"\n        res, _, _ = execute_queries([sample_query], db_name, conn)\n        calculated_tes = res[0][1]\n\n        check_tes = f\"\"\"\n        SELECT\n            CASE th_eng\n                WHEN 'High' THEN 3\n                WHEN 'Medium' THEN 2\n                WHEN 'Low' THEN 1\n                ELSE 0\n            END AS expected_tes\n        FROM treatmentbasics\n        WHERE tx_key = {txkey};\n        \"\"\"\n        res, _, _ = execute_queries([check_tes], db_name, conn)\n        expected_tes = res[0][0]\n\n        assert calculated_tes == expected_tes, f\"Test case failed: Expected TES {expected_tes}, got {calculated_tes}\"\n\n        return True\n"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "mental_health_M_6", "selected_database": "mental_health", "query": "Hi team, I'm worried we're missing the chance to help patients who are really struggling until it's too late. I want to build a new 'at-risk' watchlist named vulnerable_patient_watchlist that our care coordinators can check every morning. Let's automatically flag any patient who is both having frequent crises and seems to be socially isolated. When the system flags someone, I need the new watchlist to show the watchlist id, their patient ID, who their clinician is, exactly how many crises they've had, what their support score was, and—most importantly—when their next appointment is, and also the insertion date of the watchlist. Make sure you pull that date from the notes of their last visit so we have the most current one.", "normal_query": "As the head of clinical outreach, I am launching a new initiative to reduce critical incidents. To do this, I need to establish a new, permanent data process called 'Vulnerable Patient Watchlist Generation'. This process will create and populate a new table named vulnerable_patient_watchlist. To determine who belongs on this list, we will use our established 'Patient with High Crisis & Low Support Profile'. For every patient who meets these criteria, the new vulnerable_patient_watchlist table must store the following actionable information: a watchlist id, their unique patient ID, the ID of their lead clinician, their total crisis intervention count, their calculated SSE score, and their next scheduled appointment date as recorded during their most recent encounter, and the date of the watchlist's insertion.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE vulnerable_patient_watchlist (\n    watchlist_id SERIAL PRIMARY KEY,\n    pat_key TEXT NOT NULL,\n    clin_lead_ref TEXT,\n    last_known_crisis_count REAL,\n    social_support_score INT,\n    next_appointment_date DATE,\n    watchlist_added_on TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\nINSERT INTO vulnerable_patient_watchlist (\n    pat_key,\n    clin_lead_ref,\n    last_known_crisis_count,\n    social_support_score,\n    next_appointment_date\n)\nWITH patient_support_scores AS (\n    SELECT\n        asd.asd_key,\n        AVG(\n            CASE asd.funcassess->>'Soc_Sup'\n                WHEN 'Strong' THEN 3\n                WHEN 'Adequate' THEN 2\n                WHEN 'Limited' THEN 1\n                WHEN 'Weak' THEN 0\n                ELSE 0\n            END\n            +\n            CASE asd.funcassess->>'Rel_Qual'\n                WHEN 'Good' THEN 3\n                WHEN 'Fair' THEN 2\n                WHEN 'Poor' THEN 1\n                WHEN 'Conflicted' THEN 0\n                ELSE 0\n            END\n        ) OVER (PARTITION BY p.pat_key) AS sse_score\n    FROM patients p\n    JOIN encounters e ON p.pat_key = e.pat_ref\n    JOIN treatmentbasics tb ON e.enc_key = tb.enc_ref\n    JOIN assessmentsocialanddiagnosis asd ON e.ab_ref = asd.asd_key\n),\npatient_crisis_counts AS (\n    SELECT\n        e.pat_ref,\n        SUM(tb.crisis_int) AS total_crisis_interventions\n    FROM treatmentbasics tb\n    JOIN encounters e ON tb.enc_ref = e.enc_key\n    GROUP BY e.pat_ref\n)\nSELECT\n    p.pat_key,\n    p.clin_lead_ref,\n    pcc.total_crisis_interventions,\n    pss.sse_score,\n    TO_DATE(e.nx_appt_dt, 'DD/MM/YYYY') AS next_appointment\nFROM patients p\nJOIN assessmentbasics ab ON p.pat_key = ab.pat_owner_ref\nJOIN patient_support_scores pss ON ab.ab_key = pss.asd_key\nJOIN patient_crisis_counts pcc ON p.pat_key = pcc.pat_ref\nLEFT JOIN encounters e\n    ON p.pat_key = e.pat_ref\n    AND e.time_mark = (\n        SELECT MAX(time_mark)\n        FROM encounters\n        WHERE pat_ref = p.pat_key\n    )\nWHERE\n    pcc.total_crisis_interventions > 2\n    AND pss.sse_score < 3;"], "external_knowledge": [86, 47, 8], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    table_check_query = \"\"\"\n    SELECT EXISTS (\n        SELECT FROM information_schema.tables \n        WHERE table_name = 'vulnerable_patient_watchlist'\n    );\n    \"\"\"\n    table_exists, _, _ = execute_queries([table_check_query], db_name, conn)\n    assert table_exists[0][0], \"Table 'vulnerable_patient_watchlist' was not created.\"\n\n    verification_query = \"\"\"\n    SELECT COUNT(*)\n    FROM vulnerable_patient_watchlist\n    WHERE last_known_crisis_count > 2 AND social_support_score < 3;\n    \"\"\"\n    result, _, _ = execute_queries([verification_query], db_name, conn)\n    assert result[0][0] > 0, \"No qualifying vulnerable patients were inserted.\"\n\n    appointment_check = \"\"\"\n    SELECT COUNT(*)\n    FROM vulnerable_patient_watchlist\n    WHERE next_appointment_date IS NOT NULL;\n    \"\"\"\n    res, _, _ = execute_queries([appointment_check], db_name, conn)\n    assert res[0][0] >= 0, \"Appointment data may be missing or improperly formatted.\"\n\n    return True"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "mental_health_M_8", "selected_database": "mental_health", "query": "This is urgent. My patient, P515871, is at high risk for suicide based on the assessment I just filed. Our protocol says this should immediately flag me for intensive care coordination so we can get them help now. Can you run a script to do this protocol?", "normal_query": "I've just finished an emergency assessment for patient P515871, who has been flagged with a 'High' suicide risk. To ensure immediate action, I need to invoke our 'High-Risk Escalation Protocol'. Please execute a script to do this protocol.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- This query implements the High-Risk Escalation Protocol by updating the care_coord status for clinicians\n-- managing patients recently identified with a high suicide risk.\nUPDATE clinicians\nSET care_coord = 'Intensive'\nWHERE\n    -- The update applies to any clinician whose key is in the result of this subquery.\n    clin_key IN (\n        -- Step 1: This subquery identifies the keys of all clinicians who need to be escalated.\n        SELECT DISTINCT p.clin_lead_ref\n        FROM patients p\n        -- Step 1.1: We need to join through assessments to find the risk level.\n        JOIN (\n            -- This inner subquery is crucial: it finds only the SINGLE LATEST assessment for each patient.\n            SELECT DISTINCT ON (pat_owner_ref) pat_owner_ref, ab_key\n            FROM assessmentbasics ab\n            JOIN encounters e ON ab.ab_key = e.ab_ref\n            ORDER BY pat_owner_ref, e.time_mark DESC\n        ) AS latest_assessment ON p.pat_key = latest_assessment.pat_owner_ref\n        JOIN assessmentsymptomsandrisk asr ON latest_assessment.ab_key = asr.asr_key\n        -- Step 1.2: This is the trigger condition for the protocol.\n        WHERE asr.suic_risk = 'High' AND p.clin_lead_ref IS NOT NULL\n    )\n    -- Step 2: This condition makes the operation idempotent, preventing unnecessary updates.\n    AND care_coord IS DISTINCT FROM 'Intensive';\n"], "external_knowledge": [90], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n\n        verification_query = \"\"\"\n        SELECT DISTINCT p.clin_lead_ref, c.care_coord\n        FROM patients p\n        JOIN (\n            SELECT DISTINCT ON (ab.pat_owner_ref)\n                ab.pat_owner_ref, ab.ab_key\n            FROM assessmentbasics ab\n            JOIN encounters e ON ab.ab_key = e.ab_ref\n            ORDER BY ab.pat_owner_ref, e.time_mark DESC\n        ) latest ON p.pat_key = latest.pat_owner_ref\n        JOIN assessmentsymptomsandrisk asr ON latest.ab_key = asr.asr_key\n        JOIN clinicians c ON p.clin_lead_ref = c.clin_key\n        WHERE asr.suic_risk = 'High' AND p.clin_lead_ref IS NOT NULL;\n        \"\"\"\n\n        res, _, _ = execute_queries([verification_query], db_name, conn)\n        for clin_lead_ref, care_coord in res:\n            assert care_coord == 'Intensive', f\"Clinician {clin_lead_ref} should have care_coord = 'Intensive', got {care_coord}\"\n\n        return True"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": true, "order": false}}
{"instance_id": "mental_health_M_9", "selected_database": "mental_health", "query": "We nearly missed a serious situation with a patient named John because nobody saw their hospitalization risk score was getting dangerously high. I want to fix this now by doing a review protocol. Please help me flag those patients that is the same as John in our patient files. First, please add a new boolean column named needs_case_review to the patients table, with a default of FALSE. Then, for all patients that are currently meeting the criteria based on their latest assessment, execute an update to set the flag to TRUE. This will be our new automated safety alert.", "normal_query": "In response to a recent critical incident, our safety board has approved a new 'Mandatory Case Review Protocol'. Some patients are exceeding the hospital risk density recently, like John, and because of this, we must execute this protocol to flag and identify more patients that has the same situation as John. To implement this, I need a two-part script. First, please add a new boolean column named needs_case_review to the patients table, with a default of FALSE. Second, execute an update to set this new flag to TRUE for all patients who currently meet the protocol's criteria based on their latest assessment.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- This two-part query first modifies the patients table and then updates it\n-- to flag patients who exceed the new hospitalization risk threshold.\n\n-- Step 1: Add the new flag column to the 'patients' table.\n-- Using 'IF NOT EXISTS' prevents an error if the script is run more than once.\nALTER TABLE patients ADD COLUMN IF NOT EXISTS needs_case_review BOOLEAN DEFAULT FALSE;\n\n-- Step 2: Update the flag for all patients meeting the risk criteria.\nUPDATE patients p\nSET needs_case_review = TRUE\n-- Step 2.1: Use the 'FROM' clause to join with the latest assessment data.\nFROM (\n    -- This subquery gets the most recent hospitalization risk for every patient.\n    SELECT DISTINCT ON (pat_owner_ref)\n        pat_owner_ref,\n        CAST(REGEXP_REPLACE(hosp_risk_density, '[^0-9.]', '', 'g') AS NUMERIC) as risk_score\n    FROM assessmentbasics ab\n    JOIN encounters e ON ab.ab_key = e.ab_ref\n    ORDER BY pat_owner_ref, e.time_mark DESC\n) AS latest_risk\n-- Step 2.2: The join condition and the filter based on the new safety protocol's threshold.\nWHERE p.pat_key = latest_risk.pat_owner_ref AND latest_risk.risk_score > 1.5;\n"], "external_knowledge": [92], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n        # Step 1: Get latest hospitalization risk and flag status per patient\n        verification_query = \"\"\"\n        SELECT p.pat_key,\n            latest.risk_score,\n            p.needs_case_review\n        FROM patients p\n        JOIN (\n            SELECT DISTINCT ON (ab.pat_owner_ref)\n                ab.pat_owner_ref,\n                CAST(REGEXP_REPLACE(ab.hosp_risk_density, '[^0-9.]', '', 'g') AS NUMERIC) AS risk_score,\n                e.time_mark\n            FROM assessmentbasics ab\n            JOIN encounters e ON ab.ab_key = e.ab_ref\n            ORDER BY ab.pat_owner_ref, e.time_mark DESC\n        ) AS latest\n        ON p.pat_key = latest.pat_owner_ref;\n        \"\"\"\n        \n        res, _, _ = execute_queries([verification_query], db_name, conn)\n        \n        for pat_key, risk_score, needs_review in res:\n            risk_score = float(risk_score)\n            \n            if risk_score > 1.5:\n                assert needs_review is True, f\"Patient {pat_key} with risk {risk_score} should be flagged (needs_case_review = TRUE)\"\n            else:\n                assert needs_review in (False, None), f\"Patient {pat_key} with risk {risk_score} should not be flagged\"\n        return True"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": true, "order": false}}
{"instance_id": "mental_health_M_10", "selected_database": "mental_health", "query": "Hi, I'm Dr. Rossi. I need an urgent change to a patient's file. My patient David, that's P883117, just lost his entire support system because his wife was in a bad accident. His current support plan is useless now. We've agreed he needs to be twice as self-reliant. Can you find his last assessment, look up his 'support utilization rate' which I think is 1.5, and cut it in half? Please change it in the system to 0.75 so his official plan is up to date.", "normal_query": "I have an urgent update for my patient, David (P883117), following a major life event—his primary caregiver is incapacitated. His existing support_util_rate of '1.5 support-level/contact' is no longer viable. We have set a new therapeutic goal to halve his reliance on external support. Please execute a script to find his single most recent assessment record and update the support_util_rate value to '0.75 support-level/contact' to reflect this new plan.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- This query performs a Therapeutic Goal Adjustment for patient P425079 in response to a life crisis.\n-- It halves their target support utilization rate on their most recent assessment record.\n\n-- Step 1: Target the 'assessmentbasics' table for the update.\nUPDATE assessmentbasics\nSET\n    -- Step 2: Calculate and set the new value.\n    -- We extract the numeric part, divide it by two, and then concatenate the unit string back on.\n    support_util_rate = ROUND((\n        CAST(REGEXP_REPLACE(support_util_rate, '[^0-9.]', '', 'g') AS NUMERIC) / 2\n    )::NUMERIC, 2)::TEXT || ' support-level/contact'\nWHERE\n    -- Step 3: Ensure this update only applies to the single, most recent assessment for this specific patient.\n    ab_key = (\n        SELECT ab.ab_key\n        FROM assessmentbasics ab\n        JOIN encounters e ON ab.ab_key = e.ab_ref\n        WHERE ab.pat_owner_ref = 'P883117'\n        ORDER BY e.time_mark DESC\n        LIMIT 1\n    );\n"], "external_knowledge": [94], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n        # Step 1: Get the latest ab_key before prediction query runs\n        get_latest_abkey_query = \"\"\"\n        SELECT ab.support_util_rate\n        FROM assessmentbasics ab\n        JOIN encounters e ON ab.ab_key = e.ab_ref\n        WHERE ab.pat_owner_ref = 'P883117'\n        ORDER BY e.time_mark DESC\n        LIMIT 1;\n        \"\"\"\n        res, _, _ = execute_queries(get_latest_abkey_query, db_name, conn)\n        expected_rate = res[0][0]\n        \n        original_numeric = float(''.join(c for c in expected_rate if c.isdigit() or c == '.'))\n        expected = 1.5\n\n        original_rate = float(original_numeric * 2)\n\n        assert original_rate == expected, f\"Test failed: expected '{expected}', got '{original_rate}'\"\n\n        return True"], "category": "Management", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_1", "selected_database": "reverse_logistics", "query": "Show me how much the whole process, from transport to final disposition, cost us for every return. Round the result to 2 decimal places, biggest cost first.", "normal_query": "List the Total Return Cost (TRC) for each return case, round the value to 2 decimal places, and sort the results by TRC in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT r.casenum,\n       ROUND(\n         (r.return_details->'shipping'->>'fee')::numeric +\n         (fm.cost_breakdown->'fees'->>'restocking_fee')::numeric +\n         (fm.cost_breakdown->'fees'->>'relabeling_cost')::numeric +\n         (fm.cost_breakdown->'disposal'->>'disposal_cost')::numeric +\n         (fm.cost_breakdown->'repair_costs'->>'repair_estimate')::numeric\n       ,2) AS trc\nFROM returns r\nJOIN financial_management fm ON fm.casetag = r.casenum\nORDER BY trc DESC;"], "external_knowledge": [0], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "reverse_logistics_2", "selected_database": "reverse_logistics", "query": "Tell me the overall gain or loss from all returns: take what we got back after processing and subtract the combined out-of-pocket amount (transport, getting it sellable again, label fixes, end-of-life handling, fix-up quote). Include the net figure and both parts, rounded to two decimals.", "normal_query": "Estimate the overall net impact across all processed returns by subtracting the total handling cost (transport, reintegration, label correction, end-of-life handling, fix-up estimate) from the amount recaptured after processing; also show both components; round each to two decimals.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT ROUND(SUM((fm.cost_breakdown->'valuation'->>'recovery_value')::numeric)-SUM((r.return_details->'shipping'->>'fee')::numeric+(fm.cost_breakdown->'fees'->>'restocking_fee')::numeric+(fm.cost_breakdown->'fees'->>'relabeling_cost')::numeric+(fm.cost_breakdown->'disposal'->>'disposal_cost')::numeric+(fm.cost_breakdown->'repair_costs'->>'repair_estimate')::numeric),2) AS total_rpi,ROUND(SUM((fm.cost_breakdown->'valuation'->>'recovery_value')::numeric),2) AS total_recovery_value,ROUND(SUM((r.return_details->'shipping'->>'fee')::numeric+(fm.cost_breakdown->'fees'->>'restocking_fee')::numeric+(fm.cost_breakdown->'fees'->>'relabeling_cost')::numeric+(fm.cost_breakdown->'disposal'->>'disposal_cost')::numeric+(fm.cost_breakdown->'repair_costs'->>'repair_estimate')::numeric),2) AS total_return_cost FROM returns r JOIN financial_management fm ON fm.casetag=r.casenum;"], "external_knowledge": [1], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_3", "selected_database": "reverse_logistics", "query": "Show me the average value recovered per day since sale for the entire portfolio, along with the recovery value and number of days used in the calculation, rounded to 2 decimal places from highest to lowest.", "normal_query": "Compute the portfolio-wide average Recovery Rate per Day, rounded to two decimals; also return the aggregated Recovery Value and the aggregated Days Lapsed used in that calculation.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH per_return AS (SELECT (fm.cost_breakdown->'valuation'->>'recovery_value')::numeric AS recovery_value,NULLIF(r.dayslapsed,0)::numeric AS days_lapsed FROM returns r JOIN financial_management fm ON fm.casetag=r.casenum) SELECT ROUND(AVG(recovery_value/days_lapsed),2) AS avg_rrd,ROUND(SUM(recovery_value),2) AS total_recovery_value,ROUND(SUM(days_lapsed),2) AS total_days_lapsed FROM per_return WHERE days_lapsed IS NOT NULL;"], "external_knowledge": [2], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_M_1", "selected_database": "reverse_logistics", "query": "Find store credit refunds that are for zero dollars, mark them as “Pending,” and show me their reference numbers with the new status.", "normal_query": "Only for zero-amount reimbursements issued as store credit, set the status to “Pending” and return each affected reference with its new status.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE financial_management fm SET cost_breakdown=jsonb_set(fm.cost_breakdown,'{refund,status}','\"Pending\"',true) WHERE(fm.cost_breakdown->'refund'->>'method')='Store Credit' AND(REGEXP_REPLACE(fm.cost_breakdown->'refund'->>'refund_amount','[^0-9.\\-]','','g'))::numeric=0 RETURNING fm.creditref,fm.cost_breakdown->'refund'->>'status' AS new_status;"], "external_knowledge": [], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute('''\n        SELECT COUNT(*)\n        FROM financial_management\n        WHERE (cost_breakdown->'refund'->>'status') = 'Pending Review'\n          AND (cost_breakdown->'refund'->>'method') = 'Store Credit'\n    ''')\n    result = cursor.fetchone()[0]\n    assert isinstance(result, int), 'Result should be an integer'\n    assert result >= 0, 'Count should be non-negative'\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_4", "selected_database": "reverse_logistics", "query": "I want to know the average lost of a return, consider the environmental impact factors. Round to two decimal places.", "normal_query": "For all returns, calculate its average Sustainability Adjusted Loss (SAL), rounded to 2 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT ROUND(AVG(\n      (r.return_details->'shipping'->>'fee')::numeric +\n      (fm.cost_breakdown->'fees'->>'restocking_fee')::numeric +\n      (fm.cost_breakdown->'fees'->>'relabeling_cost')::numeric +\n      (fm.cost_breakdown->'disposal'->>'disposal_cost')::numeric +\n      (fm.cost_breakdown->'repair_costs'->>'repair_estimate')::numeric +\n      0.5 * (fm.cost_breakdown->'sustainability'->>'carbon_footprint')::numeric -\n      (fm.cost_breakdown->'valuation'->>'recovery_value')::numeric\n),2) AS avg_sal\nFROM returns r\nJOIN financial_management fm ON fm.casetag = r.casenum;"], "external_knowledge": [4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_5", "selected_database": "reverse_logistics", "query": "Show me each processing site with their average and individual processing times , rounded to one decimal place, and list the slowest sites first.", "normal_query": "List the Average Processing Time (APT) for each processing site, including individual Processing Time values in the result. Round numeric outputs to 1 decimal places. Sort the results by APT in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT rp.loccode, rp.proctime, ROUND(AVG(rp.proctime) OVER (PARTITION BY rp.loccode)::numeric, 1) AS apt_days FROM return_processing rp ORDER BY apt_days DESC;"], "external_knowledge": [5], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 1, "distinct": false, "order": true}}
{"instance_id": "reverse_logistics_6", "selected_database": "reverse_logistics", "query": "I want to know the proportion of returns having warranty claims, and round the answer to one decimal point.", "normal_query": "Calculate the Warranty Claim Ratio (WCR) for the whole business, and round the result to 1 decimal place.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n  ROUND(\n    100.0 * COUNT(*) FILTER (WHERE (r.return_details->'authorization'->>'warranty_claim') IS NOT NULL)\n    / COUNT(*)\n  ,1) AS wcr_percent\nFROM returns r;"], "external_knowledge": [6], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 1, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_7", "selected_database": "reverse_logistics", "query": "For each return, show a severity score equal to the number of signals times the level weight (use 1 for low, 2 for medium, 3 for high). Include both the signal count and the weight, and list the highest scores first.", "normal_query": "For each return, compute the Fraud Flag Severity Score (FFS) as the flag count multiplied by a level weight, using Low=1, Medium=2, High=3; also show the flag count and the applied weight; sort by FFS in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH base AS (SELECT r.casenum,(r.return_details->'fraud'->>'fraud_flags')::int AS flags,r.return_details->'fraud'->>'risk_level' AS lvl FROM returns r),scored AS (SELECT casenum,COALESCE(flags,0) AS fraud_flags,CASE lvl WHEN 'Low' THEN 1 WHEN 'Medium' THEN 2 WHEN 'High' THEN 3 ELSE NULL END AS risk_weight FROM base) SELECT casenum,fraud_flags,risk_weight,(fraud_flags*risk_weight) AS ffs FROM scored WHERE risk_weight IS NOT NULL ORDER BY ffs DESC,casenum;"], "external_knowledge": [7, 21], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "reverse_logistics_8", "selected_database": "reverse_logistics", "query": "Provide the grand total of all regulatory penalties we've accumulated to date (show only the final number).", "normal_query": "Measure the total Regulatory Compliance Penalty (RCP) incurred to date, only show the final number.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT SUM(0.2 *\n(\n(r.return_details->'shipping'->>'fee')::numeric +\n (fm.cost_breakdown->'fees'->>'restocking_fee')::numeric +\n      (fm.cost_breakdown->'fees'->>'relabeling_cost')::numeric +\n(fm.cost_breakdown->'disposal'->>'disposal_cost')::numeric +\n(fm.cost_breakdown->'repair_costs'->>'repair_estimate')::numeric\n)\n) AS total_rcp\nFROM returns r\nJOIN products p ON p.itemcode = r.itemlink\nJOIN financial_management fm ON fm.casetag = r.casenum\nWHERE p.product_traceability->'compliance'->>'regulatory_compliance' = 'Non-compliant';"], "external_knowledge": [8, 29], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_9", "selected_database": "reverse_logistics", "query": "Show the latest 100 items with a relative transport cost versus what’s typical for the same way of coming back. Include the way it came back, round to two decimals, and list newest first.", "normal_query": "For the 100 most recent returns, list each item's Return Channel Cost Index (value = its transport charge divided by the historical average for returns that came back the same way). Include the way it came back, round to two decimals, and sort by the logging time from newest to oldest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH latest AS (SELECT * FROM returns ORDER BY logtime DESC LIMIT 100),norms AS (SELECT return_channel,AVG((return_details->'shipping'->>'fee')::numeric) AS avg_channel_fee FROM returns GROUP BY return_channel) SELECT l.return_channel,ROUND((l.return_details->'shipping'->>'fee')::numeric/NULLIF(n.avg_channel_fee,0),2) AS rcci FROM latest l JOIN norms n USING(return_channel) ORDER BY l.logtime DESC;"], "external_knowledge": [9], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "reverse_logistics_M_2", "selected_database": "reverse_logistics", "query": "Suspend anyone whose combined severity score (using 1 for low, 2 for medium, 3 for high) is 9 or more, and show their IDs with the new category.", "normal_query": "Suspend all customers whose Fraud Flag Severity Score is at least 9 and the weights are Low=1, Medium=2, High=3. Return each customer ID with the updated segment category.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE customers AS c SET seg_category = 'Suspended' FROM orders o JOIN returns r ON r.srctxn = o.txnnum WHERE o.buyerlink = c.profilenum AND ((r.return_details->'fraud'->>'fraud_flags')::int * CASE (r.return_details->'fraud'->>'risk_level') WHEN 'Low' THEN 1 WHEN 'Medium' THEN 2 WHEN 'High' THEN 3 ELSE 1 END) >= 9 RETURNING c.profilenum, seg_category"], "external_knowledge": [7, 21], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute(\"\"\"\n        SELECT COUNT(*) FROM customers\n        WHERE seg_category = 'Suspended'\n    \"\"\")\n    result = cursor.fetchone()[0]\n    assert isinstance(result, int), 'Suspended count should be an integer'\n    assert result >= 0, 'Count should be non-negative'\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_M_3", "selected_database": "reverse_logistics", "query": "Make the expected expense that bring a defective return back to sellable condition increase by 15% for returns waiting more than 60 days. Show me the case numbers and new estimates (rounded to 2 decimal places).", "normal_query": "Increase the repair estimate by 15 percent for any return that has been waiting more than 60 days, and return the case number with the adjusted amount rounded to 2 decimals.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE financial_management fm\nSET cost_breakdown = jsonb_set(\n    cost_breakdown,\n    '{repair_costs,repair_estimate}',\n    to_jsonb(ROUND(((cost_breakdown->'repair_costs'->>'repair_estimate')::numeric)*1.15,2))\n)\nFROM returns r\nWHERE r.casenum = fm.casetag\n  AND r.dayslapsed > 60\nRETURNING fm.casetag AS casenum,\n          (cost_breakdown->'repair_costs'->>'repair_estimate')::numeric AS repair_estimate;"], "external_knowledge": [14, 25], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute('''\n        SELECT COUNT(*) FROM returns r\n        JOIN financial_management fm ON fm.casetag = r.casenum\n        WHERE r.dayslapsed > 60\n    ''')\n    result = cursor.fetchone()[0]\n    assert isinstance(result, int), 'Result should be integer'\n    assert result >= 0, 'Should not be negative'\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_10", "selected_database": "reverse_logistics", "query": "Show me the average cost of printing and attaching new labels for each product category, rounded to two decimals and sorted from highest to lowest.", "normal_query": "List the Average relabeling cost by product category. Round numeric outputs to 2 decimal places and sort descendingly.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT p.itemcategory,\n ROUND(AVG((fm.cost_breakdown->'fees'->>'relabeling_cost')::numeric),2) AS avg_relabel_cost\nFROM products p\nJOIN returns r ON r.itemlink = p.itemcode\nJOIN financial_management fm ON fm.casetag = r.casenum\nGROUP BY p.itemcategory\nORDER BY avg_relabel_cost DESC;"], "external_knowledge": [12], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "reverse_logistics_11", "selected_database": "reverse_logistics", "query": "Show overall disposal spend per method, most expensive first.", "normal_query": "Show the Total disposal cost by disposal method. Sort the results by the cost in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT (fm.cost_breakdown->'disposal'->>'disposal_method') AS disposal_method,\n       SUM((fm.cost_breakdown->'disposal'->>'disposal_cost')::numeric) AS total_disp_cost\nFROM financial_management fm\nGROUP BY disposal_method\nORDER BY total_disp_cost DESC;"], "external_knowledge": [13], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "reverse_logistics_12", "selected_database": "reverse_logistics", "query": "Which 5 returns cost most to fix? Show me from priciest to least pricey.", "normal_query": "Display the top 5 returns ranked by repair estimate, ordered from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT fm.casetag AS casenum,\n       (fm.cost_breakdown->'repair_costs'->>'repair_estimate')::numeric AS repair_estimate\nFROM financial_management fm\nORDER BY repair_estimate DESC\nLIMIT 5;"], "external_knowledge": [14], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "reverse_logistics_13", "selected_database": "reverse_logistics", "query": "How much do we typically recover through each return method? Give me the average amounts rounded to pennies.", "normal_query": "List the average recovery value per return channel, rounded numeric outputs to 2 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT r.return_channel,\n       ROUND(AVG((fm.cost_breakdown->'valuation'->>'recovery_value')::numeric),2) AS avg_recovery_value\nFROM returns r\nJOIN financial_management fm ON fm.casetag = r.casenum\nGROUP BY r.return_channel;"], "external_knowledge": [15, 20], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_M_4", "selected_database": "reverse_logistics", "query": "Find every case marked as finished but never officially closed, mark it closed with today's date, and tell me which case numbers were touched.", "normal_query": "Close every case whose action state is 'Completed' and whose close state is still NULL; set its close state to 'Closed', update the close date to today, and return its case number.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE case_management\nSET closestate = 'Closed',\n    closedate = CURRENT_DATE\nWHERE actionstate = 'Completed' AND closestate IS NULL\nRETURNING casetie;"], "external_knowledge": [], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute('''\n        SELECT COUNT(*) FROM case_management\n        WHERE actionstate = 'Completed' AND closestate = 'Closed'\n    ''')\n    result = cursor.fetchone()[0]\n    assert isinstance(result, int), 'Result should be integer'\n    assert result >= 0, 'Count should be non-negative'\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_14", "selected_database": "reverse_logistics", "query": "Of all returns, tell me the % flagged with 'high' fraud risk (rounded to hundredths) - show only the percentage.", "normal_query": "Figure out the percentage of returns flagged 'high' in fraud risk levels. Round numeric outputs to 2 decimal places and only show the first one.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT ROUND(\n100.0 * COUNT(*) FILTER (WHERE (r.return_details->'fraud'->>'risk_level')='High') / COUNT(*)\n,2) AS pct_high_fraud\nFROM returns r;"], "external_knowledge": [17, 21], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_15", "selected_database": "reverse_logistics", "query": "Give me the kilograms of carbon dioxide released in different disposal processing activities. Round numeric outputs to 2 decimal places and show from highest to lowest.", "normal_query": "List the Average carbon footprint by disposal method. Round numeric outputs to 2 decimal places. Show the results from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT (fm.cost_breakdown->'disposal'->>'disposal_method') AS disposal_method,\n       ROUND(AVG((fm.cost_breakdown->'sustainability'->>'carbon_footprint')::numeric),2) AS avg_carbon_fp\nFROM financial_management fm\nGROUP BY disposal_method\nORDER BY avg_carbon_fp DESC;"], "external_knowledge": [19, 23], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "reverse_logistics_16", "selected_database": "reverse_logistics", "query": "Show me how many returns we have for each warranty status and return channel combination, sorted by the count from highest to lowest.", "normal_query": "List the Count Returns per Warranty status (CNT) and return channel. Sort the results by CNT in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT r.return_channel,\n       (r.return_details->'authorization'->>'warranty_status') AS warranty_status,\n       COUNT(*) AS cnt\nFROM returns r\nGROUP BY r.return_channel, warranty_status\nORDER BY cnt DESC;"], "external_knowledge": [27, 20], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "reverse_logistics_M_5", "selected_database": "reverse_logistics", "query": "Find all electronics worth over $700, mark their subcategories as 'High Value', and tell me how many got tagged.", "normal_query": "Append the tag 'High Value' to the subcategory of electronics products with unit value greater than 700. Return the number of updated rows.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH upd AS (UPDATE products SET subcat=CASE WHEN COALESCE(subcat,'')='' THEN 'High Value' ELSE subcat||' | High Value' END WHERE LOWER(itemcategory)='electronics' AND unit_value>700 AND(subcat IS NULL OR subcat NOT ILIKE '%High Value%') RETURNING 1) SELECT COUNT(*) AS rows_updated FROM upd;"], "external_knowledge": [], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute('''\n        SELECT COUNT(*) FROM products\n        WHERE itemcategory = 'Electronics'\n          AND unit_value > 700\n          AND subcat LIKE '%High Value%'\n    ''')\n    result = cursor.fetchone()[0]\n    assert isinstance(result, int), 'Result must be an integer'\n    assert result >= 0, 'Count must be non-negative'\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_17", "selected_database": "reverse_logistics", "query": "Show me how much money we actually get back (after costs) for each type of refund, along with both the recovered amounts and what we spent processing them.", "normal_query": "Calculate the Net return profit impact per refund method. Display Recovery Value and Total Return Cost in the result.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT fm.cost_breakdown->'refund'->>'method' AS refund_method,ROUND(SUM((fm.cost_breakdown->'valuation'->>'recovery_value')::numeric),2) AS total_recovery_value,ROUND(SUM(COALESCE((r.return_details->'shipping'->>'fee')::numeric,0)+COALESCE((fm.cost_breakdown->'fees'->>'restocking_fee')::numeric,0)+COALESCE((fm.cost_breakdown->'fees'->>'repackaging_cost')::numeric,0)+COALESCE((fm.cost_breakdown->'fees'->>'relabeling_cost')::numeric,0)+COALESCE((fm.cost_breakdown->'fees'->>'qa_fee')::numeric,0)+COALESCE((fm.cost_breakdown->'repair_costs'->>'repair_estimate')::numeric,0)+COALESCE((fm.cost_breakdown->'repair_costs'->>'parts_fee')::numeric,0)+COALESCE((fm.cost_breakdown->'repair_costs'->>'labor_fee')::numeric,0)+COALESCE((fm.cost_breakdown->'disposal'->>'disposal_cost')::numeric,0)),2) AS total_return_cost,ROUND(SUM((fm.cost_breakdown->'valuation'->>'recovery_value')::numeric)-SUM(COALESCE((r.return_details->'shipping'->>'fee')::numeric,0)+COALESCE((fm.cost_breakdown->'fees'->>'restocking_fee')::numeric,0)+COALESCE((fm.cost_breakdown->'fees'->>'repackaging_cost')::numeric,0)+COALESCE((fm.cost_breakdown->'fees'->>'relabeling_cost')::numeric,0)+COALESCE((fm.cost_breakdown->'fees'->>'qa_fee')::numeric,0)+COALESCE((fm.cost_breakdown->'repair_costs'->>'repair_estimate')::numeric,0)+COALESCE((fm.cost_breakdown->'repair_costs'->>'parts_fee')::numeric,0)+COALESCE((fm.cost_breakdown->'repair_costs'->>'labor_fee')::numeric,0)+COALESCE((fm.cost_breakdown->'disposal'->>'disposal_cost')::numeric,0)),2) AS rpi FROM returns r JOIN financial_management fm ON fm.casetag=r.casenum GROUP BY refund_method;"], "external_knowledge": [24, 15], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_18", "selected_database": "reverse_logistics", "query": "How dirty and pricey is it to scrap items in different states? how me the average values, rounded to two decimals.", "normal_query": "Output the Average carbon footprint and disposal cost for each item condition state. Round numeric outputs to 2 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT qa.assessment_summary->'condition'->>'item_condition' AS item_condition,ROUND(AVG((fm.cost_breakdown->'sustainability'->>'carbon_footprint')::numeric),2) AS avg_carbon_footprint,ROUND(AVG((fm.cost_breakdown->'disposal'->>'disposal_cost')::numeric),2) AS avg_disposal_cost FROM quality_assessment AS qa JOIN financial_management AS fm ON fm.casetag=qa.inspectref GROUP BY item_condition;"], "external_knowledge": [22, 19, 23], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_M_7", "selected_database": "reverse_logistics", "query": "Find all unsatisfied customers (ratings 2 or below), flag their cases for follow-up, and give me the case numbers with their new status.", "normal_query": "Mark needsfollowup as 'Yes' for cases whose satisfaction score is less than or equal to 2. Return casetie and the new needsfollowup value.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE case_management\nSET needsfollowup = 'Yes'\nWHERE satisfscore <= 2\nRETURNING casetie, needsfollowup;"], "external_knowledge": [26], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute('''\n        SELECT COUNT(*) FROM case_management\n        WHERE satisfscore <= 2 AND needsfollowup = 'Yes'\n    ''')\n    result = cursor.fetchone()[0]\n    assert isinstance(result, int), 'Result must be an integer'\n    assert result >= 0, 'Should not be negative'\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_19", "selected_database": "reverse_logistics", "query": "Show me how many non-compliant items we have divided by disposal method, along with their average carbon footprint (rounded to 2 decimal places). Sort by the highest count of non-compliant items first.", "normal_query": "Display the count of items whose Regulatory Compliance Status is non-compliant, grouped by disposal method, with the average carbon footprint rounded to 2 decimals. Sort the list by the non-compliant count in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT (fm.cost_breakdown->'disposal'->>'disposal_method') AS disposal_method,\n COUNT(*) AS non_compliant_cnt,\n ROUND(AVG((fm.cost_breakdown->'sustainability'->>'carbon_footprint')::numeric),2) AS avg_fp\nFROM financial_management fm\nJOIN returns r ON r.casenum = fm.casetag\nJOIN products p ON p.itemcode = r.itemlink\nWHERE p.product_traceability->'compliance'->>'regulatory_compliance' = 'Non-compliant'\nGROUP BY disposal_method\nORDER BY non_compliant_cnt DESC;"], "external_knowledge": [23, 29, 19], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "reverse_logistics_M_8", "selected_database": "reverse_logistics", "query": "Find all supposedly recycled items with heavy carbon footprints (over 50kg), mark them as hazardous waste instead, and tell me how many needed reclassification.", "normal_query": "Change disposal method from 'Recycle' to 'Hazardous Waste' for records with carbon footprint greater than 50 kilograms. Return the number of affected rows.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH upd AS (\n    UPDATE financial_management\n    SET cost_breakdown = jsonb_set(\n        cost_breakdown,\n        '{disposal,disposal_method}',\n        '\"Hazardous Waste\"'\n    )\n    WHERE (cost_breakdown->'disposal'->>'disposal_method') = 'Recycle'\n      AND (cost_breakdown->'sustainability'->>'carbon_footprint')::numeric > 50\n    RETURNING 1\n)\nSELECT COUNT(*) AS rows_updated FROM upd;"], "external_knowledge": [23, 19], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute('''\n        SELECT COUNT(*) FROM financial_management\n        WHERE (cost_breakdown->'disposal'->>'disposal_method') = 'Hazardous Waste'\n          AND (cost_breakdown->'sustainability'->>'carbon_footprint')::numeric > 50\n    ''')\n    result = cursor.fetchone()[0]\n    assert isinstance(result, int), 'Expected integer count'\n    assert result >= 0, 'Count cannot be negative'\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_20", "selected_database": "reverse_logistics", "query": "When fraud risk is high and it comes back by courier, how off-normal is the shipping cost? Please round the answer to two decimals.", "normal_query": "When the fraud-risk rating is high and the item comes back by courier, how far above or below average is the shipping cost? Please round the answer to two decimals.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH courier_high AS (SELECT r.casenum,(r.return_details->'shipping'->>'fee')::numeric AS shipfee,n.avg_channel_fee FROM returns r JOIN (SELECT return_channel,AVG((return_details->'shipping'->>'fee')::numeric) AS avg_channel_fee FROM returns GROUP BY return_channel) n USING(return_channel) WHERE r.return_channel='Courier' AND(r.return_details->'fraud'->>'risk_level')='High') SELECT ROUND(AVG(shipfee/NULLIF(avg_channel_fee,0)),2) AS avg_rcci_high_courier FROM courier_high;"], "external_knowledge": [21, 20, 10, 9], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "reverse_logistics_M_10", "selected_database": "reverse_logistics", "query": "Find all automatic approvals for express processing, bump them up to manager approval, and tell me which locations had changes with both the old and new levels.", "normal_query": "Upgrade approval level from 'Automatic' to 'Manager' for processing priority 'Express'. Return loccode, old approval level, and new approval level.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH affected AS (\n    SELECT loccode, apprlevel\n    FROM return_processing\n    WHERE procprio = 'Express' AND apprlevel = 'Automatic'\n)\nUPDATE return_processing rp\nSET apprlevel = 'Manager'\nFROM affected a\nWHERE rp.loccode = a.loccode\nRETURNING rp.loccode, a.apprlevel AS old_level, rp.apprlevel AS new_level;"], "external_knowledge": [25], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute('''\n        SELECT COUNT(*) FROM return_processing\n        WHERE procprio = 'Express' AND apprlevel = 'Manager'\n    ''')\n    result = cursor.fetchone()[0]\n    assert isinstance(result, int), 'Expected integer count'\n    assert result >= 0, 'Count cannot be negative'\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "robot_fault_prediction_1", "selected_database": "robot_fault_prediction", "query": "Let's get a list of all robots that need urgent maintenance and are running too hot, showing their ID, model, urgency score, and hottest joint temp with worst cases first.", "normal_query": "Which robots currently meet the dual criteria of a high Predictive Maintenance Urgency score and a concurrent Thermal Anomaly? Show robot ID, model, maintenance urgency score, and maximum joint temperature. Sort by most critical cases first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH JointTemps AS (\n    SELECT \n        jcondoperref,\n        (jc.joint_health->'J1'->>'temperature_C')::numeric AS j1_temp,\n        (jc.joint_health->'J2'->>'temperature_C')::numeric AS j2_temp,\n        (jc.joint_health->'J3'->>'temperature_C')::numeric AS j3_temp,\n        (jc.joint_health->'J4'->>'temperature_C')::numeric AS j4_temp,\n        (jc.joint_health->'J5'->>'temperature_C')::numeric AS j5_temp,\n        (jc.joint_health->'J6'->>'temperature_C')::numeric AS j6_temp\n    FROM joint_condition jc\n), AggregatedTemps AS (\n    SELECT \n        jcondoperref,\n        GREATEST(j1_temp, j2_temp, j3_temp, j4_temp, j5_temp, j6_temp) AS max_temp,\n        (j1_temp + j2_temp + j3_temp + j4_temp + j5_temp + j6_temp) / 6.0 AS avg_temp,\n        GREATEST(j1_temp, j2_temp, j3_temp, j4_temp, j5_temp, j6_temp) - LEAST(j1_temp, j2_temp, j3_temp, j4_temp, j5_temp, j6_temp) AS temp_diff\n    FROM JointTemps\n)\nSELECT\n    rr.botcode AS robot_id,\n    rd.modelseriesval AS model,\n    (1000 / (mf.rulhours + 1)) + (mf.faultpredscore * 10) AS pmu_score,\n    at.max_temp AS max_joint_temp\nFROM robot_record AS rr\nJOIN robot_details AS rd ON rr.botcode = rd.botdetreg\nJOIN operation AS o ON rr.botcode = o.operrecref\nJOIN maintenance_and_fault AS mf ON o.operreg = mf.upkeepoperation\nJOIN AggregatedTemps AS at ON o.operreg = at.jcondoperref\nWHERE \n    ((1000 / (mf.rulhours + 1)) + (mf.faultpredscore * 10)) > 50\n    AND (at.avg_temp > 75 OR at.temp_diff > 15)\nORDER BY pmu_score DESC, max_joint_temp DESC;"], "external_knowledge": [17, 43, 4, 9, 48], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "robot_fault_prediction_3", "selected_database": "robot_fault_prediction", "query": "Find robots doing precision work that aren't accurate enough, showing their ID, job, error amount, and if they need calibration.", "normal_query": "Identify robots used in Precision-Critical Applications that are showing signs of Precision Performance Degradation. Return robot ID, its application type, relative positional error, and current calibration state.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n rr.botcode AS robot_id,\n o.apptypeval AS application_type,\n (a.poserrmmval / rd.reachmmval) AS relative_positional_error,\n p.calibstateval AS calibration_state\nFROM robot_record rr\nJOIN robot_details rd ON rr.botcode = rd.botdetreg\nJOIN operation o ON rr.botcode = o.operrecref\nJOIN actuation_data a ON o.operreg = a.actoperref\nJOIN performance_and_safety p ON a.actreg = p.effectivenessactuation\nWHERE \n LOWER(o.apptypeval) IN ('assembly', 'electronics-handling', 'inspection', 'dispensing', 'laser cutting')\n AND (a.poserrmmval / rd.reachmmval) > 0.0001\nORDER BY relative_positional_error DESC;"], "external_knowledge": [8, 41, 50], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "robot_fault_prediction_4", "selected_database": "robot_fault_prediction", "query": "Show me robots working too hard right now, listing their model, how much payload capacity they're using, and their cycle speed. Make sure the most overloaded ones are at the top of the list.", "normal_query": "List all robots currently operating under an Intensive Workload. Include the robot's model, its payload utilization ratio, and its throughput rate, sorted with the most utilized and fastest robots appearing first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH WorkloadData AS (\n SELECT \n    rr.botcode,\n    rd.modelseriesval AS model,\n    o.apptypeval,\n    o.operreg,\n    (a.payloadwval / rd.payloadcapkg) AS payload_utilization_ratio,\n    (60 / o.cycletimesecval) AS throughput_rate,\n    NTILE(5) OVER(PARTITION BY LOWER(o.apptypeval) ORDER BY (60 / o.cycletimesecval) DESC) as throughput_quintile\n FROM robot_record rr\n JOIN robot_details rd ON rr.botcode = rd.botdetreg\n JOIN operation o ON rr.botcode = o.operrecref\n JOIN actuation_data a ON o.operreg = a.actoperref\n WHERE rd.payloadcapkg IS NOT NULL AND rd.payloadcapkg > 0 AND o.cycletimesecval IS NOT NULL AND o.cycletimesecval > 0\n)\nSELECT\n model,\n payload_utilization_ratio,\n throughput_rate AS throughput_rate_cycles_per_min\nFROM WorkloadData\nWHERE payload_utilization_ratio > 0.8 AND throughput_quintile = 1\nORDER BY payload_utilization_ratio DESC, throughput_rate_cycles_per_min DESC;"], "external_knowledge": [1, 12, 58], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "robot_fault_prediction_5", "selected_database": "robot_fault_prediction", "query": "I need to plan next week's emergency maintenance schedule. Find all robots that are likely to fail sometime next week and which the system also considers a high risk to break. Show their ID, how likely they'll break, and how many days they have left.", "normal_query": "For next week's maintenance planning, identify robots that are projected to fail within the next 7 days and also have a 'High' Fault Prediction Score Tier. For each, show the robot ID, its fault prediction score, and its remaining useful life in days.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n rr.botcode AS robot_id,\n mf.faultpredscore AS fault_probability,\n (mf.rulhours / 24) AS rul_days\nFROM robot_record rr\nJOIN operation o ON rr.botcode = o.operrecref\nJOIN maintenance_and_fault mf ON o.operreg = mf.upkeepoperation\nWHERE \n mf.faultpredscore > 0.7 AND (mf.rulhours / 24) < 7\nORDER BY rul_days ASC, fault_probability DESC;"], "external_knowledge": [10, 24], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "robot_fault_prediction_6", "selected_database": "robot_fault_prediction", "query": "Let's see which manufacturers are struggling the most with their robots' health. For manufacturers with at least two robots getting worse, show me how many are affected and what their average decline rate is. Please show the manufacturer names in lowercase.", "normal_query": "I require an analysis of health degradation trends, grouped by manufacturer with more than one degrading robot, show the total count of such robots and their average health degradation rate (with manufacturer names in lowercase).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH RobotDegradation AS (\n    SELECT \n        rr.botcode, \n        rd.mfgnameval, \n        (1.0 - p.conditionindexval) / NULLIF((CURRENT_DATE - rd.instdateval), 0) AS health_degradation_rate\n    FROM robot_record AS rr\n    JOIN robot_details AS rd ON rr.botcode = rd.botdetreg\n    JOIN performance_and_safety AS p ON rr.botcode = p.effectivenessrobot\n    WHERE p.conditionindexval IS NOT NULL AND rd.instdateval IS NOT NULL\n)\nSELECT \n    LOWER(mfgnameval) AS manufacturer, \n    COUNT(botcode) AS degrading_robot_count, \n    AVG(health_degradation_rate) AS avg_degradation_rate\nFROM RobotDegradation\nWHERE health_degradation_rate > 0\nGROUP BY LOWER(mfgnameval)\nHAVING COUNT(botcode) > 1;"], "external_knowledge": [0, 19, 59], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "robot_fault_prediction_7", "selected_database": "robot_fault_prediction", "query": "Let's find robots whose controllers are overworked compared to their peers by flagging any with 'Anomalous Controller Stress'. I need to see the robot's ID, its model (in lowercase), its stress score, and what the average for its model was.", "normal_query": "Identify all robots experiencing 'Anomalous Controller Stress'. For each, display its robot ID, its model (in lowercase), its specific stress score, and the calculated average stress for its model.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ModelAvgStress AS (\n    SELECT \n        LOWER(rd.modelseriesval) AS modelseriesval_lower, \n        AVG((sc.overseerloadvalue + sc.memuseval) / 2) AS avg_stress_score\n    FROM robot_details rd\n    JOIN operation o ON rd.botdetreg = o.operrecref\n    JOIN actuation_data a ON o.operreg = a.actoperref\n    JOIN system_controller sc ON a.actreg = sc.systemoverseeractuation\n    WHERE sc.overseerloadvalue IS NOT NULL AND sc.memuseval IS NOT NULL\n    GROUP BY LOWER(rd.modelseriesval)\n)\nSELECT \n    rr.botcode AS robot_id, \n    LOWER(rd.modelseriesval) AS model, \n    (sc.overseerloadvalue + sc.memuseval) / 2 AS controller_stress_score, \n    mas.avg_stress_score\nFROM robot_record rr\nJOIN robot_details rd ON rr.botcode = rd.botdetreg\nJOIN operation o ON rr.botcode = o.operrecref\nJOIN actuation_data a ON o.operreg = a.actoperref\nJOIN system_controller sc ON a.actreg = sc.systemoverseeractuation\nJOIN ModelAvgStress mas ON LOWER(rd.modelseriesval) = mas.modelseriesval_lower\nWHERE (sc.overseerloadvalue + sc.memuseval) / 2 > (mas.avg_stress_score * 1.2)\nORDER BY controller_stress_score DESC;"], "external_knowledge": [6, 61], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "robot_fault_prediction_8", "selected_database": "robot_fault_prediction", "query": "Let's find robots that might be breaking down because their controllers are overworked. Show me any robot where its controller stress is over 20% more than average for its model, and its mechanical wear score is getting into the medium or high range. I need to see the robot's ID, who made it, the model (in lowercase), and both the stress and wear scores. Please group them by the maker and list the most stressed ones first.", "normal_query": "Identify robots exhibiting signs of mechanically significant wear potentially induced by high controller stress. A robot qualifies if it meets two criteria simultaneously: 1. Its 'Controller Stress Score' is at least 20% higher than the average score for all robots of the same model series. 2. Its 'Mechanical Wear Score' is classified as either 'High' or 'Medium' severity. For each qualifying robot, return its ID, manufacturer, model (in lowercase), its specific controller stress score, and its mechanical wear score. Sort the results by manufacturer, then by the controller stress score in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ModelAvgStress AS (\n    SELECT \n        LOWER(rd.modelseriesval) AS modelseriesval_lower, \n        AVG((sc.overseerloadvalue + sc.memuseval) / 2) AS avg_stress_score\n    FROM robot_details rd\n    JOIN operation o ON rd.botdetreg = o.operrecref\n    JOIN actuation_data a ON o.operreg = a.actoperref\n    JOIN system_controller sc ON a.actreg = sc.systemoverseeractuation\n    WHERE sc.overseerloadvalue IS NOT NULL AND sc.memuseval IS NOT NULL\n    GROUP BY LOWER(rd.modelseriesval)\n), RobotWearAndStress AS (\n    SELECT \n        rr.botcode, \n        rd.mfgnameval, \n        rd.modelseriesval, \n        (sc.overseerloadvalue + sc.memuseval) / 2 AS controller_stress_score, \n        (0.6 * AVG((j.value->>'vibration_mmps')::numeric)) + (0.4 * AVG((j.value->>'backlash_deg')::numeric)) AS mechanical_wear_score\n    FROM robot_record rr\n    JOIN robot_details rd ON rr.botcode = rd.botdetreg\n    JOIN operation o ON rr.botcode = o.operrecref\n    JOIN actuation_data a ON o.operreg = a.actoperref\n    JOIN system_controller sc ON a.actreg = sc.systemoverseeractuation\n    JOIN joint_condition jc ON o.operreg = jc.jcondoperref, \n         jsonb_each(jc.joint_health) j\n    WHERE sc.overseerloadvalue IS NOT NULL AND sc.memuseval IS NOT NULL\n    GROUP BY rr.botcode, rd.mfgnameval, rd.modelseriesval, controller_stress_score\n)\nSELECT \n    rws.botcode AS robot_id, \n    rws.mfgnameval AS manufacturer, \n    LOWER(rws.modelseriesval) AS model, \n    rws.controller_stress_score, \n    rws.mechanical_wear_score\nFROM RobotWearAndStress rws\nJOIN ModelAvgStress mas ON LOWER(rws.modelseriesval) = mas.modelseriesval_lower\nWHERE \n    rws.controller_stress_score > (mas.avg_stress_score * 1.2)\n    AND rws.mechanical_wear_score >= 0.5\nORDER BY \n    rws.mfgnameval, \n    rws.controller_stress_score DESC;"], "external_knowledge": [6, 18, 5, 15, 44, 47], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "robot_fault_prediction_10", "selected_database": "robot_fault_prediction", "query": "I'm trying to see if our heavy-duty jobs are really taking a toll on the robots. Can you compare the failure rates for two groups? First, all the robots doing high-wear stuff like welding and grinding. Second, everyone else. For each group, just tell me what percentage of them are considered a high reliability risk.", "normal_query": "For a comparative reliability analysis, calculate and contrast the percentage of robots classified with 'High Reliability Risk' between two groups: those assigned to 'High-Wear Applications' and those in all other application types. The final result should display the application group and its corresponding high-risk percentage.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH RobotReliability AS (\n    SELECT \n        o.apptypeval,\n        CASE \n            WHEN LOWER(o.apptypeval) IN ('welding', 'grinding', 'deburring', 'palletizing', 'machine tending') THEN 'High-Wear Applications'\n            ELSE 'Other Applications'\n        END AS application_group,\n        CASE \n            WHEN (o.totopshrval / NULLIF(ps.emergencystopcount, 0)) < 500 THEN 1\n            ELSE 0\n        END AS is_high_risk\n    FROM operation o\n    JOIN performance_and_safety ps ON o.operrecref = ps.effectivenessrobot\n    WHERE o.apptypeval IS NOT NULL AND o.totopshrval IS NOT NULL AND ps.emergencystopcount IS NOT NULL\n)\nSELECT \n    application_group,\n    ROUND(AVG(is_high_risk) * 100, 2) AS percentage_high_risk\nFROM RobotReliability\nGROUP BY application_group;"], "external_knowledge": [40, 46, 2], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "robot_fault_prediction_11", "selected_database": "robot_fault_prediction", "query": "I need a list of our underperforming robots. Can you find all the bots that are running slower than 180 cycles an hour, and then give me just the top 5 slowest from that list? Show me their ID, maker, model, and their actual cycles per hour score, rounded to two decimals. List the absolute slowest one at the top.", "normal_query": "For a fleet-wide performance review, identify the 5 robots with a production throughput rate lower than 180 cycles per hour. For each of these robots, display its ID, manufacturer, model, and the calculated throughput in cycles per hour, rounded to two decimal places. Sort by the lowest throughput first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    rr.botcode AS robot_id,\n    rd.mfgnameval AS manufacturer,\n    rd.modelseriesval AS model,\n    ROUND((3600 / NULLIF(o.cycletimesecval, 0))::numeric, 2) AS cycles_per_hour\nFROM robot_record rr\nJOIN robot_details rd ON rr.botcode = rd.botdetreg\nJOIN operation o ON rr.botcode = o.operrecref\nWHERE \n    o.cycletimesecval IS NOT NULL \n    AND (3600 / NULLIF(o.cycletimesecval, 0)) < 180\nORDER BY cycles_per_hour ASC\nLIMIT 5;"], "external_knowledge": [63], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "robot_fault_prediction_12", "selected_database": "robot_fault_prediction", "query": "I'm working on our fleet health dashboard and need a key baseline number. What's the average vibration level if you look at all joints on all our robots? Just give me that one number, rounded to two decimal spots, please.", "normal_query": "To establish a fleet-wide health baseline, calculate the average joint vibration (`vibration_mmps`) across all joints of all robots in the entire fleet. The final result should be a single value, rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n    ROUND(\n        AVG((v->>'vibration_mmps')::numeric)::numeric,\n        2\n    )\nFROM\n    joint_condition,\n    LATERAL jsonb_each(joint_health) AS j(k, v)\nWHERE\n    v->>'vibration_mmps' IS NOT NULL;"], "external_knowledge": [5], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "robot_fault_prediction_M_1", "selected_database": "robot_fault_prediction", "query": "We've finished the safety check for robot RB5150. Can you please zero out all its safety violation counters and also mark its calibration status as pending so we know it needs to be rechecked?", "normal_query": "Following a safety review for robot 'RB5150', reset all of its safety violation counters (zone, speed, emergency stops, and collisions) to zero and set its calibration state to 'Pending' to require re-verification.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE performance_and_safety SET zoneviolnum = 0, emergencystopcount = 0, collisioncount = 0, speedviolnum = 0, calibstateval = 'N' WHERE effectivenessrobot = 'RB5150';"], "external_knowledge": [22], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    import psycopg2\n    cursor = conn.cursor()\n    try:\n        cursor.execute(\"BEGIN;\")\n        # 1. Setup: Create a robot with non-zero safety stats and a 'Success' calibration state\n        botcode = 'RB5150'\n        operreg = 'OP_R11'\n        actreg = 'AC_R11'\n        cursor.execute(f\"INSERT INTO robot_record (botcode, rects) VALUES ('{botcode}', NOW()) ON CONFLICT (botcode) DO NOTHING;\")\n        cursor.execute(f\"INSERT INTO robot_details (botdetreg) VALUES ('{botcode}') ON CONFLICT (botdetreg) DO NOTHING;\")\n        cursor.execute(f\"INSERT INTO operation (operreg, operrecref) VALUES ('{operreg}', '{botcode}') ON CONFLICT (operreg) DO NOTHING;\")\n        cursor.execute(f\"INSERT INTO actuation_data (actreg, actoperref, actrecref) VALUES ('{actreg}', '{operreg}', '{botcode}') ON CONFLICT (actreg) DO NOTHING;\")\n        cursor.execute(f\"INSERT INTO performance_and_safety (effectivenessactuation, effectivenessrobot, zoneviolnum, emergencystopcount, collisioncount, speedviolnum, calibstateval) VALUES ('{actreg}', '{botcode}', 10, 5, 2, 8, 'Y') ON CONFLICT (effectivenessactuation) DO UPDATE SET zoneviolnum = 10, emergencystopcount = 5, collisioncount = 2, speedviolnum = 8, calibstateval = 'Y';\")\n\n        # 2. Execution\n        for sql in sol_sqls:\n            cursor.execute(sql)\n\n        # 3. Verification\n        cursor.execute(f\"SELECT zoneviolnum, emergencystopcount, collisioncount, speedviolnum, calibstateval FROM performance_and_safety WHERE effectivenessrobot = '{botcode}';\")\n        result = cursor.fetchone()\n        expected_safety_counters = (0, 0, 0, 0)\n        expected_calib_state = 'N'\n        assert (result[0], result[1], result[2], result[3]) == expected_safety_counters, f\"Expected safety counters to be {expected_safety_counters}, but got {(result[0], result[1], result[2], result[3])}\"\n        assert result[4].strip() == expected_calib_state, f\"Expected calibration state to be '{expected_calib_state}', but got '{result[4]}'.\"\n\n    finally:\n        cursor.execute(\"ROLLBACK;\")"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "robot_fault_prediction_M_2", "selected_database": "robot_fault_prediction", "query": "We're switching robot RB0042 over from welding to material handling. Can you update its job in the system and put it in manual mode so we can set it up?", "normal_query": "Robot 'RB0042' is being repurposed from 'Welding' to 'Material Handling'. Update its application type accordingly and set its operational mode to 'Manual' for reconfiguration.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE operation SET apptypeval = 'Material Handling', opermodeval = 'MANU' WHERE operrecref = 'RB0042';"], "external_knowledge": [39, 40, 23], "test_cases": ["import psycopg2\n\ndef test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    cursor = conn.cursor()\n    try:\n        cursor.execute(\"BEGIN;\")\n        # 1. Setup: Create a robot with the old application type and mode\n        cursor.execute(\"INSERT INTO robot_record (botcode, rects) VALUES ('RB0042', NOW()) ON CONFLICT (botcode) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO operation (operreg, operrecref, apptypeval, opermodeval) VALUES ('OP_R12', 'RB0042', 'Welding', 'AUTO') ON CONFLICT (operreg) DO UPDATE SET apptypeval = 'Welding', opermodeval = 'AUTO';\")\n\n        # 2. Execution: Run the provided SQL script\n        for sql in sol_sqls:\n            cursor.execute(sql)\n\n        # 3. Verification: Check if the application type and mode have been updated\n        cursor.execute(\"SELECT apptypeval, opermodeval FROM operation WHERE operrecref = 'RB0042';\")\n        result = cursor.fetchone()\n        expected_app = 'Material Handling'\n        expected_mode = 'MANU'\n        assert result[0].strip() == expected_app, f\"Expected application type to be '{expected_app}', but got '{result[0]}'.\"\n        assert result[1].strip() == expected_mode, f\"Expected mode to be '{expected_mode}', but got '{result[1]}'.\"\n\n    finally:\n        cursor.execute(\"ROLLBACK;\")"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "robot_fault_prediction_M_3", "selected_database": "robot_fault_prediction", "query": "I need to adjust the maintenance schedule for some of our heavily used robots. For any robot that does welding or grinding and is due for service in the next two weeks, can you push their maintenance date out by 7 days and set the estimated cost for the job to 1200 dollars?", "normal_query": "For all robots assigned to high-wear applications such as 'Welding' or 'Grinding' whose next maintenance is due in less than 14 days, extend the maintenance interval by an additional 7 days and standardize their estimated upkeep cost to $1200.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE maintenance_and_fault SET upkeepduedays = upkeepduedays + 7, upkeepcostest = '$1200.00' FROM operation WHERE maintenance_and_fault.upkeepoperation = operation.operreg AND LOWER(operation.apptypeval) IN ('welding', 'grinding') AND maintenance_and_fault.upkeepduedays < 14;"], "external_knowledge": [40], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    import psycopg2\n    cursor = conn.cursor()\n    try:\n        cursor.execute(\"BEGIN;\")\n        # 1. Setup: Create two robots matching the criteria and one that does not\n        # Robot 1: Welding, 10 days due\n        cursor.execute(\"INSERT INTO robot_record (botcode, rects) VALUES ('RB_WELD_1', NOW()) ON CONFLICT (botcode) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO operation (operreg, operrecref, apptypeval) VALUES ('OP_W1', 'RB_WELD_1', 'Welding') ON CONFLICT (operreg) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO actuation_data (actreg, actoperref) VALUES ('AC_W1', 'OP_W1') ON CONFLICT (actreg) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO maintenance_and_fault (upkeepactuation, upkeepoperation, upkeepduedays, upkeepcostest) VALUES ('AC_W1', 'OP_W1', 10, '$800.00') ON CONFLICT (upkeepactuation) DO UPDATE SET upkeepduedays=10, upkeepcostest='$800.00';\")\n        \n        # Robot 2: Grinding, 5 days due\n        cursor.execute(\"INSERT INTO robot_record (botcode, rects) VALUES ('RB_GRIND_1', NOW()) ON CONFLICT (botcode) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO operation (operreg, operrecref, apptypeval) VALUES ('OP_G1', 'RB_GRIND_1', 'Grinding') ON CONFLICT (operreg) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO actuation_data (actreg, actoperref) VALUES ('AC_G1', 'OP_G1') ON CONFLICT (actreg) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO maintenance_and_fault (upkeepactuation, upkeepoperation, upkeepduedays, upkeepcostest) VALUES ('AC_G1', 'OP_G1', 5, '$950.00') ON CONFLICT (upkeepactuation) DO UPDATE SET upkeepduedays=5, upkeepcostest='$950.00';\")\n\n        # Robot 3: Painting, 10 days due (should not be updated)\n        cursor.execute(\"INSERT INTO robot_record (botcode, rects) VALUES ('RB_PAINT_1', NOW()) ON CONFLICT (botcode) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO operation (operreg, operrecref, apptypeval) VALUES ('OP_P1', 'RB_PAINT_1', 'Painting') ON CONFLICT (operreg) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO actuation_data (actreg, actoperref) VALUES ('AC_P1', 'OP_P1') ON CONFLICT (actreg) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO maintenance_and_fault (upkeepactuation, upkeepoperation, upkeepduedays, upkeepcostest) VALUES ('AC_P1', 'OP_P1', 10, '$500.00') ON CONFLICT (upkeepactuation) DO UPDATE SET upkeepduedays=10, upkeepcostest='$500.00';\")\n\n        # 2. Execution\n        for sql in sol_sqls:\n            cursor.execute(sql)\n\n        # 3. Verification\n        cursor.execute(\"SELECT upkeepduedays, upkeepcostest FROM maintenance_and_fault WHERE upkeepactuation = 'AC_W1';\")\n        weld_result = cursor.fetchone()\n        assert weld_result[0] == 17, f\"Welding robot upkeep days should be 17, but got {weld_result[0]}\"\n        assert weld_result[1] == '$1200.00', f\"Welding robot cost should be $1200.00, but got {weld_result[1]}\"\n\n        cursor.execute(\"SELECT upkeepduedays, upkeepcostest FROM maintenance_and_fault WHERE upkeepactuation = 'AC_G1';\")\n        grind_result = cursor.fetchone()\n        assert grind_result[0] == 12, f\"Grinding robot upkeep days should be 12, but got {grind_result[0]}\"\n        assert grind_result[1] == '$1200.00', f\"Grinding robot cost should be $1200.00, but got {grind_result[1]}\"\n\n        cursor.execute(\"SELECT upkeepduedays, upkeepcostest FROM maintenance_and_fault WHERE upkeepactuation = 'AC_P1';\")\n        paint_result = cursor.fetchone()\n        assert paint_result[0] == 10, f\"Painting robot upkeep days should be unchanged (10), but got {paint_result[0]}\"\n        assert paint_result[1] == '$500.00', f\"Painting robot cost should be unchanged ($500.00), but got {paint_result[1]}\"\n\n    finally:\n        cursor.execute(\"ROLLBACK;\")"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "robot_fault_prediction_M_4", "selected_database": "robot_fault_prediction", "query": "It's time to retire the old robot, 'RB1055'. Can you please go through the decommissioning steps in the system? That means marking it as decommissioned, wiping its current program, and putting it into an emergency stop state so it can't be used.", "normal_query": "Initiate the full decommissioning procedure for the legacy robot 'RB1055'. This requires updating its operational mode to 'DECOMMISSIONED', clearing its currently loaded program, and setting its safety state to 'Emergency Stop' as a final precaution.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE operation SET opermodeval = 'DECOMMISSIONED', currprogval = NULL WHERE operrecref = 'RB1055';", "UPDATE performance_and_safety SET safetystateval = '✗ Emergency' WHERE effectivenessrobot = 'RB1055';"], "external_knowledge": [21, 23], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    import psycopg2\n    cursor = conn.cursor()\n    try:\n        cursor.execute(\"BEGIN;\")\n        # 1. Setup: Create a robot with an active state\n        cursor.execute(\"INSERT INTO robot_record (botcode, rects) VALUES ('RB1055', NOW()) ON CONFLICT (botcode) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO robot_details (botdetreg) VALUES ('RB1055') ON CONFLICT (botdetreg) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO operation (operreg, operrecref, opermodeval, currprogval) VALUES ('OP_R14', 'RB1055', 'AUTO', 'PRG_123') ON CONFLICT (operreg) DO UPDATE SET opermodeval = 'AUTO', currprogval = 'PRG_123';\")\n        cursor.execute(\"INSERT INTO actuation_data (actreg, actoperref, actrecref) VALUES ('AC_R14', 'OP_R14', 'RB1055') ON CONFLICT (actreg) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO performance_and_safety (effectivenessactuation, effectivenessrobot, safetystateval) VALUES ('AC_R14', 'RB1055', '✓ Normal') ON CONFLICT (effectivenessactuation) DO UPDATE SET safetystateval = '✓ Normal';\")\n\n        # 2. Execution: Run the provided SQL script\n        for sql in sol_sqls:\n            cursor.execute(sql)\n\n        # 3. Verification: Check if the mode is 'DECOMMISSIONED' and safety state is 'Emergency Stop'\n        cursor.execute(\"SELECT opermodeval FROM operation WHERE operrecref = 'RB1055';\")\n        op_result = cursor.fetchone()\n        cursor.execute(\"SELECT safetystateval FROM performance_and_safety WHERE effectivenessrobot = 'RB1055';\")\n        safety_result = cursor.fetchone()\n        \n        expected_mode = 'DECOMMISSIONED'\n        expected_safety = '✗ Emergency'\n        assert op_result[0].strip() == expected_mode, f'Expected mode to be \"{expected_mode}\", but got \"{op_result[0]}\".'\n        assert safety_result[0].strip() == expected_safety, f'Expected safety state to be \"{expected_safety}\", but got \"{safety_result[0]}\".'\n\n    finally:\n        cursor.execute(\"ROLLBACK;\")"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "robot_fault_prediction_M_5", "selected_database": "robot_fault_prediction", "query": "Hey, we fixed that 'SRVO-050' alarm on robot 'RB2073'. Can you clear that fault from the system? And since it's fixed, let's also reset the fault prediction score back to a really low number, like 0.05, and clear out what the system thought was going to fail.", "normal_query": "For robot 'RB2073', clear the active fault 'SRVO-050' as the underlying issue is resolved. Concurrently, reset the fault prediction metrics by setting the prediction score to a baseline low value of 0.05 and nullifying the fault type estimation.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE maintenance_and_fault\nSET \n    faultcodeval = NULL,\n    faultpredscore = 0.05,\n    faulttypeestimation = NULL\nFROM operation\nWHERE maintenance_and_fault.upkeepoperation = operation.operreg AND operation.operrecref = 'RB2073' AND maintenance_and_fault.faultcodeval = 'SRVO-050';"], "external_knowledge": [24], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    import psycopg2\n    cursor = conn.cursor()\n    try:\n        cursor.execute(\"BEGIN;\")\n        # 1. Setup: Create a robot with the specified fault\n        botcode = 'RB2073'\n        operreg = 'OP_R15'\n        actreg = 'AC_R15'\n        cursor.execute(f\"INSERT INTO robot_record (botcode, rects) VALUES ('{botcode}', NOW()) ON CONFLICT (botcode) DO NOTHING;\")\n        cursor.execute(f\"INSERT INTO robot_details (botdetreg) VALUES ('{botcode}') ON CONFLICT (botdetreg) DO NOTHING;\")\n        cursor.execute(f\"INSERT INTO operation (operreg, operrecref) VALUES ('{operreg}', '{botcode}') ON CONFLICT (operreg) DO NOTHING;\")\n        cursor.execute(f\"INSERT INTO actuation_data (actreg, actoperref, actrecref) VALUES ('{actreg}', '{operreg}', '{botcode}') ON CONFLICT (actreg) DO NOTHING;\")\n        cursor.execute(f\"INSERT INTO maintenance_and_fault (upkeepactuation, upkeepoperation, faultcodeval, faultpredscore, faulttypeestimation) VALUES ('{actreg}', '{operreg}', 'SRVO-050', 0.8, 'Motor') ON CONFLICT (upkeepactuation) DO UPDATE SET faultcodeval = 'SRVO-050', faultpredscore = 0.8, faulttypeestimation = 'Motor';\")\n\n        # 2. Execution: Run the provided SQL script\n        for sql in sol_sqls:\n            cursor.execute(sql)\n\n        # 3. Verification: Check that the fault is cleared and prediction score is reset\n        cursor.execute(f\"SELECT faultcodeval, faultpredscore, faulttypeestimation FROM maintenance_and_fault WHERE upkeepoperation = '{operreg}';\")\n        result = cursor.fetchone()\n        expected = (None, 0.05, None)\n        assert result == expected, f\"Expected fault status to be {expected}, but got {result}\"\n\n    finally:\n        cursor.execute(\"ROLLBACK;\")"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "robot_fault_prediction_M_7", "selected_database": "robot_fault_prediction", "query": "To help us keep a constant eye on our riskiest robots, can you create a special saved list called `vw_high_risk_robots`? It should always show the bots that are considered a high safety risk, along with their ID, who made them, the model, how many times they've crashed, and their overall incident rate.", "normal_query": "As per our new safety directive, create a permanent database view named `vw_high_risk_robots`. This view must provide a live list of all robots classified as a 'High Safety-Risk Unit'. The view shall include the robot's ID, its manufacturer, model, total collision count, and its calculated Safety Incident Rate.", "preprocess_sql": [], "clean_up_sqls": ["DROP VIEW IF EXISTS vw_high_risk_robots;"], "sol_sql": ["CREATE OR REPLACE VIEW vw_high_risk_robots AS\nSELECT\n    rd.botdetreg AS robot_id,\n    rd.mfgnameval AS manufacturer,\n    rd.modelseriesval AS model,\n    ps.collisioncount,\n    ((ps.collisioncount + ps.zoneviolnum + ps.speedviolnum) * 1000.0 / NULLIF(o.totopshrval, 0)) AS safety_incident_rate\nFROM\n    robot_details AS rd\nJOIN\n    operation AS o ON rd.botdetreg = o.operrecref\nJOIN\n    performance_and_safety AS ps ON rd.botdetreg = ps.effectivenessrobot\nWHERE\n    ps.collisioncount > 5 OR\n    ((ps.collisioncount + ps.zoneviolnum + ps.speedviolnum) * 1000.0 / NULLIF(o.totopshrval, 0)) > 2.0;"], "external_knowledge": [49, 7], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    import psycopg2\n    cursor = conn.cursor()\n    try:\n        cursor.execute(\"BEGIN;\")\n        # 1. Setup Data\n        cursor.execute(\"INSERT INTO robot_record (botcode, rects) VALUES ('RB_RISK_1', NOW()), ('RB_SAFE_1', NOW()) ON CONFLICT (botcode) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO robot_details (botdetreg, mfgnameval, modelseriesval) VALUES ('RB_RISK_1', 'MfgA', 'M1'), ('RB_SAFE_1', 'MfgB', 'M2') ON CONFLICT (botdetreg) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO operation (operreg, operrecref, totopshrval) VALUES ('OP_RISK_1', 'RB_RISK_1', 2000), ('OP_SAFE_1', 'RB_SAFE_1', 1000) ON CONFLICT (operreg) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO actuation_data (actreg, actoperref, actrecref) VALUES ('AC_RISK_1', 'OP_RISK_1', 'RB_RISK_1'), ('AC_SAFE_1', 'OP_SAFE_1', 'RB_SAFE_1') ON CONFLICT (actreg) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO performance_and_safety (effectivenessactuation, effectivenessrobot, collisioncount, zoneviolnum, speedviolnum) VALUES ('AC_RISK_1', 'RB_RISK_1', 6, 1, 1), ('AC_SAFE_1', 'RB_SAFE_1', 1, 0, 0) ON CONFLICT (effectivenessactuation) DO UPDATE SET collisioncount = EXCLUDED.collisioncount;\")\n\n        # 2. Execution\n        for sql in sol_sqls:\n            cursor.execute(sql)\n\n        # 3. Verification\n        cursor.execute('SELECT robot_id FROM vw_high_risk_robots;')\n        results = [row[0] for row in cursor.fetchall()]\n        assert 'RB_RISK_1' in results, 'High-risk robot RB_RISK_1 should be in the view'\n        assert 'RB_SAFE_1' not in results, 'Safe robot RB_SAFE_1 should not be in the view'\n\n    finally:\n        cursor.execute('DROP VIEW IF EXISTS vw_high_risk_robots;')\n        cursor.execute(\"ROLLBACK;\")"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "robot_fault_prediction_M_8", "selected_database": "robot_fault_prediction", "query": "Let's start tracking how often our robots get overloaded. Can you add a new field called `overload_frequency_ph` to the main performance and safety table? Once you've added it, you'll need to go back and fill it in for all the robots by calculating their overload frequency from their existing data.", "normal_query": "To enhance our performance monitoring, we need to begin tracking the 'Overload Frequency' KPI directly on the performance records. First, modify the `performance_and_safety` table to add a new column named `overload_frequency_ph` with a `REAL` data type. After adding the column, immediately populate it for all existing records by calculating their historical Overload Frequency based on their total overload events and operating hours.", "preprocess_sql": ["ALTER TABLE performance_and_safety DROP COLUMN IF EXISTS overload_frequency_ph;"], "clean_up_sqls": ["ALTER TABLE performance_and_safety DROP COLUMN IF EXISTS overload_frequency_ph;"], "sol_sql": ["ALTER TABLE performance_and_safety ADD COLUMN overload_frequency_ph REAL;", "UPDATE performance_and_safety ps\nSET overload_frequency_ph = (ps.overloadcnt / NULLIF(o.totopshrval / 1000.0, 0))\nFROM operation o\nWHERE ps.effectivenessrobot = o.operrecref;"], "external_knowledge": [13], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    import psycopg2\n    cursor = conn.cursor()\n    try:\n        cursor.execute(\"BEGIN;\")\n        # 0. Pre-cleanup\n        cursor.execute('ALTER TABLE performance_and_safety DROP COLUMN IF EXISTS overload_frequency_ph;')\n\n        # 1. Setup Data\n        cursor.execute(\"INSERT INTO robot_record (botcode, rects) VALUES ('RB_OVL_1', NOW()) ON CONFLICT (botcode) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO robot_details (botdetreg) VALUES ('RB_OVL_1') ON CONFLICT (botdetreg) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO operation (operreg, operrecref, totopshrval) VALUES ('OP_OVL_1', 'RB_OVL_1', 2000) ON CONFLICT (operreg) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO actuation_data (actreg, actoperref, actrecref) VALUES ('AC_OVL_1', 'OP_OVL_1', 'RB_OVL_1') ON CONFLICT (actreg) DO NOTHING;\")\n        cursor.execute(\"INSERT INTO performance_and_safety (effectivenessactuation, effectivenessrobot, overloadcnt) VALUES ('AC_OVL_1', 'RB_OVL_1', 10) ON CONFLICT (effectivenessactuation) DO UPDATE SET overloadcnt = 10;\")\n\n        # 2. Execution\n        for sql in sol_sqls:\n            cursor.execute(sql)\n\n        # 3. Verification\n        cursor.execute('SELECT overload_frequency_ph FROM performance_and_safety WHERE effectivenessrobot = \\'RB_OVL_1\\';')\n        result = cursor.fetchone()[0]\n        # Calculation: 10 overloads / (2000 hours / 1000) = 10 / 2 = 5.0\n        assert round(result, 2) == 5.00, f'Expected overload_frequency_ph to be 5.0, but got {result}'\n    finally:\n        cursor.execute('ALTER TABLE performance_and_safety DROP COLUMN IF EXISTS overload_frequency_ph;')\n        cursor.execute(\"ROLLBACK;\")"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_1", "selected_database": "exchange_traded_funds", "query": "Could you get me a ranked list of all income funds that are premier? For each one, I need the ticker symbol, its short name, its premier rank, and the calculated secure income efficiency score. Please sort the list by the efficiency score, from highest to lowest.", "normal_query": "Generate a ranked list of all premier income funds. For each fund, provide its ticker symbol, short label, its premier rank, and its calculated secure income efficiency score. This has to be ordered from the highest to the lowest score.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH BondQuality AS (\n    SELECT\n        fundlink,\n        SUM(ba.allocationpct) FILTER (WHERE br.creditmark IN ('us_government', 'aaa', 'aa')) AS high_quality_alloc\n    FROM\n        bond_allocations ba\n    INNER JOIN\n        bond_ratings br ON ba.ratinglink = br.ratekey\n    GROUP BY\n        fundlink\n),\nFundRatios AS (\n    SELECT\n        tickersym,\n        shortlabel,\n        -- The NULLIF function prevents division-by-zero errors if the net expense is 0.\n        (fundmetrics ->> 'Yield_Rate')::numeric / NULLIF((fundmetrics ->> 'Expense_Net')::numeric, 0) AS yter\n    FROM\n        funds\n    WHERE\n        (fundmetrics ->> 'Yield_Rate')::numeric > 0\n)\nSELECT\n    f.tickersym,\n    f.shortlabel,\n    -- The RANK() window function assigns a rank to each fund based on its final score.\n    RANK() OVER (ORDER BY (fr.yter * bq.high_quality_alloc) DESC) AS premier_rank,\n    (fr.yter * bq.high_quality_alloc) AS secure_income_score\nFROM\n    funds f\nINNER JOIN\n    FundRatios fr ON f.tickersym = fr.tickersym\nINNER JOIN\n    BondQuality bq ON f.tickersym = bq.fundlink\nWHERE\n    fr.yter > 15 AND bq.high_quality_alloc > 0.6\nORDER BY\n    premier_rank;"], "external_knowledge": [55, 42, 19, 14, 3], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "exchange_traded_funds_2", "selected_database": "exchange_traded_funds", "query": "Hey, can you pull up the performance history for the 'AADR' fund? I want to see how it did against its category each year. For every year, also show me what the outperformance was in the year before, and what the year-over-year outperformance change was. Just list it all out by year for me.", "normal_query": "I need to analyze the performance trend for the fund with ticker 'AADR'. Please calculate the annual fund outperformance for each calendar year. Additionally, for each year, show the previous year's outperformance and the year-over-year outperformance change. The output should contain the calendar year, the current outperformance, the previous year's outperformance, and the year-over-year change, sorted by year.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH FundOutperformance AS (\n    SELECT\n        calendaryear,\n        fundperf - categoryperf AS outperformance\n    FROM\n        annual_returns\n    WHERE\n        portfolioref = 'AADR' AND fundperf IS NOT NULL AND categoryperf IS NOT NULL\n)\n-- Step 2: Use the LAG window function to get the previous year's outperformance.\nSELECT\n    calendaryear,\n    outperformance,\n    -- The LAG function retrieves the 'outperformance' value from the previous row within the ordered set.\n    LAG(outperformance, 1) OVER (ORDER BY calendaryear) AS previous_year_outperformance,\n    -- This calculation shows the year-over-year change in performance momentum.\n    outperformance - LAG(outperformance, 1) OVER (ORDER BY calendaryear) AS yoy_outperformance_change\nFROM\n    FundOutperformance\nORDER BY\n    calendaryear;"], "external_knowledge": [0, 70], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "exchange_traded_funds_3", "selected_database": "exchange_traded_funds", "query": "I'm worried about interest rates going up and want to find bond funds that are safer than their peers. Can you identify funds that are much less sensitive to rate changes‚Äîat least 1.5 years less‚Äîthan what's typical for their category? For each of these funds, please list its ticker symbol, name, and category. Also, show me its specific sensitivity value, the average for its category, and its advantage to which their duration is shorter than the average duration of its category. Sort the results by the advantage , from highest to lowest.", "normal_query": "I need to perform a peer analysis on fixed-income funds based on interest rate sensitivity. For each fund category, first calculate the category average duration. Then, identify all funds whose own duration is at least 1.5 years lower than this average. For this final list of funds, please display the ticker symbol, short label, product class, the specific fund's duration, the calculated category average duration, and the fund's duration advantage. Sort the results in descending order by the duration advantage.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH CategoryDuration AS (\n    SELECT\n        productclass,\n        -- Calculate the average duration for all funds within a category.\n        AVG((allocweights -> 'bond_characteristics' ->> 'Duration_Yrs')::numeric) AS avg_category_duration\n    FROM\n        funds\n    WHERE\n        -- Ensure we only include funds where duration is a valid number.\n        jsonb_typeof(allocweights -> 'bond_characteristics' -> 'Duration_Yrs') = 'number'\n    GROUP BY\n        productclass\n),\n-- Step 2: Get the duration for each individual fund.\nFundDuration AS (\n    SELECT\n        tickersym,\n        productclass,\n        (allocweights -> 'bond_characteristics' ->> 'Duration_Yrs')::numeric AS fund_duration\n    FROM\n        funds\n    WHERE\n        jsonb_typeof(allocweights -> 'bond_characteristics' -> 'Duration_Yrs') = 'number'\n)\n-- Step 3: Join the individual fund data with the category averages and find the outliers.\nSELECT\n    fd.tickersym,\n    f.shortlabel,\n    fd.productclass,\n    ROUND(fd.fund_duration, 2) AS fund_duration,\n    ROUND(cd.avg_category_duration, 2) AS category_avg_duration,\n    -- Calculate how much lower the fund's duration is compared to its peers.\n    ROUND(cd.avg_category_duration - fd.fund_duration, 2) AS duration_advantage\nFROM\n    FundDuration fd\nJOIN\n    CategoryDuration cd ON fd.productclass = cd.productclass\nJOIN\n    funds f ON fd.tickersym = f.tickersym\nWHERE\n    -- Filter for funds that have a duration at least 1.5 years lower than their category average.\n    fd.fund_duration < (cd.avg_category_duration - 1.5)\n    AND cd.avg_category_duration IS NOT NULL\nORDER BY\n    duration_advantage DESC;"], "external_knowledge": [25, 71, 72], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "exchange_traded_funds_4", "selected_database": "exchange_traded_funds", "query": "I'm looking for resilient funds that do well no matter what the market is doing. Can you find funds that tend to beat their peers when the market is up, but also protect against losses better than their peers when the market is down? For each fund, show me its ticker symbol, its average outperformance in good years, and its average outperformance in bad years. Also, calculate the fund's difference in average outperformance for both scenarios. Only show me funds with at least three good and three bad years of data. Sort the list by the biggest difference first.", "normal_query": "I want to identify funds that perform well in different market cycles. Please calculate the average upside outperformance and the average downside outperformance for each fund. Also, compute the capture differential. Display the fund's ticker symbol, its average upside outperformance, its average downside outperformance, and the capture differential. Only include funds with at least three up years and three down years of history. Sort the results by the capture differential in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FundRelativePerformance AS (\n    SELECT portfolioref,\n           fundperf - categoryperf AS outperformance,\n           CASE WHEN categoryperf > 0 THEN 'up_year' ELSE 'down_year' END AS year_type\n    FROM annual_returns\n    WHERE fundperf IS NOT NULL AND categoryperf IS NOT NULL\n)\nSELECT portfolioref,\n       ROUND(AVG(outperformance) FILTER (WHERE year_type = 'up_year')::numeric, 6) AS avg_upside_outperformance,\n       ROUND(AVG(outperformance) FILTER (WHERE year_type = 'down_year')::numeric, 6) AS avg_downside_outperformance,\n       ROUND((AVG(outperformance) FILTER (WHERE year_type = 'up_year')::numeric) - (AVG(outperformance) FILTER (WHERE year_type = 'down_year')::numeric), 6) AS capture_differential\nFROM FundRelativePerformance\nGROUP BY portfolioref\nHAVING COUNT(*) FILTER (WHERE year_type = 'up_year') >= 3\n   AND COUNT(*) FILTER (WHERE year_type = 'down_year') >= 3\nORDER BY capture_differential DESC;"], "external_knowledge": [0, 73, 74, 75], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 6, "distinct": false, "order": true}}
{"instance_id": "exchange_traded_funds_5", "selected_database": "exchange_traded_funds", "query": "I'm worried about some of our funds being too big and hard to trade. For each fund, please tell me what is its liquidity pressure? For the top 100 funds, I want to see a list showing the fund's ticker symbol, its size, ratio of turnover, and its calculated days to trade turnover. Show me the riskiest ones at the top of the list.", "normal_query": "I need to assess the liquidity risk for our funds. Please calculate the portfolio liquidity Pressure for each fund, which should be expressed in days. For the top 100 funds with the highest pressure, please display the ticker symbol, net worth, turnover ratio, and the calculated liquidity pressure days. The results should be sorted by the liquidity pressure days in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n    f.tickersym,\n    (f.fundmetrics ->> 'Net_Worth')::bigint AS networth,\n    ROUND((f.fundmetrics ->> 'Turnover_Ratio')::numeric, 2) AS turnoverratio,\n    -- Formula for Portfolio Liquidity Pressure (KB ID: 58) in days\n    -- (Assets to be Traded Annually) / (Average Daily Value Traded)\n    ROUND(\n        ((f.fundmetrics ->> 'Net_Worth')::bigint * (f.fundmetrics ->> 'Turnover_Ratio')::numeric) /\n        NULLIF(\n            ((f.tradingdata -> 'volume_metrics' ->> 'Vol_3M')::numeric *\n             252),\n            0\n        ),\n        2\n    ) AS liquidity_pressure_days\nFROM funds f\nWHERE\n    jsonb_typeof(f.fundmetrics -> 'Net_Worth') = 'number'\n    AND jsonb_typeof(f.fundmetrics -> 'Turnover_Ratio') = 'number'\n    AND jsonb_typeof(f.tradingdata -> 'volume_metrics' -> 'Vol_3M') = 'number'\n        AND (f.fundmetrics ->> 'Turnover_Ratio')::numeric > 0\nORDER BY liquidity_pressure_days DESC\nLIMIT 100"], "external_knowledge": [76, 58], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "exchange_traded_funds_6", "selected_database": "exchange_traded_funds", "query": "Hey, can you help me check funds that changed its investment style? I'm looking for funds with long histories and want to compare their 3-year vs 10-year Beta and R-squared values. For each one, give me the ticker, how much the Beta and R-squared changed, and include a quick summary of their classifications too. Then just show the top 100 funds with the biggest Beta drift, sorted from highest to lowest based on the absolute value of that drift.", "normal_query": "I need to analyze the style drift for funds with long track records. Please compare the 3-year and 10-year Beta and R-squared values for each fund. The output should include the fund's ticker symbol, the calculated Beta Drift, the R-Squared Drift, and a summary of the classifications. Sort the top 100 results by the absolute value of the Beta drift in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH RiskProfiles AS (\n    SELECT\n        investmentref,\n        (risk3y -> 'risk_measures_3y' ->> 'Beta_3Y')::numeric AS beta3y,\n        (risk10y -> 'risk_measures_10y' ->> 'Beta_10Y')::numeric AS beta10y,\n        (risk3y -> 'risk_measures_3y' ->> 'R_Squared_3Y')::numeric AS rsquared3y,\n        (risk10y -> 'risk_measures_10y' ->> 'R_Squared_10Y')::numeric AS rsquared10y\n    FROM risk_metrics\n    WHERE\n        jsonb_typeof(risk3y -> 'risk_measures_3y' -> 'Beta_3Y') = 'number' AND\n        jsonb_typeof(risk10y -> 'risk_measures_10y' -> 'Beta_10Y') = 'number' AND\n        jsonb_typeof(risk3y -> 'risk_measures_3y' -> 'R_Squared_3Y') = 'number' AND\n        jsonb_typeof(risk10y -> 'risk_measures_10y' -> 'R_Squared_10Y') = 'number'\n)\nSELECT\n    investmentref,\n    ROUND(beta3y - beta10y, 2) AS beta_drift,\n    ROUND(rsquared3y - rsquared10y, 2) AS rsquared_drift,\n    CASE \n        WHEN ABS(beta3y - beta10y) > 0.15 \n             OR ABS(rsquared3y - rsquared10y) > 10\n        THEN 'Drifted'\n        ELSE 'No Significant Drift'\n    END AS drift_analysis\nFROM RiskProfiles\nORDER BY ABS(beta3y - beta10y) DESC\nLIMIT 100"], "external_knowledge": [77, 78, 79], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "exchange_traded_funds_7", "selected_database": "exchange_traded_funds", "query": "I want to find the top-performing fund in each category. But only look at categories that have at least 10 funds in them, and for each category, find the fund that has the highest score for peer-group comparison. For each one, show me the category name, the ticker of the best fund, its short label, and its final score. Sort everything by the highest scores first, and just give me the top 100 results.", "normal_query": "I want to find the category dominator for each fund category. For each fund category with at least 10 funds, identify the fund with the highest composite score. The output should list the category, the ticker symbol of the dominating fund, its short label, and its final composite score. Please sort the results by the composite score in descending order and limit the output to the top 100.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FundBaseMetrics AS ( SELECT f.tickersym, f.productclass, (r.risk5y -> 'risk_measures_5y' ->> 'Alpha_5Y')::numeric AS alpha5y, (r.risk3y -> 'risk_measures_3y' ->> 'Sharpe_Ratio_3Y')::numeric AS sharperatio3y, (f.fundmetrics ->> 'Expense_Net')::numeric AS expensenet FROM funds f JOIN risk_metrics r ON f.tickersym = r.investmentref WHERE f.productclass IS NOT NULL AND jsonb_typeof(r.risk5y -> 'risk_measures_5y' -> 'Alpha_5Y') = 'number' AND jsonb_typeof(r.risk3y -> 'risk_measures_3y' -> 'Sharpe_Ratio_3Y') = 'number' AND jsonb_typeof(f.fundmetrics -> 'Expense_Net') = 'number' ), RankedMetrics AS ( SELECT tickersym, productclass, PERCENT_RANK() OVER (PARTITION BY productclass ORDER BY alpha5y ASC) AS alpha_rank, PERCENT_RANK() OVER (PARTITION BY productclass ORDER BY sharperatio3y ASC) AS sharpe_rank, PERCENT_RANK() OVER (PARTITION BY productclass ORDER BY expensenet DESC) AS expense_rank FROM FundBaseMetrics ), FinalScores AS ( SELECT tickersym, productclass, ROUND(((alpha_rank + sharpe_rank + expense_rank) / 3.0)::NUMERIC, 4) AS composite_score, RANK() OVER (PARTITION BY productclass ORDER BY (alpha_rank + sharpe_rank + expense_rank) DESC) AS category_rank FROM RankedMetrics ) SELECT fs.productclass, fs.tickersym, f.shortlabel, fs.composite_score FROM FinalScores fs JOIN funds f ON fs.tickersym = f.tickersym WHERE fs.category_rank = 1 AND fs.productclass IN ( SELECT productclass FROM funds WHERE productclass IS NOT NULL GROUP BY productclass HAVING COUNT(*) >= 10 ) ORDER BY fs.composite_score DESC LIMIT 100"], "external_knowledge": [26, 24, 10, 80, 81], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "exchange_traded_funds_8", "selected_database": "exchange_traded_funds", "query": "I'd like to run a regression to see how portfolio turnover relates to manager skill. For each product class that has more than 25 funds, calculate the slope between alpha and turnover, and also show how good the fit is. I want to see the product class name, how many funds are in it, the slope value, and the R-squared for the regression. Sort the results by the slope from highest to lowest, and just show the top 100.", "normal_query": "I want to conduct a regression analysis to determine the relationship between portfolio turnover and manager skill. For each productclass with more than 25 funds, calculate the alpha-turnover slope and the fit quality. The output should include the productclass, the number of funds, the calculated slope, and the R-squared value for the fit. Please sort the top 100 results by the alpha-turnover slope in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ModelData AS (\n    SELECT\n        f.productclass,\n        (f.fundmetrics ->> 'Turnover_Ratio')::numeric AS turnover,\n        (r.risk3y -> 'risk_measures_3y' ->> 'Alpha_3Y')::numeric AS alpha\n    FROM funds f\n    JOIN risk_metrics r ON f.tickersym = r.investmentref\n    WHERE\n        f.productclass IS NOT NULL\n        AND jsonb_typeof(f.fundmetrics -> 'Turnover_Ratio') = 'number'\n        AND jsonb_typeof(r.risk3y -> 'risk_measures_3y' -> 'Alpha_3Y') = 'number'\n)\n-- Step 2: Group by category and apply the regression function.\nSELECT\n    productclass,\n    COUNT(*) as num_funds,\n    -- regr_slope calculates the slope of the linear regression line (Y = slope*X + intercept).\n    ROUND(regr_slope(alpha, turnover)::numeric, 4) AS alpha_turnover_slope,\n    -- regr_r2 calculates the R-squared of the regression, indicating how well turnover explains alpha.\n    ROUND(regr_r2(alpha, turnover)::numeric, 4) AS fit_quality_r_squared\nFROM ModelData\nGROUP BY productclass\nHAVING COUNT(*) > 25 -- Need enough data points for a meaningful regression.\nORDER BY alpha_turnover_slope DESC\nLIMIT 100"], "external_knowledge": [10, 12, 82, 83], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "exchange_traded_funds_9", "selected_database": "exchange_traded_funds", "query": "Can you find funds that is a potential value investment? For the top 100 that qualify, show me their ticker, short label, and where their price stands right now. Sort them from the lowest to the highest price position.", "normal_query": "Please screen for funds that match the contrarian value play profile. For the top 100 qualifying funds, please display the ticker symbol, short label, and the calculated price position. Sort the results by the price position in ascending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FundCharacteristics AS (\n    SELECT\n        p.productref,\n        f.fundmetrics,\n        -- Price Position in 52-Week Range (KB ID: 4)\n        ROUND(\n            (\n                ((p.pricerange52w -> 'high_metrics' ->> 'High_52W')::numeric + \n                 (p.pricerange52w -> 'high_metrics' ->> 'High_Delta')::numeric) \n                - (p.pricerange52w -> 'low_metrics' ->> 'Low_52W')::numeric\n            ) / NULLIF(\n                ((p.pricerange52w -> 'high_metrics' ->> 'High_52W')::numeric - \n                 (p.pricerange52w -> 'low_metrics' ->> 'Low_52W')::numeric), 0\n            ),\n            8\n        ) * 100 AS price_range_position\n    FROM performance p\n    JOIN funds f ON p.productref = f.tickersym\n    WHERE\n        jsonb_typeof(p.pricerange52w -> 'high_metrics' -> 'High_52W') = 'number'\n        AND jsonb_typeof(f.fundmetrics -> 'Expense_Net') = 'number'\n        AND jsonb_typeof(f.fundmetrics -> 'Benchmark_Exp') = 'number'\n)\nSELECT\n    fc.productref,\n    f.shortlabel,\n    fc.price_range_position\nFROM FundCharacteristics fc\nJOIN funds f ON fc.productref = f.tickersym\nWHERE\n    fc.price_range_position < 25 -- Out of favor (KB ID: 47)\n    AND jsonb_typeof(fc.fundmetrics -> 'Turnover_Ratio') = 'number'\n    AND (fc.fundmetrics ->> 'Turnover_Ratio')::numeric < 30 -- Low-Turnover Strategy (KB ID: 12)\n    AND (fc.fundmetrics ->> 'Expense_Net')::numeric < (fc.fundmetrics ->> 'Benchmark_Exp')::numeric -- Low Cost (Negative Relative Expense, KB ID: 1)\nORDER BY price_range_position ASC\nLIMIT 100"], "external_knowledge": [1, 4, 47, 12], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 8, "distinct": false, "order": true}}
{"instance_id": "exchange_traded_funds_10", "selected_database": "exchange_traded_funds", "query": "I have a hunch that funds that tell us more about their stock valuations actually perform better. Can we test this? Let's split all the funds into two groups: those that share their valuation numbers and those that don't. For each of those two groups, tell me how many funds are in it and what the typical 1-year return was. I'm curious to see if there's a difference.", "normal_query": "I want to compare the performance of funds based on their data transparency. Please create two groups of funds: 'Transparent' and 'Opaque', based on their valuation data availability. For each group, calculate the fund_count and the Median 1-Year Return.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH FundTransparency AS (\n    SELECT\n        -- A fund is 'Transparent' if it has numeric data for both P/E and P/B ratios.\n        CASE\n            WHEN jsonb_typeof(f.valuationratios -> 'valuation_metrics' -> 'PE_Ratio') = 'number'\n             AND jsonb_typeof(f.valuationratios -> 'valuation_metrics' -> 'PB_Ratio') = 'number'\n            THEN 'Transparent'\n            ELSE 'Opaque'\n        END AS transparency_type,\n        -- Safely extract the 1-year return.\n        (p.returnmetrics -> 'fund_returns' ->> 'Return_1Y')::numeric as r1y\n    FROM funds f\n    JOIN performance p ON f.tickersym = p.productref\n    WHERE\n        jsonb_typeof(p.returnmetrics -> 'fund_returns' -> 'Return_1Y') = 'number'\n)\n-- Step 2: Group by the transparency type and calculate the median return.\nSELECT\n    transparency_type,\n    COUNT(*) as fund_count,\n    -- Use median (percentile_cont) as it's more robust to outliers than average.\n    ROUND(percentile_cont(0.5) WITHIN GROUP (ORDER BY r1y)::NUMERIC, 4) AS median_1y_return\nFROM\n    FundTransparency\nGROUP BY\n    transparency_type;"], "external_knowledge": [84, 85], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 4, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_11", "selected_database": "exchange_traded_funds", "query": "Can you find those rare funds that basically produce positive alpha but are passive? I'd like to see the top 10 of these, ranked by the alpha for 5-years in descending order. For each one, show me the ticker symbol, the company that runs it, and their 5-year alpha score. Also, at the end, just tell me the total number of these funds you found.", "normal_query": "I want to find all passive alpha generators. Please provide a list of the top 10, showing their ticker symbol, parent group, and their 5-year alpha. The list should be sorted by the 5-year alpha in descending order. Also, provide a separate summary count of the total number of passive alpha generators found.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FundClassification AS (\n    -- Step 1: For each fund, determine if it's a \"Market-Tracking Fund\" (R-Squared > 90)\n    -- and an \"Alpha Generator\" (5Y Alpha > 0) by extracting data from JSONB columns.\n    SELECT\n        f.tickersym,\n        f.parentgroup,\n        (r.risk3y->'risk_measures_3y'->>'R_Squared_3Y')::numeric > 90 AS is_market_tracker,\n        (r.risk5y->'risk_measures_5y'->>'Alpha_5Y')::numeric AS alpha5y\n    FROM funds f\n    JOIN risk_metrics r ON f.tickersym = r.investmentref\n    WHERE (r.risk3y->'risk_measures_3y'->>'R_Squared_3Y') IS NOT NULL\n      AND (r.risk5y->'risk_measures_5y'->>'Alpha_5Y') IS NOT NULL\n),\nPassiveAlphaGenerators AS (\n    -- Step 2: Filter the above to get the final list of funds that meet both criteria.\n    -- This CTE is used by both the tabular and scalar parts of the query below.\n    SELECT tickersym, parentgroup, alpha5y\n    FROM FundClassification\n    WHERE is_market_tracker AND alpha5y > 0\n)\n-- The first SELECT must be wrapped in parentheses to use its own ORDER BY and LIMIT.\n(\n    SELECT\n        tickersym,\n        parentgroup,\n        alpha5y::text AS value\n    FROM PassiveAlphaGenerators\n    ORDER BY alpha5y DESC\n    LIMIT 10\n)\nUNION ALL\n-- The second SELECT provides the scalar count, formatted to match the column structure.\nSELECT\n    'Total Passive Alpha Generators Found' AS tickersym,\n    NULL AS parentgroup,\n    COUNT(*)::text AS value\nFROM PassiveAlphaGenerators"], "external_knowledge": [10, 11, 16], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "exchange_traded_funds_12", "selected_database": "exchange_traded_funds", "query": "I want to see which fund managers are actually worth their high fees. Can you figure out a score that shows if a manager's skill outweighs their cost? Show me the top 10 funds with the best scores, listing their ticker symbol and the score itself, sorted from highest to lowest. Also, can you tell me what percentage of all funds are actually providing a positive value for their cost?", "normal_query": "I need to rank funds based on their active manager value. Show me a list of the top 10 funds with the highest AMV, displaying their ticker symbol and the calculated AMV score, sorted from highest to lowest. Additionally, provide a scalar metric showing the percentage of all funds that have a positive AMV.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FundCalculations AS (\n    SELECT\n        f.tickersym,\n        -- Correct Information Ratio per KB definition (3Y Avg Return difference over 3Y Volatility)\n        (\n            (r.risk3y->'risk_measures_3y'->>'Avg_Return_3Y')::numeric\n            - (p.returnmetrics->'benchmark_returns'->>'Bench_Return_3Y')::numeric\n        )\n        / NULLIF((r.risk3y->'risk_measures_3y'->>'Volatility_3Y')::numeric, 0) AS info_ratio,\n        -- Relative Expense Ratio (KB ID: 1)\n        (f.fundmetrics->>'Expense_Net')::numeric - (f.fundmetrics->>'Benchmark_Exp')::numeric AS relative_expense\n    FROM funds f\n    JOIN risk_metrics r ON f.tickersym = r.investmentref\n    JOIN performance p ON f.tickersym = p.productref\n    WHERE (r.risk3y->'risk_measures_3y'->>'Avg_Return_3Y') IS NOT NULL\n      AND (p.returnmetrics->'benchmark_returns'->>'Bench_Return_3Y') IS NOT NULL\n      AND (r.risk3y->'risk_measures_3y'->>'Volatility_3Y') IS NOT NULL\n      AND (f.fundmetrics->>'Expense_Net') IS NOT NULL\n      AND (f.fundmetrics->>'Benchmark_Exp') IS NOT NULL\n),\nAmvScores AS (\n    SELECT\n        tickersym,\n        ROUND(info_ratio - (relative_expense * 10), 4) AS amv\n    FROM FundCalculations\n    WHERE info_ratio IS NOT NULL AND relative_expense IS NOT NULL\n)\n(\n    SELECT\n        tickersym,\n        amv::text AS value\n    FROM AmvScores\n    ORDER BY amv DESC\n    LIMIT 10\n)\nUNION ALL\n(\n    SELECT\n        'Percentage of Funds with Positive AMV' AS tickersym,\n        ( (SUM(CASE WHEN amv > 0 THEN 1 ELSE 0 END)::decimal / COUNT(*)) * 100 )::numeric(5,2)::text AS value\n    FROM AmvScores\n);"], "external_knowledge": [1, 6, 34], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "exchange_traded_funds_13", "selected_database": "exchange_traded_funds", "query": "How many funds out there are both generating excess returns successfully and its managers confidently put a lot of money into their best ideas? Just give me the total number.", "normal_query": "I need to know the total number of focused alpha leaders.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FocusedAlphaLeaders AS (\n    SELECT\n        f.tickersym\n    FROM funds f\n    JOIN risk_metrics r ON f.tickersym = r.investmentref\n    JOIN holdings h ON f.tickersym = h.instrumentref\n    WHERE \n        (r.risk5y->'risk_measures_5y'->>'Alpha_5Y')::numeric > 0 \n        AND h.positionrank = 1 \n        AND h.holdingpct > 0.08\n)\nSELECT COUNT(DISTINCT tickersym) AS total_focused_alpha_leaders \nFROM FocusedAlphaLeaders \nLIMIT 100"], "external_knowledge": [10, 13, 45], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_14", "selected_database": "exchange_traded_funds", "query": "I want to know the average Information Ratio after adjusting for how consistent it is. I just need that one final number.", "normal_query": "I want to calculate the average consistency-adjusted information ratio, and please provide it as a single scalar value.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FundComponents AS (\n    SELECT\n        f.tickersym,\n        (\n            (r.risk3y->'risk_measures_3y'->>'Avg_Return_3Y')::numeric\n            - (p.returnmetrics->'benchmark_returns'->>'Bench_Return_3Y')::numeric\n        )\n        / NULLIF((r.risk3y->'risk_measures_3y'->>'Volatility_3Y')::numeric, 0) AS info_ratio,\n        (p.histstats->>'Positive_Years')::numeric / NULLIF((p.histstats->>'Positive_Years')::numeric + (p.histstats->>'Negative_Years')::numeric, 0) * 100 AS positive_return_consistency\n    FROM funds f\n    JOIN performance p ON f.tickersym = p.productref\n    JOIN risk_metrics r ON f.tickersym = r.investmentref\n    WHERE (r.risk3y->'risk_measures_3y'->>'Avg_Return_3Y')::numeric\n            - (p.returnmetrics->'benchmark_returns'->>'Bench_Return_3Y')::numeric IS NOT NULL\n      AND (p.histstats->>'Positive_Years') IS NOT NULL\n),\nCairScores AS (\n    SELECT\n        tickersym,\n        info_ratio * (positive_return_consistency/100) AS cair,\n        positive_return_consistency\n    FROM FundComponents\n    WHERE info_ratio IS NOT NULL\n      AND positive_return_consistency IS NOT NULL\n)\nSELECT\n    AVG(cair)::numeric(6,2) AS avg_cair_for_consistent_funds\nFROM CairScores;"], "external_knowledge": [6, 7, 32, 86], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_16", "selected_database": "exchange_traded_funds", "query": "What's the best score any fund has gotten for generating steady income efficiently? Just give me the highest number and I only need the top score across the board.", "normal_query": "I need to find the single highest secure income efficiency score across all funds. Please provide only the maximum SIES value as a single scalar result.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FundYTER AS (\n    SELECT \n        tickersym, \n        (fundmetrics->>'Yield_Rate')::numeric / NULLIF((fundmetrics->>'Expense_Net')::numeric, 0) AS yter\n    FROM funds \n    WHERE (fundmetrics->>'Yield_Rate')::numeric > 0\n),\nBondQuality AS (\n    SELECT \n        b.fundlink, \n        SUM(b.allocationpct) \n            FILTER (WHERE r.creditmark IN ('us_government', 'aaa', 'aa')) AS high_quality_pct\n    FROM bond_allocations b \n    JOIN bond_ratings r ON b.ratinglink = r.ratekey\n    GROUP BY b.fundlink\n),\nSiesScores AS (\n    SELECT \n        fy.tickersym, \n        (fy.yter * bq.high_quality_pct) AS sies_score\n    FROM FundYTER fy \n    JOIN BondQuality bq ON fy.tickersym = bq.fundlink\n    WHERE fy.yter IS NOT NULL AND bq.high_quality_pct IS NOT NULL\n)\nSELECT \n    MAX(sies_score)::numeric(10,2) AS highest_sies_score_achieved \nFROM SiesScores \nLIMIT 100"], "external_knowledge": [3, 19, 55], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_17", "selected_database": "exchange_traded_funds", "query": "How many funds are truly standing out from the crowd with their active strategies? Just give me the total number ‚Äî one single value.", "normal_query": "I want a total count of all true active differentiators. Please provide a single scalar value for the total count.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FundMetrics AS (\n    SELECT\n        f.tickersym,\n        (1 - ((r.risk3y->'risk_measures_3y'->>'R_Squared_3Y')::numeric / 100)) / NULLIF((f.fundmetrics->>'Turnover_Ratio')::numeric, 0) AS amf,\n        (r.risk3y->'risk_measures_3y'->>'Alpha_3Y')::numeric / NULLIF((r.risk3y->'risk_measures_3y'->>'Volatility_3Y')::numeric * POWER(1 - ((r.risk3y->'risk_measures_3y'->>'R_Squared_3Y')::numeric/100), 0.5), 0) AS appraisal_ratio,\n        EXISTS (\n            SELECT 1\n            FROM holdings h\n            WHERE h.instrumentref = f.tickersym\n              AND h.positionrank = 1\n              AND h.holdingpct > 0.08\n        ) AS is_high_conviction\n    FROM funds f\n    JOIN risk_metrics r ON f.tickersym = r.investmentref\n    WHERE (f.fundmetrics->>'Turnover_Ratio') IS NOT NULL\n      AND (r.risk3y->'risk_measures_3y'->>'Alpha_3Y') IS NOT NULL\n),\nDifferentiators AS (\n    SELECT tickersym\n    FROM FundMetrics\n    WHERE is_high_conviction\n      AND amf > 0.5\n      AND appraisal_ratio > 0.2\n)\nSELECT COUNT(*) AS total_differentiators_found\nFROM Differentiators;"], "external_knowledge": [13, 50, 52, 68], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_18", "selected_database": "exchange_traded_funds", "query": "How many funds are really going against the grain and is a potential value investment? Just give me the final count ‚Äî one number is all I need.", "normal_query": "I need the total number of funds that qualify as a contrarian value play. Please provide the final count as a single scalar value.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ContrarianFunds AS (\n    SELECT f.tickersym\n    FROM funds f\n    JOIN performance p ON f.tickersym = p.productref\n    WHERE\n        (\n            ((p.pricerange52w -> 'high_metrics' ->> 'High_52W')::numeric +\n             (p.pricerange52w -> 'high_metrics' ->> 'High_Delta')::numeric) -\n             (p.pricerange52w -> 'low_metrics' ->> 'Low_52W')::numeric\n        ) / NULLIF((p.pricerange52w -> 'range_metrics' ->> 'Range_Move')::numeric, 0) * 100 < 25\n        AND (f.fundmetrics->>'Turnover_Ratio')::numeric < 0.3\n        AND ((f.fundmetrics->>'Expense_Net')::numeric - (f.fundmetrics->>'Benchmark_Exp')::numeric) < 0\n)\nSELECT COUNT(*) AS total_contrarian_funds_found\nFROM ContrarianFunds;"], "external_knowledge": [1, 4, 12, 47], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_19", "selected_database": "exchange_traded_funds", "query": "Which exchange moves the most money overall? Let's first figure out the average daily trading volume for each fund. Then, total those numbers for every exchange. Just tell me the one exchange where the most trading happens ‚Äî the biggest player in terms of total value traded.", "normal_query": "I need to identify the most liquid exchange. To do this, first calculate the average daily value traded for every fund. Then, sum this value for all funds on each exchange. Finally, return the name of the single exchange with the highest total value traded", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FundValueTraded AS (\n    SELECT\n        f.listingvenue,\n        (f.tradingdata->'volume_metrics'->>'Vol_3M')::numeric * \n        (f.tradingdata->'moving_averages'->>'MA_200')::numeric AS advt\n    FROM funds f\n    WHERE \n        (f.tradingdata->'volume_metrics'->>'Vol_3M') IS NOT NULL AND \n        (f.tradingdata->'moving_averages'->>'MA_200') IS NOT NULL\n),\nExchangeLiquidity AS (\n    SELECT\n        e.tradingvenue, \n        SUM(fv.advt) AS total_advt\n    FROM FundValueTraded fv \n    JOIN exchanges e ON fv.listingvenue = e.marketcode\n    GROUP BY e.tradingvenue\n)\nSELECT \n    tradingvenue AS most_liquid_exchange_by_total_volume\nFROM ExchangeLiquidity \nORDER BY total_advt DESC \nLIMIT 1"], "external_knowledge": [76], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_20", "selected_database": "exchange_traded_funds", "query": "How much money is being wasted on fees by funds that are basically just hugging the index? For every one of those closet indexers, add up the fees that aren‚Äôt really earning their keep. In the end, just give me one total number ‚Äî the grand total of all those wasted fees, rounded to 2 decimal places.", "normal_query": "I need to calculate the total wasted fee amount for all funds classified as a closet indexer. Finally, sum these amounts together and provide a single, rounded scalar value to 2 decimal values.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nWITH FundMetrics AS (\n    SELECT\n        f.tickersym,\n        (f.fundmetrics->>'Expense_Net')::numeric - (f.fundmetrics->>'Benchmark_Exp')::numeric AS relative_expense,\n        (f.fundmetrics->>'Net_Worth')::numeric as net_worth,\n        (r.risk3y->'risk_measures_3y'->>'R_Squared_3Y')::numeric > 90 AS is_market_tracker\n    FROM funds f \n    JOIN risk_metrics r ON f.tickersym = r.investmentref\n    WHERE \n        (r.risk3y->'risk_measures_3y'->>'R_Squared_3Y') IS NOT NULL AND \n        (f.fundmetrics->>'Expense_Net') IS NOT NULL AND \n        (f.fundmetrics->>'Net_Worth') IS NOT NULL\n),\nClosetIndexerFees AS (\n    SELECT \n        tickersym, \n        (relative_expense * net_worth) AS wasted_fee_amount\n    FROM FundMetrics\n    WHERE is_market_tracker AND relative_expense > 0\n)\nSELECT \n    ROUND(SUM(wasted_fee_amount), 2) AS total_wasted_fees_across_all_closet_indexers \nFROM ClosetIndexerFees \nLIMIT 100\n"], "external_knowledge": [1, 60, 86], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_M_1", "selected_database": "exchange_traded_funds", "query": "It's time to refresh our list of funds with positive momentum. Can you wipe the old daily_golden_cross_leaders table clean (create it if it doesn't exists) and then fill it up again with all the funds whose short-term price average is above their long-term average? For each one, I need the ticker symbol, who runs it, the momentum score, and today's date.", "normal_query": "Please run the daily job to update the daily_golden_cross_leaders table. First, ensure the table exists. Then, clear out all existing data from it. Finally, populate it with all funds that are currently showing a golden cross signal. The table should contain the fund's ticker symbol, parent group, the calculated short-term momentum indicator value, and today's date as the report date.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Step 1 (DDL): Create the summary table if it doesn't already exist.\nCREATE TABLE IF NOT EXISTS daily_golden_cross_leaders (\n    tickersym VARCHAR(20),\n    parentgroup VARCHAR(255),\n    momentum_value NUMERIC(10, 4),\n    report_date DATE\n);\n\n-- Step 2 (TCL/DML): Clear old data\nTRUNCATE TABLE daily_golden_cross_leaders;\n\n-- Step 3 (DML): Insert new data safely\nINSERT INTO daily_golden_cross_leaders (tickersym, parentgroup, momentum_value, report_date)\nSELECT\n    tickersym,\n    parentgroup,\n    ((tradingdata->'moving_averages'->>'MA_50')::numeric -\n     (tradingdata->'moving_averages'->>'MA_200')::numeric) AS momentum_value,\n    CURRENT_DATE\nFROM funds\nWHERE\n    jsonb_typeof(tradingdata->'moving_averages'->'MA_50') = 'number' AND\n    jsonb_typeof(tradingdata->'moving_averages'->'MA_200') = 'number' AND\n    (tradingdata->'moving_averages'->>'MA_50')::numeric >\n    (tradingdata->'moving_averages'->>'MA_200')::numeric;"], "external_knowledge": [5, 17], "test_cases": ["\ndef test_case(pred_sqls, sol_sql, db_name, conn):\n        from datetime import date\n\n        for sql in pred_sqls:\n            _, _, _ = execute_queries(sql, db_name, conn)\n\n        table_exists_sql = \"\"\"\n        SELECT EXISTS (\n            SELECT FROM information_schema.tables \n            WHERE table_name = 'daily_golden_cross_leaders'\n        );\n        \"\"\"\n        result, _, _ = execute_queries(table_exists_sql, db_name, conn)\n        assert result[0][0], \"Table 'daily_golden_cross_leaders' was not created.\"\n\n        count_sql = \"SELECT COUNT(*) FROM daily_golden_cross_leaders;\"\n        result, _, _ = execute_queries(count_sql, db_name, conn)\n        assert result[0][0] > 0, \"No data inserted into 'daily_golden_cross_leaders'.\"\n\n        check_sql = \"\"\"\n        SELECT tickersym, parentgroup, momentum_value, report_date \n        FROM daily_golden_cross_leaders \n        ORDER BY tickersym LIMIT 1;\n        \"\"\"\n        result, _, _ = execute_queries(check_sql, db_name, conn)\n\n        expected_ticker = result[0][0]\n\n        get_momentum = f\"\"\"\n        SELECT (tradingdata->'moving_averages'->'MA_50')::numeric - \n        (tradingdata->'moving_averages'->'MA_200')::numeric as momentum\n        FROM funds\n        WHERE tickersym = '{expected_ticker}'\n        \"\"\"\n\n        momentum, _, _ = execute_queries(get_momentum, db_name, conn)\n        \n        expected_momentum = momentum[0][0]\n        expected_date = date.today()\n\n        assert abs(result[0][2] - expected_momentum) < 0.001, f\"Expected momentum ~{expected_momentum}, got {result[0][2]}\"\n        assert result[0][3] == expected_date, f\"Expected date {expected_date}, got {result[0][3]}\"\n\n        print(\"Golden cross leader test passed.\")\n        return True\n\n"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_M_2", "selected_database": "exchange_traded_funds", "query": "I need a tool that can calculate the stock-picking skill for any fund. Can you build a function called get_appraisal_ratio that I can use on any ticker? It should figure out the fund's appraisal ratio over the last 3 years and be smart enough not to break if the data isn't perfect like nulls or invalid data.", "normal_query": "Please create a reusable SQL function named get_appraisal_ratio that takes a fund's ticker symbol as input. This function should calculate the fund's 3-year appraisal ratio. Inside the function, handle potential null or invalid data to avoid errors.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION get_appraisal_ratio(ticker VARCHAR)\nRETURNS NUMERIC AS $$\nDECLARE\n    v_alpha3y NUMERIC;\n    v_vol3y NUMERIC;\n    v_rsquared3y NUMERIC;\n    v_appraisal_ratio NUMERIC;\nBEGIN\n    SELECT\n        (risk3y->'risk_measures_3y'->>'Alpha_3Y')::numeric,\n        (risk3y->'risk_measures_3y'->>'Volatility_3Y')::numeric,\n        (risk3y->'risk_measures_3y'->>'R_Squared_3Y')::numeric / 100\n    INTO\n        v_alpha3y,\n        v_vol3y,\n        v_rsquared3y\n    FROM risk_metrics\n    WHERE investmentref = ticker;\n\n    IF v_vol3y IS NULL OR v_vol3y = 0 OR v_rsquared3y IS NULL OR v_rsquared3y >= 1 OR v_alpha3y IS NULL THEN\n        RETURN NULL;\n    END IF;\n\n    v_appraisal_ratio := v_alpha3y / (v_vol3y * POWER(1 - v_rsquared3y, 0.5));\n    RETURN v_appraisal_ratio;\nEND;\n$$ LANGUAGE plpgsql;"], "external_knowledge": [50, 10], "test_cases": ["\ndef test_case(pred_sqls, sol_sql, db_name, conn):\n    \"\"\"Test the execution and correctness of golden cross leader query\"\"\"\n    import math\n\n    get_ticker = f\"\"\"\n    SELECT investmentref FROM risk_metrics LIMIT 1;\n    \"\"\"\n\n    result, _, _ = execute_queries(get_ticker, db_name, conn)\n    assert result, \"No investment reference found in risk_metrics table.\"\n    ticker = result[0][0]\n\n    get_risk_data = f\"\"\"\n    SELECT risk3y->'risk_measures_3y' FROM risk_metrics WHERE investmentref = '{ticker}';\n    \"\"\"\n    result, _, _ = execute_queries(get_risk_data, db_name, conn)\n    assert result, f\"No risk data found for ticker {ticker}.\"\n    risk_values = result[0][0]\n\n    print(f\"Risk values for {ticker}: {risk_values}\")\n    alpha = float(risk_values['Alpha_3Y'])\n    rsq = float(risk_values['R_Squared_3Y']) / 100.0\n    vol = float(risk_values['Volatility_3Y'])\n\n    if vol == 0 or rsq >= 1:\n        expected = None\n    else:\n        expected = alpha / (vol * math.sqrt(1 - rsq))\n        expected = round(expected, 6)\n\n    get_func_result = f\"SELECT get_appraisal_ratio('{ticker}');\"\n    result, _, _ = execute_queries(get_func_result, db_name, conn)\n    assert result, \"Function get_appraisal_ratio did not return any result.\"\n    actual = result[0][0]\n    if actual is not None:\n        actual = round(float(actual), 6)\n\n    assert expected == actual, f\"Expected {expected}, got {actual}\"\n    print(f\"Appraisal Ratio matched: {actual}\")\n\n    return 1\n"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_M_3", "selected_database": "exchange_traded_funds", "query": "I want to create a pre-calculated list named vw_reliable_core_holdings. This list should contain all funds that are reliable as a core portfolio holding. For each qualifying fund, please include its ticker symbol, short label, parent group, product class, and launch date.", "normal_query": "I want to create a pre-calculated list named vw_reliable_core_holdings. This list should contain all funds classified as a reliable core holding. For each qualifying fund, please include its ticker symbol, short label, parent group, product class, and launch date.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE MATERIALIZED VIEW IF NOT EXISTS vw_reliable_core_holdings AS\n-- Step 1: The SELECT statement encapsulates the full logic for identifying \"Reliable Core Holdings\".\nWITH ConsistentOutperformers AS (\n    -- Sub-step 1a: Identify funds that are \"Consistent Outperformers\" (high consistency + positive alpha).\n    SELECT\n        f.tickersym\n    FROM funds f\n    JOIN risk_metrics r ON f.tickersym = r.investmentref\n    JOIN performance p ON f.tickersym = p.productref\n    WHERE\n        jsonb_typeof(r.risk5y->'risk_measures_5y'->'Alpha_5Y') = 'number'\n        AND (r.risk5y->'risk_measures_5y'->>'Alpha_5Y')::numeric > 0 -- Alpha Generator (ID: 10)\n        AND jsonb_typeof(p.histstats->'Positive_Years') = 'number'\n        AND jsonb_typeof(p.histstats->'Negative_Years') = 'number'\n        AND ((p.histstats->>'Positive_Years')::numeric / NULLIF((p.histstats->>'Positive_Years')::numeric + (p.histstats->>'Negative_Years')::numeric, 0)) > 0.8 -- Positive Return Consistency (ID: 7)\n),\nTotalReturnFunds AS (\n    -- Sub-step 1b: Identify funds that are \"Total Return Funds\" (have both equity weight and yield).\n    SELECT\n        tickersym\n    FROM funds\n    WHERE\n        jsonb_typeof(allocweights->'asset_allocation'->'Equity_Weight') = 'number'\n        AND (allocweights->'asset_allocation'->>'Equity_Weight')::numeric > 0\n        AND jsonb_typeof(fundmetrics->'Yield_Rate') = 'number'\n        AND (fundmetrics->>'Yield_Rate')::numeric > 0\n)\n-- Step 2: The final SELECT joins these two sets to find the funds that are in both,\n-- fulfilling the definition of a \"Reliable Core Holding\".\nSELECT\n    f.tickersym,\n    f.shortlabel,\n    f.parentgroup,\n    f.productclass,\n    f.launchdate\nFROM funds f\nWHERE\n    f.tickersym IN (SELECT tickersym FROM ConsistentOutperformers)\n    AND f.tickersym IN (SELECT tickersym FROM TotalReturnFunds);"], "external_knowledge": [46, 15, 9], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n        query_view = \"\"\"\n        SELECT tickersym FROM vw_reliable_core_holdings;\n        \"\"\"\n        result, _, _ = execute_queries(query_view, db_name, conn)\n\n        tickers_in_view = [row[0] for row in result]\n        assert tickers_in_view, \"No tickers found in vw_reliable_core_holdings.\"\n\n        for ticker in tickers_in_view:\n            print(f\"Validating ticker: {ticker}\")\n\n            risk_sql = f\"\"\"\n            SELECT (risk5y->'risk_measures_5y'->>'Alpha_5Y')::numeric\n            FROM risk_metrics\n            WHERE investmentref = '{ticker}';\n            \"\"\"\n            risk_result, _, _ = execute_queries(risk_sql, db_name, conn)\n            alpha = float(risk_result[0][0])\n            assert alpha > 0, f\"Alpha not positive for ticker {ticker}: {alpha}\"\n\n            perf_sql = f\"\"\"\n            SELECT (histstats->>'Positive_Years')::numeric,\n                (histstats->>'Negative_Years')::numeric\n            FROM performance\n            WHERE productref = '{ticker}';\n            \"\"\"\n            perf_result, _, _ = execute_queries(perf_sql, db_name, conn)\n            pos_years = float(perf_result[0][0])\n            neg_years = float(perf_result[0][1])\n            total = pos_years + neg_years\n            consistency = pos_years / total if total != 0 else 0\n            assert consistency > 0.8, f\"Consistency too low for {ticker}: {consistency:.2f}\"\n\n            fund_sql = f\"\"\"\n            SELECT \n                (allocweights->'asset_allocation'->>'Equity_Weight')::numeric,\n                (fundmetrics->>'Yield_Rate')::numeric\n            FROM funds\n            WHERE tickersym = '{ticker}';\n            \"\"\"\n            fund_result, _, _ = execute_queries(fund_sql, db_name, conn)\n            equity_weight = float(fund_result[0][0])\n            yield_rate = float(fund_result[0][1])\n            assert equity_weight > 0, f\"Equity weight is 0 for {ticker}\"\n            assert yield_rate > 0, f\"Yield rate is 0 for {ticker}\"\n\n            print(f\"Passed: {ticker} | alpha={alpha}, consistency={consistency:.2f}, equity={equity_weight}, yield={yield_rate}\")\n\n        print(\"All tickers in vw_reliable_core_holdings passed the validation checks.\")\n        return True\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_M_4", "selected_database": "exchange_traded_funds", "query": "Let's make a refreshable summary table called family_risk_summary to track the family risk profile for each fund company. For each company, store the family name, and can you calculate their average market risk (3-year beta), their median risk-adjusted return (3-year Sharpe Ratio), and the number of funds that generates more than 0 alpha in a 5 year period? When the query is run, it should just update the table with these fresh calculations and the current time.", "normal_query": "I need to perform an upsert operation on a summary table named family_risk_summary. This table should store a family risk profile for each fund family. For each family, calculate and store the family name, the average 3-year beta, the median 3-year sharpe ratio, a total count of their alpha generator funds, and a timestamp of when the record was last updated.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Step 1 (DDL): Create a new table to hold the summary data.\nCREATE TABLE IF NOT EXISTS family_risk_summary (\n    family_name VARCHAR(255) PRIMARY KEY,\n    avg_beta_3y NUMERIC(5, 2),\n    median_sharpe_3y NUMERIC(5, 2),\n    alpha_generator_count INTEGER,\n    last_updated TIMESTAMP\n);\n\n-- Step 2 (DML): Calculate the metrics and insert/update the summary table.\nWITH FamilyMetrics AS (\n    -- Sub-step 2a: Extract the raw risk metrics for each fund and its family.\n    SELECT\n        f.parentgroup,\n        (r.risk3y->'risk_measures_3y'->>'Beta_3Y')::numeric AS beta,\n        (r.risk3y->'risk_measures_3y'->>'Sharpe_Ratio_3Y')::numeric AS sharpe,\n        CASE WHEN (r.risk5y->'risk_measures_5y'->>'Alpha_5Y')::numeric > 0 THEN 1 ELSE 0 END AS is_alpha_generator\n    FROM funds f\n    JOIN risk_metrics r ON f.tickersym = r.investmentref\n    WHERE f.parentgroup IS NOT NULL\n      AND jsonb_typeof(r.risk3y->'risk_measures_3y'->'Beta_3Y') = 'number'\n      AND jsonb_typeof(r.risk3y->'risk_measures_3y'->'Sharpe_Ratio_3Y') = 'number'\n      AND jsonb_typeof(r.risk5y->'risk_measures_5y'->'Alpha_5Y') = 'number'\n)\n-- Sub-step 2b: Use advanced aggregation functions to calculate the desired stats for each family.\nINSERT INTO family_risk_summary (family_name, avg_beta_3y, median_sharpe_3y, alpha_generator_count, last_updated)\nSELECT\n    parentgroup,\n    AVG(beta) AS avg_beta_3y,\n    -- PERCENTILE_CONT is used to calculate the median, which is robust to outliers.\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY sharpe) AS median_sharpe_3y,\n    SUM(is_alpha_generator) AS alpha_generator_count,\n    NOW()\nFROM FamilyMetrics\nGROUP BY parentgroup\n-- Sub-step 2c: The ON CONFLICT clause makes this an \"upsert\". If a family record exists, it's updated; otherwise, it's inserted.\nON CONFLICT (family_name) DO UPDATE\nSET\n    avg_beta_3y = EXCLUDED.avg_beta_3y,\n    median_sharpe_3y = EXCLUDED.median_sharpe_3y,\n    alpha_generator_count = EXCLUDED.alpha_generator_count,\n    last_updated = NOW();"], "external_knowledge": [10, 22, 24], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n        import math\n        import numpy\n\n        for sql in pred_sqls:\n            execute_queries(sql, db_name, conn)\n\n        summary_sql = \"\"\"\n        SELECT family_name, avg_beta_3y, median_sharpe_3y, alpha_generator_count\n        FROM family_risk_summary\n        LIMIT 1;\n        \"\"\"\n        result, _, _ = execute_queries(summary_sql, db_name, conn)\n        assert result, \"family_risk_summary is empty.\"\n\n        family_name, avg_beta_summary, median_sharpe_summary, alpha_gen_summary = result[0]\n        print(f\"üîç Validating family: {family_name}\")\n\n        raw_sql = f\"\"\"\n        SELECT\n            (r.risk3y->'risk_measures_3y'->>'Beta_3Y')::numeric AS beta,\n            (r.risk3y->'risk_measures_3y'->>'Sharpe_Ratio_3Y')::numeric AS sharpe,\n            CASE WHEN (r.risk5y->'risk_measures_5y'->>'Alpha_5Y')::numeric > 0 THEN 1 ELSE 0 END AS is_alpha_generator\n        FROM funds f\n        JOIN risk_metrics r ON f.tickersym = r.investmentref\n        WHERE f.parentgroup = '{family_name}'\n        AND jsonb_typeof(r.risk3y->'risk_measures_3y'->'Beta_3Y') = 'number'\n        AND jsonb_typeof(r.risk3y->'risk_measures_3y'->'Sharpe_Ratio_3Y') = 'number'\n        AND jsonb_typeof(r.risk5y->'risk_measures_5y'->'Alpha_5Y') = 'number';\n        \"\"\"\n        data_rows, _, _ = execute_queries(raw_sql, db_name, conn)\n        assert data_rows, f\"No source data found for family '{family_name}'.\"\n\n        betas = [float(row[0]) for row in data_rows if row[0] is not None]\n        sharpes = [float(row[1]) for row in data_rows if row[1] is not None]\n        alpha_gens = [int(row[2]) for row in data_rows]\n\n        expected_avg_beta = round(sum(betas) / len(betas), 2) if betas else None\n        expected_median_sharpe = round(numpy.median(numpy.array(sharpes)), 2) if sharpes else None\n        expected_alpha_count = sum(alpha_gens)\n\n        assert math.isclose(avg_beta_summary, expected_avg_beta, abs_tol=0.01), \\\n            f\"Avg beta mismatch: expected {expected_avg_beta}, got {avg_beta_summary}\"\n        assert math.isclose(median_sharpe_summary, expected_median_sharpe, abs_tol=0.01), \\\n            f\"Median sharpe mismatch: expected {expected_median_sharpe}, got {median_sharpe_summary}\"\n        assert alpha_gen_summary == expected_alpha_count, \\\n            f\"Alpha generator count mismatch: expected {expected_alpha_count}, got {alpha_gen_summary}\"\n\n        print(f\"Passed: {family_name} | beta={avg_beta_summary}, sharpe={median_sharpe_summary}, alpha_count={alpha_gen_summary}\")\n        return True"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_M_6", "selected_database": "exchange_traded_funds", "query": "Let's rebuild our summary of funds that are basically just expensive index trackers. Can you wipe the closet_indexer_summary table clean and then fill it again (create it if the table doesn't exists)? For every fund that just copies the market but still charges more than its benchmark, I want to see how much money is being wasted in extra fees. Put the fund's ticker, the company name, and that wasted fee amount into the table.", "normal_query": "I need to refresh the closet_indexer_summary table. Please ensure the table exists, then clear all its existing data. Afterward, identify all closet indexer funds, calculate their total wasted fee amount, and insert the fund's ticker symbol, its family name, and the calculated amount into the table.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Step 1 (DDL): Define the summary table to hold the analysis results.\nCREATE TABLE IF NOT EXISTS closet_indexer_summary (\n    fund_ticker VARCHAR(20) PRIMARY KEY,\n    family_name VARCHAR(255),\n    wasted_fee_amount_usd NUMERIC(20, 2)\n);\n\n-- Step 2 (TCL/DDL): Clear any existing data from the table before the new load.\n-- TRUNCATE is faster than DELETE for clearing an entire table.\nTRUNCATE TABLE closet_indexer_summary;\n\n-- Step 3 (DML): Insert the freshly calculated data into the now-empty summary table.\nINSERT INTO closet_indexer_summary (fund_ticker, family_name, wasted_fee_amount_usd)\nWITH ClosetIndexers AS (\n    -- Sub-step 3a: Identify funds that meet the definition of a \"Closet Indexer\".\n    -- This requires being a \"Market-Tracking Fund\" (ID: 11) with a positive \"Relative Expense Ratio\" (ID: 1).\n    SELECT\n        f.tickersym,\n        f.parentgroup,\n        (f.fundmetrics->>'Net_Worth')::numeric AS net_worth,\n        ((f.fundmetrics->>'Expense_Net')::numeric - (f.fundmetrics->>'Benchmark_Exp')::numeric) AS relative_expense\n    FROM funds f\n    JOIN risk_metrics r ON f.tickersym = r.investmentref\n    WHERE\n        jsonb_typeof(r.risk3y->'risk_measures_3y'->'R_Squared_3Y') = 'number'\n        AND (r.risk3y->'risk_measures_3y'->>'R_Squared_3Y')::numeric > 90 -- Market-Tracking Fund\n        AND jsonb_typeof(f.fundmetrics->'Expense_Net') = 'number'\n        AND jsonb_typeof(f.fundmetrics->'Benchmark_Exp') = 'number'\n        AND ((f.fundmetrics->>'Expense_Net')::numeric - (f.fundmetrics->>'Benchmark_Exp')::numeric) > 0 -- Positive Relative Expense\n        AND jsonb_typeof(f.fundmetrics->'Net_Worth') = 'number' -- Ensure net worth is available for calculation.\n)\n-- Sub-step 3b: Calculate the \"Wasted Fee Amount\" (ID: 86) and select the final columns for insertion.\nSELECT\n    tickersym,\n    parentgroup,\n    (net_worth * relative_expense) AS wasted_fee_amount_usd\nFROM ClosetIndexers;\n"], "external_knowledge": [86, 60, 1, 11], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n        import math\n\n        fetch_summary_sql = \"\"\"\n            SELECT fund_ticker, family_name, wasted_fee_amount_usd\n            FROM closet_indexer_summary\n            LIMIT 1;\n        \"\"\"\n        result, _, _ = execute_queries(fetch_summary_sql, db_name, conn)\n        assert result, \"closet_indexer_summary is empty.\"\n\n        fund_ticker, family_name, wasted_fee_recorded = result[0]\n        print(f\"Verifying fund: {fund_ticker} ({family_name})\")\n\n        fetch_fund_data_sql = f\"\"\"\n            SELECT\n                (f.fundmetrics->>'Net_Worth')::numeric,\n                (f.fundmetrics->>'Expense_Net')::numeric,\n                (f.fundmetrics->>'Benchmark_Exp')::numeric\n            FROM funds f\n            JOIN risk_metrics r ON f.tickersym = r.investmentref\n            WHERE f.tickersym = '{fund_ticker}'\n                AND (r.risk3y->'risk_measures_3y'->>'R_Squared_3Y')::numeric > 90 -- Market-Tracking\n                AND ((f.fundmetrics->>'Expense_Net')::numeric - (f.fundmetrics->>'Benchmark_Exp')::numeric) > 0\n                AND f.fundmetrics->>'Net_Worth' IS NOT NULL;\n        \"\"\"\n        rows, _, _ = execute_queries(fetch_fund_data_sql, db_name, conn)\n        assert rows, f\"Fund {fund_ticker} does not meet Closet Indexer criteria anymore.\"\n\n        net_worth, Expense_Net, Benchmark_Exp = map(float, rows[0])\n        relative_expense = Expense_Net - Benchmark_Exp\n        expected_wasted_fee = round(net_worth * relative_expense, 2)\n\n        assert math.isclose(wasted_fee_recorded, expected_wasted_fee, abs_tol=0.01), (\n            f\"Wasted fee mismatch: expected {expected_wasted_fee}, got {wasted_fee_recorded}\"\n        )\n\n        print(f\"Fund '{fund_ticker}' has correct wasted fee amount: ${wasted_fee_recorded:.2f}\")\n        return True\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_M_7", "selected_database": "exchange_traded_funds", "query": "I need to clean up the annual_returns table by moving old data into a separate archive. Let‚Äôs create a procedure called archive_old_returns that takes a number of years as input‚Äîthis tells it how far back we want to keep. Any return records older than that should be moved to annual_returns_archive, and then deleted from the main table. This is important because we use the archived data to calculate long-term outperformance. Once the procedure is ready, run it to archive everything older than 10 years.", "normal_query": "I need to move historical data from annual_returns which is an active table, to an archive table called annual_returns_archive. You should create a procedure, named archive_old_returns, and it must be designed to take an integer for the number of years to retain. It should move records from annual_returns to annual_returns_archive if they are older than the specified retention period, and then delete the moved records. This is important because the archived data is the basis for calculating annual fund outperformance. Please create the procedure and then CALL it to archive all records older than 10 years.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nCREATE TABLE IF NOT EXISTS annual_returns_archive (\n    yearlyid INTEGER,\n    portfolioref VARCHAR,\n    calendaryear INTEGER,\n    fundperf NUMERIC,\n    categoryperf NUMERIC,\n    archived_at TIMESTAMP\n);\n\n-- Step 2 (DDL): Create the procedure that encapsulates the archiving logic.\nCREATE OR REPLACE PROCEDURE archive_old_returns(p_years_to_keep INTEGER)\nLANGUAGE plpgsql\nAS $$\nBEGIN\n    -- Sub-step 2a: Insert the old records into the archive table.\n    -- The threshold is calculated based on the current year minus the number of years to keep.\n    INSERT INTO annual_returns_archive (yearlyid, portfolioref, calendaryear, fundperf, categoryperf, archived_at)\n    SELECT\n        yearlyid,\n        portfolioref,\n        calendaryear,\n        fundperf,\n        categoryperf,\n        NOW()\n    FROM annual_returns\n    WHERE calendaryear < (EXTRACT(YEAR FROM NOW()) - p_years_to_keep);\n\n    -- Sub-step 2b: Delete the same records from the primary annual_returns table.\n    DELETE FROM annual_returns\n    WHERE calendaryear < (EXTRACT(YEAR FROM NOW()) - p_years_to_keep);\n\n    -- Commit the transaction implicitly upon successful completion.\nEND;\n$$;\n\n-- Step 3: Execute the procedure to run the archiving process.\nCALL archive_old_returns(10);\n"], "external_knowledge": [0], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n        import datetime\n        current_year = datetime.datetime.now().year\n        cutoff_year = current_year - 10\n\n        count_old_returns_sql = f\"\"\"\n            SELECT COUNT(*) FROM annual_returns\n            WHERE calendaryear < {cutoff_year};\n        \"\"\"\n        pre_count_result, _, _ = execute_queries(count_old_returns_sql, db_name, conn)\n        old_record_count = pre_count_result[0][0]\n        assert old_record_count == 0, \"No old records to archive; test is inconclusive.\"\n\n        \n        count_new_returns_sql = f\"\"\"\n            SELECT COUNT(*) FROM annual_returns_archive\n            WHERE calendaryear < {cutoff_year};\n        \"\"\"\n        count_result, _, _ = execute_queries(count_new_returns_sql, db_name, conn)\n        assert count_result[0][0] > 0, \"Archive failed\"\n        \n        return True\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_M_8", "selected_database": "exchange_traded_funds", "query": "I want to automatically get an alert when a fund's investment strategy seems to be changing. Can you set up a trigger function called log_style_drift that keeps a log for us? Whenever a fund's risk numbers get updated in the risk_metrics table, I want the function to check if its market risk or its correlation to the benchmark has shifted significantly. If it has, please add a new line to a style_drift_log table with the fund's ticker symbol, the old and new risk values, and when it happened. If the style_drift_log table does not exist, please create it first.", "normal_query": "I need you to implement a trigger to monitor for style drift in funds, and this function should be called log_style_drift. First, create a new table named style_drift_log to record these events. Then, create a trigger function that fires after any update on the risk_metrics table.  If the change meets the definition of style drift, it should insert a new record into style_drift_log containing the fund's ticker symbol, the old and new beta/R-squared values, and the current timestamp.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Step 1 (DDL): Define the log table to capture style drift events.\nCREATE TABLE IF NOT EXISTS style_drift_log (\n    log_id SERIAL PRIMARY KEY,\n    fund_ticker VARCHAR(20),\n    old_beta NUMERIC,\n    new_beta NUMERIC,\n    old_rsquared NUMERIC,\n    new_rsquared NUMERIC,\n    log_timestamp TIMESTAMP\n);\n\n-- Step 2 (DDL): Create the trigger function that contains the logging logic.\nCREATE OR REPLACE FUNCTION log_style_drift()\nRETURNS TRIGGER AS $$\nDECLARE\n    -- Declare variables to hold old and new risk values.\n    old_beta_3y NUMERIC := (OLD.risk3y->'risk_measures_3y'->>'Beta_3Y')::numeric;\n    new_beta_3y NUMERIC := (NEW.risk3y->'risk_measures_3y'->>'Beta_3Y')::numeric;\n    old_rsquared_3y NUMERIC := (OLD.risk3y->'risk_measures_3y'->>'R_Squared_3Y')::numeric;\n    new_rsquared_3y NUMERIC := (NEW.risk3y->'risk_measures_3y'->>'R_Squared_3Y')::numeric;\nBEGIN\n    -- Sub-step 2a: Check if the change in beta or r-squared exceeds the thresholds defined in \"Style Drift\" (ID: 79).\n    IF ABS(new_beta_3y - old_beta_3y) > 0.15 OR ABS(new_rsquared_3y - old_rsquared_3y) > 10 THEN\n        -- Sub-step 2b: If a significant drift is detected, insert a record into the log table.\n        INSERT INTO style_drift_log (fund_ticker, old_beta, new_beta, old_rsquared, new_rsquared, log_timestamp)\n        VALUES (NEW.investmentref, old_beta_3y, new_beta_3y, old_rsquared_3y, new_rsquared_3y, NOW());\n    END IF;\n    -- Return NEW to allow the original UPDATE operation to proceed.\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Step 3 (DDL): Create the trigger and attach it to the risk_metrics table.\n-- This trigger will fire AFTER every UPDATE on the table.\nCREATE OR REPLACE TRIGGER trg_risk_metrics_style_drift\nAFTER UPDATE ON risk_metrics\nFOR EACH ROW\nEXECUTE FUNCTION log_style_drift();"], "external_knowledge": [77, 78, 79], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n        import time\n        import json\n\n        select_sql = \"\"\"\n            SELECT investmentref, risk3y\n            FROM risk_metrics\n            WHERE risk3y->'risk_measures_3y' IS NOT NULL\n            LIMIT 1;\n        \"\"\"\n        result, _, _ = execute_queries(select_sql, db_name, conn)\n        ticker, risk3y = result[0]\n\n        beta = float(risk3y['risk_measures_3y']['Beta_3Y'])\n        rsq = float(risk3y['risk_measures_3y']['R_Squared_3Y'])\n\n        new_beta = beta + 0.2  \n        new_rsq = rsq - 11     \n\n        risk3y['risk_measures_3y']['Beta_3Y'] = new_beta\n        risk3y['risk_measures_3y']['R_Squared_3Y'] = new_rsq\n\n        updated_json = json.dumps(risk3y)\n\n        update_sql = f\"\"\"\n            UPDATE risk_metrics\n            SET risk3y = '{updated_json}'\n            WHERE investmentref = '{ticker}';\n        \"\"\"\n        execute_queries(update_sql, db_name, conn)\n\n        time.sleep(0.5)\n\n        check_sql = f\"\"\"\n            SELECT fund_ticker, old_beta, new_beta, old_rsquared, new_rsquared\n            FROM style_drift_log\n            WHERE fund_ticker = '{ticker}'\n            ORDER BY log_timestamp DESC\n            LIMIT 1;\n        \"\"\"\n        result, _, _ = execute_queries(check_sql, db_name, conn)\n        assert result, f\"No style drift log found for {ticker}\"\n\n        log_entry = result[0]\n        assert abs(log_entry[2] - log_entry[1]) > 0.15 or abs(log_entry[4] - log_entry[3]) > 10, \\\n            f\"Trigger fired but changes were not significant: {log_entry}\"\n\n        return True"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_M_9", "selected_database": "exchange_traded_funds", "query": "Let's add a new performance stat to the funds table. First, make sure there's a column called rrei_score, and that's where we'll store each fund's index of risk-return efficiency. Then, set up a function called calculate_rrei that figures out this score based on the fund's ticker. Once that's ready, go ahead and fill in the rrei_score for every fund by calling the function for each one.", "normal_query": "I want to enrich the funds table with a new calculated metric. First, add a new rrei_score column if it's not already there. Next, create a function calculate_rrei that computes the risk-return efficiency index for a given fund ticker. Finally, update the funds table to populate the rrei_score for every fund by calling this new function.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["ALTER TABLE funds ADD COLUMN IF NOT EXISTS rrei_score NUMERIC(10, 4);\n\n-- Step 2 (DDL): Create the function to encapsulate the calculation logic.\n-- This function takes a fund's ticker symbol and returns its RREI score.\nCREATE OR REPLACE FUNCTION calculate_rrei(p_tickersym VARCHAR)\nRETURNS NUMERIC AS $$\nDECLARE\n    -- Declare variables to hold the intermediate values.\n    v_roc NUMERIC;\n    v_volatility_3y NUMERIC;\nBEGIN\n    -- Sub-step 2a: Calculate 'Return on Cost (ROC)' (Knowledge ID: 2).\n    SELECT (p.returnmetrics->'fund_returns'->>'Return_1Y')::numeric /\n           NULLIF((f.fundmetrics->>'Expense_Net')::numeric, 0)\n    INTO v_roc\n    FROM funds f\n    JOIN performance p ON f.tickersym = p.productref\n    WHERE f.tickersym = p_tickersym;\n\n    -- Sub-step 2b: Get the 3-year volatility.\n    SELECT (risk3y->'risk_measures_3y'->>'Volatility_3Y')::numeric\n    INTO v_volatility_3y\n    FROM risk_metrics\n    WHERE investmentref = p_tickersym;\n\n    -- Sub-step 2c: Return the final RREI score, handling potential division by zero.\n    IF v_volatility_3y IS NULL OR v_volatility_3y = 0 OR v_roc IS NULL THEN\n        RETURN NULL;\n    ELSE\n        RETURN v_roc / v_volatility_3y;\n    END IF;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Step 3 (DML): Update the funds table by applying the new function to each fund.\nUPDATE funds\nSET rrei_score = calculate_rrei(tickersym);"], "external_knowledge": [39, 2], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n        import math\n\n        pick_sql = \"\"\"\n            SELECT f.tickersym\n            FROM funds f\n            JOIN performance p ON f.tickersym = p.productref\n            JOIN risk_metrics r ON f.tickersym = r.investmentref\n            WHERE jsonb_typeof(f.fundmetrics->'Expense_Net') = 'number'\n                AND jsonb_typeof(p.returnmetrics->'fund_returns'->'Return_1Y') = 'number'\n                AND jsonb_typeof(r.risk3y->'risk_measures_3y'->'Volatility_3Y') = 'number'\n            LIMIT 1;\n        \"\"\"\n        result, _, _ = execute_queries(pick_sql, db_name, conn)\n        ticker = result[0][0]\n\n        fetch_sql = f\"\"\"\n            SELECT \n                (p.returnmetrics->'fund_returns'->>'Return_1Y')::numeric AS r1y,\n                (f.fundmetrics->>'Expense_Net')::numeric AS expensenet,\n                (r.risk3y->'risk_measures_3y'->>'Volatility_3Y')::numeric AS vol3y\n            FROM funds f\n            JOIN performance p ON f.tickersym = p.productref\n            JOIN risk_metrics r ON f.tickersym = r.investmentref\n            WHERE f.tickersym = '{ticker}';\n        \"\"\"\n        result, _, _ = execute_queries(fetch_sql, db_name, conn)\n        r1y, expensenet, vol3y = result[0]\n\n        if expensenet == 0 or vol3y == 0 or r1y is None or expensenet is None or vol3y is None:\n            expected = None\n        else:\n            roc = r1y / expensenet\n            expected = round(roc / vol3y, 4)\n\n        func_sql = f\"SELECT calculate_rrei('{ticker}');\"\n        result, _, _ = execute_queries(func_sql, db_name, conn)\n        actual_func_result = result[0][0]\n        if actual_func_result is not None:\n            actual_func_result = round(float(actual_func_result), 4)\n\n        table_sql = f\"SELECT rrei_score FROM funds WHERE tickersym = '{ticker}';\"\n        result, _, _ = execute_queries(table_sql, db_name, conn)\n        actual_table_result = result[0][0]\n        if actual_table_result is not None:\n            actual_table_result = round(float(actual_table_result), 4)\n\n        assert math.isclose(actual_func_result, expected, rel_tol=1e-6), \\\n            f\"Function result mismatch: expected {expected}, got {actual_func_result}\"\n        assert math.isclose(actual_table_result, expected, rel_tol=1e-6), \\\n            f\"Table update mismatch: expected {expected}, got {actual_table_result}\"\n\n        return True"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "exchange_traded_funds_M_10", "selected_database": "exchange_traded_funds", "query": "Let's create and keep a summary table called family_sector_focus up-to-date. I want to see which single industry each fund company is most heavily invested in. For each company, give the company's name, find their top industry and the average investment percentage in it and then either add them to the table or update their existing entry with this new info and the time of the update.", "normal_query": "I want to insert a new summary record or update an existing one, ensuring data freshness without duplicates to track the family sector concentration profile. Ensure a table named family_sector_focus exists. Then, for each fund family, insert or update the table with the family name, the top sector name, the average weight in that sector, and the current timestamp.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\nCREATE TABLE IF NOT EXISTS family_sector_focus (\n    family_name VARCHAR(255) PRIMARY KEY,\n    top_sector_name TEXT,\n    avg_weight_in_top_sector NUMERIC(5, 4),\n    last_updated TIMESTAMP\n);\n\n-- Step 2 (DML): Calculate and upsert the data into the summary table.\nWITH FamilySectorWeights AS (\n    -- Sub-step 2a: Join funds, sectors, and allocations to get the weight of each sector for each family.\n    SELECT\n        f.parentgroup,\n        s.industrytag,\n        sa.weightpct\n    FROM funds f\n    JOIN sector_allocations sa ON f.tickersym = sa.productlink\n    JOIN sectors s ON sa.sectorlink = s.secid\n    WHERE f.parentgroup IS NOT NULL\n),\nRankedFamilySectors AS (\n    -- Sub-step 2b: For each family, calculate the average weight per sector and rank them to find the top one.\n    -- The ROW_NUMBER() window function is perfect for ranking within groups.\n    SELECT\n        parentgroup,\n        industrytag,\n        AVG(weightpct) as avg_weight,\n        ROW_NUMBER() OVER(PARTITION BY parentgroup ORDER BY AVG(weightpct) DESC) as rn\n    FROM FamilySectorWeights\n    GROUP BY parentgroup, industrytag\n)\n-- Sub-step 2c: Insert the top-ranked sector for each family into our summary table.\n-- The ON CONFLICT clause handles the \"upsert\" logic: if the family_name already exists, it updates the record.\nINSERT INTO family_sector_focus (family_name, top_sector_name, avg_weight_in_top_sector, last_updated)\nSELECT\n    parentgroup,\n    industrytag,\n    avg_weight,\n    NOW() -- Use the current timestamp for the update time.\nFROM RankedFamilySectors\nWHERE rn = 1\nON CONFLICT (family_name) DO UPDATE\nSET\n    top_sector_name = EXCLUDED.top_sector_name,\n    avg_weight_in_top_sector = EXCLUDED.avg_weight_in_top_sector,\n    last_updated = NOW();\n"], "external_knowledge": [28, 87], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n        import math\n\n        pick_family_sql = \"\"\"\n            SELECT f.parentgroup\n            FROM funds f\n            JOIN sector_allocations sa ON f.tickersym = sa.productlink\n            WHERE f.parentgroup IS NOT NULL\n            LIMIT 1;\n        \"\"\"\n        result, _, _ = execute_queries(pick_family_sql, db_name, conn)\n        family_name = result[0][0]\n\n        manual_check_sql = f\"\"\"\n            SELECT s.industrytag, AVG(sa.weightpct) AS avg_weight\n            FROM funds f\n            JOIN sector_allocations sa ON f.tickersym = sa.productlink\n            JOIN sectors s ON sa.sectorlink = s.secid\n            WHERE f.parentgroup = '{family_name}'\n            GROUP BY s.industrytag\n            ORDER BY avg_weight DESC\n            LIMIT 1;\n        \"\"\"\n        result, _, _ = execute_queries(manual_check_sql, db_name, conn)\n        expected_sector, expected_avg_weight = result[0]\n        expected_avg_weight = round(float(expected_avg_weight), 4)\n\n        fetch_summary_sql = f\"\"\"\n            SELECT top_sector_name, avg_weight_in_top_sector\n            FROM family_sector_focus\n            WHERE family_name = '{family_name}';\n        \"\"\"\n        result, _, _ = execute_queries(fetch_summary_sql, db_name, conn)\n        actual_sector, actual_avg_weight = result[0]\n        actual_avg_weight = round(float(actual_avg_weight), 4)\n\n        assert actual_sector == expected_sector,             f\"Sector mismatch: expected {expected_sector}, got {actual_sector}\"\n        assert math.isclose(actual_avg_weight, expected_avg_weight, rel_tol=1e-6),             f\"Average weight mismatch: expected {expected_avg_weight}, got {actual_avg_weight}\"\n\n        print(f\"Sector focus test passed for '{family_name}': sector = {actual_sector}, avg_weight = {actual_avg_weight}\")\n        return True\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "disaster_relief_1", "selected_database": "disaster_relief", "query": "We need to pinpoint which operations must be flagged as critical. Let's find any operation responding to a 'Catastrophic' level disaster. For these, show me their ID, the area they are in, and that final disaster severity score, rounded to two decimals.", "normal_query": "Generate a report of all operations that require escalation to 'Critical' priority. An operation qualifies if it is responding to a disaster classified as 'Catastrophic'. The report must include the operation's reference ID, the affected area, and the specific DSI value, rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH PIS_Calc AS (\n    SELECT\n        distregistry,\n        (10 * COALESCE((impact_summary -> 'population_impact' ->> 'casualties')::real, 0)) + (3 * COALESCE((impact_summary -> 'population_impact' ->> 'injured')::real, 0)) + (2 * COALESCE((impact_summary -> 'population_impact' ->> 'missing')::real, 0)) + (1 * COALESCE((impact_summary -> 'population_impact' ->> 'displaced')::real, 0)) AS population_impact_score\n    FROM disasterevents\n), IDI_Calc AS (\n    SELECT\n        distregistry,\n        (COALESCE((impact_summary -> 'infrastructure_damage' ->> 'infra_damage_pct')::real, 0) + COALESCE((impact_summary -> 'infrastructure_damage' ->> 'power_outage_pct')::real, 0) + COALESCE((impact_summary -> 'infrastructure_damage' ->> 'water_damage_pct')::real, 0)) / 3 AS infrastructure_damage_index\n    FROM disasterevents\n), DSI_Calculation AS (\n    SELECT\n        p.distregistry,\n        (0.6 * p.population_impact_score) + (0.4 * i.infrastructure_damage_index) AS DSI\n    FROM PIS_Calc p\n    JOIN IDI_Calc i ON p.distregistry = i.distregistry\n)\nSELECT\n  hr.hr_ops_ref,\n  de.affectedarea,\n  ROUND(dsi.DSI::numeric, 2)\nFROM DSI_Calculation AS dsi\nJOIN disasterevents de ON dsi.distregistry = de.distregistry\nJOIN humanresources AS hr ON de.distregistry = hr.hrdistref\nWHERE\n  dsi.DSI > 15000;"], "external_knowledge": [51, 16, 8, 2], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "disaster_relief_2", "selected_database": "disaster_relief", "query": "For planning purposes, I need to get a handle on our resource situation across all disasters. Could you pull a report for every event showing what it was and where it happened, and then calculate two key things for me? First, how many days will our current supplies last, considering food and water as separate constraints? Second, what is the current shelter shortfall? Please round any calculated numbers to two decimal places.", "normal_query": "Provide a logistical status report for all recorded disasters. The report must contain the disaster ID, hazard type, and affected area, along with two calculated metrics, rounded to two decimal places: the 'Supply Sufficiency (Days)' and the 'Shelter Gap'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n  T1.distregistry,\n  T1.haztype,\n  T1.affectedarea,\n  ROUND(LEAST(\n    (T2.foodtons * 1000) / NULLIF(((T1.impact_summary -> 'population_impact' ->> 'affected')::real * 2), 0),\n    T2.water_liters / NULLIF(((T1.impact_summary -> 'population_impact') ->> 'affected')::real * 3, 0)\n  )::numeric, 2) AS supply_sufficiency_days,\n  GREATEST(0, COALESCE((T1.impact_summary -> 'population_impact' ->> 'displaced')::int, 0) - COALESCE(T2.shelter_units, 0)) AS shelter_gap\nFROM disasterevents AS T1\nINNER JOIN supplies AS T2\n  ON T1.distregistry = T2.supply_dist_ref;"], "external_knowledge": [3, 12], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "disaster_relief_3", "selected_database": "disaster_relief", "query": "I need to flag any disaster that was a Mass Casualty Incident. Can you pull a list of them and show me the ID and location for each?", "normal_query": "Identify all disasters classified as a Mass Casualty Incident (MCI). For each qualifying disaster, return its ID and the affected area.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT T1.distregistry, T1.affectedarea FROM disasterevents AS T1 JOIN humanresources AS T2 ON T1.distregistry = T2.hrdistref WHERE (COALESCE((T1.impact_summary -> 'population_impact' ->> 'casualties')::real, 0) + COALESCE((T1.impact_summary -> 'population_impact' ->> 'injured')::real, 0)) > 100 OR (COALESCE((T2.staffing_details -> 'staff_counts' ->> 'medical')::real, 0) / NULLIF(COALESCE((T1.impact_summary -> 'population_impact' ->> 'casualties')::real, 0) + COALESCE((T1.impact_summary -> 'population_impact' ->> 'injured')::real, 0), 0)) < 0.01;"], "external_knowledge": [21, 9], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "disaster_relief_4", "selected_database": "disaster_relief", "query": "How are our distribution hubs holding up? Let's create a report for each hub. I want a strain score that considers both how busy the hub is internally and how severe the disasters are that it's serving. If that final score is over a million, flag it as 'Overwhelmed', otherwise it's 'Normal'. Show me the hub ID, the final score rounded to two decimals, and its status.", "normal_query": "Let's assess the status of our distribution hubs. For each hub, provide its ID and its calculated Hub Strain Index (HSI), rounded to two decimal places. Also include a 'status' column classifying the hub as 'Overwhelmed' if its HSI exceeds 1,000,000, and 'Normal' otherwise.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH DisasterDSIs AS (\n    SELECT\n        distregistry,\n        (0.6 * ((10 * COALESCE((impact_summary -> 'population_impact' ->> 'casualties')::real, 0)) + (3 * COALESCE((impact_summary -> 'population_impact' ->> 'injured')::real, 0)) + (2 * COALESCE((impact_summary -> 'population_impact' ->> 'missing')::real, 0)) + (1 * COALESCE((impact_summary -> 'population_impact' ->> 'displaced')::real, 0)))) + (0.4 * ((COALESCE((impact_summary -> 'infrastructure_damage' ->> 'infra_damage_pct')::real, 0) + COALESCE((impact_summary -> 'infrastructure_damage' ->> 'power_outage_pct')::real, 0) + COALESCE((impact_summary -> 'infrastructure_damage' ->> 'water_damage_pct')::real, 0)) / 3)) AS dsi\n    FROM disasterevents\n), HubExternalDemand AS (\n    SELECT \n        s.supplyhubref AS hub_id,\n        AVG(d.dsi) AS avg_dsi_served\n    FROM supplies s\n    JOIN DisasterDSIs d ON s.supply_dist_ref = d.distregistry\n    GROUP BY s.supplyhubref\n), HubStrain AS (\n    SELECT \n        h.hubregistry,\n        (0.5 * ((COALESCE(h.hubutilpct, 0) / 100) + (COALESCE(h.stockturnrate, 0) / 10))) + (0.5 * COALESCE(hed.avg_dsi_served, 0)) AS hsi\n    FROM distributionhubs h\n    LEFT JOIN HubExternalDemand hed ON h.hubregistry = hed.hub_id\n)\nSELECT \n    hubregistry, \n    ROUND(hsi::numeric, 2) as hub_strain_index,\n    CASE\n        WHEN hsi > 1000000 THEN 'Overwhelmed'\n        ELSE 'Normal'\n    END AS status\nFROM HubStrain;"], "external_knowledge": [7, 16, 8, 2, 31], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "disaster_relief_5", "selected_database": "disaster_relief", "query": "I'm worried about operations running out of cash. Can you identify which are in a Funding Crisis? For that list, show me their ID, funding status, and exactly how many days they have left, to one decimal place.", "normal_query": "Identify all ongoing operations that are in a Funding Crisis. The report should include the operation ID, its funding state, and the specific Budget Runway in days, rounded to one decimal place.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH FinancialMetrics AS (\n    SELECT\n        f.fin_ops_ref AS opsregistry,\n        f.funding_state,\n        (REPLACE(REPLACE(f.budgetallotusd, '$', ''), ',', ''))::numeric AS allocated_budget,\n        f.ops_costs_usd AS total_operational_costs,\n        f.ops_costs_usd / NULLIF((CURRENT_DATE - o.ops_start_date), 0) AS daily_burn_rate\n    FROM financials f\n    JOIN operations o ON f.fin_ops_ref = o.opsregistry\n    WHERE o.opsstatus != 'Completed'\n),\nBudgetRunway AS (\n    SELECT\n        opsregistry,\n        funding_state,\n        CASE\n            WHEN daily_burn_rate > 0\n            THEN (allocated_budget - total_operational_costs) / daily_burn_rate\n            ELSE NULL\n        END AS runway_days\n    FROM FinancialMetrics\n)\nSELECT\n    opsregistry,\n    funding_state,\n    ROUND(runway_days::numeric, 1) AS runway_days\nFROM BudgetRunway\nWHERE\n    funding_state = 'Critical' AND runway_days < 7;"], "external_knowledge": [24, 11, 4], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 1, "distinct": false, "order": false}}
{"instance_id": "disaster_relief_6", "selected_database": "disaster_relief", "query": "I want to do a post-mortem on missions that didn't go well. Can you pull up the full coordination and evaluation details for any completed operations we've flagged as a Failing Operation?", "normal_query": "For post-mission analysis, retrieve the complete coordination and evaluation records for all completed operations that are classified as a Failing Operation.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH LTV_Calc AS (\n    SELECT\n        hr.hr_ops_ref AS opsregistry,\n        (t.delivery_metrics -> 'delivery_capacity' ->> 'daily_tons')::real /\n        NULLIF(t.vehiclecount, 0) AS ltv\n    FROM transportation t\n    JOIN humanresources hr ON t.transportdistref = hr.hrdistref\n),\nRES_Calc AS (\n    SELECT\n        b.beneopsref AS opsregistry,\n        (0.4 * COALESCE((t.delivery_metrics -> 'delivery_performance' ->> 'success_rate')::real, 0)) +\n        (0.3 * COALESCE(b.bene_feedbackscore, 0) * 10) +\n        (0.3 * COALESCE(b.distequityidx, 0) * 100) AS res\n    FROM beneficiariesandassessments b\n    JOIN humanresources hr ON b.beneopsref = hr.hr_ops_ref\n    JOIN transportation t ON hr.hrdistref = t.transportdistref\n),\nFailing_Ops_To_Select AS (\n    SELECT\n        o.opsregistry\n    FROM operations o\n    JOIN LTV_Calc ltv ON o.opsregistry = ltv.opsregistry\n    JOIN RES_Calc res ON o.opsregistry = res.opsregistry\n    WHERE\n        o.opsstatus = 'Completed'\n        AND (ltv.ltv < 1.0 OR res.res < 40)\n)\nSELECT c.*\nFROM coordinationandevaluation c\nJOIN Failing_Ops_To_Select d ON c.coordopsref = d.opsregistry;"], "external_knowledge": [32, 6, 14], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "disaster_relief_7", "selected_database": "disaster_relief", "query": "Let's find our top performers. Can you identify all missions that qualify as a Highly Effective Operation? Please list their ID, along with their RES and CQI scores. Sort the list to show the most effective ones at the top.", "normal_query": "Generate a report identifying all operations designated as a Highly Effective Operation. The report must include the operation ID, its Response Effectiveness Score (RES), and its Coordination Quality Index (CQI), sorted by RES in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH RES_Calc AS (SELECT b.beneopsref AS opsregistry, (0.4 * COALESCE((t.delivery_metrics -> 'delivery_performance' ->> 'success_rate')::real, 0)) + (0.3 * COALESCE(b.bene_feedbackscore, 0) * 10) + (0.3 * COALESCE(b.distequityidx, 0) * 100) AS res FROM beneficiariesandassessments AS b JOIN humanresources AS hr ON b.beneopsref = hr.hr_ops_ref JOIN transportation AS t ON hr.hrdistref = t.transportdistref), CQI_Calc AS (SELECT coordopsref AS opsregistry, (COALESCE(report_compliance, 0) + COALESCE(stakeholder_satisf, 0) * 20 + COALESCE(dataqualityvalue, 0) * 20) / 3.0 AS cqi FROM coordinationandevaluation) SELECT res_c.opsregistry, res_c.res::numeric AS response_effectiveness_score, cqi_c.cqi::numeric AS coordination_quality_index FROM RES_Calc AS res_c JOIN CQI_Calc AS cqi_c ON res_c.opsregistry = cqi_c.opsregistry WHERE res_c.res > 75 AND cqi_c.cqi > 75 ORDER BY res_c.res DESC;"], "external_knowledge": [28, 14, 15], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "disaster_relief_8", "selected_database": "disaster_relief", "query": "Let's find any areas on the brink of a major health crisis. Can you run the numbers and find all operations where the public health risk score is over 70? This score should be based on things like disease risk, sanitation, and medical staff ratios. For any you find, just list the operation's ID and that final risk score, rounded to two decimal places.", "normal_query": "Produce a report identifying all operations that should be flagged for a Public Health Emergency. An operation is flagged if its calculated Public Health Risk Score (PHRS) exceeds 70. For each flagged operation, show the operation ID and its calculated PHRS, rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH PublicHealthData AS (\n  SELECT\n    hr.hr_ops_ref AS opsregistry,\n    (\n      COALESCE((hr.staffing_details -> 'staff_counts' ->> 'medical') :: real, 0) / NULLIF(\n        COALESCE((de.impact_summary -> 'population_impact' ->> 'casualties') :: real, 0) + COALESCE((de.impact_summary -> 'population_impact' ->> 'injured') :: real, 0),\n        0\n      )\n    ) AS msr,\n    CASE (eh.health_environment_profile -> 'health' ->> 'disease_risk')\n      WHEN 'High' THEN 3\n      WHEN 'Medium' THEN 2\n      ELSE 1\n    END AS disease_risk_score,\n    100 - COALESCE((eh.health_environment_profile -> 'environment' ->> 'sanitation_coverage') :: real, 100) AS sanitation_risk,\n    100 - COALESCE(\n      REPLACE(\n        (eh.health_environment_profile -> 'environment' ->> 'water_quality_index'),\n        ' WQI',\n        ''\n      ) :: real,\n      100\n    ) AS water_quality_risk\n  FROM disasterevents AS de\n  JOIN humanresources AS hr\n    ON de.distregistry = hr.hrdistref\n  JOIN environmentandhealth AS eh\n    ON de.distregistry = eh.env_dist_ref\n),\nPHRS_Calc AS (\n  SELECT\n    opsregistry,\n    (\n      (1 - COALESCE(msr, 0)) * 40 +\n      disease_risk_score * 10 +\n      sanitation_risk * 0.15 +\n      water_quality_risk * 0.15\n    ) AS phrs\n  FROM PublicHealthData\n)\nSELECT\n  opsregistry,\n  ROUND(phrs::numeric, 2) as public_health_risk_score\nFROM PHRS_Calc\nWHERE\n  phrs > 70;"], "external_knowledge": [26, 19, 9], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "disaster_relief_9", "selected_database": "disaster_relief", "query": "A simple list of stuck operations isn't enough; we need to know which ones are the biggest emergencies. Can you rank all operations in Logistical Gridlock by how critical they are using the Gridlock Severity Index? Then, show me a list of the worst ones at the top, with their location and that severity score, rounded to two decimals.", "normal_query": "Generate a prioritized report of all operations in Logistical Gridlock. The report should rank operations by a calculated 'Gridlock Severity Index'. Return the operation ID, affected area, and the index, rounded to two decimal places, sorted with the most severe cases first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH Gridlocked_Ops_Details AS (\n  SELECT\n    T1.opsregistry,\n    CASE LOWER(T1.priority_rank::TEXT)\n      WHEN 'critical' THEN 1\n      WHEN 'high' THEN 2\n      WHEN 'medium' THEN 3\n      WHEN 'low' THEN 4\n      ELSE 5\n    END AS priority_rank_numeric,\n    T3.affectedarea,\n    CASE LOWER(T3.impact_summary -> 'communication_and_transport' ->> 'transport_access')\n      WHEN 'minimal' THEN 1.0\n      WHEN 'limited' THEN 0.5\n      ELSE 0.1\n    END AS transport_restriction_score,\n    (T2.staffing_details -> 'staff_counts' ->> 'total')::int AS total_personnel\n  FROM operations AS T1\n  INNER JOIN humanresources AS T2\n    ON T1.opsregistry = T2.hr_ops_ref\n  INNER JOIN disasterevents AS T3\n    ON T2.hrdistref = T3.distregistry\n  WHERE\n    LOWER(T1.supply_flow_state::TEXT) = 'disrupted' AND LOWER(T3.impact_summary -> 'communication_and_transport' ->> 'transport_access') = 'minimal'\n)\nSELECT\n  opsregistry,\n  affectedarea,\n  ROUND(\n    ( (10 - priority_rank_numeric) * 0.5 ) + \n    ( total_personnel * 0.2 ) + \n    ( transport_restriction_score * 30 )\n  , 2) AS gridlock_severity_index\nFROM Gridlocked_Ops_Details\nORDER BY gridlock_severity_index DESC;"], "external_knowledge": [25, 40], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "disaster_relief_10", "selected_database": "disaster_relief", "query": "Give me a bird's-eye view of how our regions are performing. Can you produce a summary showing each region (in lowercase), their average coordination quality score (rounded to two decimals), and a simple count of how many disasters they've handled?", "normal_query": "I need a regional performance summary. For each region tag (in lowercase), calculate and display the average Coordination Quality Index (CQI), rounded to two decimal places, and the total count of disasters that have occurred in that region.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH CQI_Calc AS (\n    SELECT \n        T1.region_tag,\n        T1.distregistry,\n        (COALESCE(T2.report_compliance, 0) + COALESCE(T2.stakeholder_satisf, 0) * 20 + COALESCE(T2.dataqualityvalue, 0) * 20) / 3.0 AS cqi\n    FROM disasterevents AS T1\n    INNER JOIN coordinationandevaluation AS T2 ON T1.distregistry = T2.coord_dist_ref\n)\nSELECT\n  LOWER(region_tag) AS region,\n  ROUND(AVG(cqi)::numeric, 2) AS average_cqi,\n  COUNT(distregistry) AS disaster_count\nFROM CQI_Calc\nGROUP BY\n  LOWER(region_tag);"], "external_knowledge": [15], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "disaster_relief_11", "selected_database": "disaster_relief", "query": "Let's get a big picture metric on our efficiency. For all the missions we've officially wrapped up, how much did it cost us on average to help a single person? Just give me that final dollar amount, rounded to two decimal places.", "normal_query": "For all 'Completed' operations, calculate the overall 'Cost Per Beneficiary (CPB)'. The final value should be rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH CompletedOpsCosts AS (\n  SELECT\n    COALESCE(f.ops_costs_usd, 0) + COALESCE(f.transportcostsusd, 0) + COALESCE(f.storage_costs_usd, 0) + COALESCE(f.personnelcostsusd, 0) AS total_cost,\n    (de.impact_summary -> 'population_impact' ->> 'affected')::numeric AS affected_population\n  FROM operations AS o\n  JOIN financials AS f\n    ON o.opsregistry = f.fin_ops_ref\n  JOIN humanresources AS hr\n    ON o.opsregistry = hr.hr_ops_ref\n  JOIN disasterevents AS de\n    ON hr.hrdistref = de.distregistry\n  WHERE\n    LOWER(o.opsstatus::TEXT) = 'completed'\n)\nSELECT\n  ROUND(\n    (SUM(total_cost) / NULLIF(SUM(affected_population), 0))::numeric,\n    2\n  )\nFROM CompletedOpsCosts;"], "external_knowledge": [52], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "disaster_relief_12", "selected_database": "disaster_relief", "query": "I want to know how our vehicles are holding up in the absolute worst conditions. For all those disasters where we could barely get in—the ones marked with 'Minimal' transport access—what's our average vehicle breakdown rate? Just give me that single percentage, rounded to one decimal.", "normal_query": "Determine the average vehicle breakdown rate for all transportation units involved in disasters where transport access is rated as 'Minimal'. Present the result as a percentage rounded to one decimal place.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n  ROUND(\n    AVG((T1.delivery_metrics -> 'vehicle_metrics' ->> 'break_rate') :: numeric),\n    1\n  )\nFROM transportation AS T1\nINNER JOIN disasterevents AS T2\n  ON T1.transportdistref = T2.distregistry\nWHERE\n  LOWER(T2.impact_summary -> 'communication_and_transport' ->> 'transport_access') = 'minimal';"], "external_knowledge": [53], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 1, "distinct": false, "order": false}}
{"instance_id": "disaster_relief_M_1", "selected_database": "disaster_relief", "query": "Let's add a supply status flag to each disaster record to make our dashboards easier to read. Can you update our records based on how many days their supplies will last? If it's less than two days, tag it 'Critical'. If it's under five, call it 'Low'. Anything else is 'Adequate'. Please also store the calculated number of days in there.", "normal_query": "Update the impact_summary JSONB field for all disaster events to include a new 'supply_status' object. This object should contain the calculated supply sufficiency in days and a status classification based on that value. Classify the status as 'Critical' for a runway under 2 days, 'Low' for under 5 days, and 'Adequate' otherwise.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH supply_calc AS (\n    SELECT \n        d.distregistry,\n        LEAST(\n            (s.foodtons * 1000) / NULLIF(((d.impact_summary -> 'population_impact' ->> 'affected')::real * 2), 0),\n            s.water_liters / NULLIF(((d.impact_summary -> 'population_impact' ->> 'affected')::real * 3), 0)\n        ) AS sufficiency_days\n    FROM disasterevents d\n    JOIN supplies s ON d.distregistry = s.supply_dist_ref\n)\nUPDATE disasterevents de\nSET impact_summary = jsonb_set(\n    COALESCE(de.impact_summary, '{}'::jsonb),\n    '{supply_status}',\n    jsonb_build_object(\n        'status', CASE \n            WHEN sc.sufficiency_days < 2 THEN 'Critical'\n            WHEN sc.sufficiency_days < 5 THEN 'Low'\n            ELSE 'Adequate'\n        END,\n        'days', sc.sufficiency_days\n    )\n)\nFROM supply_calc sc\nWHERE de.distregistry = sc.distregistry\nRETURNING de.distregistry;"], "external_knowledge": [3, 23], "test_cases": [], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "disaster_relief_M_2", "selected_database": "disaster_relief", "query": "We need a standard way to figure out our cost per ton for aid delivery. Let's create a reusable function, call it calculate_adc, that does this for us. It should take a transportation ID, find the associated costs and total tons delivered, and return the cost per ton. Also, build in a sanity check: if delivery tons are missing or zero, it should throw an error instead of dividing by zero.", "normal_query": "Create a PL/pgSQL function named calculate_adc that calculates the aid delivery cost per ton. The function should accept a single transportation ID as an input parameter. It must compute the total transport costs associated with that ID and divide it by the total tons delivered. The function must include validation to raise an exception if the total delivery tons are null or zero.", "preprocess_sql": [], "clean_up_sqls": ["DROP FUNCTION IF EXISTS calculate_adc(VARCHAR(20));"], "sol_sql": ["CREATE OR REPLACE FUNCTION calculate_adc(p_transport_id VARCHAR(20))\nRETURNS DECIMAL AS $$\nDECLARE\n    total_cost DECIMAL;\n    total_tons DECIMAL;\nBEGIN\n    SELECT SUM(f.transportcostsusd) INTO total_cost\n    FROM financials f\n    JOIN transportation t ON f.findistref = t.transportdistref\n    WHERE t.transportregistry = p_transport_id;\n\n    SELECT (delivery_metrics -> 'delivery_capacity' ->> 'total_tons')::DECIMAL INTO total_tons\n    FROM transportation \n    WHERE transportregistry = p_transport_id;\n\n    IF total_tons IS NULL OR total_tons <= 0 THEN\n        RAISE EXCEPTION 'Invalid or zero delivery tons for ID %',\n        p_transport_id;\n    END IF;\n    \n    RETURN total_cost / total_tons;\nEND;\n$$ LANGUAGE plpgsql;"], "external_knowledge": [10], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    cursor = conn.cursor()\n\n    try:\n        cursor.execute(\"SELECT calculate_adc('TRANS_PK3RF');\")\n        result = cursor.fetchone()[0]\n        assert result > 0, f\"Function returned a non-positive value: {result}\"\n    except Exception as e:\n        raise AssertionError(f\"Function failed on a valid ID 'TRANS_PK3RF'. Error: {e}\")\n\n    try:\n        cursor.execute(\"BEGIN;\")\n        cursor.execute(\"INSERT INTO transportation (transportregistry, transportdistref, delivery_metrics) VALUES ('TEST_ID_0_TONS', 'DIST_BI3UF', '{\\\"delivery_capacity\\\": {\\\"total_tons\\\": 0}}');\")\n        cursor.execute(\"INSERT INTO financials (financeregistry, findistref, transportcostsusd) VALUES ('FIN_TEST_0', 'DIST_BI3UF', 1000);\")\n        \n        try:\n            cursor.execute(\"SELECT calculate_adc('TEST_ID_0_TONS');\")\n            raise AssertionError(\"The function did not raise an exception when it should have.\")\n        except Exception as e:\n            assert 'Invalid or zero delivery tons' in str(e), f\"Incorrect exception message: {e}\"\n\n    finally:\n        cursor.execute(\"ROLLBACK;\")\n\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "disaster_relief_M_3", "selected_database": "disaster_relief", "query": "I want to set up some automatic financial red flags. Can you build a function that checks all our active operations and alerts us based on two rules? First, if an active operation has already spent more than 20 percent of its total budget, raise a 'High Burn Rate' alert. Second, calculate how many days of funding they have left at their current spending rate; if it's less than a week, raise a 'Funding Crisis' alert.", "normal_query": "Create a function named get_financial_alerts that returns a table of alerts for active operations. It should generate a 'High Burn Rate' alert if an operation's total costs exceed 20% of its allocated budget. It should also generate a 'Funding Crisis' alert if the calculated budget runway, based on the daily burn rate, is less than 7 days.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION get_financial_alerts() RETURNS TABLE (ops_id VARCHAR, alert_type TEXT, message TEXT) AS $$ DECLARE op_record RECORD; days_active REAL; burn_rate REAL; total_budget REAL; days_remaining REAL; BEGIN FOR op_record IN SELECT o.opsregistry, o.ops_start_date, f.ops_costs_usd, CAST(REPLACE(REPLACE(f.budgetallotusd, '$', ''), ',', '') AS REAL) AS budget FROM operations AS o JOIN financials AS f ON o.opsregistry = f.fin_ops_ref WHERE o.opsstatus = 'Active' LOOP IF op_record.budget IS NULL OR op_record.budget <= 0 THEN CONTINUE; END IF; total_budget := op_record.budget; days_active := GREATEST(1.0, EXTRACT(EPOCH FROM (NOW() - op_record.ops_start_date)) / 86400.0); IF op_record.ops_costs_usd > (total_budget * 0.20) THEN ops_id := op_record.opsregistry; alert_type := 'High Burn Rate'; message := 'Operation has spent more than 20% of its total budget.'; RETURN NEXT; END IF; IF total_budget > op_record.ops_costs_usd AND op_record.ops_costs_usd >= 0 THEN IF days_active > 0 THEN burn_rate := op_record.ops_costs_usd / days_active; IF burn_rate > 0 THEN days_remaining := (total_budget - op_record.ops_costs_usd) / burn_rate; IF days_remaining < 7.0 THEN ops_id := op_record.opsregistry; alert_type := 'Funding Crisis'; message := 'Operation has less than 7 days of funding remaining. Days left: ' || ROUND(days_remaining::NUMERIC, 2); RETURN NEXT; END IF; END IF; END IF; END IF; END LOOP; RETURN; END; $$ LANGUAGE plpgsql;"], "external_knowledge": [4, 24], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    cursor = conn.cursor()\n    try:\n        cursor.execute(\"BEGIN;\")\n        for sql in sol_sqls:\n            cursor.execute(sql)\n        cursor.execute(\"INSERT INTO operations (opsregistry, opsstatus, ops_start_date) VALUES ('OPS_OK', 'Active', NOW() - INTERVAL '10 days');\")\n        cursor.execute(\"INSERT INTO financials (financeregistry, fin_ops_ref, budgetallotusd, ops_costs_usd) VALUES ('FIN_OK', 'OPS_OK', '$100,000.00', 10000);\")\n        cursor.execute(\"INSERT INTO operations (opsregistry, opsstatus, ops_start_date) VALUES ('OPS_BURN', 'Active', NOW() - INTERVAL '5 days');\")\n        cursor.execute(\"INSERT INTO financials (financeregistry, fin_ops_ref, budgetallotusd, ops_costs_usd) VALUES ('FIN_BURN', 'OPS_BURN', '$100,000.00', 25000);\")\n        cursor.execute(\"INSERT INTO operations (opsregistry, opsstatus, ops_start_date) VALUES ('OPS_CRISIS', 'Active', NOW() - INTERVAL '30 days');\")\n        cursor.execute(\"INSERT INTO financials (financeregistry, fin_ops_ref, budgetallotusd, ops_costs_usd) VALUES ('FIN_CRISIS', 'OPS_CRISIS', '$100,000.00', 95000);\")\n        cursor.execute(\"INSERT INTO operations (opsregistry, opsstatus, ops_start_date) VALUES ('OPS_INACTIVE', 'Completed', NOW() - INTERVAL '30 days');\")\n        cursor.execute(\"INSERT INTO financials (financeregistry, fin_ops_ref, budgetallotusd, ops_costs_usd) VALUES ('FIN_INACTIVE', 'OPS_INACTIVE', '$100,000.00', 99000);\")\n        cursor.execute(\"SELECT ops_id, alert_type FROM get_financial_alerts() WHERE ops_id IN ('OPS_OK', 'OPS_BURN', 'OPS_CRISIS', 'OPS_INACTIVE') ORDER BY ops_id, alert_type;\")\n        results = cursor.fetchall()\n        expected = [('OPS_BURN', 'High Burn Rate'), ('OPS_CRISIS', 'Funding Crisis'), ('OPS_CRISIS', 'High Burn Rate')]\n        assert sorted(results) == sorted(expected), f\"Function did not produce the correct alerts for the controlled test data. Expected {expected}, got {results}\"\n        return 1\n    finally:\n        cursor.execute(\"ROLLBACK;\")"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_1", "selected_database": "households", "query": "I want to find the most 'plush' region. To do this, first figure out a 'comfort score' for each house by dividing its number of bathrooms by its number of residents—but only for houses where we know the bathroom count. Then, find the average comfort score for each region. Once you've identified the single region with the best average score, go back and add up the car counts for every household in that specific region and tell me the grand total.", "normal_query": "A 'Comfort Index' is calculated for each household by dividing its bathroom count by its resident count. Find the region (`locregion`) with the highest average 'Comfort Index', considering only households with a known bathroom count and at least one resident. For this top-ranking region, calculate the total number of cars ('Auto_Count') owned by all its households combined.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ComfortIndex AS (SELECT h.locregion, AVG((p.dwelling_specs ->> 'Bath_Count')::numeric / NULLIF(h.residentcount, 0)) AS avg_comfort FROM households h JOIN properties p ON h.housenum = p.houselink WHERE (p.dwelling_specs ->> 'Bath_Count') IS NOT NULL GROUP BY h.locregion), TopRegion AS (SELECT locregion FROM ComfortIndex ORDER BY avg_comfort DESC LIMIT 1) SELECT SUM((t.vehicleinventory -> 'vehicle_counts' ->> 'Auto_Count')::int) FROM transportation_assets t JOIN households h ON t.housetag = h.housenum WHERE h.locregion = (SELECT locregion FROM TopRegion);"], "external_knowledge": [15, 36], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_2", "selected_database": "households", "query": "Let's find the biggest welfare fraud hotspot. A family is a red flag if they get aid but have a lot of new vehicles (more than 2, newest from 2010+). I want to know which region has the highest concentration of these red-flag families as a percentage of their total population. Just give me the name of that region.", "normal_query": "To identify potential fraud hotspots, first define a 'High-Risk Household' as one that is 'Supported' (socsupport is 'Yes') and 'High-Mobility' (total vehicles > 2, newest vehicle 2010 or later). Then, for each region, calculate the percentage of its total households that are 'High-Risk'. Finally, return the `locregion` with the highest percentage of 'High-Risk' households.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH HighRisk AS (SELECT h.housenum, h.locregion FROM households h JOIN service_types s ON h.serviceplan = s.serviceref JOIN transportation_assets t ON h.housenum = t.housetag WHERE s.socsupport = 'Yes' AND (COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Auto_Count')::int, 0) + COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Bike_Count')::int, 0) + COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Motor_Count')::int, 0)) > 2 AND (t.vehicleinventory ->> 'Newest_Year' IN ('2010 to 2013', '2012 To 2013', '2014 or newer'))), RegionalCounts AS (SELECT locregion, COUNT(*) AS high_risk_count FROM HighRisk GROUP BY locregion), TotalCounts AS (SELECT locregion, COUNT(*) AS total_count FROM households GROUP BY locregion) SELECT rc.locregion FROM RegionalCounts rc JOIN TotalCounts tc ON rc.locregion = tc.locregion ORDER BY (rc.high_risk_count::numeric / tc.total_count) DESC LIMIT 1;"], "external_knowledge": [9, 10, 14, 23, 24], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "households_3", "selected_database": "households", "query": "We're looking for welfare fraud. Flag any family that gets aid and also has a lot of newish vehicles. Specifically, a 'high-mobility' family has more than two vehicles in total, and their newest one is from 2010 or later. Give me a unique list of the flagged household IDs.", "normal_query": "The government is investigating potential welfare fraud. A household is flagged for review if it meets two conditions simultaneously: 1) its `socsupport` status is 'Yes', AND 2) it is a 'High-Mobility' household. A 'High-Mobility' household is defined as one where the sum of all vehicles is greater than 2, AND its 'Newest_Year' is from 2010 or later.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT DISTINCT h.housenum FROM households h JOIN service_types s ON h.serviceplan = s.serviceref JOIN transportation_assets t ON h.housenum = t.housetag WHERE s.socsupport = 'Yes' AND (COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Auto_Count')::int, 0) + COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Bike_Count')::int, 0) + COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Motor_Count')::int, 0)) > 2 AND (t.vehicleinventory ->> 'Newest_Year' IN ('2010 to 2013', '2012 To 2013', '2014 or newer'));"], "external_knowledge": [9, 10, 14, 23, 24], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": true, "order": false}}
{"instance_id": "households_4", "selected_database": "households", "query": "Let's find all the 'roomy' apartments. A place is 'roomy' if it has more than 20 square meters per person. To figure out the total square meters, assume every bathroom is 10 sq meters and every bedroom (use the `Room_Count` field) is 15 sq meters—if a count is missing, just treat it as zero. After you get the total area, divide it by the number of people in that house, making sure to handle cases with no residents. Finally, just tell me how many apartments in total are 'roomy'.", "normal_query": "An urban planning initiative gives a 'Space Bonus'. To qualify, an apartment must have more than 20 square meters per resident. The total square meters is calculated as (the number of bathrooms * 10) plus (the number of bedrooms * 15), using the `Bath_Count` and `Room_Count` fields from `dwelling_specs` respectively. Any missing counts should be treated as 0. This total area is then divided by the household's resident count, avoiding any division-by-zero errors. Return the total count of apartments that qualify for the bonus.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT COUNT(*) FROM (SELECT ((COALESCE((p.dwelling_specs ->> 'Bath_Count')::numeric, 0) * 10 + COALESCE((p.dwelling_specs ->> 'Room_Count')::numeric, 0) * 15) / NULLIF(h.residentcount, 0)) AS sq_meters_per_resident FROM properties p JOIN households h ON p.houselink = h.housenum WHERE LOWER(p.dwelling_specs ->> 'Dwelling_Class') = 'apartment') AS space_details WHERE sq_meters_per_resident > 20;"], "external_knowledge": [27, 28, 29], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_5", "selected_database": "households", "query": "I need to find our longest-standing 'large, wealthy' family. A family qualifies if they own their home, are in the top two income brackets, and have more than 4 people. From that list, find the one with the lowest household ID (our oldest record) and tell me their region and zone.", "normal_query": "Identify 'affluent, large' families, defined as owner-occupied households in the top two income brackets with more than 4 residents. From this group, find the household that has been in the system the longest (i.e., has the lowest `housenum`). For this specific household, list its `locregion` and `loczone`.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH AffluentLarge AS (SELECT h.housenum, h.locregion, h.loczone FROM households h WHERE (h.socioeconomic ->> 'Income_Bracket' IN ('More than R$ 4,400', 'More than R$ 2,640 and less than R$ 4,400')) AND (UPPER(h.socioeconomic ->> 'Tenure_Type') = 'OWNED') AND h.residentcount > 4) SELECT locregion, loczone::INTEGER FROM AffluentLarge ORDER BY housenum ASC LIMIT 1;"], "external_knowledge": [1, 2, 21], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "households_6", "selected_database": "households", "query": "In the Taguatinga area, find all the 'overcrowded' homes (more than 3 people per bedroom). Once you have that list, figure out the average number of vehicles (cars, bikes, motorcycles all counted) that this specific group owns. Give me the final number, rounded.", "normal_query": "For the 'Taguatinga' region, calculate the 'crowding score' for each household. Identify all households with a score over 3. For this specific group of 'overcrowded' households, determine the average number of total vehicles they own (sum of autos, bikes, and motors), rounded to an integer.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH Overcrowded AS (SELECT h.housenum FROM households h JOIN properties p ON h.housenum = p.houselink WHERE LOWER(h.locregion) = 'taguatinga' AND (p.dwelling_specs ->> 'Room_Count')::numeric > 0 AND (h.residentcount / (p.dwelling_specs ->> 'Room_Count')::numeric) > 3) SELECT ROUND(AVG(COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Auto_Count')::int, 0) + COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Bike_Count')::int, 0) + COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Motor_Count')::int, 0))) FROM transportation_assets t JOIN Overcrowded o ON t.housetag = o.housenum;"], "external_knowledge": [11, 14], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 0, "distinct": false, "order": false}}
{"instance_id": "households_7", "selected_database": "households", "query": "Let's compare crowded city homes to crowded country homes. How many more crowded city homes are there than crowded country homes? Give me the difference.", "normal_query": "We want to compare two groups of households: 'Urban Crowded' and 'Rural Crowded', based on our established definitions for 'Urban' and 'Crowded' households. Calculate the count for each group and return the difference (Urban Crowded count - Rural Crowded count).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH CrowdedStatus AS (SELECT CASE WHEN LOWER(i.wateraccess) IN ('yes', 'available at least in one room') AND LOWER(i.roadsurface) IN ('asphalt', 'concrete') THEN 'Urban' ELSE 'Rural' END AS location_type FROM households h JOIN properties p ON h.housenum = p.houselink JOIN infrastructure i ON p.infralink = i.infraref WHERE (p.dwelling_specs ->> 'Room_Count')::numeric > 0 AND (h.residentcount / (p.dwelling_specs ->> 'Room_Count')::numeric) > 2) SELECT COUNT(CASE WHEN location_type = 'Urban' THEN 1 END) - COUNT(CASE WHEN location_type = 'Rural' THEN 1 END) FROM CrowdedStatus;"], "external_knowledge": [3, 4, 11, 22, 25], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_8", "selected_database": "households", "query": "Let's score each house's infrastructure. First, find the average score for each region. Then, tell me which region has the best average score and which has the worst, in a single line like 'BestRegion | WorstRegion'.", "normal_query": "Calculate the 'Infrastructure Score' for each household. Then, for each region, find the average score. Finally, list the region with the highest average score and the region with the lowest average score as a single string: '[Highest Region] | [Lowest Region]'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH InfraScores AS (SELECT h.locregion, (CASE WHEN LOWER(i.wateraccess) IN ('yes', 'available at least in one room') THEN 3 ELSE 0 END) + (CASE WHEN LOWER(i.roadsurface) IN ('asphalt', 'concrete') THEN 2 ELSE 0 END) + (CASE WHEN LOWER(i.parkavail) = 'available' THEN 1 ELSE 0 END) AS score FROM households h JOIN properties p ON h.housenum = p.houselink JOIN infrastructure i ON p.infralink = i.infraref), AvgScores AS (SELECT locregion, AVG(score) as avg_score FROM InfraScores GROUP BY locregion) SELECT (SELECT locregion FROM AvgScores ORDER BY avg_score DESC LIMIT 1) || ' | ' || (SELECT locregion FROM AvgScores ORDER BY avg_score ASC LIMIT 1);"], "external_knowledge": [3, 4, 5, 13], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_9", "selected_database": "households", "query": "Let's score each house's infrastructure. First, find the average score for each region. Then, tell me which region has the best average score and which has the worst, in a single line like 'BestRegion | WorstRegion'.", "normal_query": "Calculate the 'Infrastructure Score' for each household. Then, for each region, find the average score. Finally, list the region with the highest average score and the region with the lowest average score as a single string: '[Highest Region] | [Lowest Region]'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH InfraScores AS (SELECT TRIM(UPPER(h.locregion)) AS region, (CASE WHEN LOWER(i.wateraccess) IN ('yes', 'available at least in one room') THEN 3 ELSE 0 END) + (CASE WHEN LOWER(i.roadsurface) IN ('asphalt', 'concrete') THEN 2 ELSE 0 END) + (CASE WHEN LOWER(i.parkavail) = 'available' THEN 1 ELSE 0 END) AS score FROM households h JOIN properties p ON h.housenum = p.houselink JOIN infrastructure i ON p.infralink = i.infraref), AvgScores AS (SELECT region, AVG(score) as avg_score FROM InfraScores GROUP BY region) SELECT (SELECT region FROM AvgScores ORDER BY avg_score DESC LIMIT 1) || ' | ' || (SELECT region FROM AvgScores ORDER BY avg_score ASC LIMIT 1);"], "external_knowledge": [3, 4, 5, 13], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_10", "selected_database": "households", "query": "I'm looking for the hotspot of 'high-tech, high-mobility' families. These are people with lots of newish vehicles (more than 2, newest from 2005+) who also live in modern homes (house, apt, condo) with available TV service. Find all these families, then tell me which single region has the most of them.", "normal_query": "Identify households that are both 'High-Mobility' (more than 2 total vehicles, newest from 2005 or later) and 'High-Tech' (living in a modern dwelling like a house, apartment, or condo with available TV service). After finding this group, determine which `locregion` has the highest count of these specific households.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH TargetHouseholds AS (SELECT h.housenum, h.locregion FROM households h JOIN transportation_assets t ON h.housenum = t.housetag JOIN properties p ON h.housenum = p.houselink JOIN amenities a ON h.housenum = a.houseid WHERE (COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Auto_Count')::int, 0) + COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Bike_Count')::int, 0) + COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Motor_Count')::int, 0)) > 2 AND LOWER(t.vehicleinventory ->> 'Newest_Year') IN ('2005 to 2009', '2010 to 2013', '2012 to 2013', '2014 or newer') AND LOWER(p.dwelling_specs ->> 'Dwelling_Class') IN ('brickwork house', 'apartment') AND LOWER(a.cablestatus) IN ('avail', 'available', 'yes')) SELECT locregion FROM TargetHouseholds GROUP BY locregion ORDER BY COUNT(*) DESC LIMIT 1;"], "external_knowledge": [6, 7, 10, 14, 23, 26], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "households_11", "selected_database": "households", "query": "Let's find our top 10 families by their financial health score, which considers their income, spending habits, and homeownership status. After you get that list, tell me what percentage of them has a private garage.", "normal_query": "Using the 'Socioeconomic Index' (SEI), identify the top 10 households with the highest scores. Then, for this elite group, calculate the percentage of them that have access to a 'Private Garage'.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH HouseholdScores AS (SELECT h.housenum, i.parkavail, CASE h.socioeconomic ->> 'Income_Bracket' WHEN 'More than R$ 4,400' THEN 6 WHEN 'More than R$ 2,640 and less than R$ 4,400' THEN 5 WHEN 'More than R$ 1,760 and less than R$ 2,640' THEN 4 WHEN 'More than R$ 880 and less than R$ 1,760' THEN 3 WHEN 'More than R$ 880 and less' THEN 2 WHEN 'Has no income' THEN 1 ELSE 0 END AS income_score, CASE UPPER(h.socioeconomic ->> 'Tenure_Type') WHEN 'OWNED' THEN 3 WHEN 'RENTED' THEN 1 ELSE 0 END AS tenure_score, COALESCE((h.socioeconomic ->> 'Expend_Coeff')::numeric, 0.5) AS expend_coeff FROM households h JOIN properties p ON h.housenum = p.houselink JOIN infrastructure i ON p.infralink = i.infraref WHERE h.socioeconomic ->> 'Income_Bracket' IS NOT NULL AND (h.socioeconomic ->> 'Income_Bracket') <> 'Has no income'), SEI_Scores AS (SELECT housenum, parkavail, (0.4 * income_score + 0.4 * (1 - expend_coeff) + 0.2 * tenure_score) AS sei FROM HouseholdScores), Top10 AS (SELECT parkavail FROM SEI_Scores ORDER BY sei DESC LIMIT 10) SELECT ROUND(COALESCE(AVG(CASE WHEN UPPER(parkavail) = 'AVAILABLE' THEN 1.0 ELSE 0.0 END) * 100, 0), 2) FROM Top10;"], "external_knowledge": [1, 2, 5, 12, 19], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "households_12", "selected_database": "households", "query": "Find all the families that don't get social aid or domestic help and own more than one vehicle. From those, figure out which type of home has the highest average 'prosperity score'. Then, tell me the total number of vehicles owned by families in that home type.", "normal_query": "Identify all 'independent' households (defined as those receiving no social support or domestic help and owning more than one vehicle). Among them, find the dwelling class with the highest average 'Household Prosperity Score'. For that top-ranking dwelling class, what is the total number of vehicles owned by the 'independent' households living there?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH IndependentHouseholds AS (SELECT h.housenum, h.residentcount, LOWER(p.dwelling_specs ->> 'Dwelling_Class') AS dwelling_class, (COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Auto_Count')::int, 0) + COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Bike_Count')::int, 0) + COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Motor_Count')::int, 0)) AS total_vehicles, CASE LOWER(TRIM(h.socioeconomic ->> 'Income_Bracket')) WHEN 'low income' THEN 1 WHEN 'lower middle' THEN 2 WHEN 'middle income' THEN 3 WHEN 'upper middle' THEN 4 WHEN 'high income' THEN 5 WHEN 'very high income' THEN 6 ELSE 0 END AS income_score FROM households h LEFT JOIN service_types s ON h.serviceplan = s.serviceref JOIN transportation_assets t ON h.housenum = t.housetag JOIN properties p ON h.housenum = p.houselink WHERE (s.serviceref IS NULL OR (LOWER(s.domestichelp) = 'no domestic workers' AND s.socsupport = 'No')) AND (COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Auto_Count')::int, 0) + COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Bike_Count')::int, 0) + COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Motor_Count')::int, 0)) > 1), TopDwelling AS (SELECT dwelling_class FROM IndependentHouseholds WHERE income_score > 0 GROUP BY dwelling_class ORDER BY AVG(residentcount * income_score) DESC LIMIT 1) SELECT SUM(total_vehicles) FROM IndependentHouseholds WHERE dwelling_class = (SELECT dwelling_class FROM TopDwelling);"], "external_knowledge": [14, 32, 18, 42], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_13", "selected_database": "households", "query": "I want to find the region with the highest concentration of families on social support. For each region, figure out what percentage of its total families receive aid. Then, just show me the name of the region with the top percentage and the percentage itself, rounded to two decimal places.", "normal_query": "For each region, calculate the ratio of households receiving social support to the total number of households in that region. Return the name of the region with the highest percentage, along with the ratio expressed as a percentage rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH SupportCounts AS (SELECT locregion, COUNT(CASE WHEN LOWER(s.socsupport) = 'yes' THEN 1 END) AS support_count, COUNT(*) AS total_count FROM households h LEFT JOIN service_types s ON h.serviceplan = s.serviceref GROUP BY h.locregion) SELECT locregion, ROUND((support_count::numeric / NULLIF(total_count, 0)) * 100, 2) AS support_percentage FROM SupportCounts ORDER BY support_percentage DESC LIMIT 1;"], "external_knowledge": [9, 24], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "households_14", "selected_database": "households", "query": "Let's find the biggest spender among our 'comfortable' families. A family is 'comfortable' if they have a high living score and enough bathrooms for their size. First, make a list of all these families. Then, from that list, find the one with the highest spending number and tell me their household ID.", "normal_query": "A 'Comfortable Household' is defined as one with a 'Living Condition Score' over 3 and a bathroom-to-resident ratio over 0.5. First, identify all such households based on these criteria. Then, from this group, find the household with the highest 'Expenditure Coefficient' and return its `housenum`.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH LivingScores AS (SELECT h.housenum, h.socioeconomic ->> 'Expend_Coeff' AS expend_coeff, (0.5 * (CASE UPPER(p.dwelling_specs ->> 'Dwelling_Class') WHEN 'APARTMENT' THEN 4 WHEN 'BRICKWORK HOUSE' THEN 3 ELSE 1 END) + 0.5 * ((CASE WHEN UPPER(i.wateraccess) = 'MUNICIPAL PIPED' THEN 4 ELSE 1 END + CASE WHEN UPPER(i.roadsurface) IN ('PAVED ASPHALT', 'CONCRETE') THEN 4 ELSE 1 END + CASE WHEN UPPER(i.parkavail) = 'PRIVATE GARAGE' THEN 4 ELSE 1 END) / 3.0)) AS lcs, COALESCE((p.dwelling_specs ->> 'Bath_Count')::numeric, 0) / NULLIF(h.residentcount, 0) AS bathroom_ratio FROM households h JOIN properties p ON h.housenum = p.houselink JOIN infrastructure i ON p.infralink = i.infraref) SELECT housenum::BIGINT FROM LivingScores WHERE lcs > 2 AND bathroom_ratio > 0.5 AND expend_coeff IS NOT NULL ORDER BY expend_coeff::numeric DESC LIMIT 1;"], "external_knowledge": [2, 3, 4, 5, 6, 12, 13, 15, 20, 29], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "households_15", "selected_database": "households", "query": "I need to find the region with the biggest 'at-risk' population. A household is 'at-risk' if they get social aid AND they're very overcrowded (more than 4 people per bedroom). For every region, calculate what percentage of their families are 'at-risk'. Then, just tell me the name of the region with the highest percentage.", "normal_query": "Identify 'At-Risk' households, defined as those receiving social support and having a household density (residents per bedroom) over 4. Then, for each region, calculate the percentage of all its households that are 'At-Risk'. Finally, return the region with the highest percentage of 'At-Risk' households.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH AtRisk AS (SELECT h.housenum, TRIM(LOWER(h.locregion)) AS locregion FROM households h JOIN service_types s ON h.serviceplan = s.serviceref JOIN properties p ON h.housenum = p.houselink WHERE s.socsupport = 'Yes' AND (p.dwelling_specs ->> 'Room_Count')::numeric > 0 AND (h.residentcount / (p.dwelling_specs ->> 'Room_Count')::numeric) > 4), RegionalCounts AS (SELECT locregion, COUNT(*) AS at_risk_count FROM AtRisk GROUP BY locregion), TotalCounts AS (SELECT TRIM(LOWER(locregion)) AS locregion, COUNT(*) AS total_count FROM households GROUP BY TRIM(LOWER(locregion))) SELECT rc.locregion FROM RegionalCounts rc JOIN TotalCounts tc ON rc.locregion = tc.locregion ORDER BY (rc.at_risk_count::numeric / tc.total_count) DESC LIMIT 1;"], "external_knowledge": [9, 11, 24, 25], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "households_16", "selected_database": "households", "query": "What's the income bracket for household number 3?", "normal_query": "What is the income classification of the household with number 3?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT socioeconomic ->> 'Income_Bracket' AS income_classification FROM households WHERE housenum = 3;"], "external_knowledge": [2], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_17", "selected_database": "households", "query": "How many wealthy families who own their own homes live in the Taguatinga area?", "normal_query": "How many households in the 'Taguatinga' region are owner-occupied and fall within the top two income brackets?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT COUNT(*) FROM households WHERE TRIM(LOWER(locregion)) = 'taguatinga' AND LOWER(socioeconomic ->> 'Tenure_Type') = 'owned' AND LOWER(socioeconomic ->> 'Income_Bracket') IN ('high income', 'very high income');"], "external_knowledge": [1, 2, 21], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_18", "selected_database": "households", "query": "Which modern-style homes (like brickwork houses or apartments) in the 'Guará' area also have TV service? List their household numbers in order.", "normal_query": "List the household numbers for modern dwellings in the 'Guará' region, sorted by household number. A modern dwelling is defined as a specific dwelling type with available TV service.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT DISTINCT h.housenum FROM households h JOIN properties p ON h.housenum = p.houselink JOIN amenities a ON h.housenum = a.houseid WHERE TRIM(LOWER(h.locregion)) = 'guará' AND LOWER(p.dwelling_specs ->> 'Dwelling_Class') IN ('brickwork house', 'apartment', 'condominium') AND LOWER(a.cablestatus) IN ('avail', 'available', 'yes') ORDER BY h.housenum ASC;"], "external_knowledge": [6, 7, 26], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": true, "order": true}}
{"instance_id": "households_19", "selected_database": "households", "query": "Which household in an urban area owns the most cars?", "normal_query": "What is the household number with the most passenger vehicles among all households in urban areas (defined by infrastructure)?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH UrbanHouseholds AS (SELECT h.housenum FROM households h JOIN properties p ON h.housenum = p.houselink JOIN infrastructure i ON p.infralink = i.infraref WHERE LOWER(i.wateraccess) LIKE 'yes,%' AND LOWER(i.roadsurface) = 'asphalt, concrete') SELECT t.housetag FROM transportation_assets t JOIN UrbanHouseholds u ON t.housetag = u.housenum ORDER BY COALESCE((t.vehicleinventory -> 'vehicle_counts' ->> 'Auto_Count')::integer, 0) DESC LIMIT 1;"], "external_knowledge": [3, 4, 22, 36], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "households_20", "selected_database": "households", "query": "How many families in each region get government help? List the regions from the one with the most helped families to the one with the least.", "normal_query": "Count the number of households receiving social support in each region, sorted by count in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT TRIM(UPPER(h.locregion)) AS locregion, COUNT(*) AS supported_count FROM households h JOIN service_types s ON h.serviceplan = s.serviceref WHERE UPPER(s.socsupport) = 'YES' GROUP BY TRIM(UPPER(h.locregion)) ORDER BY supported_count DESC;"], "external_knowledge": [9, 24], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "households_M_1", "selected_database": "households", "query": "The city has a $10k fund to give cable to modern homes that don't have it. It costs $75 per house. First, check if we have enough money to cover everyone who's eligible. If we do, go ahead and update their status to 'Subscribed'. After you're done, tell me exactly how many homes got the upgrade.", "normal_query": "A municipal program with a budget of $10,000 aims to upgrade cable infrastructure for 'modern dwellings' that currently have 'No Service Available'. If the cost per household is $75, first determine if the total cost for all eligible households is within budget. If it is, perform the update to set their cable status to 'Subscribed'. Finally, return the total number of households that were successfully updated (which will be 0 if the budget was exceeded).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH Eligible AS (SELECT a.houseid FROM amenities a JOIN properties p ON a.houseid = p.houselink WHERE UPPER(a.cablestatus) = 'NOT AVAILABLE' AND UPPER(p.dwelling_specs ->> 'Dwelling_Class') IN ('BRICKWORK HOUSE', 'APARTMENT', 'CONDOMINIUM')), Cost AS (SELECT COUNT(*) * 75 AS total_cost FROM Eligible) SELECT CASE WHEN (SELECT total_cost FROM Cost) <= 10000 THEN (SELECT COUNT(*) FROM Eligible) ELSE 0 END;"], "external_knowledge": [6, 7, 26], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    pred_result, _, _ = execute_queries(pred_sqls, db_name, conn)\n    sol_result, _, _ = execute_queries(sol_sqls, db_name, conn)\n    assert pred_result == sol_result, f\"Prediction {pred_result} does not match solution {sol_result}\""], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_M_2", "selected_database": "households", "query": "I want to delete vehicle records for families with no income info, but only if it's a small cleanup. First, check what percentage of our total vehicle data this would remove. If it's less than 5%, go ahead and delete them, then tell me how many you deleted. If it's 5% or more, don't delete anything and just tell me '0'.", "normal_query": "As a data quality measure, we need to purge transportation assets for households with a null income bracket. However, to prevent accidental mass deletion, this operation is only permitted if the number of affected records is less than 5% of the total transportation assets. First, calculate the percentage of records that would be deleted. If this percentage is below 5%, proceed with the deletion and return the count of deleted records. Otherwise, return 0.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ToDelete AS (SELECT housenum FROM households WHERE socioeconomic ->> 'Income_Bracket' IS NULL), AffectedCount AS (SELECT COUNT(t.housetag) AS cnt FROM transportation_assets t JOIN ToDelete d ON t.housetag = d.housenum), TotalCount AS (SELECT COUNT(*) AS total FROM transportation_assets), Percentage AS (SELECT (AffectedCount.cnt::numeric / TotalCount.total) * 100 AS pct FROM AffectedCount, TotalCount), DeletedRows AS (DELETE FROM transportation_assets WHERE housetag IN (SELECT housenum FROM ToDelete) AND (SELECT pct FROM Percentage) < 5 RETURNING *) SELECT COUNT(*) FROM DeletedRows;"], "external_knowledge": [2, 31], "test_cases": [], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_M_4", "selected_database": "households", "query": "Add a new family: household 5000, Taguatinga, zone 315, 3 people, no social services, owns their home.", "normal_query": "Register a new household with number 5000 in 'Taguatinga', zone 315, with 3 residents, no service plan, and owned tenure.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["INSERT INTO households (housenum, residentcount, locregion, loczone, serviceplan, socioeconomic) VALUES (5000, 3, 'Taguatinga', 315, NULL, jsonb_build_object('Tenure_Type', 'OWNED')) ON CONFLICT (housenum) DO UPDATE SET residentcount = EXCLUDED.residentcount, locregion = EXCLUDED.locregion, loczone = EXCLUDED.loczone, serviceplan = EXCLUDED.serviceplan, socioeconomic = EXCLUDED.socioeconomic;"], "external_knowledge": [1, 32], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    # This test case verifies the state of the DB after each operation\n    cleanup = [\"DELETE FROM households WHERE housenum = 5000;\"]\n    check_sql = \"SELECT COUNT(*) FROM households WHERE housenum = 5000;\"\n\n    # Test prediction\n    execute_queries(cleanup, db_name, conn)\n    execute_queries(pred_sqls, db_name, conn)\n    pred_count, _, _ = execute_queries([check_sql], db_name, conn)\n    assert pred_count[0][0] == 1, \"Prediction did not insert the record correctly.\"\n\n    # Test solution\n    execute_queries(cleanup, db_name, conn)\n    execute_queries(sol_sqls, db_name, conn)\n    sol_count, _, _ = execute_queries([check_sql], db_name, conn)\n    assert sol_count[0][0] == 1, \"Solution did not insert the record correctly.\""], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_M_5", "selected_database": "households", "query": "Remove all vehicle records for families where we don't have their income information.", "normal_query": "Purge transportation assets for households with no income classification.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["DELETE FROM transportation_assets WHERE housetag IN (SELECT housenum FROM households WHERE socioeconomic ->> 'Income_Bracket' IS NULL);"], "external_knowledge": [2, 31], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    # This test case verifies a DELETE statement.\n    # The main DELETE query has already been executed by the evaluation framework.\n    \n    # Step 1: Define the verification query.\n    # This query counts how many records still exist that *should* have been deleted.\n    verification_query = \"\"\"SELECT COUNT(*) FROM transportation_assets WHERE housetag IN (SELECT housenum FROM households WHERE socioeconomic ->> 'Income_Bracket' IS NULL);\"\"\"\n    \n    try:\n        # Use the provided 'execute_queries' function to run the verification query.\n        result, _, _ = execute_queries([verification_query], db_name, conn)\n        \n        # Extract the count from the result.\n        count_after_delete = result[0][0]\n        \n        # Step 2: Assert that the count is 0.\n        assert count_after_delete == 0, f\"Test failed: Expected 0 records after deletion, but found {count_after_delete}.\"\n\n    except Exception as e:\n        assert False, f\"An error occurred during test case execution: {e}\""], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_M_6", "selected_database": "households", "query": "What type of building does household 1182 live in?", "normal_query": "What is the dwelling type of household number 1182?", "preprocess_sql": [], "clean_up_sqls": ["DROP VIEW IF EXISTS V_Dwelling_Type_1182;"], "sol_sql": ["CREATE OR REPLACE VIEW \"V_Dwelling_Type_1182\" AS SELECT p.dwelling_specs ->> 'Dwelling_Class' AS dwelling_type FROM households h JOIN properties p ON h.housenum = p.houselink WHERE h.housenum = 1182;", "SELECT * FROM \"V_Dwelling_Type_1182\";"], "external_knowledge": [6], "test_cases": [], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_M_7", "selected_database": "households", "query": "Count how many small living spaces (apartments or studios) house only one or two people.", "normal_query": "How many households are in compact dwellings (Apartment or Studio) with fewer than 3 residents?", "preprocess_sql": [], "clean_up_sqls": ["DROP VIEW IF EXISTS V_Compact_Household_Count;"], "sol_sql": ["CREATE OR REPLACE VIEW \"V_Compact_Household_Count\" AS SELECT COUNT(*) as compact_household_count FROM households h JOIN properties p ON h.housenum = p.houselink WHERE LOWER(p.dwelling_specs ->> 'Dwelling_Class') IN ('apartment', 'studio') AND h.residentcount < 3;", "SELECT * FROM \"V_Compact_Household_Count\";"], "external_knowledge": [6, 39], "test_cases": [], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_M_8", "selected_database": "households", "query": "How many well-maintained houses (like brickwork houses and apartments) also have TV service available?", "normal_query": "What is the total number of dwellings considered well-maintained, based on their type and available TV services?", "preprocess_sql": [], "clean_up_sqls": ["DROP VIEW IF EXISTS V_Well_Maintained_Dwellings_Count;"], "sol_sql": ["CREATE OR REPLACE VIEW \"V_Well_Maintained_Dwellings_Count\" AS SELECT COUNT(*) as well_maintained_count FROM properties p JOIN amenities a ON p.houselink = a.houseid WHERE LOWER(p.dwelling_specs ->> 'Dwelling_Class') IN ('brickwork house', 'apartment') AND LOWER(a.cablestatus) IN ('avail', 'available', 'yes');", "SELECT * FROM \"V_Well_Maintained_Dwellings_Count\";"], "external_knowledge": [7, 38, 43], "test_cases": [], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_M_9", "selected_database": "households", "query": "We have a $5000 monthly budget to survey premium city families. A family is 'premium' if they own a high-income home, have great infrastructure (city water, paved roads), and aren't overcrowded (no more than 2 people per bedroom). If each survey costs $150, what's the total cost, and are we over or under budget? Give me a summary like 'Cost: $XXXX, Budget Status: Within Budget'.", "normal_query": "A research institute has a total monthly budget of $5000 to identify and survey 'premium urban households'. A household qualifies as 'premium urban' if it meets three criteria: 1) It is owner-occupied with an income level of 'High Income' or 'Very High Income'. 2) It has piped water and resides on roads with asphalt or concrete surfaces. 3) The household's resident-to-bedroom ratio does not exceed 2. If the cost to survey each qualifying household is $150, calculate the total survey cost and determine if it is within the monthly budget. The final output should be a single string: 'Cost: [Total Cost], Budget Status: [Within Budget/Exceeds Budget]'.", "preprocess_sql": [], "clean_up_sqls": ["DROP VIEW IF EXISTS V_Survey_Budget_Analysis;"], "sol_sql": ["CREATE OR REPLACE VIEW \"V_Survey_Budget_Analysis\" AS WITH PremiumHouseholds AS (SELECT h.housenum FROM households h JOIN properties p ON h.housenum = p.houselink JOIN infrastructure i ON p.infralink = i.infraref WHERE (h.socioeconomic ->> 'Income_Bracket' IN ('High Income', 'Very High Income')) AND LOWER(h.socioeconomic ->> 'Tenure_Type') = 'owned' AND LOWER(i.wateraccess) IN ('yes', 'available at least in one room') AND LOWER(i.roadsurface) IN ('asphalt', 'concrete') AND (h.residentcount / NULLIF((p.dwelling_specs ->> 'Room_Count')::numeric, 0)) <= 2), CostCalculation AS (SELECT COUNT(*) * 150 AS total_cost FROM PremiumHouseholds) SELECT 'Cost: ' || total_cost || ', Budget Status: ' || CASE WHEN total_cost <= 5000 THEN 'Within Budget' ELSE 'Exceeds Budget' END AS budget_summary FROM CostCalculation;", "SELECT * FROM \"V_Survey_Budget_Analysis\";"], "external_knowledge": [1, 2, 3, 4, 11, 21, 22, 25], "test_cases": [], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "households_M_10", "selected_database": "households", "query": "I need to find the 5th most crowded house. First, figure out the 'people per bedroom' number for all crowded houses (more than 2 people/bedroom). Then, convert that number to a 'strain index' by multiplying it by 15. Finally, tell me the ID of the house that ranks 5th on this new strain index list.", "normal_query": "To assess housing strain, we define a 'density score' as residents per bedroom. For international comparison, this score needs to be converted to a 'strain index' where 1 unit of density equals 15 'strain points'. Generate a ranked list of households with a density score greater than 2, showing their household number and the calculated strain index (as an integer). From this list, identify the household number with the 5th highest strain index.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH HouseholdMetrics AS (SELECT h.housenum, (h.residentcount / NULLIF((p.dwelling_specs ->> 'Room_Count')::numeric, 0)) AS density FROM households h JOIN properties p ON h.housenum = p.houselink WHERE (p.dwelling_specs ->> 'Room_Count')::numeric > 0), StrainIndex AS (SELECT housenum, (density * 15)::int AS strain_index FROM HouseholdMetrics WHERE density > 2 ORDER BY strain_index DESC) SELECT housenum FROM StrainIndex OFFSET 4 LIMIT 1;"], "external_knowledge": [11, 25], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 0, "distinct": false, "order": true}}
{"instance_id": "planets_data_1", "selected_database": "planets_data", "query": "I'm curious about the gravity on bigger, rocky worlds. For all the confirmed super-earths found by watching their star's light dip (no matter how the method is written), what's their average surface gravity? Just give me the number, rounded to two decimal spots.", "normal_query": "What is the average planet surface gravity for all confirmed exoplanets that are larger than Earth but no more than ten times its mass, have a density greater than 3 g/cm³, and were discovered by observing the dimming of their host star? The check for the discovery method must be case-insensitive. Provide the result as a single scalar value rounded to 2 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH SuperEarths AS (\n    SELECT \n        p.\"planetref\",\n        pp.\"massjup\" * 317.83 AS mass_earth,\n        pp.\"radjup\" * 11.209 AS radius_earth\n    FROM planets p\n    JOIN physical_properties pp ON p.\"planetref\" = pp.\"objectlink\"\n    WHERE LOWER(p.\"discmethod\") IN ('transit', 'tr', 'transit method', 'photometry', 'photometric')\n      AND pp.\"densvalue\" > 3 \n      AND (pp.\"massjup\" * 317.83) BETWEEN 1 AND 10\n      AND pp.\"radjup\" IS NOT NULL\n)\nSELECT \n    ROUND(CAST(AVG(mass_earth / POWER(radius_earth, 2)) AS numeric), 2) AS avg_surface_gravity\nFROM SuperEarths;"], "external_knowledge": [1, 2, 4, 21, 24, 36], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "planets_data_2", "selected_database": "planets_data", "query": "I'm looking for Jupiter-like planets that are both scorching hot and spinning backwards, but only those where we know the star's mass and the planet's orbital distance. Can you list them out for me? I want to see their names, how long their year is, their orbital tilt, and how fast they're zipping around their star in kilometers per second. Put the fastest ones at the top.", "normal_query": "Generate a table of all hot jupiter planets that are also in a retrograde orbit, and for which the host star's mass and the planet's semi-major axis are known. Please display the host star's name, its orbital period in days, its inclination in degrees, and its calculated orbital velocity in km/s. Sort the results in descending order of the orbital velocity.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    s.\"hostplname\",\n    oc.\"period\",\n    oc.\"inclination\",\n    (SQRT((6.67430E-11 * (s.stellarprops->'physical'->>'Mass_Value')::real * 1.98847E30) / (oc.\"semimajor\" * 1.496E11)) / 1000) AS orbital_velocity_kms\nFROM planets p\nJOIN stars s ON p.\"hostlink\" = s.\"stellarref\"\nJOIN orbital_characteristics oc ON p.\"planetref\" = oc.\"bodylink\"\nJOIN physical_properties pp ON p.\"planetref\" = pp.\"objectlink\"\nWHERE pp.\"massjup\" > 0.1 \n  AND oc.\"period\" < 10\n  AND oc.\"inclination\" >= 90\n  AND (s.stellarprops->'physical'->>'Mass_Value')::real IS NOT NULL\n  AND oc.\"semimajor\" IS NOT NULL\nORDER BY orbital_velocity_kms DESC;"], "external_knowledge": [11, 20, 22, 23, 31], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "planets_data_4", "selected_database": "planets_data", "query": "Let's fact-check Kepler's law on star systems with multiple planets. For each of these systems, take the planet that's farthest out (based on a known semi-major axis) and use its orbit to calculate its star's mass, but only if its orbital period is also known and positive. Show me the star's name, its official mass, and the mass we just calculated. Then, sort them to show the ones where our calculation was closest to the real value first.", "normal_query": "For each star that hosts a multi-planetary system, calculate the kepler's third law verification value for its outermost planet (the one with the largest known semi-major axis). Only include planets with a known and positive orbital period for this calculation. Display the host star's name, the recorded mass from the database, and the calculated mass. Order the results by the absolute difference between the recorded and calculated mass, from smallest to largest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH outermost_planets AS (\n    SELECT \n        p.\"hostlink\",\n        MAX(oc.\"semimajor\") as max_semimajor\n    FROM planets p\n    JOIN stars s ON p.\"hostlink\" = s.\"stellarref\"\n    JOIN orbital_characteristics oc ON p.\"planetref\" = oc.\"bodylink\"\n    WHERE s.\"compcount\" > 1 AND oc.\"semimajor\" IS NOT NULL\n    GROUP BY p.\"hostlink\"\n)\nSELECT \n    s.\"hostplname\",\n    (s.stellarprops->'physical'->>'Mass_Value')::real AS recorded_mass,\n    (POWER(oc.\"semimajor\", 3) / POWER(oc.\"period\" / 365.25, 2)) AS calculated_mass\nFROM planets p\nJOIN stars s ON p.\"hostlink\" = s.\"stellarref\"\nJOIN orbital_characteristics oc ON p.\"planetref\" = oc.\"bodylink\"\nJOIN outermost_planets op ON p.\"hostlink\" = op.\"hostlink\" AND oc.\"semimajor\" = op.max_semimajor\nWHERE oc.\"period\" IS NOT NULL AND oc.\"period\" > 0\nORDER BY ABS((s.stellarprops->'physical'->>'Mass_Value')::real - (POWER(oc.\"semimajor\", 3) / POWER(oc.\"period\" / 365.25, 2)));"], "external_knowledge": [15, 28], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "planets_data_5", "selected_database": "planets_data", "query": "Of all the stars that have a measured mass and a positive radius, which one is the most tightly packed? Give me its name. You'll need to convert solar mass to kg using 1.98847E30 and solar radius to meters using 6.957E8 to do the calculation.", "normal_query": "For all stars with a known mass and a known, positive radius, what is the name of the one with the highest calculated stellar density? Provide the name as a single text result. Note: To calculate density in SI units, use the conversion factors 1.98847E30 for solar mass to kg, and 6.957E8 for solar radius to meters.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \"hostplname\"\nFROM stars\nWHERE (stellarprops->'physical'->>'Mass_Value')::real IS NOT NULL AND (stellarprops->'physical'->>'Radius_Value')::real IS NOT NULL AND (stellarprops->'physical'->>'Radius_Value')::real > 0\nORDER BY ((stellarprops->'physical'->>'Mass_Value')::real * 1.98847E30) / (4.0/3.0 * PI() * POWER((stellarprops->'physical'->>'Radius_Value')::real * 6.957E8, 3)) DESC\nLIMIT 1;"], "external_knowledge": [12], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "planets_data_6", "selected_database": "planets_data", "query": "Show me a list of planets that have highly eccentric orbits and are flagged for having a minimum mass measurement (look for 'msini', regardless of case). For each one that has a known mass and is orbiting a star with a known, positive mass, I'd like to see the planet's name, its star's name, its eccentricity value, and the planet-to-star mass ratio. To calculate the ratio, use 1.898E27 kg as the mass of Jupiter and 1.98847E30 kg as the mass of the Sun. Please show the ratio with 5 decimal places and sort the list from the smallest ratio to the largest.", "normal_query": "I want a report on all planets that have a high eccentricity orbit and also have their minimum mass status flagged (case-insensitive match for 'msini'). For each planet with a known mass, whose host star also has a known and positive mass, show its full name, its host star, its eccentricity, and calculate its planet-star mass ratio to 5 decimal places. Use 1.898E27 kg for Jupiter's mass and 1.98847E30 kg for the Sun's mass for the ratio calculation. Order the results by the mass ratio in ascending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    s.\"hostplname\" || ' ' || p.\"completter\" AS planet_name,\n    s.\"hostplname\",\n    oc.\"eccentricity\",\n    ROUND(CAST((pp.\"massjup\" * 1.898E27) / ((s.stellarprops->'physical'->>'Mass_Value')::real * 1.98847E30) as numeric), 5) as mass_ratio\nFROM planets p\nJOIN stars s ON p.\"hostlink\" = s.\"stellarref\"\nJOIN orbital_characteristics oc ON p.\"planetref\" = oc.\"bodylink\"\nJOIN physical_properties pp ON p.\"planetref\" = pp.\"objectlink\"\nJOIN data_quality_tracking dqt ON p.\"planetref\" = dqt.\"targetlink\"\nWHERE oc.\"eccentricity\" > 0.25\n  AND LOWER(dqt.\"masssource\") = 'msini'\n  AND pp.\"massjup\" IS NOT NULL\n  AND (s.stellarprops->'physical'->>'Mass_Value')::real IS NOT NULL AND (s.stellarprops->'physical'->>'Mass_Value')::real > 0\nORDER BY mass_ratio ASC;"], "external_knowledge": [18, 27, 34], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 5, "distinct": false, "order": true}}
{"instance_id": "planets_data_7", "selected_database": "planets_data", "query": "For all the big gassy planets found by Kepler's second-chance mission (matching 'k2' however it's capitalized), what's their average surface temperature, basically? Only include cases where we know the star's temperature, the star's radius, and the planet's orbital distance, and all are positive numbers. Give me that in kelvin, rounded to a whole number. Note that you'll need to use 6.957E8 to convert solar radius to meters and 1.496E11 to convert AU to meters.", "normal_query": "Find the average planetary equilibrium temperature for all gas giant planets discovered by the successor to the original Kepler mission (case-insensitive match for 'k2'). Only include planets for which the host star's temperature and radius, and the planet's semi-major axis, are all known and positive. Express the result in kelvin and round to the nearest whole number. Note: for the temperature calculation, use conversion factors of 6.957E8 for solar radius to meters and 1.496E11 for AU to meters.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    ROUND(AVG((s.stellarprops->'physical'->>'Temp_Value')::real * SQRT((s.stellarprops->'physical'->>'Radius_Value')::real * 6.957E8 / (2 * oc.\"semimajor\" * 1.496E11)))) AS avg_equilibrium_temp\nFROM planets p\nJOIN stars s ON p.\"hostlink\" = s.\"stellarref\"\nJOIN physical_properties pp ON p.\"planetref\" = pp.\"objectlink\"\nJOIN orbital_characteristics oc ON p.\"planetref\" = oc.\"bodylink\"\nJOIN planet_instrument_observations pio ON p.\"planetref\" = pio.\"subjectlink\"\nJOIN instruments_surveys ins ON pio.\"facilitylink\" = ins.\"instrumentref\"\nWHERE pp.\"massjup\" > 0.1 \n  AND LOWER(ins.\"facilityname\") = 'k2'\n  AND (s.stellarprops->'physical'->>'Temp_Value')::real IS NOT NULL AND (s.stellarprops->'physical'->>'Temp_Value')::real > 0\n  AND (s.stellarprops->'physical'->>'Radius_Value')::real IS NOT NULL AND (s.stellarprops->'physical'->>'Radius_Value')::real > 0\n  AND oc.\"semimajor\" IS NOT NULL AND oc.\"semimajor\" > 0;"], "external_knowledge": [5, 20, 50], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 0, "distinct": false, "order": false}}
{"instance_id": "planets_data_8", "selected_database": "planets_data", "query": "When planets found by the star wobble method (no matter how it's capitalized) pass in front of their star, what's the biggest dimming effect we could see? Only consider planets that have a known radius and orbit a star with a known, positive radius. Tell me that maximum dip in brightness as a percentage with four decimal places, and also name the planet and star responsible. You'll need to use the conversion that 1 solar radius is 109.2 Earth radii.", "normal_query": "What is the maximum transit depth, expressed as a percentage to 4 decimal places, for any planet discovered via the radial velocity method (case-insensitive match) where both the planet's radius and the host star's radius are known and positive? Also, provide the full name of the planet and its host star. Note: To compare radii, use the conversion factor 1 solar radius = 109.2 Earth radii.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH TransitDepths AS (\n    SELECT \n        s.\"hostplname\",\n        s.\"hostplname\" || ' ' || p.\"completter\" AS planet_name,\n        POWER((pp.\"radjup\" * 11.209) / ((s.stellarprops->'physical'->>'Radius_Value')::real * 109.2), 2) * 100 AS transit_depth_percent\n    FROM planets p\n    JOIN stars s ON p.\"hostlink\" = s.\"stellarref\"\n    JOIN physical_properties pp ON p.\"planetref\" = pp.\"objectlink\"\n    WHERE LOWER(p.\"discmethod\") IN ('radvel', 'rv', 'rv method', 'radial velocity', 'doppler')\n      AND (s.stellarprops->'physical'->>'Radius_Value')::real IS NOT NULL AND (s.stellarprops->'physical'->>'Radius_Value')::real > 0\n      AND pp.\"radjup\" IS NOT NULL\n)\nSELECT \n    hostplname, \n    planet_name, \n    ROUND(CAST(transit_depth_percent AS numeric), 4)\nFROM TransitDepths\nORDER BY transit_depth_percent DESC\nLIMIT 1;"], "external_knowledge": [9, 37], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 4, "distinct": false, "order": true}}
{"instance_id": "planets_data_9", "selected_database": "planets_data", "query": "Find me the rocky super-earth that has the strongest gravity pull of them all, considering only planets with known mass, radius, and density. Once you've pinpointed that planet, tell me what its mass is as a fraction of its star's mass, assuming the star's mass is also known and positive. I need that final number in scientific format, with 7-digit precision.", "normal_query": "Determine the planet-star mass ratio for the specific super-earth that exhibits the highest planet surface gravity. Only consider planets with known mass, radius, and density, orbiting stars with a known and positive mass. The final ratio should be a single value, expressed in scientific notation with 7 digits of precision.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH PlanetPropertiesInEarthUnits AS (\n    SELECT \n        p.\"planetref\",\n        p.\"hostlink\",\n        pp.\"massjup\" * 317.83 AS mass_earth,\n        pp.\"radjup\" * 11.209 AS radius_earth,\n        pp.\"densvalue\",\n        pp.\"massjup\"\n    FROM planets p\n    JOIN physical_properties pp ON p.\"planetref\" = pp.\"objectlink\"\n    WHERE pp.\"massjup\" IS NOT NULL AND pp.\"radjup\" IS NOT NULL AND pp.\"densvalue\" > 3\n),\nSuperEarths AS (\n    SELECT * \n    FROM PlanetPropertiesInEarthUnits\n    WHERE mass_earth BETWEEN 1 AND 10\n),\nPlanetGravity AS (\n    SELECT\n        planetref,\n        hostlink,\n        massjup,\n        mass_earth / (radius_earth * radius_earth) AS surface_gravity\n    FROM SuperEarths\n    WHERE radius_earth > 0\n),\nTargetPlanet AS (\n    SELECT hostlink, massjup\n    FROM PlanetGravity\n    ORDER BY surface_gravity DESC\n    LIMIT 1\n)\nSELECT \n    to_char((tp.massjup * 1.898E27) / ((s.stellarprops->'physical'->>'Mass_Value')::real * 1.98847E30), '9.999999E9')\nFROM TargetPlanet tp\nJOIN stars s ON tp.hostlink = s.\"stellarref\"\nWHERE (s.stellarprops->'physical'->>'Mass_Value')::real IS NOT NULL AND (s.stellarprops->'physical'->>'Mass_Value')::real > 0;"], "external_knowledge": [1, 2, 4, 18, 21, 24], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 6, "distinct": false, "order": false}}
{"instance_id": "planets_data_10", "selected_database": "planets_data", "query": "On average, how far away are the stars that have those big, puffy gas planets? I only want to include stars where we have a solid distance number, not just a 'less than' value, and where the brightness measurement isn't messed up by other stars nearby. Show the result in light-years with two decimal points.", "normal_query": "What is the average distance in light-years to host stars of inflated gas giant planets? Only include host stars where the distance measurement is not an upper limit value and the photometric magnitude measurement is not affected by a blended measurement. Give the answer to 2 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    ROUND(CAST(AVG(s.\"stellardist\" * 3.26156) as numeric), 2) AS avg_dist_ly\nFROM planets p\nJOIN stars s ON p.\"hostlink\" = s.\"stellarref\"\nJOIN physical_properties pp ON p.\"planetref\" = pp.\"objectlink\"\nJOIN data_quality_tracking dqt ON p.\"planetref\" = dqt.\"targetlink\"\nWHERE pp.\"massjup\" > 0.1 \n  AND pp.\"densvalue\" < 0.5\n  AND (dqt.limitflags->'stellar_limits'->>'Dist_Lim')::int != 1 \n  AND (s.stellarprops->'photometry'->>'Mag_Blend')::int = 0;"], "external_knowledge": [0, 20, 32, 35, 38], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "planets_data_11", "selected_database": "planets_data", "query": "Find me the planetary systems that are really tightly packed, but only if their closest-in planet is also super fast. For these systems, I want to see the star's name and the average orbital period ratio calculated using the geometric mean, with three decimal points. Sort the list by that average ratio, from highest to lowest.", "normal_query": "Identify compact systems where the innermost planet is a short-period planet. For each such system, list the host star name and calculate the geometric mean of all orbital period ratios between adjacent planets, rounded to 3 decimal places. Order the result by this geometric mean descending.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH SystemPlanetPeriods AS (\n    SELECT \n        s.\"hostplname\",\n        oc.\"period\",\n        ROW_NUMBER() OVER(PARTITION BY s.\"hostplname\" ORDER BY oc.\"period\" ASC) as rn\n    FROM planets p\n    JOIN stars s ON p.\"hostlink\" = s.\"stellarref\"\n    JOIN orbital_characteristics oc ON p.\"planetref\" = oc.\"bodylink\"\n    WHERE s.\"compcount\" > 1\n), PeriodRatios AS (\n    SELECT \n        s1.\"hostplname\",\n        s2.\"period\" / s1.\"period\" as ratio\n    FROM SystemPlanetPeriods s1\n    JOIN SystemPlanetPeriods s2 ON s1.\"hostplname\" = s2.\"hostplname\" AND s1.rn = s2.rn - 1\n), InnermostCheck AS (\n    SELECT \"hostplname\"\n    FROM SystemPlanetPeriods\n    WHERE rn = 1 AND \"period\" < 10\n)\nSELECT \n    pr.\"hostplname\",\n    ROUND(CAST(EXP(AVG(LN(pr.ratio))) as numeric), 3) as geometric_mean_ratio\nFROM PeriodRatios pr\nWHERE pr.ratio < 3\n  AND pr.\"hostplname\" IN (SELECT \"hostplname\" FROM InnermostCheck)\nGROUP BY pr.\"hostplname\"\nHAVING COUNT(*) = (\n    SELECT s.\"compcount\" - 1 FROM stars s WHERE s.\"hostplname\" = pr.\"hostplname\"\n)\nORDER BY geometric_mean_ratio DESC;"], "external_knowledge": [17, 22, 28, 33], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "planets_data_12", "selected_database": "planets_data", "query": "How many different stars have planets that were discovered by the ttv method, where they look for wobbles in a planet's transit schedule? Make sure you find 'ttv' regardless of its case.", "normal_query": "What is the total number of distinct host stars for which a planet was found by analyzing timing deviations in an already known transit? The search for the facility name 'ttv' must be case-insensitive.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT COUNT(DISTINCT p.\"hostlink\")\nFROM planets p\nJOIN planet_instrument_observations pio ON p.\"planetref\" = pio.\"subjectlink\"\nJOIN instruments_surveys ins ON pio.\"facilitylink\" = ins.\"instrumentref\"\nWHERE LOWER(ins.\"facilityname\") = 'ttv';"], "external_knowledge": [50], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": true, "order": false}}
{"instance_id": "planets_data_13", "selected_database": "planets_data", "query": "Find the planet with the biggest mass-to-size ratio, only looking at planets where we have a measured mass and a measured, non-zero radius. Then tell me its escape velocity in kilometers per second. Just give me a whole number.", "normal_query": "Calculate the planet escape velocity in km/s for the planet with the highest confirmed mass-radius relationship value. Only consider planets with a known, non-zero mass and radius. Provide the result rounded to the nearest integer.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH PlanetMetrics AS (\n    SELECT \n        pp.\"massjup\",\n        pp.\"radjup\",\n        (pp.\"massjup\" * 317.83) / (pp.\"radjup\" * 11.209) AS mrr\n    FROM physical_properties pp\n    WHERE pp.\"massjup\" IS NOT NULL \n      AND pp.\"radjup\" IS NOT NULL \n      AND pp.\"radjup\" > 0\n)\nSELECT \n    ROUND(SQRT(2 * 6.67430E-11 * pm.\"massjup\" * 1.898E27 / (pm.\"radjup\" * 7.1492E7)) / 1000)\nFROM PlanetMetrics pm\nORDER BY pm.mrr DESC\nLIMIT 1;"], "external_knowledge": [1, 2, 10, 16], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 0, "distinct": false, "order": true}}
{"instance_id": "planets_data_14", "selected_database": "planets_data", "query": "Can you count up how many stars were observed with each type of light filter? Make sure to lump all the different ways of writing 'v-band' together, and do the same for 'kepler-band', ignoring capitalization. Just ignore the 'k-band' ones for now. Then show me the cleaned-up filter name and how many stars for each, with the most-used filter at the top.", "normal_query": "For each photometric band in the 'stars' table, count the number of stars observed. Standardize the band names (case-insensitively): 'v (johnson)', 'johnson', 'v', 'johnson v', and 'v-band' should all be grouped as 'v-band'; 'kepler-band', 'kepler', 'kep-b', and 'kep' as 'kepler-band'. Ignore 'k-band' and any nulls. Show the standardized band name and the count, ordered by count descending.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH StandardizedBands AS (\n    SELECT\n        CASE\n            WHEN LOWER(TRIM(stellarprops->'photometry'->>'Photo_Band')) IN ('v (johnson)', 'johnson', 'v', 'johnson v', 'v-band') THEN 'V-Band'\n            WHEN LOWER(TRIM(stellarprops->'photometry'->>'Photo_Band')) IN ('kepler-band', 'kepler', 'kep-b', 'kep') THEN 'Kepler-Band'\n            ELSE NULL\n        END AS standardized_band\n    FROM stars\n)\nSELECT \n    standardized_band, \n    COUNT(*)\nFROM StandardizedBands\nWHERE standardized_band IS NOT NULL\nGROUP BY standardized_band\nORDER BY COUNT(*) DESC;"], "external_knowledge": [44], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "planets_data_15", "selected_database": "planets_data", "query": "If you look at stars that only have rocky planets and no gas giants, what is their average brightness compared to our sun? Only include planets where we know their density or mass, and only stars where we know their radius and temperature. I need the number with 4 decimal places.", "normal_query": "What is the average stellar luminosity of stars that host at least one rocky planet, but have no gas giant planets in the system? This analysis should only consider planets with a known density or mass and stars with a known radius and temperature. Calculate the result relative to the sun and provide it to 4 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH SystemPlanetTypes AS (\n    SELECT \n        p.\"hostlink\",\n        MAX(CASE WHEN pp.\"densvalue\" > 3 THEN 1 ELSE 0 END) as has_rocky,\n        MAX(CASE WHEN pp.\"massjup\" > 0.1 THEN 1 ELSE 0 END) as has_gas_giant\n    FROM planets p\n    JOIN physical_properties pp ON p.\"planetref\" = pp.\"objectlink\"\n    WHERE pp.\"densvalue\" IS NOT NULL OR pp.\"massjup\" IS NOT NULL\n    GROUP BY p.\"hostlink\"\n), TargetStars AS (\n    SELECT \"hostlink\"\n    FROM SystemPlanetTypes\n    WHERE has_rocky = 1 AND has_gas_giant = 0\n)\nSELECT \n    ROUND(CAST(AVG(POWER((s.stellarprops->'physical'->>'Radius_Value')::real, 2) * POWER((s.stellarprops->'physical'->>'Temp_Value')::real / 5778, 4)) AS numeric), 4) AS avg_luminosity\nFROM stars s\nJOIN TargetStars ts ON s.\"stellarref\" = ts.\"hostlink\"\nWHERE (s.stellarprops->'physical'->>'Radius_Value')::real IS NOT NULL AND (s.stellarprops->'physical'->>'Temp_Value')::real IS NOT NULL;"], "external_knowledge": [3, 20, 21], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 4, "distinct": false, "order": false}}
{"instance_id": "planets_data_16", "selected_database": "planets_data", "query": "How many planets did kepler find where the star's temperature reading is wonky because of other nearby stars?", "normal_query": "Count the number of planets whose discovery is attributed to the kepler mission and are part of a system with a blended measurement for stellar temperature.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT COUNT(p.\"planetref\")\nFROM planets p\nJOIN stars s ON p.\"hostlink\" = s.\"stellarref\"\nJOIN planet_instrument_observations pio ON p.\"planetref\" = pio.\"subjectlink\"\nJOIN instruments_surveys ins ON pio.\"facilitylink\" = ins.\"instrumentref\"\nWHERE LOWER(ins.\"facilityname\") = 'kep'\n  AND (s.stellarprops->'physical'->>'Temp_Blend')::int = 1;"], "external_knowledge": [38, 39, 46], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "planets_data_17", "selected_database": "planets_data", "query": "Can you give me the coordinates for the star '55 cnc' on an hr diagram, matching the name regardless of its case? I need its temperature and its luminosity relative to the sun, with the luminosity value having 3 decimal points.", "normal_query": "Provide a hertzsprung-russell (hr) diagram position for the star '55 cnc' (case-insensitive match). List its effective temperature and its calculated stellar luminosity. Round the luminosity to 3 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    (stellarprops->'physical'->>'Temp_Value')::real AS effective_temperature,\n    ROUND(CAST(POWER((stellarprops->'physical'->>'Radius_Value')::real, 2) * POWER((stellarprops->'physical'->>'Temp_Value')::real / 5778, 4) as numeric), 3) AS stellar_luminosity\nFROM stars\nWHERE LOWER(\"hostplname\") = '55 cnc';"], "external_knowledge": [3, 13], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": false}}
{"instance_id": "planets_data_18", "selected_database": "planets_data", "query": "I want to find the hottest planet that isn't a 'hot jupiter'. Only look at planets where we know the star's temperature and radius and the planet's orbital distance so you can do the calculation. Can you tell me its name, its star, and its temperature in kelvin? Please round the temperature to a whole number. You'll need to convert star radius from solar radii to meters using 6.957E8 and orbital distance from AU to meters using 1.496E11.", "normal_query": "Find the planet with the highest planetary equilibrium temperature that is not classified as a hot jupiter. Only include planets for which the host star's temperature and radius, and the planet's semi-major axis, are all known and valid for the calculation. Return the planet's letter, its host star name, and its calculated equilibrium temperature in kelvin, rounded to the nearest integer. Note that for unit consistency, you should use the conversion factors 6.957E8 for solar radius to meters and 1.496E11 for AU to meters.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n    p.\"completter\",\n    s.\"hostplname\",\n    ROUND((s.stellarprops->'physical'->>'Temp_Value')::real * SQRT((s.stellarprops->'physical'->>'Radius_Value')::real * 6.957E8 / (2 * oc.\"semimajor\" * 1.496E11))) AS equilibrium_temp\nFROM planets p\nJOIN stars s ON p.\"hostlink\" = s.\"stellarref\"\nJOIN orbital_characteristics oc ON p.\"planetref\" = oc.\"bodylink\"\nJOIN physical_properties pp ON p.\"planetref\" = pp.\"objectlink\"\nWHERE NOT (pp.\"massjup\" > 0.1 AND oc.\"period\" < 10)\nAND (s.stellarprops->'physical'->>'Temp_Value')::real IS NOT NULL\nAND (s.stellarprops->'physical'->>'Radius_Value')::real IS NOT NULL\nAND oc.\"semimajor\" IS NOT NULL AND oc.\"semimajor\" > 0\nORDER BY equilibrium_temp DESC\nLIMIT 1;"], "external_knowledge": [5, 20, 22, 23], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 0, "distinct": false, "order": true}}
{"instance_id": "planets_data_19", "selected_database": "planets_data", "query": "For how many planets do we have a size measurement, but we know it's just a 'less-than-or-equal-to' kind of number because it's marked as an upper limit?", "normal_query": "How many planets have a value for planetary radius, but this value is not a confirmed measurement and is instead flagged as an upper boundary? ", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT COUNT(*)\nFROM physical_properties pp\nJOIN data_quality_tracking dqt ON pp.\"objectlink\" = dqt.\"targetlink\"\nWHERE pp.\"radjup\" IS NOT NULL\n  AND (dqt.limitflags->'planetary_limits'->>'Rad_Lim')::int = 1;"], "external_knowledge": [35, 68], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "planets_data_20", "selected_database": "planets_data", "query": "For the star that looks brightest in our night sky, what's its standard gravitational parameter, μ? Give me the answer in scientific notation, with three digits of precision. You'll need to use G = 6.67430E-11 and convert the star's mass from solar masses to kg using the factor 1.98847E30.", "normal_query": "Calculate the gravitational parameter (μ) for the star that appears brightest from Earth. Provide the result in scientific notation with 3 digits of precision. Note that the Gravitational constant 'G' is 6.67430E-11 and the conversion factor for solar mass to kilograms is 1.98847E30.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH BrightestStar AS (\n    SELECT (stellarprops->'physical'->>'Mass_Value')::real as massvalue\n    FROM stars\n    WHERE (stellarprops->'photometry'->>'Opt_Mag')::real IS NOT NULL AND (stellarprops->'physical'->>'Mass_Value')::real IS NOT NULL\n    ORDER BY (stellarprops->'photometry'->>'Opt_Mag')::real ASC\n    LIMIT 1\n)\nSELECT TO_CHAR(6.67430E-11 * massvalue * 1.98847E30, '9.99EEEE')\nFROM BrightestStar;"], "external_knowledge": [19, 41], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "planets_data_M_1", "selected_database": "planets_data", "query": "Let's clean up the discovery methods because they're a mess. Can you make a new view called `v_discovery_method_summary`? It should list every planet's id, its original discovery method from the table, and a new column with a neat, standardized category like 'radial velocity' or 'transit' that works no matter how the original method is capitalized.", "normal_query": "Create a view named `v_discovery_method_summary`. This view should contain the planet's reference id, its original discovery method string, and a new `discovery_category` column. The new column should perform a case-insensitive standardization of the various discovery method names into unified categories such as 'radial velocity', 'transit', and 'imaging', based on the known variations.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE VIEW v_discovery_method_summary AS\nSELECT\n    \"planetref\",\n    \"discmethod\" AS original_method,\n    CASE \n        WHEN LOWER(TRIM(\"discmethod\")) IN ('radvel', 'rv', 'rv method', 'radial velocity', 'doppler') THEN 'Radial Velocity'\n        WHEN LOWER(TRIM(\"discmethod\")) IN ('transit', 'tr', 'transit method', 'photometry', 'photometric') THEN 'Transit'\n        WHEN LOWER(TRIM(\"discmethod\")) IN ('direct imaging', 'di', 'imaging', 'img', 'direct') THEN 'Imaging'\n        WHEN LOWER(TRIM(\"discmethod\")) IN ('ttv', 'transit timing variations', 'transit timing', 'ttv method', 'timing var') THEN 'TTV'\n        WHEN LOWER(TRIM(\"discmethod\")) IN ('microlensing', 'ml', '\\u03bclens', 'lensing', 'gravitational') THEN 'Microlensing'\n        ELSE 'Other'\n    END AS discovery_category\nFROM\n    planets;"], "external_knowledge": [36, 37, 51], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Drop the view for cleanup to ensure a fresh start\n    drop_view = \"DROP VIEW IF EXISTS v_discovery_method_summary;\"\n    execute_queries(drop_view, db_name, conn)\n    \n    # Execute the CREATE VIEW statement provided by the LLM\n    execute_queries(pred_sqls, db_name, conn)\n    \n    # 1. Verify that the view object was created in the database\n    check_view_sql = \"SELECT to_regclass('v_discovery_method_summary');\"\n    view_check_result = execute_queries(check_view_sql, db_name, conn)\n    assert view_check_result and view_check_result[0] and view_check_result[0][0] is not None, \"The view 'v_discovery_method_summary' was not created.\"\n    \n    # 2. Verify the row count matches the base table, ensuring no planets were lost\n    view_count_res = execute_queries(\"SELECT COUNT(*) FROM v_discovery_method_summary;\", db_name, conn)\n    base_count_res = execute_queries(\"SELECT COUNT(*) FROM planets;\", db_name, conn)\n    assert view_count_res[0][0][0] == base_count_res[0][0][0], \"View row count does not match the base 'planets' table count.\"\n\n    # 3. Verify the standardization for a specific, known planet from the sample data\n    # Planet with planetref=1 has discmethod='RadVel'\n    test_query = \"SELECT discovery_category FROM v_discovery_method_summary WHERE planetref = 1;\"\n    category_result = execute_queries(test_query, db_name, conn)\n    assert category_result and category_result[0][0][0] == 'Radial Velocity', \"Standardization for 'RadVel' method is incorrect.\""], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "planets_data_M_2", "selected_database": "planets_data", "query": "I need a new table called `planet_properties_earth_units` to keep track of planet sizes in a way that's easier to compare to home. It should have a reference to the planet, its mass in earths, and its radius in earths. Once you've made the table, go ahead and fill it up with all the planets we have the right data for.", "normal_query": "Create a table named `planet_properties_earth_units`. The table should store the planet's reference id (as a foreign key to the `planets` table), the planet mass in earth units, and the planet radius in earth units. After creating the table, populate it with data for all planets that have known jupiter-mass and jupiter-radius values.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE planet_properties_earth_units (\n    planet_ref INTEGER PRIMARY KEY,\n    mass_earth_units NUMERIC(10, 4),\n    radius_earth_units NUMERIC(10, 4),\n    FOREIGN KEY (planet_ref) REFERENCES planets(planetref)\n);\n", "INSERT INTO planet_properties_earth_units (planet_ref, mass_earth_units, radius_earth_units)\nSELECT \n    p.\"planetref\",\n    pp.\"massjup\" * 317.83,\n    pp.\"radjup\" * 11.209\nFROM planets p\nJOIN physical_properties pp ON p.\"planetref\" = pp.\"objectlink\"\nWHERE pp.\"massjup\" IS NOT NULL AND pp.\"radjup\" IS NOT NULL;"], "external_knowledge": [1, 2], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # DROP the table for cleanup\n    drop_table = \"DROP TABLE IF EXISTS planet_properties_earth_units;\"\n    execute_queries(drop_table, db_name, conn)\n    # pred sql result\n    pred_result = execute_queries(pred_sqls, db_name, conn)\n    # check if the table was created\n    check_table_sql = \"SELECT to_regclass('planet_properties_earth_units');\"\n    table_check = execute_queries(check_table_sql, db_name, conn)[0]\n    assert table_check[0][0] is not None, \"The table 'planet_properties_earth_units' was not created.\"\n    # check if table was populated\n    count_query = \"SELECT COUNT(*) FROM planet_properties_earth_units;\"\n    count_check = execute_queries(count_query, db_name, conn)[0]\n    assert count_check[0][0] > 0, \"The table 'planet_properties_earth_units' was not populated.\""], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "planets_data_M_3", "selected_database": "planets_data", "query": "Let's create a summary view for all the star systems; call it `v_system_overview`. For each star, I want to see its name, how big it is, how hot it is, and then two numbers: how many of its planets were found by the 'wobble' method and how many were found by the 'dimming' method (ignoring capitalization for both).", "normal_query": "I need a new view called `v_system_overview`. This view should list each host star and include its name, its stellar radius, its temperature, and two separate counts of its planets: one for discoveries via radial velocity and one for discoveries via the transit method (both matched case-insensitively).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE VIEW v_system_overview AS\nSELECT \n    s.\"hostplname\",\n    (s.stellarprops->'physical'->>'Radius_Value')::real as radiusvalue,\n    (s.stellarprops->'physical'->>'Temp_Value')::real as tempvalue,\n    COUNT(CASE WHEN LOWER(TRIM(p.\"discmethod\")) IN ('radvel', 'rv', 'rv method', 'radial velocity', 'doppler') THEN 1 ELSE NULL END) AS radial_velocity_discoveries,\n    COUNT(CASE WHEN LOWER(TRIM(p.\"discmethod\")) IN ('transit', 'tr', 'transit method', 'photometry', 'photometric') THEN 1 ELSE NULL END) AS transit_discoveries\nFROM stars s\nLEFT JOIN planets p ON s.\"stellarref\" = p.\"hostlink\"\nGROUP BY s.\"stellarref\", s.\"hostplname\", s.stellarprops;"], "external_knowledge": [36, 37], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # DROP the view for cleanup\n    drop_view = \"DROP VIEW IF EXISTS v_system_overview;\"\n    execute_queries(drop_view, db_name, conn)\n    # pred sql result\n    pred_result = execute_queries(pred_sqls, db_name, conn)[0]\n    # check if the view was created\n    check_view_sql = \"SELECT to_regclass('v_system_overview');\"\n    view_check = execute_queries(check_view_sql, db_name, conn)[0]\n    assert view_check[0][0] is not None, \"The view 'v_system_overview' was not created.\"\n    # query the view for a specific star\n    view_query = \"SELECT radial_velocity_discoveries FROM v_system_overview WHERE hostplname = '11 Com';\"\n    pred_view_data = execute_queries(view_query, db_name, conn)[0]\n    assert pred_view_data[0][0] == 1, \"Data in the view is incorrect.\""], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "planets_data_M_4", "selected_database": "planets_data", "query": "Make me a table called `high_precision_params` that flags planets with super-accurate measurements. It needs to link to the planet and then have three true/false columns: one for a high-precision mass, one for radius, and one for period. Then, fill the table with every planet for which we can calculate at least one of these uncertainty values, even if all flags end up being false.", "normal_query": "Create a table `high_precision_params`. The table should contain a reference to the planet and boolean flags indicating if its mass, radius, and period are high-precision. Populate this table for all planets that have at least one valid, non-null uncertainty measurement for either mass, radius, or period, regardless of whether it qualifies as high-precision.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE high_precision_params (\n    planet_ref INTEGER PRIMARY KEY REFERENCES planets(planetref),\n    has_hp_mass BOOLEAN,\n    has_hp_radius BOOLEAN,\n    has_hp_period BOOLEAN\n);\n", "INSERT INTO high_precision_params (planet_ref, has_hp_mass, has_hp_radius, has_hp_period)\nWITH RelativeUncertainties AS (\n    SELECT\n        p.\"planetref\",\n        ABS((dqt.\"masserr1\" - dqt.\"masserr2\") / 2 / pp.\"massjup\" * 100) AS mass_uncertainty,\n        ABS((dqt.\"raderr1\" - dqt.\"raderr2\") / 2 / pp.\"radjup\" * 100) AS radius_uncertainty,\n        ABS((dqt.\"perioderr1\" - dqt.\"perioderr2\") / 2 / oc.\"period\" * 100) AS period_uncertainty\n    FROM planets p\n    LEFT JOIN orbital_characteristics oc ON p.\"planetref\" = oc.\"bodylink\"\n    LEFT JOIN physical_properties pp ON p.\"planetref\" = pp.\"objectlink\"\n    LEFT JOIN data_quality_tracking dqt ON p.\"planetref\" = dqt.\"targetlink\"\n)\nSELECT \n    ru.\"planetref\",\n    (ru.mass_uncertainty < 5),\n    (ru.radius_uncertainty < 5),\n    (ru.period_uncertainty < 5)\nFROM RelativeUncertainties ru\nWHERE ru.mass_uncertainty IS NOT NULL \n   OR ru.radius_uncertainty IS NOT NULL \n   OR ru.period_uncertainty IS NOT NULL;"], "external_knowledge": [8, 29], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # DROP table for cleanup\n    execute_queries(\"DROP TABLE IF EXISTS high_precision_params;\", db_name, conn)\n    # Run the creation and insertion\n    execute_queries(pred_sqls, db_name, conn)\n    # Check if table exists\n    check_sql = \"SELECT to_regclass('high_precision_params');\"\n    check_res = execute_queries(check_sql, db_name, conn)[0]\n    assert check_res[0][0] is not None, \"Table 'high_precision_params' was not created.\"\n    # Check if data was inserted\n    count_res = execute_queries(\"SELECT COUNT(*) FROM high_precision_params;\", db_name, conn)[0]\n    assert count_res[0][0] > 0, \"Table was not populated.\"\n    # Check a specific known case, e.g., planet 1\n    planet_1_res = execute_queries(\"SELECT has_hp_mass, has_hp_radius, has_hp_period FROM high_precision_params WHERE planet_ref = 1;\", db_name, conn)[0]\n    # Based on data, mass err for planet 1 is large, radius is null, period is precise.\n    assert planet_1_res[0] == (False, None, True), \"Incorrect boolean flags for planet 1.\""], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "planets_data_M_5", "selected_database": "planets_data", "query": "I want to categorize all the stars with a known mass by how big they are. Can you make a new table called `star_classifications` for this? It needs to link to the star and give it a label. Call them 'massive' if they're huge (over 3 solar masses), 'intermediate' if they're a fair bit bigger than the sun, 'sun-like' if they're in the same ballpark as ours (down to about 0.8 solar masses), and 'low-mass' for all the little ones. Then fill the table with these labels.", "normal_query": "Create a new table `star_classifications` with columns for `stellarref` and `class_name`. The `stellarref` should be a foreign key to the `stars` table. Then, for all stars with a known stellar mass value, populate this table by assigning a `class_name` based on that mass: 'massive' for stars more than three times the sun's mass, 'intermediate' for those between that and one-and-a-half solar masses, 'sun-like' for those down to eighty percent of the sun's mass, and 'low-mass' for anything smaller.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE star_classifications (\n    stellar_ref INTEGER PRIMARY KEY,\n    class_name VARCHAR(20),\n    FOREIGN KEY (stellar_ref) REFERENCES stars(stellarref)\n);\n", "INSERT INTO star_classifications (stellar_ref, class_name)\nSELECT\n    \"stellarref\",\n    CASE \n        WHEN (stellarprops->'physical'->>'Mass_Value')::real > 3 THEN 'Massive'\n        WHEN (stellarprops->'physical'->>'Mass_Value')::real > 1.5 AND (stellarprops->'physical'->>'Mass_Value')::real <= 3 THEN 'Intermediate'\n        WHEN (stellarprops->'physical'->>'Mass_Value')::real > 0.8 AND (stellarprops->'physical'->>'Mass_Value')::real <= 1.5 THEN 'Sun-like'\n        WHEN (stellarprops->'physical'->>'Mass_Value')::real <= 0.8 THEN 'Low-mass'\n        ELSE 'Unknown'\n    END\nFROM stars\nWHERE (stellarprops->'physical'->>'Mass_Value')::real IS NOT NULL;"], "external_knowledge": [43], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Cleanup\n    execute_queries(\"DROP TABLE IF EXISTS star_classifications;\", db_name, conn)\n    # Run creation/insertion\n    execute_queries(pred_sqls, db_name, conn)\n    # Check creation\n    check_res = execute_queries(\"SELECT to_regclass('star_classifications');\", db_name, conn)[0]\n    assert check_res[0][0] is not None, \"Table 'star_classifications' was not created.\"\n    # Check data for star '11 Com' (mass 2.7)\n    class_res = execute_queries(\"SELECT sc.class_name FROM star_classifications sc JOIN stars s ON sc.stellar_ref = s.stellarref WHERE s.hostplname = '11 Com';\", db_name, conn)[0]\n    assert class_res[0][0] == 'Intermediate', \"Classification for '11 Com' is incorrect.\"\n    # Check data for star '14 And' (mass 2.2)\n    class_res_2 = execute_queries(\"SELECT sc.class_name FROM star_classifications sc JOIN stars s ON sc.stellar_ref = s.stellarref WHERE s.hostplname = '14 And';\", db_name, conn)[0]\n    assert class_res_2[0][0] == 'Intermediate', \"Classification for '14 And' is incorrect.\""], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "planets_data_M_6", "selected_database": "planets_data", "query": "Can you make me a pre-compiled list of all the super puffy gas planets where we can actually calculate their temperature? Let's call it `v_inflated_giants_report`. The list should only include planets where the star's temperature and radius and the planet's orbital distance are known. For those planets, show their name, star, mass and radius in jupiter units, density, and estimated temperature, with the temperature as a whole number.", "normal_query": "Please create a materialized view called `v_inflated_giants_report`. This view should contain all planets classified as inflated gas giants for which a planetary equilibrium temperature can be calculated (i.e., the host star's temperature and radius, and the planet's semi-major axis are all known). For each such planet, include its name, its host star, its mass in jupiter units, its radius in jupiter units, its density, and its planetary equilibrium temperature, rounded to the nearest integer.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE MATERIALIZED VIEW v_inflated_giants_report AS\nSELECT\n    p.\"completter\",\n    s.\"hostplname\",\n    pp.\"massjup\",\n    pp.\"radjup\",\n    pp.\"densvalue\",\n    ROUND((s.stellarprops->'physical'->>'Temp_Value')::real * SQRT((s.stellarprops->'physical'->>'Radius_Value')::real * 6.957E8 / (2 * oc.\"semimajor\" * 1.496E11))) AS equilibrium_temp\nFROM planets p\nJOIN stars s ON p.\"hostlink\" = s.\"stellarref\"\nJOIN physical_properties pp ON p.\"planetref\" = pp.\"objectlink\"\nJOIN orbital_characteristics oc ON p.\"planetref\" = oc.\"bodylink\"\nWHERE pp.\"massjup\" > 0.1 AND pp.\"densvalue\" < 0.5\nAND (s.stellarprops->'physical'->>'Temp_Value')::real IS NOT NULL\nAND (s.stellarprops->'physical'->>'Radius_Value')::real IS NOT NULL\nAND oc.\"semimajor\" IS NOT NULL AND oc.\"semimajor\" > 0;"], "external_knowledge": [5, 20, 32], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Cleanup\n    execute_queries(\"DROP MATERIALIZED VIEW IF EXISTS v_inflated_giants_report;\", db_name, conn)\n    # Create view\n    execute_queries(pred_sqls, db_name, conn)\n    # Check creation\n    check_res = execute_queries(\"SELECT to_regclass('v_inflated_giants_report');\", db_name, conn)[0]\n    assert check_res[0][0] is not None, \"View 'v_inflated_giants_report' was not created.\"\n    # Query view and check column count\n    view_res = execute_queries(\"SELECT * FROM v_inflated_giants_report;\", db_name, conn)[0]\n    assert len(view_res[0]) == 6, \"View has incorrect number of columns.\"\n    # Check that all densities are < 0.5\n    density_check = execute_queries(\"SELECT MAX(densvalue) FROM v_inflated_giants_report;\", db_name, conn)[0]\n    assert float(density_check[0][0]) < 0.5, \"View contains planets that are not Inflated Gas Giants.\""], "category": "Management", "high_level": false, "conditions": {"decimal": 0, "distinct": false, "order": false}}
{"instance_id": "planets_data_M_7", "selected_database": "planets_data", "query": "Let's make a new table called `planet_types` to label every planet. It should have the planet's id and its type. Use our standard rulebook to classify them: check first for the special types like 'hot jupiter' and 'super-earth', then for the basic 'rocky' or 'gas giant' types. If a planet doesn't fit any of those buckets, just label it 'unknown'. Go ahead and fill the table after you create it.", "normal_query": "Create a new table `planet_types` that contains the planet's reference id and a string representing its type. Then, populate this table by classifying each planet according to the established hierarchical definitions for 'hot jupiter', 'super-earth', 'rocky planet', and 'gas giant', assigning 'unknown' to any that do not fit a category.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE planet_types (\n    planet_ref INTEGER PRIMARY KEY REFERENCES planets(planetref),\n    type_name VARCHAR(50)\n);\n", "INSERT INTO planet_types(planet_ref, type_name)\nSELECT \n    p.\"planetref\",\n    CASE\n        WHEN pp.\"massjup\" > 0.1 AND oc.\"period\" < 10 THEN 'Hot Jupiter'\n        WHEN pp.\"densvalue\" > 3 AND (pp.\"massjup\" * 317.83) BETWEEN 1 AND 10 THEN 'Super-Earth'\n        WHEN pp.\"densvalue\" > 3 THEN 'Rocky Planet'\n        WHEN pp.\"massjup\" > 0.1 THEN 'Gas Giant'\n        ELSE 'Unknown'\n    END AS type_name\nFROM planets p\nLEFT JOIN physical_properties pp ON p.\"planetref\" = pp.\"objectlink\"\nLEFT JOIN orbital_characteristics oc ON p.\"planetref\" = oc.\"bodylink\";"], "external_knowledge": [1, 20, 21, 22, 23, 24], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Cleanup\n    execute_queries(\"DROP TABLE IF EXISTS planet_types;\", db_name, conn)\n    # Run SQL\n    execute_queries(pred_sqls, db_name, conn)\n    # Check creation\n    check_res = execute_queries(\"SELECT to_regclass('planet_types');\", db_name, conn)[0]\n    assert check_res[0][0] is not None, \"Table 'planet_types' was not created.\"\n    # Check count\n    count_res = execute_queries(\"SELECT COUNT(*) FROM planet_types;\", db_name, conn)[0]\n    planets_count = execute_queries(\"SELECT COUNT(*) FROM planets;\", db_name, conn)[0]\n    assert count_res[0][0] == planets_count[0][0], \"Table was not populated with all planets.\"\n    # Check a known Super-Earth candidate like Kepler-10 b (planetref might differ, let's check a property)\n    super_earth_count = execute_queries(\"SELECT COUNT(*) FROM planet_types WHERE type_name = 'Super-Earth';\", db_name, conn)[0]\n    assert super_earth_count[0][0] > 0, \"No Super-Earths were classified.\""], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "planets_data_M_8", "selected_database": "planets_data", "query": "I want a quick way to see which planets have dodgy star measurements. Make a view called `v_uncertain_measurements`. It should list any planet where the star's mass, size, or temperature reading might be mixed with another star's light. Show me the planet's name, its star, and then three flags telling me 'yes' or 'no' for whether the mass is blended, the radius is blended, and the temp is blended.", "normal_query": "Create a view called `v_uncertain_measurements`. This view should list all planets that have a blended measurement for their stellar mass, radius, or temperature. Include the planet's letter, host star name, and boolean flags indicating which of the three measurements (mass, radius, temperature) are blended.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE VIEW v_uncertain_measurements AS\nSELECT\n    p.\"completter\",\n    s.\"hostplname\",\n    CASE WHEN (s.stellarprops->'physical'->>'Mass_Blend')::int = 1 THEN TRUE ELSE FALSE END AS is_mass_blended,\n    CASE WHEN (s.stellarprops->'physical'->>'Rad_Blend')::int = 1 THEN TRUE ELSE FALSE END AS is_radius_blended,\n    CASE WHEN (s.stellarprops->'physical'->>'Temp_Blend')::int = 1 THEN TRUE ELSE FALSE END AS is_temp_blended\nFROM planets p\nJOIN stars s ON p.\"hostlink\" = s.\"stellarref\"\nWHERE (s.stellarprops->'physical'->>'Mass_Blend')::int = 1 OR (s.stellarprops->'physical'->>'Rad_Blend')::int = 1 OR (s.stellarprops->'physical'->>'Temp_Blend')::int = 1;"], "external_knowledge": [38], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Cleanup\n    execute_queries(\"DROP VIEW IF EXISTS v_uncertain_measurements;\", db_name, conn)\n    # Create view\n    execute_queries(pred_sqls, db_name, conn)\n    # Check creation\n    check_res = execute_queries(\"SELECT to_regclass('v_uncertain_measurements');\", db_name, conn)[0]\n    assert check_res[0][0] is not None, \"View 'v_uncertain_measurements' was not created.\"\n    # Check that at least one flag is true for every row\n    flag_check = execute_queries(\"SELECT COUNT(*) FROM v_uncertain_measurements WHERE NOT is_mass_blended AND NOT is_radius_blended AND NOT is_temp_blended;\", db_name, conn)[0]\n    assert flag_check[0][0] == 0, \"View contains rows where no measurement is blended.\""], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "planets_data_M_9", "selected_database": "planets_data", "query": "Let's make a table for studying tightly-packed solar systems, call it `system_period_ratios`. It should have the star's id, the inner planet's orbital period, the outer planet's orbital period, and the ratio between them. Go ahead and fill it up with this info for every neighboring pair of planets that have known periods, in any system that has more than one planet.", "normal_query": "Create a table `system_period_ratios` to analyze compact systems. It should store `hostlink`, the `inner_planet_period`, `outer_planet_period`, and the calculated orbital period ratio. Populate this table for all adjacent planet pairs with known orbital periods in systems where the star has more than one planet.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE TABLE system_period_ratios (\n    host_link INTEGER,\n    inner_planet_period NUMERIC,\n    outer_planet_period NUMERIC,\n    period_ratio NUMERIC\n);\n", "INSERT INTO system_period_ratios (host_link, inner_planet_period, outer_planet_period, period_ratio)\nWITH PlanetRanks AS (\n    SELECT \n        p.\"hostlink\",\n        oc.\"period\",\n        ROW_NUMBER() OVER(PARTITION BY p.\"hostlink\" ORDER BY oc.\"period\") as rnum\n    FROM planets p\n    JOIN orbital_characteristics oc ON p.\"planetref\" = oc.\"bodylink\"\n    JOIN stars s ON p.\"hostlink\" = s.\"stellarref\"\n    WHERE s.\"compcount\" > 1 AND oc.\"period\" IS NOT NULL\n)\nSELECT \n    p1.\"hostlink\",\n    p1.\"period\" as inner_planet_period,\n    p2.\"period\" as outer_planet_period,\n    p2.\"period\" / p1.\"period\" as period_ratio\nFROM PlanetRanks p1\nJOIN PlanetRanks p2 ON p1.\"hostlink\" = p2.\"hostlink\" AND p1.rnum = p2.rnum - 1;"], "external_knowledge": [17, 28, 33], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Cleanup\n    execute_queries(\"DROP TABLE IF EXISTS system_period_ratios;\", db_name, conn)\n    # Run SQL\n    execute_queries(pred_sqls, db_name, conn)\n    # Check creation\n    check_res = execute_queries(\"SELECT to_regclass('system_period_ratios');\", db_name, conn)[0]\n    assert check_res[0][0] is not None, \"Table 'system_period_ratios' was not created.\"\n    # Check that inner period is always less than outer period\n    period_check = execute_queries(\"SELECT COUNT(*) FROM system_period_ratios WHERE inner_planet_period >= outer_planet_period;\", db_name, conn)[0]\n    assert period_check[0][0] == 0, \"Table contains rows where inner period is not less than outer period.\""], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "planets_data_M_10", "selected_database": "planets_data", "query": "I need a pre-calculated summary of how we're finding planets. Please create a materialized view called `v_discovery_stats`. It should list every discovery technique after you've cleaned them up by trimming spaces and ignoring case. For each one, show how many planets we found with it, the average distance to those planets in light-years (calculated only from planets with known distances and shown with two decimal points), and the very last date any record for that method was updated.", "normal_query": "Create a materialized view `v_discovery_stats`. The view should list each distinct discovery method, after cleaning the text by trimming whitespace and converting to lowercase. It should also provide the total count of planets discovered with that method, the average stellar distance in light-years for those discoveries (only for planets with a known distance) rounded to two decimal places, and the most recent update timestamp associated with any planet of that discovery method.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE MATERIALIZED VIEW v_discovery_stats AS\nSELECT\n    TRIM(LOWER(p.\"discmethod\")) AS discmethod,\n    COUNT(p.\"planetref\") AS num_planets,\n    ROUND(CAST(AVG(s.\"stellardist\" * 3.26156) AS numeric), 2) AS avg_distance_ly,\n    MAX(dqt.\"updatestamp\") AS last_updated\nFROM planets p \nLEFT JOIN stars s ON p.\"hostlink\" = s.\"stellarref\"\nLEFT JOIN data_quality_tracking dqt ON p.\"planetref\" = dqt.\"targetlink\"\nWHERE p.\"discmethod\" IS NOT NULL\nGROUP BY TRIM(LOWER(p.\"discmethod\"));"], "external_knowledge": [0], "test_cases": ["from datetime import date\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Cleanup\n    execute_queries(\"DROP MATERIALIZED VIEW IF EXISTS v_discovery_stats;\", db_name, conn)\n    # Create view\n    execute_queries(pred_sqls, db_name, conn)\n    # Check creation\n    check_res = execute_queries(\"SELECT to_regclass('v_discovery_stats');\", db_name, conn)[0]\n    assert check_res[0][0] is not None, \"View 'v_discovery_stats' was not created.\"\n    # Check a value for 'radvel' (the standardized form of 'Radial Velocity')\n    rv_stats = execute_queries(\"SELECT num_planets FROM v_discovery_stats WHERE discmethod = 'radvel';\", db_name, conn)[0]\n    assert rv_stats[0][0] > 0, \"View data for 'radvel' seems incorrect or missing.\""], "category": "Management", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "museum_artifact_1", "selected_database": "museum_artifact", "query": "I'm worried about our items that are most at risk. Can you pull a list of artifacts that are both considered high-value (let's say historical significance over 8 and cultural score over 20) and are also officially listed with a 'High' or 'Medium' risk level? For each one, show its ID, title, its actual risk level, and both of those scores.", "normal_query": "Generate a report of artifacts that are both high-risk (level 'High' or 'Medium') and high-value (historical significance > 8 and cultural score > 20). The report should include the artifact's ID, title, risk level, historical significance, and cultural score.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH HighRiskArtifacts AS (\n    SELECT art_concern as \"ARTregID\", risk_level\n    FROM \"RiskAssessments\"\n    WHERE LOWER(risk_level) IN ('high', 'medium')\n),\nHighValueArtifacts AS (\n    SELECT \"ART_link\" as \"ARTregID\", \"HIST_sign\", (rating_profile->>'cultural_score')::numeric as cultural_score\n    FROM \"ArtifactRatings\"\n    WHERE \"HIST_sign\" > 8 AND (rating_profile->>'cultural_score')::numeric > 20\n)\nSELECT \n    ac.\"ARTregID\",\n    ac.art_title,\n    hra.risk_level,\n    hva.\"HIST_sign\",\n    hva.cultural_score\nFROM \"ArtifactsCore\" ac\nJOIN HighRiskArtifacts hra ON ac.\"ARTregID\" = hra.\"ARTregID\"\nJOIN HighValueArtifacts hva ON ac.\"ARTregID\" = hva.\"ARTregID\";"], "external_knowledge": [], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "museum_artifact_2", "selected_database": "museum_artifact", "query": "I'm concerned about environmental damage. Can we find the artifacts most at risk by calculating a score based on their average sensitivity to the environment? Only show me the ones where the score is over 4. For those, I need the artifact's ID, name, its exact score, and a list of all specific sensitivities rated 'High'. Please sort the list to show the highest-risk items first.", "normal_query": "Identify artifacts with dangerously high environmental risks by calculating their Environmental Risk Factor (ERF). The report should include the artifact's ID, its name, the calculated ERF score, and a JSON summary of all its 'High' sensitivity ratings. Only include artifacts where the ERF score exceeds 4, and sort the results from highest to lowest risk.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ERF_Calculation AS (\n    SELECT\n        \"ART_link\",\n        (\n            (CASE LOWER(env_handling_sensitivity -> 'environment' ->> 'light') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 0 END) +\n            (CASE LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 0 END) +\n            (CASE LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 0 END) +\n            (CASE LOWER(env_handling_sensitivity -> 'environment' ->> 'vibration') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 0 END) +\n            (CASE LOWER(env_handling_sensitivity -> 'environment' ->> 'pollutANTS') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 0 END) +\n            (CASE LOWER(env_handling_sensitivity -> 'biological' ->> 'pest') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 0 END) +\n            (CASE LOWER(env_handling_sensitivity -> 'handling_transport' ->> 'handling') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 0 END) +\n            (CASE LOWER(env_handling_sensitivity -> 'handling_transport' ->> 'transport') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 0 END) +\n            (CASE LOWER(env_handling_sensitivity -> 'context' ->> 'display') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 0 END) +\n            (CASE LOWER(env_handling_sensitivity -> 'context' ->> 'storage') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 0 END)\n        ) / 10.0 AS erf_score\n    FROM \"SensitivityData\"\n),\nHighSensitivities AS (\n    SELECT\n        \"ART_link\",\n        jsonb_agg(jsonb_build_object('type', sensitivity_type, 'level', 'High')) as high_sensitivity_summary\n    FROM (\n        SELECT \"ART_link\", 'light', env_handling_sensitivity -> 'environment' ->> 'light' FROM \"SensitivityData\" UNION ALL\n        SELECT \"ART_link\", 'temperature', env_handling_sensitivity -> 'environment' ->> 'temperature' FROM \"SensitivityData\" UNION ALL\n        SELECT \"ART_link\", 'humidity', env_handling_sensitivity -> 'environment' ->> 'humidity' FROM \"SensitivityData\" UNION ALL\n        SELECT \"ART_link\", 'vibration', env_handling_sensitivity -> 'environment' ->> 'vibration' FROM \"SensitivityData\" UNION ALL\n        SELECT \"ART_link\", 'pollutants', env_handling_sensitivity -> 'environment' ->> 'pollutants' FROM \"SensitivityData\" UNION ALL\n        SELECT \"ART_link\", 'pest', env_handling_sensitivity -> 'biological' ->> 'pest' FROM \"SensitivityData\" UNION ALL\n        SELECT \"ART_link\", 'handling', env_handling_sensitivity -> 'handling_transport' ->> 'handling' FROM \"SensitivityData\" UNION ALL\n        SELECT \"ART_link\", 'transport', env_handling_sensitivity -> 'handling_transport' ->> 'transport' FROM \"SensitivityData\" UNION ALL\n        SELECT \"ART_link\", 'display', env_handling_sensitivity -> 'context' ->> 'display' FROM \"SensitivityData\" UNION ALL\n        SELECT \"ART_link\", 'storage', env_handling_sensitivity -> 'context' ->> 'storage' FROM \"SensitivityData\"\n    ) AS unpivoted(\"ART_link\", sensitivity_type, sensitivity_value)\n    WHERE LOWER(sensitivity_value) = 'high'\n    GROUP BY \"ART_link\"\n)\nSELECT\n    ac.\"ARTregID\",\n    ac.art_title,\n    e.erf_score,\n    hs.high_sensitivity_summary\nFROM \"ArtifactsCore\" AS ac\nJOIN ERF_Calculation AS e ON ac.\"ARTregID\" = e.\"ART_link\"\nLEFT JOIN HighSensitivities AS hs ON ac.\"ARTregID\" = hs.\"ART_link\"\nWHERE e.erf_score > 4\nORDER BY e.erf_score DESC;"], "external_knowledge": [1, 2], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "museum_artifact_3", "selected_database": "museum_artifact", "query": "To help us plan conservation, I need to focus on artifacts from the 'Ming' and 'Qing' dynasties. Please calculate a priority score for each one, and then assign a priority level. I'd like a report showing the artifact's ID, title, dynasty, the calculated score, and the final priority level. Please sort it to show the highest scores at the top.", "normal_query": "Generate a conservation priority report for artifacts from the 'Ming' and 'Qing' dynasties. For each artifact, calculate its Conservation Priority Index (CPI) and determine its Priority Level. The report should include the Artifact ID, Title, Dynasty, CPI Score, and Priority Level, sorted by CPI Score in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH CPIData AS (\n    SELECT\n        t1.\"ARTregID\",\n        t1.art_title,\n        t1.\"DYNASTY\",\n        t2.\"HIST_sign\",\n        (t2.rating_profile->>'research_score')::numeric AS \"researchScore\",\n        (t2.rating_profile->>'cultural_score')::numeric AS \"CULTscore\",\n        CASE LOWER(t1.conserve_status)\n            WHEN 'excellent' THEN 1\n            WHEN 'good'      THEN 3\n            WHEN 'fair'      THEN 5\n            WHEN 'poor'      THEN 7\n            WHEN 'critical'  THEN 10\n            ELSE NULL\n        END AS status_numeric\n    FROM \"ArtifactsCore\" AS t1\n    JOIN \"ArtifactRatings\" AS t2 ON t1.\"ARTregID\" = t2.\"ART_link\"\n),\nCPICalculation AS (\n    SELECT\n        \"ARTregID\",\n        art_title,\n        \"DYNASTY\",\n        ( (\"HIST_sign\" + \"researchScore\" + \"CULTscore\") * (10 - status_numeric) ) / 30.0 AS cpi_score\n    FROM CPIData\n    WHERE status_numeric IS NOT NULL \n      AND \"HIST_sign\" IS NOT NULL \n      AND \"researchScore\" IS NOT NULL \n      AND \"CULTscore\" IS NOT NULL\n)\nSELECT\n    \"ARTregID\",\n    art_title,\n    \"DYNASTY\",\n    cpi_score,\n    CASE\n        WHEN cpi_score > 7 THEN 'High Priority'\n        WHEN cpi_score > 4 THEN 'Medium Priority'\n        ELSE 'Low Priority'\n    END AS \"Conservation Priority Level\"\nFROM CPICalculation\nWHERE LOWER(\"DYNASTY\") IN ('ming', 'qing')\nORDER BY cpi_score DESC;"], "external_knowledge": [0, 21, 56], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "museum_artifact_4", "selected_database": "museum_artifact", "query": "I need a report on how well we're funding artifact conservation across different dynasties. For each artifact, show its dynasty, its priority score, the specific budget we've assigned, and whether that funding is 'Sufficient' or 'Insufficient'.", "normal_query": "For each artifact with a known dynasty, create a report showing its dynasty, its CPI score, its specific budget allocation, and its generalized budget status.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH artifact_cpi AS (\n    SELECT \n        ac.\"ARTregID\",\n        ac.\"DYNASTY\",\n        ar.\"HIST_sign\",\n        (ar.rating_profile->>'research_score')::numeric AS \"researchScore\",\n        (ar.rating_profile->>'cultural_score')::numeric AS \"CULTscore\",\n        CASE LOWER(ac.\"conserve_status\") \n            WHEN 'excellent' THEN 1 WHEN 'good' THEN 3 WHEN 'fair' THEN 5 \n            WHEN 'poor' THEN 7 WHEN 'critical' THEN 10 ELSE 5 \n        END AS \"conserve_status_num\"\n    FROM \"ArtifactsCore\" ac\n    LEFT JOIN \"ArtifactRatings\" ar ON ac.\"ARTregID\" = ar.\"ART_link\"\n),\ncalculated_cpi AS (\n    SELECT \n        ac.*,\n        COALESCE((\n            (ac.\"HIST_sign\" + ac.\"researchScore\" + ac.\"CULTscore\") * \n            (10 - ac.\"conserve_status_num\")\n        ) / 30.0, 0) AS \"CPI\"\n    FROM artifact_cpi ac\n),\nbudget_allocation AS (\n    SELECT DISTINCT ON (ca.art_exam) \n        ca.art_exam AS \"ARTregID\",\n        cm.budget_alloc AS \"budget_allocation\",\n        CASE \n            WHEN LOWER(cm.budget_alloc) IN ('sufficient', 'adequate') THEN 'Sufficient'\n            WHEN LOWER(cm.budget_alloc) = 'insufficient' THEN 'Insufficient'\n            ELSE 'Unknown'\n        END AS \"budget_status\"\n    FROM \"ConditionAssessments\" ca\n    JOIN \"EnvironmentalReadingsCore\" erc ON ca.case_exam = erc.case_link\n    JOIN \"ConservationAndMaintenance\" cm ON erc.monitor_code = cm.monitor_link\n    ORDER BY ca.art_exam, erc.\"readTS\" DESC\n)\nSELECT \n    cpi.\"DYNASTY\",\n    cpi.\"CPI\",\n    ba.budget_allocation,\n    ba.budget_status\nFROM calculated_cpi cpi\nLEFT JOIN budget_allocation ba ON cpi.\"ARTregID\" = ba.\"ARTregID\"\nWHERE cpi.\"DYNASTY\" IS NOT NULL;"], "external_knowledge": [0, 9, 17, 68], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "museum_artifact_5", "selected_database": "museum_artifact", "query": "Can you whip up a fast list for me? I want to see if any artifacts are deteriorating too quickly. For each one, give me the ID, name, current temp and humidity, a count of its high sensitivities, and a 'Yes' or 'No' on whether it's in this danger zone. Don't skip any artifacts, even if data's missing. Sort it all by artifact ID.", "normal_query": "Check if artifacts are in an Accelerated Deterioration Scenario. The report should show each artifact's ID, its name, the current temperature and humidity in its display case, how many high sensitivities it has, and a 'Yes' or 'No' for the scenario. Include all artifacts and sort by artifact ID.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH SensitivityInfo AS (\n    SELECT\n        sd.\"ART_link\",\n        (\n            CASE WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'light') = 'high' THEN 1 ELSE 0 END +\n            CASE WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'temperature') = 'high' THEN 1 ELSE 0 END +\n            CASE WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'humidity') = 'high' THEN 1 ELSE 0 END +\n            CASE WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'vibration') = 'high' THEN 1 ELSE 0 END +\n            CASE WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'pollutants') = 'high' THEN 1 ELSE 0 END\n        ) AS high_sensitivity_count,\n        (\n            (CASE WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'light') = 'low' THEN 1 WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'light') = 'medium' THEN 5 WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'light') = 'high' THEN 10 ELSE 1 END) +\n            (CASE WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'temperature') = 'low' THEN 1 WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'temperature') = 'medium' THEN 5 WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'temperature') = 'high' THEN 10 ELSE 1 END) +\n            (CASE WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'humidity') = 'low' THEN 1 WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'humidity') = 'medium' THEN 5 WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'humidity') = 'high' THEN 10 ELSE 1 END) +\n            (CASE WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'vibration') = 'low' THEN 1 WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'vibration') = 'medium' THEN 5 WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'vibration') = 'high' THEN 10 ELSE 1 END) +\n            (CASE WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'pollutants') = 'low' THEN 1 WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'pollutants') = 'medium' THEN 5 WHEN LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'pollutants') = 'high' THEN 10 ELSE 1 END)\n        ) / 5.0 AS ERF\n    FROM \"SensitivityData\" sd\n),\nLatestReadings AS (\n    SELECT DISTINCT ON (ca.\"art_exam\")\n        ca.\"art_exam\" AS \"ARTregID\",\n        erc.\"TEMPc\" AS current_temp,\n        erc.\"RH\" AS current_humidity\n    FROM \"ConditionAssessments\" AS ca\n    JOIN \"EnvironmentalReadingsCore\" AS erc ON ca.\"case_exam\" = erc.\"case_link\"\n    ORDER BY ca.\"art_exam\", erc.\"readTS\" DESC\n),\nMDRAggregate AS (\n    SELECT\n        ac.\"ARTregID\",\n        ((ac.\"ageYears\" * si.ERF * POWER(lr.current_humidity - 50, 2) * lr.current_temp) / 100000.0) AS \"MDR\",\n        si.high_sensitivity_count,\n        lr.current_temp,\n        lr.current_humidity\n    FROM \"ArtifactsCore\" ac\n    LEFT JOIN SensitivityInfo si ON ac.\"ARTregID\" = si.\"ART_link\"\n    LEFT JOIN LatestReadings lr ON ac.\"ARTregID\" = lr.\"ARTregID\"\n)\nSELECT \n    ac.\"ARTregID\",\n    ac.art_title,\n    mdr.current_temp,\n    mdr.current_humidity,\n    COALESCE(mdr.high_sensitivity_count, 0) AS high_sensitivity_count,\n    CASE \n        WHEN mdr.\"MDR\" > 5 AND COALESCE(mdr.high_sensitivity_count, 0) >= 2 THEN 'Yes'\n        ELSE 'No'\n    END AS \"accelerated_deterioration\"\nFROM \"ArtifactsCore\" ac\nLEFT JOIN MDRAggregate mdr ON ac.\"ARTregID\" = mdr.\"ARTregID\"\nORDER BY ac.\"ARTregID\" ASC;"], "external_knowledge": [1, 2, 7, 14], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "museum_artifact_6", "selected_database": "museum_artifact", "query": "Can you whip up a fast rundown for me? I need all the showcase IDs that have unstable environmental conditions. Get every unique ID and line them up in alphabetical order.", "normal_query": "Generate a list of all unique showcase IDs that are experiencing an Environmental Instability Event. The list should be sorted alphabetically by ID.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT DISTINCT s.\"caseID\"\nFROM \"Showcases\" s\nJOIN \"EnvironmentalReadingsCore\" erc ON s.\"caseID\" = erc.\"case_link\"\nWHERE (erc.\"tempVar24\" > 1.0 OR erc.\"RHvar\" > 3)\nORDER BY s.\"caseID\" ASC;"], "external_knowledge": [13], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": true, "order": true}}
{"instance_id": "museum_artifact_7", "selected_database": "museum_artifact", "query": "Can you sniff out all the showcase IDs that could be heading toward a problem? We'll say a showcase is a problem if its environmental stability score drops below 5, OR if it has at least three major maintenance issues like a poor seal, overdue status, or a filter/silica needing replacement. Show me just the showcase IDs for these problem cases, lined up alphabetically.", "normal_query": "Identify all showcase IDs that are at risk of failure. A showcase is considered 'At Risk' if its calculated environmental stability score is less than 5, OR if it has at least three major maintenance issues ('Poor' seal state, 'Overdue' maintenance status, 'Replace Now' filter, or 'Replace Now' silica). The report should list the IDs of the at-risk showcases, sorted alphabetically.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH LatestReadings AS (\n    SELECT DISTINCT ON (\"case_link\")\n        \"case_link\",\n        \"tempVar24\",\n        \"RHvar\"\n    FROM \"EnvironmentalReadingsCore\"\n    ORDER BY \"case_link\", \"readTS\" DESC\n), ShowcaseRiskAssessment AS (\n    SELECT \n        s.\"caseID\",\n        CASE \n            WHEN (10 - (COALESCE(lr.\"tempVar24\", 0) + (COALESCE(lr.\"RHvar\", 0) / 5.0) + COALESCE((s.case_environment_profile->'physical_state'->>'leak_rate_per_day')::numeric, 0))) < 5 \n                  OR ((CASE WHEN LOWER(s.case_environment_profile->'physical_state'->>'seal_state') = 'poor' THEN 1 ELSE 0 END) + \n                      (CASE WHEN LOWER(s.case_environment_profile->'maintenance'->>'maint_status') = 'overdue' THEN 1 ELSE 0 END) + \n                      (CASE WHEN LOWER(s.case_environment_profile->'maintenance'->>'filter_status') = 'replace now' THEN 1 ELSE 0 END) + \n                      (CASE WHEN LOWER(s.case_environment_profile->'maintenance'->>'silica_status') = 'replace now' THEN 1 ELSE 0 END)) >= 3\n            THEN 'At Risk'\n            ELSE 'Stable'\n        END AS risk_status\n    FROM \"Showcases\" s\n    LEFT JOIN LatestReadings lr ON s.\"caseID\" = lr.\"case_link\"\n)\nSELECT \n    \"caseID\" AS showcase_id\nFROM ShowcaseRiskAssessment\nWHERE risk_status = 'At Risk'\nORDER BY \"caseID\";"], "external_knowledge": [5, 16, 61], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": true, "order": true}}
{"instance_id": "museum_artifact_8", "selected_database": "museum_artifact", "query": "Can you pull together a rundown of artifacts with 'Medium' or 'High' humidity sensitivity? I need the registry number, name, material type, and that sensitivity level for each. Plus, check if they're 'Over Exposure' or 'Within Limits' using a more cautious humidity threshold, and line them up by registry number.", "normal_query": "List all artifacts with 'Medium' or 'High' Humidity Sensitivity. For each, show the registry number, title, material type, and sensitivity level. Also, determine if they are 'Over Exposure' or 'Within Limits' based on a secondary, more cautious humidity threshold, and sort by registry number.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH artifact_humidity_data AS (\n    SELECT \n        ac.\"ARTregID\",\n        ac.\"art_title\",\n        ac.\"MatKind\",\n        (sd.env_handling_sensitivity -> 'environment' ->> 'humidity') AS \"humidity_sensitivity\",\n        erc.\"RH\" AS \"current_RH\"\n    FROM \"ArtifactsCore\" ac\n    JOIN \"SensitivityData\" sd ON ac.\"ARTregID\" = sd.\"ART_link\"\n    LEFT JOIN \"ConditionAssessments\" ca ON ac.\"ARTregID\" = ca.\"art_exam\"\n    LEFT JOIN \"Showcases\" s ON ca.\"case_exam\" = s.\"caseID\"\n    LEFT JOIN \"EnvironmentalReadingsCore\" erc ON s.\"caseID\" = erc.\"case_link\"\n    WHERE LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'humidity') IN ('medium', 'high')\n      AND erc.\"RH\" IS NOT NULL\n)\nSELECT \n    \"ARTregID\",\n    \"art_title\",\n    \"MatKind\",\n    \"humidity_sensitivity\",\n    CASE \n        WHEN LOWER(\"humidity_sensitivity\") = 'high' AND \"current_RH\" > 55 THEN 'Over Exposure'\n        WHEN LOWER(\"humidity_sensitivity\") = 'medium' AND \"current_RH\" > 60 THEN 'Over Exposure'\n        ELSE 'Within Limits'\n    END AS \"exposure_status\"\nFROM artifact_humidity_data\nORDER BY \"ARTregID\";"], "external_knowledge": [23, 57, 58], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "museum_artifact_9", "selected_database": "museum_artifact", "query": "We need a priority list based on comprehensive risk. First, calculate a total environmental threat score for all artifacts. I'm only interested in those officially at the second-highest risk level. From that group, find the top 10 with the highest threat scores. Give me their registration IDs and their scores, sorted from highest to lowest.", "normal_query": "Identify the top 10 artifacts in greatest danger by calculating their Total Environmental Threat Level (TETL). The report should only consider artifacts at the second-highest risk level and list their registration IDs and TETL scores, sorted from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH RiskLevelOrder AS (\n  SELECT 'High' AS risk_level, 3 AS risk_order\n  UNION ALL SELECT 'Medium', 2\n  UNION ALL SELECT 'Low', 1\n), SecondHighestRisk AS (\n  SELECT LOWER(risk_level) as risk_level FROM RiskLevelOrder ORDER BY risk_order DESC LIMIT 1 OFFSET 1\n), ArtifactEnvData AS (\n  SELECT\n    T1.\"ARTregID\", T1.\"ageYears\",\n    T2.env_handling_sensitivity -> 'environment' ->> 'light' AS light_sense,\n    (T2.env_handling_sensitivity -> 'environment' ->> 'temperature') AS \"tempSense\",\n    (T2.env_handling_sensitivity -> 'environment' ->> 'humidity') AS \"humiditySense\",\n    T6.\"TEMPc\", T6.\"RH\", T7.\"lux\", T7.\"visLxh\"\n  FROM \"ArtifactsCore\" AS T1\n  JOIN \"SensitivityData\" AS T2 ON T1.\"ARTregID\" = T2.\"ART_link\"\n  JOIN \"RiskAssessments\" AS T3 ON T1.\"ARTregID\" = T3.art_concern\n  JOIN \"ConditionAssessments\" AS T4 ON T1.\"ARTregID\" = T4.art_exam\n  JOIN \"Showcases\" AS T5 ON T4.\"case_exam\" = T5.\"caseID\"\n  JOIN \"EnvironmentalReadingsCore\" AS T6 ON T5.\"caseID\" = T6.case_link\n  JOIN \"LightAndRadiationReadings\" AS T7 ON T6.\"monitor_code\" = T7.env_link\n  WHERE LOWER(T3.\"risk_level\") = (SELECT risk_level FROM SecondHighestRisk)\n), Calculations AS (\n  SELECT\n    \"ARTregID\",\n    ((CASE WHEN LOWER(light_sense) = 'low' THEN 1 WHEN LOWER(light_sense) = 'medium' THEN 5 WHEN LOWER(light_sense) = 'high' THEN 10 ELSE 0 END) + \n     (CASE WHEN LOWER(\"tempSense\") = 'low' THEN 1 WHEN LOWER(\"tempSense\") = 'medium' THEN 5 WHEN LOWER(\"tempSense\") = 'high' THEN 10 ELSE 0 END) + \n     (CASE WHEN LOWER(\"humiditySense\") = 'low' THEN 1 WHEN LOWER(\"humiditySense\") = 'medium' THEN 5 WHEN LOWER(\"humiditySense\") = 'high' THEN 10 ELSE 0 END)) / 3.0 AS erf,\n    (\"lux\" * (CASE WHEN LOWER(light_sense) = 'low' THEN 1 WHEN LOWER(light_sense) = 'medium' THEN 5 WHEN LOWER(light_sense) = 'high' THEN 10 ELSE 0 END) * \"visLxh\") / 1000.0 AS ler,\n    \"ageYears\", \"RH\", \"TEMPc\"\n  FROM ArtifactEnvData\n), FinalTETL AS (\n  SELECT\n    \"ARTregID\",\n    (\"ageYears\" * erf * POWER((COALESCE(\"RH\", 50) - 50), 2) * COALESCE(\"TEMPc\", 20)) / 100000.0 AS mdr,\n    erf, ler\n  FROM Calculations\n)\nSELECT\n  \"ARTregID\",\n  erf + ler + (mdr * 2) AS tetl_score\nFROM FinalTETL\nWHERE erf IS NOT NULL AND ler IS NOT NULL AND mdr IS NOT NULL\nORDER BY tetl_score DESC\nLIMIT 10;"], "external_knowledge": [31], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "museum_artifact_10", "selected_database": "museum_artifact", "query": "Let's figure out our next rotation plan. For all items currently in a resting state, I need to see their ID, name, material, and how many months it's been on display. Then, calculate its maximum safe display time. Using our standard method, compute its final rotation priority score. To make things clear, add a final column that flags it for 'Immediate Rotation' if needed, otherwise just label it as 'Monitor'.", "normal_query": "Generate a rotation schedule based on the Exhibition Rotation Priority Score (ERPS). The report should only include artifacts in 'Resting' state and show their ID, name, material, current display duration, and Display Safety Duration (DSD) limit. It must also include the ERPS value and a final recommendation ('Immediate Rotation' or 'Monitor').", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH BaseData AS (\n    SELECT\n        ac.\"ARTregID\",\n        ac.art_title,\n        ac.\"MatKind\",\n        ac.conserve_status,\n        ar.rating_profile,\n        sd.env_handling_sensitivity,\n        lrr.lux,\n        lrr.\"visLxh\",\n        ur.\"displayMonths\"\n    FROM \"ArtifactsCore\" ac\n    JOIN \"ArtifactRatings\" ar ON ac.\"ARTregID\" = ar.\"ART_link\"\n    JOIN \"SensitivityData\" sd ON ac.\"ARTregID\" = sd.\"ART_link\"\n    JOIN \"ConditionAssessments\" ca ON ac.\"ARTregID\" = ca.art_exam\n    JOIN \"Showcases\" s ON ca.case_exam = s.\"caseID\"\n    JOIN \"EnvironmentalReadingsCore\" erc ON s.\"caseID\" = erc.case_link\n    JOIN \"UsageRecords\" ur ON erc.monitor_code = ur.env_link\n    JOIN \"LightAndRadiationReadings\" lrr ON erc.monitor_code = lrr.env_link\n    WHERE lrr.lux IS NOT NULL AND lrr.\"visLxh\" IS NOT NULL AND ur.\"displayMonths\" IS NOT NULL AND LOWER(ur.rotate_sched) = 'resting'\n),\nCalculations AS (\n    SELECT\n        \"ARTregID\",\n        art_title,\n        \"MatKind\",\n        \"displayMonths\",\n        COALESCE((((rating_profile->>'exhibit_value')::numeric + (rating_profile->>'research_score')::numeric + (rating_profile->>'cultural_score')::numeric) * \n        (10 - CASE LOWER(conserve_status) WHEN 'excellent' THEN 1 WHEN 'good' THEN 3 WHEN 'fair' THEN 5 WHEN 'poor' THEN 7 WHEN 'critical' THEN 10 ELSE 5 END)) / 30.0, 0) AS \"CPI\",\n        (36.0 * \n        (10 - CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'high' THEN 10 ELSE 0 END) * \n        (10 - CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'high' THEN 10 ELSE 0 END) * \n        (10 - CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'high' THEN 10 ELSE 0 END)\n        ) / 1000.0 AS \"DSD\",\n        (lux * \n        (CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'high' THEN 10 ELSE 0 END) * \n        \"visLxh\") / 1000.0 AS \"LER\"\n    FROM BaseData\n)\nSELECT\n  \"ARTregID\" AS \"Artifact ID\",\n  art_title AS \"Name\",\n  \"MatKind\" AS \"Material Type\",\n  \"displayMonths\" AS \"Current Display Duration\",\n  \"DSD\" AS \"Display Safety Duration (DSD)\",\n  ((\"DSD\" - \"displayMonths\") * (\"LER\" + 1) * (\"CPI\" + 1)) / 100.0 AS \"ERPS\",\n  CASE WHEN ((\"DSD\" - \"displayMonths\") * (\"LER\" + 1) * (\"CPI\" + 1)) / 100.0 < 0 THEN 'Immediate Rotation' ELSE 'Monitor' END AS \"Recommendation\"\nFROM Calculations\nORDER BY \"ERPS\" ASC\nLIMIT 5;"], "external_knowledge": [0, 1, 4, 8, 38, 52], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "museum_artifact_11", "selected_database": "museum_artifact", "query": "I'm trying to figure out our data storage plan. Can you analyze the environmental readings for 2023, 2024, and 2025? First, for each year, find the average temperature. Then, for each of those years, count how many days the temperature was off by a specific amount—a deviation that's more than zero but no more than 4 degrees from that year's average. Give me a table with the year, its average temperature, and the total count of these anomaly days.", "normal_query": "To assist with data partitioning strategy, generate a report showing the annual environmental anomaly count for the years 2023-2025. An anomaly is defined as a daily reading where the temperature deviation from that year's annual average is greater than 0°C and less than or equal to 4°C. The report should show the year, the calculated average temperature for that year, and the total count of anomaly days.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ValidReadings AS (\n    SELECT \"readTS\", \"TEMPc\"\n    FROM \"EnvironmentalReadingsCore\"\n    WHERE \"TEMPc\" IS NOT NULL \n    AND \"TEMPc\" BETWEEN -50 AND 100\n),\nYearlyAverages AS (\n    SELECT EXTRACT(YEAR FROM \"readTS\") as read_year,\n           AVG(\"TEMPc\") as avg_temp_for_year\n    FROM ValidReadings\n    WHERE EXTRACT(YEAR FROM \"readTS\") IN (2023, 2024, 2025)\n    GROUP BY EXTRACT(YEAR FROM \"readTS\")\n),\nAnomalousReadings AS (\n    SELECT\n        vr.\"readTS\",\n        ya.read_year\n    FROM ValidReadings vr\n    JOIN YearlyAverages ya ON EXTRACT(YEAR FROM vr.\"readTS\") = ya.read_year\n    WHERE ABS(vr.\"TEMPc\" - ya.avg_temp_for_year) > 0 AND ABS(vr.\"TEMPc\" - ya.avg_temp_for_year) <= 4\n),\nAnomalyDayCounts AS (\n    SELECT\n        read_year,\n        COUNT(DISTINCT CAST(\"readTS\" AS DATE)) as anomaly_day_count\n    FROM AnomalousReadings\n    GROUP BY read_year\n)\nSELECT\n    ya.read_year,\n    ROUND(COALESCE(ya.avg_temp_for_year, 0)::numeric, 2) as annual_avg_temp,\n    COALESCE(adc.anomaly_day_count, 0) as anomaly_day_count\nFROM YearlyAverages ya\nLEFT JOIN AnomalyDayCounts adc ON ya.read_year = adc.read_year\nORDER BY ya.read_year;"], "external_knowledge": [], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": true, "order": true}}
{"instance_id": "museum_artifact_12", "selected_database": "museum_artifact", "query": "I'm trying to find our most valuable artifacts (let's define that as a cultural score over 80) that might be at risk due to a weird environment. Can you find any of them that are in a showcase where the average temperature is more than 1 degree different from the average temperature of all other showcases in that same hall? For any you find, list the artifact's ID, its title, material, its showcase's temperature, what the hall's average temp is, and the exact deviation.", "normal_query": "Identify artifacts that are both a 'High-Value Artifact' (cultural score > 80) and an 'Environmental Outlier'. An outlier is defined as an artifact whose showcase has an average temperature that deviates by more than 1°C from the average temperature of all showcases in its hall. For these artifacts, show their ID, title, material, their showcase's temperature, the hall's average temperature, and the temperature deviation.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ValidReadings AS (\n    SELECT \n        erc.case_link,\n        erc.\"TEMPc\"\n    FROM \"EnvironmentalReadingsCore\" erc\n    WHERE erc.\"TEMPc\" IS NOT NULL\n),\nShowcaseAvgTemp AS (\n    SELECT \n        s.\"caseID\", \n        s.hall_ref,\n        AVG(vr.\"TEMPc\") AS showcase_avg_temp\n    FROM \"Showcases\" s\n    JOIN ValidReadings vr ON s.\"caseID\" = vr.case_link\n    GROUP BY s.\"caseID\", s.hall_ref\n),\nEnvironmentalOutliers AS (\n    SELECT \n        st.\"caseID\",\n        st.hall_ref,\n        st.showcase_avg_temp,\n        (SELECT AVG(vr.\"TEMPc\") \n         FROM \"Showcases\" s_inner\n         JOIN ValidReadings vr ON s_inner.\"caseID\" = vr.case_link\n         WHERE s_inner.hall_ref = st.hall_ref AND s_inner.\"caseID\" != st.\"caseID\"\n        ) AS hall_avg_temp,\n        ABS(st.showcase_avg_temp - \n            (SELECT AVG(vr.\"TEMPc\") \n             FROM \"Showcases\" s_inner\n             JOIN ValidReadings vr ON s_inner.\"caseID\" = vr.case_link\n             WHERE s_inner.hall_ref = st.hall_ref AND s_inner.\"caseID\" != st.\"caseID\"\n            )\n        ) as deviation\n    FROM ShowcaseAvgTemp st\n),\nHighValueArtifacts AS (\n    SELECT \n      ar.\"ART_link\" as \"ARTregID\",\n      ac.art_title,\n      ac.\"MatKind\",\n      ca.case_exam as \"caseID\"\n    FROM \"ArtifactRatings\" ar\n    JOIN \"ArtifactsCore\" ac ON ar.\"ART_link\" = ac.\"ARTregID\"\n    JOIN \"ConditionAssessments\" ca ON ac.\"ARTregID\" = ca.art_exam\n    WHERE (ar.rating_profile->>'cultural_score')::numeric > 80\n)\nSELECT \n    hva.\"ARTregID\", \n    hva.art_title, \n    hva.\"MatKind\",\n    eo.showcase_avg_temp,\n    eo.hall_avg_temp,\n    eo.deviation\nFROM HighValueArtifacts hva\nJOIN EnvironmentalOutliers eo ON hva.\"caseID\" = eo.\"caseID\"\nWHERE eo.deviation > 1;"], "external_knowledge": [], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "museum_artifact_13", "selected_database": "museum_artifact", "query": "Figure out the conservation priority score for every artifact, showing their ID, name, and score, ordered from highest to lowest priority.", "normal_query": "Calculate the Conservation Priority Index (CPI) for each artifact. The report should include the artifact ID, title, and the CPI score, sorted in descending order by CPI.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT ac.\"ARTregID\" AS artifact_id, ac.art_title, ROUND(((ar.\"HIST_sign\" + (ar.rating_profile ->> 'research_score')::INTEGER + (ar.rating_profile ->> 'cultural_score')::INTEGER) * (10 - CASE LOWER(ac.conserve_status) WHEN 'excellent' THEN 1 WHEN 'good' THEN 3 WHEN 'fair' THEN 5 WHEN 'poor' THEN 7 WHEN 'critical' THEN 10 END))::NUMERIC / 30, 2) AS cpi_score\nFROM \"ArtifactsCore\" ac\nJOIN \"ArtifactRatings\" ar ON ac.\"ARTregID\" = ar.\"ART_link\"\nWHERE ac.conserve_status IS NOT NULL\nORDER BY cpi_score DESC;"], "external_knowledge": [0, 21], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "order": true, "distinct": false}}
{"instance_id": "museum_artifact_14", "selected_database": "museum_artifact", "query": "Find all really valuable artifacts, group them by their historical period, and show the period, how many valuable artifacts there are, and their average cultural score, ordered from highest to lowest count.", "normal_query": "Identify all High-Value Artifacts and group them by dynasty. The report should show the dynasty, a count of high-value artifacts, and their average cultural score, sorted by the count in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ArtifactPercentiles AS (\n    SELECT\n        \"ART_link\",\n        NTILE(10) OVER (ORDER BY \"HIST_sign\" DESC) as hist_percentile,\n        NTILE(10) OVER (ORDER BY (rating_profile->>'cultural_score')::numeric DESC) as cultural_percentile\n    FROM \"ArtifactRatings\"\n),\nHighValueArtifacts AS (\n    SELECT \"ART_link\" as \"ARTregID\"\n    FROM ArtifactPercentiles\n    WHERE hist_percentile = 1 OR cultural_percentile = 1\n)\nSELECT\n    ac.\"DYNASTY\",\n    COUNT(hva.\"ARTregID\") AS high_value_count,\n    ROUND(AVG(CAST(ar.rating_profile ->> 'cultural_score' AS INTEGER))::NUMERIC, 2) AS avg_cultural_score\nFROM HighValueArtifacts hva\nJOIN \"ArtifactsCore\" ac ON hva.\"ARTregID\" = ac.\"ARTregID\"\nJOIN \"ArtifactRatings\" ar ON hva.\"ARTregID\" = ar.\"ART_link\"\nWHERE ac.\"DYNASTY\" IS NOT NULL\nGROUP BY ac.\"DYNASTY\"\nORDER BY high_value_count DESC;"], "external_knowledge": [11, 51], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "order": true, "distinct": false}}
{"instance_id": "museum_artifact_15", "selected_database": "museum_artifact", "query": "I want to find artifacts that we've put in the wrong place. Can you make a list of any artifact that has a 'Medium' sensitivity to something (like light, temp, or humidity) and is in a showcase that we've classified as having a 'Medium' level of that same thing? Show me the artifact's name, material, the showcase it's in, which specific sensitivity is the problem, what the sensitivity level is, and what the showcase's environment profile is.", "normal_query": "Identify environmental mismatches by finding artifacts whose specific environmental sensitivities are incompatible with the typical environment of their showcase. A 'mismatch' occurs if an artifact with 'Medium' sensitivity to an environmental factor (e.g., humidity) is in a showcase classified with a 'Medium' level of that same factor. The report should list the artifact's title, its material, the showcase ID, the mismatched sensitivity type, the sensitivity level, and the environment profile.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ValidReadings AS (\n    SELECT \n        erc.case_link,\n        erc.\"TEMPc\",\n        erc.\"RH\",\n        lr.lux\n    FROM \"EnvironmentalReadingsCore\" erc\n    LEFT JOIN \"LightAndRadiationReadings\" lr ON erc.\"monitor_code\" = lr.env_link\n    WHERE erc.\"TEMPc\" IS NOT NULL \n      AND erc.\"RH\" IS NOT NULL \n      AND erc.case_link IS NOT NULL\n),\nShowcaseEnvironment AS (\n    SELECT \n        case_link as \"caseID\",\n        CASE WHEN AVG(\"TEMPc\") > 20 THEN 'high' WHEN AVG(\"TEMPc\") < 18 THEN 'low' ELSE 'medium' END as temp_profile,\n        CASE WHEN AVG(\"RH\") > 55 THEN 'high' WHEN AVG(\"RH\") < 45 THEN 'low' ELSE 'medium' END as humidity_profile,\n        CASE WHEN AVG(COALESCE(lux, 0)) > 150 THEN 'high' WHEN AVG(COALESCE(lux, 0)) < 50 THEN 'low' ELSE 'medium' END as light_profile\n    FROM ValidReadings\n    GROUP BY case_link\n),\nArtifactSensitivities AS (\n    SELECT\n        \"ART_link\",\n        LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') AS temp_sensitivity,\n        LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') AS humidity_sensitivity,\n        LOWER(env_handling_sensitivity -> 'environment' ->> 'light') AS light_sensitivity\n    FROM \"SensitivityData\"\n),\nMismatches AS (\n    SELECT\n        ac.art_title,\n        ac.\"MatKind\",\n        s.\"caseID\",\n        'Temperature' AS mismatch_type, \n        arts.temp_sensitivity as sensitivity_level,\n        s.temp_profile as env_profile\n    FROM \"ArtifactsCore\" ac\n    JOIN \"ConditionAssessments\" ca ON ac.\"ARTregID\" = ca.art_exam\n    JOIN ShowcaseEnvironment s ON ca.\"case_exam\" = s.\"caseID\"\n    JOIN ArtifactSensitivities arts ON ac.\"ARTregID\" = arts.\"ART_link\"\n    WHERE arts.temp_sensitivity = 'medium' AND s.temp_profile = 'medium'\n    UNION ALL\n    SELECT\n        ac.art_title,\n        ac.\"MatKind\",\n        s.\"caseID\",\n        'Humidity' AS mismatch_type,\n        arts.humidity_sensitivity as sensitivity_level,\n        s.humidity_profile as env_profile\n    FROM \"ArtifactsCore\" ac\n    JOIN \"ConditionAssessments\" ca ON ac.\"ARTregID\" = ca.art_exam\n    JOIN ShowcaseEnvironment s ON ca.\"case_exam\" = s.\"caseID\"\n    JOIN ArtifactSensitivities arts ON ac.\"ARTregID\" = arts.\"ART_link\"\n    WHERE arts.humidity_sensitivity = 'medium' AND s.humidity_profile = 'medium'\n    UNION ALL\n    SELECT\n        ac.art_title,\n        ac.\"MatKind\",\n        s.\"caseID\",\n        'Light' AS mismatch_type,\n        arts.light_sensitivity as sensitivity_level,\n        s.light_profile as env_profile\n    FROM \"ArtifactsCore\" ac\n    JOIN \"ConditionAssessments\" ca ON ac.\"ARTregID\" = ca.art_exam\n    JOIN ShowcaseEnvironment s ON ca.\"case_exam\" = s.\"caseID\"\n    JOIN ArtifactSensitivities arts ON ac.\"ARTregID\" = arts.\"ART_link\"\n    WHERE arts.light_sensitivity = 'medium' AND s.light_profile = 'medium'\n)\nSELECT \n    art_title, \n    \"MatKind\", \n    \"caseID\", \n    mismatch_type, \n    sensitivity_level,\n    env_profile\nFROM Mismatches\nORDER BY art_title;"], "external_knowledge": [60], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "museum_artifact_16", "selected_database": "museum_artifact", "query": "Group artifacts by how vulnerable their organic material is and their environmental risk score. Show me what they're made of, their vulnerability status, their average environmental risk, and a count for each group, ordered by the highest average risk.", "normal_query": "Cluster artifacts by their Organic Material Vulnerability and Environmental Risk Factor (ERF). The report should show material type, vulnerability status, average ERF, and artifact count, sorted by average ERF descending.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    ac.\"MatKind\", \n    CASE WHEN LOWER(ac.\"MatKind\") IN ('wood', 'textile', 'paper') AND LOWER(sd.\"ENVsense\") = 'high' THEN 'Vulnerable' ELSE 'Non-Vulnerable' END AS vulnerability_status, \n    ROUND(AVG((\n        CASE LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'light') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 5 END +\n        CASE LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'temperature') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 5 END +\n        CASE LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'humidity') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 5 END +\n        CASE LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'vibration') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 5 END +\n        CASE LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'pollutants') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 5 END +\n        CASE LOWER(sd.env_handling_sensitivity -> 'biological' ->> 'pest') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 5 END +\n        CASE LOWER(sd.env_handling_sensitivity -> 'handling_transport' ->> 'handling') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 5 END +\n        CASE LOWER(sd.env_handling_sensitivity -> 'handling_transport' ->> 'transport') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 5 END +\n        CASE LOWER(sd.env_handling_sensitivity -> 'context' ->> 'display') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 5 END +\n        CASE LOWER(sd.env_handling_sensitivity -> 'context' ->> 'storage') WHEN 'low' THEN 1 WHEN 'medium' THEN 5 WHEN 'high' THEN 10 ELSE 5 END\n    )::NUMERIC / 10), 2) AS avg_erf, \n    COUNT(*) AS artifact_count\nFROM \"ArtifactsCore\" ac\nJOIN \"SensitivityData\" sd ON ac.\"ARTregID\" = sd.\"ART_link\"\nGROUP BY ac.\"MatKind\", vulnerability_status\nORDER BY avg_erf DESC;"], "external_knowledge": [1, 2, 20], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "order": true, "distinct": false}}
{"instance_id": "museum_artifact_17", "selected_database": "museum_artifact", "query": "I'm looking for ticking time bombs in our collection. Can you find all the artifacts that are currently listed as 'Good' or 'Excellent', but are at a high risk of light damage? Let's say 'high risk' means they have 'High' light sensitivity and they've already soaked up more than 70,000 lux-hours of light. For each one you find, show me its name, dynasty, its exact light sensitivity, the current lux level, and its total light exposure. Please list the ones with the most total exposure first.", "normal_query": "Generate a report of artifacts with a 'Good' or 'Excellent' conservation status that are at high risk of light damage. A high-risk artifact is defined as having 'High' light sensitivity AND has a cumulative light exposure exceeding 70,000 lxh. The report should include the artifact's title, dynasty, light sensitivity level, current lux, and cumulative exposure (visLxh), sorted by cumulative exposure in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH HighRiskCandidates AS (\n    SELECT \n        ac.\"ARTregID\",\n        ac.art_title,\n        ac.\"DYNASTY\",\n        sd.env_handling_sensitivity -> 'environment' ->> 'light' AS light_sensitivity\n    FROM \"ArtifactsCore\" ac\n    JOIN \"SensitivityData\" sd ON ac.\"ARTregID\" = sd.\"ART_link\"\n    WHERE LOWER(ac.conserve_status) IN ('good', 'excellent') \n    AND LOWER(sd.env_handling_sensitivity -> 'environment' ->> 'light') = 'high'\n),\nLatestReadings AS (\n    SELECT \n        ca.art_exam AS \"ARTregID\",\n        lr.lux,\n        lr.\"visLxh\"\n    FROM \"ConditionAssessments\" ca\n    JOIN \"LightAndRadiationReadings\" lr ON ca.light_link = lr.rad_id\n    WHERE (ca.art_exam, lr.rad_id) IN (\n        SELECT art_exam, MAX(light_link) \n        FROM \"ConditionAssessments\" \n        WHERE light_link IS NOT NULL\n        GROUP BY art_exam\n    )\n)\nSELECT \n    hrc.art_title,\n    hrc.\"DYNASTY\",\n    hrc.light_sensitivity,\n    lr.lux,\n    lr.\"visLxh\"\nFROM HighRiskCandidates hrc\nJOIN LatestReadings lr ON hrc.\"ARTregID\" = lr.\"ARTregID\"\nWHERE lr.\"visLxh\" > 70000\nORDER BY lr.\"visLxh\" DESC;"], "external_knowledge": [], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "museum_artifact_18", "selected_database": "museum_artifact", "query": "What's the single highest risk score for any artifact, considering both its conservation priority and environmental sensitivity?", "normal_query": "What is the maximum Artifact Vulnerability Score (AVS) found among all artifacts in the collection?", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH CPI_Calc AS (\n  SELECT\n    T1.\"ARTregID\",\n    ((T2.\"HIST_sign\" + (T2.rating_profile->>'research_score')::numeric + (T2.rating_profile->>'cultural_score')::numeric)) * (10 - \n      CASE LOWER(T1.\"conserve_status\") \n        WHEN 'excellent' THEN 1 WHEN 'good' THEN 3 WHEN 'fair' THEN 5 \n        WHEN 'poor' THEN 7 WHEN 'critical' THEN 10 ELSE 0 \n      END) / 30.0 AS CPI\n  FROM \"ArtifactsCore\" AS T1 JOIN \"ArtifactRatings\" AS T2 ON T1.\"ARTregID\" = T2.\"ART_link\"\n),\nERF_Calc AS (\n  SELECT\n    \"ART_link\",\n    ((\n      CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'high' THEN 10 ELSE 0 END + \n      CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'high' THEN 10 ELSE 0 END + \n      CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'high' THEN 10 ELSE 0 END + \n      CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'vibration') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'vibration') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'vibration') = 'high' THEN 10 ELSE 0 END + \n      CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'pollutants') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'pollutants') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'pollutants') = 'high' THEN 10 ELSE 0 END\n    ) / 5.0) AS ERF\n  FROM \"SensitivityData\"\n)\nSELECT MAX(T2.CPI * T3.ERF)\nFROM \"ArtifactsCore\" AS T1\nJOIN CPI_Calc AS T2 ON T1.\"ARTregID\" = T2.\"ARTregID\"\nJOIN ERF_Calc AS T3 ON T1.\"ARTregID\" = T3.\"ART_link\";"], "external_knowledge": [0, 1, 2, 3, 21], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "museum_artifact_19", "selected_database": "museum_artifact", "query": "How well do our showcases generally suit the artifacts from the Ming dynasty? I'm looking for a single number that tells me the average compatibility.", "normal_query": "Calculate the average Artifact Exhibition Compatibility (AEC) for all artifacts from the 'Ming' dynasty.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH ERF_Calc AS (\n  SELECT\n    \"ART_link\",\n    ((CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'high' THEN 10 ELSE 0 END) + \n     (CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'high' THEN 10 ELSE 0 END) + \n     (CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'high' THEN 10 ELSE 0 END)) / 3.0 AS ERF\n  FROM \"SensitivityData\"\n),\nSESR_Calc AS (\n  SELECT\n    T1.\"caseID\",\n    10 - (T2.\"tempVar24\" + T2.\"RHvar\" / 5.0 + (T1.case_environment_profile->'physical_state'->>'leak_rate_per_day')::numeric) / 3.0 AS SESR\n  FROM \"Showcases\" AS T1\n  JOIN \"EnvironmentalReadingsCore\" AS T2 ON T1.\"caseID\" = T2.\"case_link\"\n)\nSELECT AVG(10 - ABS(T4.ERF - T5.SESR))\nFROM \"ArtifactsCore\" AS T1\nJOIN \"ConditionAssessments\" AS T2 ON T1.\"ARTregID\" = T2.art_exam\nJOIN \"Showcases\" AS T3 ON T2.\"case_exam\" = T3.\"caseID\"\nJOIN ERF_Calc AS T4 ON T1.\"ARTregID\" = T4.\"ART_link\"\nJOIN SESR_Calc AS T5 ON T3.\"caseID\" = T5.\"caseID\"\nWHERE LOWER(T1.\"DYNASTY\") = 'ming';"], "external_knowledge": [6, 2, 5, 1], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "museum_artifact_20", "selected_database": "museum_artifact", "query": "Can you give me a total count of display cases that are at risk of failing? I'm talking about any case with a very unstable environment or at least three major maintenance problems.", "normal_query": "Calculate the total number of showcases that are currently considered a Showcase Failure Risk.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH SESR_Calc AS (\n  SELECT\n    T1.\"caseID\",\n    AVG(10 - (T2.\"tempVar24\" + T2.\"RHvar\" / 5.0 + (T1.case_environment_profile->'physical_state'->>'leak_rate_per_day')::numeric) / 3.0) AS SESR\n  FROM \"Showcases\" AS T1\n  JOIN \"EnvironmentalReadingsCore\" AS T2 ON T1.\"caseID\" = T2.\"case_link\"\n  WHERE T2.\"tempVar24\" IS NOT NULL AND T2.\"RHvar\" IS NOT NULL AND (T1.case_environment_profile->'physical_state'->>'leak_rate_per_day') IS NOT NULL\n  GROUP BY T1.\"caseID\"\n), Maint_Issues AS (\n  SELECT\n    \"caseID\",\n    (CASE WHEN LOWER(case_environment_profile -> 'physical_state' ->> 'seal_state') = 'poor' THEN 1 ELSE 0 END) + \n    (CASE WHEN LOWER(case_environment_profile -> 'maintenance' ->> 'maint_status') = 'overdue' THEN 1 ELSE 0 END) + \n    (CASE WHEN LOWER(case_environment_profile -> 'maintenance' ->> 'filter_status') = 'replace now' THEN 1 ELSE 0 END) + \n    (CASE WHEN LOWER(case_environment_profile -> 'maintenance' ->> 'silica_status') = 'replace now' THEN 1 ELSE 0 END) AS issue_count\n  FROM \"Showcases\"\n)\nSELECT COUNT(DISTINCT T1.\"caseID\")\nFROM \"Showcases\" AS T1\nJOIN SESR_Calc AS T2 ON T1.\"caseID\" = T2.\"caseID\"\nJOIN Maint_Issues AS T3 ON T1.\"caseID\" = T3.\"caseID\"\nWHERE T2.SESR < 4 OR T3.issue_count >= 3;"], "external_knowledge": [16, 5, 61], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 0, "distinct": true, "order": false}}
{"instance_id": "museum_artifact_M_1", "selected_database": "museum_artifact", "query": "I need to add a maintenance alert. Find every artifact that has a condition report on file, and for each one, append a timestamped alert saying 'Alert (Conservation Emergency): Immediate action recommended' to its maintenance log.", "normal_query": "For all artifacts that have an existing condition assessment record, append a timestamped alert with the text 'Alert (Conservation Emergency): Immediate action recommended' to their maintenance log.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE \"ConservationAndMaintenance\"\nSET \"maintLog\" = COALESCE(\"maintLog\", '') || E'\\n' || 'Alert (Conservation Emergency): Immediate action recommended as of ' || to_char(now(), 'YYYY-MM-DD HH24:MI:SS')\nWHERE maint_id IN (\n    SELECT cm.maint_id\n    FROM \"ConditionAssessments\" ca\n    JOIN \"Showcases\" s ON ca.case_exam = s.\"caseID\"\n    JOIN \"EnvironmentalReadingsCore\" erc ON s.\"caseID\" = erc.case_link\n    JOIN \"ConservationAndMaintenance\" cm ON erc.monitor_code = cm.monitor_link\n)\nRETURNING maint_id;"], "external_knowledge": [], "test_cases": [], "category": "Management", "high_level": true, "conditions": {"decimal": 0, "order": false, "distinct": false}}
{"instance_id": "museum_artifact_M_2", "selected_database": "museum_artifact", "query": "Write a function called 'calculate_cpi' that figures out how important an artifact is for conservation, using its historical, research, and cultural value, plus its current condition, and gives back a final score.", "normal_query": "Create a function named 'calculate_cpi' to compute a conservation priority score. This score should be based on historical significance, research value, cultural importance, and conservation condition, returning a numeric value.", "preprocess_sql": [], "clean_up_sqls": ["DROP FUNCTION IF EXISTS calculate_cpi(SMALLINT, INT, SMALLINT, VARCHAR);"], "sol_sql": ["CREATE OR REPLACE FUNCTION calculate_cpi(\n    hist_sign_rating SMALLINT, \n    research_val INT, \n    cult_score_val SMALLINT, \n    conserve_status_text VARCHAR\n) \nRETURNS NUMERIC AS $$\nDECLARE\n    status_val INT;\nBEGIN\n    status_val := CASE LOWER(conserve_status_text)\n        WHEN 'excellent' THEN 1\n        WHEN 'good' THEN 3\n        WHEN 'fair' THEN 5\n        WHEN 'poor' THEN 7\n        WHEN 'critical' THEN 10\n        ELSE 5\n    END;\n    RETURN ((hist_sign_rating + research_val + cult_score_val) * (10 - status_val))::NUMERIC / 30;\nEND;\n$$ LANGUAGE plpgsql;"], "external_knowledge": [0], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn, **kwargs):\n    import psycopg2\n    # First, create the function using the submission's SQL\n    execute_queries(pred_sqls, db_name, conn)\n    # Test cases: (hist_sign, research_val, cult_score, conserve_status, expected_cpi)\n    test_values = [\n        (5, 7, 8, 'Excellent', 6.0),\n        (3, 5, 6, 'Good', 3.266666),\n        (8, 9, 10, 'Critical', 0.0),\n        (4, 6, 5, 'Poor', 1.5)\n    ]\n    with conn.cursor() as cursor:\n        for hist, res, cult, stat, expected in test_values:\n            try:\n                # FIX: Add explicit type casts to match the function signature exactly.\n                cursor.execute(f\"SELECT calculate_cpi({hist}::smallint, {res}::integer, {cult}::smallint, '{stat}'::varchar);\")\n                result = cursor.fetchone()[0]\n                assert abs(float(result) - expected) < 1e-5, f\"For {stat}, expected ~{expected} but got {result}\"\n            except psycopg2.Error as e:\n                assert False, f\"Function call failed for input ({hist}, {res}, {cult}, '{stat}'): {e}\"\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "order": false, "distinct": false}}
{"instance_id": "museum_artifact_M_3", "selected_database": "museum_artifact", "query": "Put a rule on the artifact ratings table to make sure the historical importance score stays between 1 and 10.", "normal_query": "Add a check constraint named 'hist_sign_rating_check' to the 'ArtifactRatings' table. This constraint should ensure the historical significance score is between 1 and 10, inclusive.", "preprocess_sql": ["UPDATE \"ArtifactRatings\" SET \"HIST_sign\" = 11 WHERE \"ART_link\" = 'ART54317';"], "clean_up_sqls": ["ALTER TABLE \"ArtifactRatings\" DROP CONSTRAINT IF EXISTS hist_sign_rating_check;", "UPDATE \"ArtifactRatings\" SET \"HIST_sign\" = 7 WHERE \"ART_link\" = 'ART54317';"], "sol_sql": ["DELETE FROM \"ArtifactRatings\" WHERE \"HIST_sign\" < 1 OR \"HIST_sign\" > 10;", "ALTER TABLE \"ArtifactRatings\" DROP CONSTRAINT IF EXISTS hist_sign_rating_check;", "ALTER TABLE \"ArtifactRatings\" ADD CONSTRAINT hist_sign_rating_check CHECK (\"HIST_sign\" >= 1 AND \"HIST_sign\" <= 10);", "SELECT 1 AS success;"], "external_knowledge": [22], "test_cases": [], "category": "Management", "high_level": false, "conditions": {"decimal": 0, "order": false, "distinct": false}}
{"instance_id": "museum_artifact_M_4", "selected_database": "museum_artifact", "query": "We need to decide which artifacts to put into storage next. Figure out the five most urgent candidates for rotation based on a priority score, but with a twist: if an artifact is made of organic material like textile, wood, or paper, increase its final score by multiplying it by 1.2 because it's more fragile. I just need the artifact titles and their final adjusted scores, with the most urgent one (the one with the lowest score) first.", "normal_query": "Create a database view named 'V_Top5_Rotation_Priority' that provides a priority list of the top 5 artifacts for exhibition rotation. The standard Exhibition Rotation Priority Score (ERPS) calculation needs to be adjusted: for artifacts made of 'Organic' materials ('Textile', 'Wood', 'Paper'), their final ERPS score should be multiplied by 1.2 (a 20% vulnerability factor). The view should include the artifact's title and its final, adjusted ERPS score, sorted by the adjusted ERPS in ascending order (lowest score is highest priority).", "preprocess_sql": [], "clean_up_sqls": ["DROP VIEW IF EXISTS V_Top5_Rotation_Priority;", "DROP VIEW IF EXISTS V_Rotation_Priority_Analysis;"], "sol_sql": ["CREATE OR REPLACE VIEW \"V_Top5_Rotation_Priority\" AS\nSELECT\n  T1.\"art_title\",\n  (\n    (\n      COALESCE(\n        (36 * (10 - \n          (CASE WHEN LOWER(T_sens.\"env_handling_sensitivity\" -> 'environment' ->> 'light') = 'low' THEN 1 \n               WHEN LOWER(T_sens.\"env_handling_sensitivity\" -> 'environment' ->> 'light') = 'medium' THEN 5 \n               WHEN LOWER(T_sens.\"env_handling_sensitivity\" -> 'environment' ->> 'light') = 'high' THEN 10 ELSE 0 END)) * (10 - \n          (CASE WHEN LOWER(T_sens.\"env_handling_sensitivity\" -> 'environment' ->> 'temperature') = 'low' THEN 1 \n               WHEN LOWER(T_sens.\"env_handling_sensitivity\" -> 'environment' ->> 'temperature') = 'medium' THEN 5\n               WHEN LOWER(T_sens.\"env_handling_sensitivity\" -> 'environment' ->> 'temperature') = 'high' THEN 10 ELSE 0 END)) * (10 - \n          (CASE WHEN LOWER(T_sens.\"env_handling_sensitivity\" -> 'environment' ->> 'humidity') = 'low' THEN 1 \n               WHEN LOWER(T_sens.\"env_handling_sensitivity\" -> 'environment' ->> 'humidity') = 'medium' THEN 5 \n               WHEN LOWER(T_sens.\"env_handling_sensitivity\" -> 'environment' ->> 'humidity') = 'high' THEN 10 ELSE 0 END))\n        ) / 1000.0, 0)\n      - COALESCE(T_usage.\"displayMonths\", 0)\n    ) *\n    (COALESCE(\n        (T_light.\"lux\" * \n          (CASE WHEN LOWER(T_sens.\"env_handling_sensitivity\" -> 'environment' ->> 'light') = 'low' THEN 1 \n                WHEN LOWER(T_sens.\"env_handling_sensitivity\" -> 'environment' ->> 'light') = 'medium' THEN 5\n                WHEN LOWER(T_sens.\"env_handling_sensitivity\" -> 'environment' ->> 'light') = 'high' THEN 10 \n                ELSE 0 END) * T_light.\"visLxh\") / 1000.0, 0) + 1\n    ) *\n    (\n      ((T2.\"HIST_sign\" + (T2.\"rating_profile\"->>'research_score')::numeric + (T2.\"rating_profile\"->>'cultural_score')::numeric)) * (10 - \n      CASE LOWER(T1.\"conserve_status\")\n        WHEN 'excellent' THEN 1 WHEN 'good' THEN 3 WHEN 'fair' THEN 5\n        WHEN 'poor' THEN 7 WHEN 'critical' THEN 10 ELSE 0\n      END) / 30.0 + 1\n    ) / 100.0\n  ) * (CASE WHEN LOWER(T1.\"MatKind\") IN ('textile', 'wood', 'paper') THEN 1.2 ELSE 1.0 END) AS adjusted_ERPS\nFROM \"ArtifactsCore\" AS T1\nJOIN \"ArtifactRatings\" AS T2 ON T1.\"ARTregID\" = T2.\"ART_link\"\nINNER JOIN \"ConditionAssessments\" AS T_assess ON T1.\"ARTregID\" = T_assess.\"art_exam\"\nLEFT JOIN \"LightAndRadiationReadings\" AS T_light ON T_assess.\"light_link\" = T_light.\"rad_id\"\nLEFT JOIN \"SensitivityData\" AS T_sens ON T1.\"ARTregID\" = T_sens.\"ART_link\"\nLEFT JOIN \"Showcases\" AS T_case ON T_assess.\"case_exam\" = T_case.\"caseID\"\nLEFT JOIN \"EnvironmentalReadingsCore\" AS T_env ON T_case.\"caseID\" = T_env.\"case_link\"\nLEFT JOIN \"UsageRecords\" AS T_usage ON T_env.\"monitor_code\" = T_usage.\"env_link\"\nORDER BY adjusted_ERPS ASC\nLIMIT 5;", "SELECT * FROM \"V_Top5_Rotation_Priority\";"], "external_knowledge": [0, 1, 4, 8, 38, 52, 62, 63], "test_cases": [], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "museum_artifact_M_5", "selected_database": "museum_artifact", "query": "Can you find all the artifacts made of materials like textile or paper that are both extremely valuable and highly sensitive to their environment? For each one, show its name and its exact sensitivity levels for light, temperature, and humidity.", "normal_query": "Create a database view named 'V_Precious_Vulnerable_Organic_Items' to identify all artifacts that are both a High-Value Artifact and meet the Organic Material Vulnerability criteria. The view should display each artifact's title and its specific sensitivity ratings for light, temperature, and humidity.", "preprocess_sql": [], "clean_up_sqls": ["DROP VIEW IF EXISTS V_Precious_Vulnerable_Organic_Items;", "DROP VIEW IF EXISTS V_High_Light_Risk_Artifact_Status;"], "sol_sql": ["CREATE OR REPLACE VIEW \"V_Precious_Vulnerable_Organic_Items\" AS\nWITH HighValueArtifacts AS (\n  SELECT \"ART_link\" AS art_id\n  FROM \"ArtifactRatings\"\n  WHERE (rating_profile->>'cultural_score')::numeric > 8\n  UNION\n  SELECT \"art_concern\"\n  FROM \"RiskAssessments\"\n  WHERE \"evacPrio\" = 'Priority 1'\n)\nSELECT\n  T1.\"art_title\",\n  T3.env_handling_sensitivity -> 'environment' ->> 'light' AS light_sensitivity,\n  T3.env_handling_sensitivity -> 'environment' ->> 'temperature' AS temperature_sensitivity,\n  T3.env_handling_sensitivity -> 'environment' ->> 'humidity' AS humidity_sensitivity\nFROM \"ArtifactsCore\" AS T1\nJOIN \"SensitivityData\" AS T3 ON T1.\"ARTregID\" = T3.\"ART_link\"\nJOIN HighValueArtifacts AS T_high ON T1.\"ARTregID\" = T_high.art_id\nWHERE LOWER(T1.\"MatKind\") IN ('textile', 'paper', 'wood')\n  AND LOWER(T3.\"ENVsense\") = 'high';", "SELECT * FROM \"V_Precious_Vulnerable_Organic_Items\";"], "external_knowledge": [11, 20], "test_cases": [], "category": "Management", "high_level": true, "conditions": {"decimal": 0, "distinct": false, "order": false}}
{"instance_id": "museum_artifact_M_6", "selected_database": "museum_artifact", "query": "I need to know which of our halls are a security nightmare. Find any hall that has a high visitor impact score (say, over 15) but at the same time has a low security score (less than 8). For each of those problem halls, show me the hall ID and both of those scores so I can see what's going on.", "normal_query": "Create a view named 'V_High_Threat_Halls' to identify 'High Threat' exhibition halls, defined as those with a Visitor Impact Risk (VIR) score greater than 15 and a calculated Security Score below 8. The view should show the hall's ID, its VIR score, and its Security Score.", "preprocess_sql": [], "clean_up_sqls": ["DROP VIEW IF EXISTS V_High_Threat_Halls;", "DROP VIEW IF EXISTS V_Critical_Artifacts_In_High_Threat_Halls;"], "sol_sql": ["CREATE OR REPLACE VIEW \"V_High_Threat_Halls\" AS\nWITH HallScores AS (\n    SELECT\n        \"Hall_ID\",\n        ((security_visitor_overview->'visitor_statistics'->>'avg_daily_visitors')::numeric * \n         CASE LOWER(security_visitor_overview->'visitor_statistics'->>'visitor_flow') WHEN 'low' THEN 1 WHEN 'medium' THEN 3 WHEN 'high' THEN 5 END * \n         (security_visitor_overview->'visitor_statistics'->>'avg_dwell_minutes')::numeric) / 1000.0 AS vir_score,\n        (\n            CASE LOWER(security_visitor_overview->'security'->>'alarm_status') WHEN 'armed' THEN 3 ELSE 0 END +\n            CASE LOWER(security_visitor_overview->'security'->>'cctv_coverage') WHEN 'full' THEN 3 ELSE 0 END +\n            CASE LOWER(security_visitor_overview->'security'->>'access_control') WHEN 'active' THEN 2 ELSE 0 END +\n            CASE LOWER(security_visitor_overview->'security'->>'motion_detection') WHEN 'active' THEN 2 ELSE 0 END\n        ) AS security_score\n    FROM \"ExhibitionHalls\"\n)\nSELECT \n    \"Hall_ID\", \n    vir_score, \n    security_score\nFROM HallScores\nWHERE vir_score > 15 AND security_score < 8;", "SELECT * FROM \"V_High_Threat_Halls\";"], "external_knowledge": [64, 65], "test_cases": [], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "museum_artifact_M_7", "selected_database": "museum_artifact", "query": "Out of all our textiles, how many are in a 'Poor' state of environmental compliance? Assume the ideal is 20 degrees Celsius and 50 percent humidity to make the call.", "normal_query": "Create a function named 'get_poor_compliance_count_by_material' that accepts a material type (e.g., 'textile') and calculates the number of artifacts of that material with a 'Poor' Compliance Level. The calculation should be based on the Environmental Compliance Index (ECI) with an ideal temperature of 20°C and ideal humidity of 50%.", "preprocess_sql": [], "clean_up_sqls": ["DROP FUNCTION IF EXISTS get_poor_compliance_count_by_material(TEXT);", "DROP VIEW IF EXISTS V_Compliance_Report_By_Material;"], "sol_sql": ["CREATE OR REPLACE FUNCTION get_poor_compliance_count_by_material(material_type TEXT) RETURNS BIGINT AS $$\nDECLARE\n    poor_count BIGINT;\nBEGIN\n    WITH ERF_Calc AS (\n      SELECT\n        \"ART_link\",\n        ((CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'high' THEN 10 ELSE 0 END) + \n         (CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'high' THEN 10 ELSE 0 END) + \n         (CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'high' THEN 10 ELSE 0 END)) / 3.0 AS ERF\n      FROM \"SensitivityData\"\n    ),\n    ECI_Calc AS (\n      SELECT\n        10 - (ABS(T4.\"TEMPc\" - 20) + ABS(T4.\"RH\" - 50) / 5.0 + T5.ERF / 2.0) AS ECI\n      FROM \"ArtifactsCore\" AS T1\n      JOIN \"ConditionAssessments\" AS T2 ON T1.\"ARTregID\" = T2.\"art_exam\"\n      JOIN \"EnvironmentalReadingsCore\" AS T4 ON T2.\"case_exam\" = T4.\"case_link\"\n      JOIN ERF_Calc AS T5 ON T1.\"ARTregID\" = T5.\"ART_link\"\n      WHERE LOWER(T1.\"MatKind\") = LOWER(material_type) AND T4.\"TEMPc\" IS NOT NULL AND T4.\"RH\" IS NOT NULL\n    )\n    SELECT COUNT(*) INTO poor_count\n    FROM ECI_Calc\n    WHERE ECI <= 4;\n\n    RETURN poor_count;\nEND;\n$$ LANGUAGE plpgsql;", "SELECT get_poor_compliance_count_by_material('textile');"], "external_knowledge": [39, 2, 60, 1], "test_cases": [], "category": "Management", "high_level": true, "conditions": {"decimal": 0, "distinct": false, "order": false}}
{"instance_id": "museum_artifact_M_8", "selected_database": "museum_artifact", "query": "Can you tell me the exact number of our important dynasty artifacts (from Ming, Han, or Tang) that are aging too quickly and are considered at risk?", "normal_query": "Create a function named 'get_dynasty_artifacts_at_risk_count' to calculate the total number of artifacts classified as a Dynasty Artifact at Risk.", "preprocess_sql": [], "clean_up_sqls": ["DROP FUNCTION IF EXISTS get_dynasty_artifacts_at_risk_count();", "DROP VIEW IF EXISTS V_Dynasty_Risk_Report;"], "sol_sql": ["CREATE OR REPLACE FUNCTION get_dynasty_artifacts_at_risk_count() RETURNS BIGINT AS $$\nDECLARE\n    risk_count BIGINT;\nBEGIN\n    WITH ERF_Calc AS (\n      SELECT \"ART_link\", (COALESCE(CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'light') = 'high' THEN 10 ELSE 0 END, 0) + COALESCE(CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'temperature') = 'high' THEN 10 ELSE 0 END, 0) + COALESCE(CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'humidity') = 'high' THEN 10 ELSE 0 END, 0) + COALESCE(CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'vibration') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'vibration') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'vibration') = 'high' THEN 10 ELSE 0 END, 0) + COALESCE(CASE WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'pollutants') = 'low' THEN 1 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'pollutants') = 'medium' THEN 5 WHEN LOWER(env_handling_sensitivity -> 'environment' ->> 'pollutants') = 'high' THEN 10 ELSE 0 END, 0)) / 5.0 AS ERF\n      FROM \"SensitivityData\"\n    ), MDR_Calc AS (\n      SELECT T1.\"ARTregID\", (T1.\"ageYears\" * T3.ERF * POWER((T4.\"RH\" - 50), 2) * T4.\"TEMPc\") / 100000.0 AS MDR\n      FROM \"ArtifactsCore\" AS T1\n      JOIN \"ConditionAssessments\" AS T2 ON T1.\"ARTregID\" = T2.\"art_exam\"\n      JOIN ERF_Calc AS T3 ON T1.\"ARTregID\" = T3.\"ART_link\"\n      JOIN \"EnvironmentalReadingsCore\" AS T4 ON T2.\"case_exam\" = T4.\"case_link\"\n      WHERE T4.\"RH\" IS NOT NULL AND T4.\"TEMPc\" IS NOT NULL\n    ), LER_Calc AS (\n      SELECT T2.\"art_exam\" AS ARTregID, (T4.\"lux\" * (CASE WHEN LOWER(T5.env_handling_sensitivity -> 'environment' ->> 'light') = 'low' THEN 1 WHEN LOWER(T5.env_handling_sensitivity -> 'environment' ->> 'light') = 'medium' THEN 5 WHEN LOWER(T5.env_handling_sensitivity -> 'environment' ->> 'light') = 'high' THEN 10 ELSE 0 END) * T4.\"visLxh\") / 1000.0 AS LER\n      FROM \"ConditionAssessments\" AS T2\n      JOIN \"LightAndRadiationReadings\" AS T4 ON T2.\"light_link\" = T4.\"rad_id\"\n      JOIN \"SensitivityData\" AS T5 ON T2.\"art_exam\" = T5.\"ART_link\"\n      WHERE T4.\"lux\" IS NOT NULL\n    ), DynastyValueArtifacts AS (\n      SELECT T1.\"ARTregID\"\n      FROM \"ArtifactsCore\" AS T1 JOIN \"ArtifactRatings\" AS T2 ON T1.\"ARTregID\" = T2.\"ART_link\"\n      WHERE LOWER(T1.\"DYNASTY\") IN ('ming', 'han', 'tang') AND (T2.rating_profile->>'research_score')::numeric > 8\n    ), Final_Calc AS (\n      SELECT\n        dva.\"ARTregID\",\n        COALESCE(mdr.MDR, 0) AS MDR,\n        COALESCE(ler.LER, 0) AS LER,\n        COALESCE(erf.ERF, 0) AS ERF\n      FROM DynastyValueArtifacts dva\n      LEFT JOIN MDR_Calc mdr ON dva.\"ARTregID\" = mdr.\"ARTregID\"\n      LEFT JOIN LER_Calc ler ON dva.\"ARTregID\" = ler.ARTregID\n      LEFT JOIN ERF_Calc erf ON dva.\"ARTregID\" = erf.\"ART_link\"\n    )\n    SELECT COUNT(T1.\"ARTregID\") INTO risk_count\n    FROM Final_Calc AS T1\n    WHERE (T1.MDR * (1 + ((T1.ERF + T1.LER + (T1.MDR * 2)) / 20.0))) > 3;\n    RETURN risk_count;\nEND;\n$$ LANGUAGE plpgsql;", "SELECT get_dynasty_artifacts_at_risk_count();"], "external_knowledge": [1, 2, 7, 8, 18, 31, 37, 46], "test_cases": [], "category": "Management", "high_level": true, "conditions": {"decimal": 0, "distinct": false, "order": false}}
{"instance_id": "museum_artifact_M_9", "selected_database": "museum_artifact", "query": "I need a list of our most neglected showcases. Find every single one that has all three of these problems at once: the filter needs replacing now, the silica needs replacing now, and its general maintenance is overdue. For that list of problem showcases, tell me their ID, which hall they're in, and a count of how many high-sensitivity artifacts are stuck inside them.", "normal_query": "Create a view named 'V_Chronic_Maintenance_Backlog_Showcases' to identify showcases with chronic maintenance backlogs. A 'chronic backlog' is defined as a showcase where the filter is overdue ('Replace Now'), the silica is exhausted ('Replace Now'), and the general maintenance status is 'Overdue'. For these showcases, the view should list their ID, hall ID, and the number of high-sensitivity artifacts they contain.", "preprocess_sql": [], "clean_up_sqls": ["DROP VIEW IF EXISTS V_Chronic_Maintenance_Backlog_Showcases;", "DROP FUNCTION IF EXISTS get_worst_backlog_showcase_env();"], "sol_sql": ["CREATE OR REPLACE VIEW \"V_Chronic_Maintenance_Backlog_Showcases\" AS\nWITH ChronicBacklogShowcases AS (\n    SELECT \n        \"caseID\",\n        \"hall_ref\"\n    FROM \"Showcases\"\n    WHERE \n        LOWER(case_environment_profile -> 'maintenance' ->> 'filter_status') = 'replace now'\n        AND LOWER(case_environment_profile -> 'maintenance' ->> 'silica_status') = 'replace now'\n        AND LOWER(case_environment_profile -> 'maintenance' ->> 'maint_status') = 'overdue'\n),\nArtifactsInShowcases AS (\n    SELECT\n        ca.\"case_exam\" as \"caseID\",\n        ac.\"ARTregID\"\n    FROM \"ConditionAssessments\" ca\n    JOIN \"ArtifactsCore\" ac ON ca.\"art_exam\" = ac.\"ARTregID\"\n    WHERE ca.\"case_exam\" IN (SELECT \"caseID\" FROM ChronicBacklogShowcases)\n),\nHighSensitivityArtifacts AS (\n    SELECT \"ART_link\"\n    FROM \"SensitivityData\"\n    WHERE LOWER(\"ENVsense\") = 'high'\n)\nSELECT \n    cbs.\"caseID\",\n    cbs.\"hall_ref\",\n    COUNT(hsa.\"ART_link\") AS high_sensitivity_artifact_count\nFROM ChronicBacklogShowcases cbs\nLEFT JOIN ArtifactsInShowcases ais ON cbs.\"caseID\" = ais.\"caseID\"\nLEFT JOIN HighSensitivityArtifacts hsa ON ais.\"ARTregID\" = hsa.\"ART_link\"\nGROUP BY cbs.\"caseID\", cbs.\"hall_ref\"\nORDER BY high_sensitivity_artifact_count DESC;", "SELECT * FROM \"V_Chronic_Maintenance_Backlog_Showcases\";"], "external_knowledge": [], "test_cases": [], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "museum_artifact_M_10", "selected_database": "museum_artifact", "query": "We've got a $25,000 budget to catch up on overdue cleanings. I need a priority list. Figure out the risk score for every late artifact, and also estimate a treatment cost based on its material and complexity. Then, tell me which artifacts we can afford to fix, starting with the highest risk ones, without going over our budget. List the artifact name, its risk score, and its estimated cost.", "normal_query": "Create a materialized view named 'MV_Prioritized_Maintenance_Plan' to generate a prioritized maintenance plan for overdue cleanings within a simulated budget of $25,000. Calculate the 'cost of treatment' for each overdue artifact based on its material and treatment complexity. The view should list the artifacts that can be treated within the budget, ordered by their Conservation Backlog Risk (CBR), and show their title, CBR score, and calculated cost.", "preprocess_sql": [], "clean_up_sqls": ["DROP MATERIALIZED VIEW IF EXISTS MV_Prioritized_Maintenance_Plan;", "DROP FUNCTION IF EXISTS get_remaining_maintenance_budget();"], "sol_sql": ["DROP MATERIALIZED VIEW IF EXISTS \"MV_Prioritized_Maintenance_Plan\";", "CREATE MATERIALIZED VIEW \"MV_Prioritized_Maintenance_Plan\" AS\nWITH CPI_Calc AS (\n  SELECT\n    T1.\"ARTregID\",\n    ((T2.\"HIST_sign\" + (T2.rating_profile->>'research_score')::numeric + (T2.rating_profile->>'cultural_score')::numeric)) * (10 - \n      CASE LOWER(T1.\"conserve_status\")\n        WHEN 'excellent' THEN 1 WHEN 'good' THEN 3 WHEN 'fair' THEN 5\n        WHEN 'poor' THEN 7 WHEN 'critical' THEN 10 ELSE 0\n      END) / 30.0 AS CPI\n  FROM \"ArtifactsCore\" AS T1 JOIN \"ArtifactRatings\" AS T2 ON T1.\"ARTregID\" = T2.\"ART_link\"\n),\nOverdueArtifacts AS (\n    SELECT \n        T1.\"ARTregID\",\n        T1.\"art_title\",\n        T1.\"MatKind\",\n        (T3.CPI * (CURRENT_DATE - T2.\"lastClean\")) / 100.0 AS CBR,\n        ar.rating_profile->>'treatment_complexity' as treatment_complexity\n    FROM \"ArtifactsCore\" AS T1\n    JOIN \"ConditionAssessments\" AS ca ON T1.\"ARTregID\" = ca.\"art_exam\"\n    JOIN \"ConservationAndMaintenance\" AS T2 ON ca.\"cond_id\" = T2.\"maint_id\"\n    INNER JOIN CPI_Calc AS T3 ON T1.\"ARTregID\" = T3.\"ARTregID\"\n    INNER JOIN \"ArtifactRatings\" ar ON T1.\"ARTregID\" = ar.\"ART_link\"\n    WHERE T2.\"nextClean\" < CURRENT_DATE\n),\nCostedArtifacts AS (\n    SELECT \n        \"art_title\", \n        CBR, \n        CASE \n            WHEN LOWER(\"MatKind\") = 'textile' AND LOWER(treatment_complexity) = 'complex' THEN 7500\n            WHEN LOWER(\"MatKind\") = 'textile' THEN 4000\n            WHEN LOWER(\"MatKind\") IN ('wood', 'paper') AND LOWER(treatment_complexity) = 'complex' THEN 5000\n            WHEN LOWER(\"MatKind\") IN ('wood', 'paper') THEN 2500\n            WHEN LOWER(treatment_complexity) = 'complex' THEN 4000\n            WHEN LOWER(treatment_complexity) = 'moderate' THEN 2000\n            ELSE 1000\n        END as treatment_cost\n    FROM OverdueArtifacts\n),\nPrioritizedPlan AS (\n    SELECT \n        \"art_title\", \n        CBR, \n        treatment_cost,\n        SUM(treatment_cost) OVER (ORDER BY CBR DESC, \"art_title\") as cumulative_cost\n    FROM CostedArtifacts\n)\nSELECT \n    \"art_title\",\n    CBR,\n    treatment_cost\nFROM PrioritizedPlan\nWHERE cumulative_cost <= 25000\nORDER BY CBR DESC;", "SELECT * FROM \"MV_Prioritized_Maintenance_Plan\";"], "external_knowledge": [0, 66, 67], "test_cases": [], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "fake_account_1", "selected_database": "fake_account", "query": "Which accounts are growing their networks the fastest? Give me that blended growth score rounded to three decimal places and sort from fastest to slowest.", "normal_query": "Compute the Network Growth Velocity (NGV) for every account using its follower and following growth rates, rounded to three decimal places, and list accounts with the highest NGV first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    acct_node AS account_id,\n    ROUND(SQRT(POWER(COALESCE(\"FollGrow\",0),2)+POWER(COALESCE(\"FingGrow\",0),2))::numeric,3) AS ngv\nFROM network_metrics\nORDER BY ngv DESC;"], "external_knowledge": [2], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "fake_account_2", "selected_database": "fake_account", "query": "Show me the top 10 accounts by their logins-per-day score. For each account, take the biggest lifecycle login total you've ever seen for it, divide by its age in days (skip age missing or ≤ 0), round the score to 3 decimals, and show just the account ID plus that score.", "normal_query": "Compute each account's Account Activity Frequency (AAF) per the domain definition. For each account, use the highest recorded lifecycle session total across all session snapshots, exclude anyone with age in days missing or ≤ 0, then return the ten accounts with the greatest AAF in descending order, showing the account identifier and AAF rounded to three decimals.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH sess AS (SELECT s.acct_gate AS account_id,MAX((s.session_telemetry->'session_stats'->>'session_count')::numeric) AS session_cnt FROM security_sessions s GROUP BY s.acct_gate),aaf AS (SELECT a.acct_ref AS account_id,ROUND(sess.session_cnt/NULLIF(a.\"AGE_D\",0),3) AS aaf FROM accounts a JOIN sess ON sess.account_id=a.acct_ref WHERE a.\"AGE_D\" IS NOT NULL AND a.\"AGE_D\">0) SELECT account_id,aaf FROM aaf ORDER BY aaf DESC,account_id LIMIT 10;"], "external_knowledge": [0], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "fake_account_3", "selected_database": "fake_account", "query": "List each account together with their attempts to evade detection. Calculate that sneakiness score, rounded to 3 decimal places, and rank them from most to least sneaky.", "normal_query": "List each account along with its Technical Evasion Index (TEI), rounded to three decimal places, and sort them descending.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    acct_gate AS account_id,\n    ROUND(\n        0.4 * (REGEXP_REPLACE(session_telemetry->>'vpn_usage_pct', '%', '')::numeric / 100)\n        + 0.3* ((session_telemetry->'ip_reputation'->>'proxy_hits')::numeric/10)\n        + 0.3* ((session_telemetry->'ip_reputation'->>'country_count')::numeric/20)\n    ,3) AS tei\nFROM security_sessions\nORDER BY tei DESC;"], "external_knowledge": [7], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "fake_account_4", "selected_database": "fake_account", "query": "Bucket every user into four sneakiness tiers based on how much they rely on tricks like VPNs, and list each account with the tier number.", "normal_query": "Divide all accounts into quartiles based on their TEI values and return each account id with its TEI quartile.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH tei_vals AS (SELECT acct_gate AS account_id, 0.4* (REGEXP_REPLACE(session_telemetry->>'vpn_usage_pct', '%','')::numeric / 100) + 0.3*((session_telemetry->'ip_reputation'->>'proxy_hits')::numeric/10) + 0.3*((session_telemetry->'ip_reputation'->>'country_count')::numeric/20) AS tei FROM security_sessions) SELECT account_id, NTILE(4) OVER(ORDER BY tei) AS tei_quartile FROM tei_vals;"], "external_knowledge": [70, 7], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "fake_account_5", "selected_database": "fake_account", "query": "How many users land in each risk level based on the number of an account's attempts to evade detection?", "normal_query": "Count how many accounts fall into each TEI Risk Category (low, moderate, high, very high).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH tei_vals AS (\n    SELECT \n        acct_gate AS account_id,\n        0.4* (REGEXP_REPLACE(session_telemetry->>'vpn_usage_pct', '%', '')::numeric / 100)\n        +0.3*((session_telemetry->'ip_reputation'->>'proxy_hits')::numeric/10)\n        +0.3*((session_telemetry->'ip_reputation'->>'country_count')::numeric/20) AS tei\n    FROM security_sessions\n),\nquart AS (\n    SELECT account_id,\n           NTILE(4) OVER(ORDER BY tei) AS q\n    FROM tei_vals\n)\nSELECT \n    CASE q \n        WHEN 1 THEN 'Low Risk'\n        WHEN 2 THEN 'Moderate Risk'\n        WHEN 3 THEN 'High Risk'\n        WHEN 4 THEN 'Very High Risk'\n    END AS tei_risk_category,\n    COUNT(*) AS account_count\nFROM quart\nGROUP BY tei_risk_category\nORDER BY account_count DESC;"], "external_knowledge": [79, 70, 7], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "fake_account_6", "selected_database": "fake_account", "query": "List the top twenty accounts by the highest overall exposure metric based on multiple signals. Only include users whose required inputs are all present; sort high to low; show the user ID and the metric rounded to three decimals.", "normal_query": "Show the twenty accounts with the highest Security Risk Score (SRS). Compute the score only for accounts where all three inputs (risk value, trust value, impact value) are present; sort by the unrounded score descending; display the account identifier and the score rounded to three decimals.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH srs_calc AS (SELECT rm.acct_risk AS account_id,(0.4*(rm.risk_profile->'risk_scores'->>'risk_value')::numeric+0.3*(1-(rm.risk_profile->'risk_scores'->>'trust_score')::numeric)+0.3*(rm.risk_profile->'risk_scores'->>'impact_score')::numeric) AS srs_raw FROM risk_and_moderation rm WHERE(rm.risk_profile->'risk_scores'->>'risk_value') IS NOT NULL AND(rm.risk_profile->'risk_scores'->>'trust_score') IS NOT NULL AND(rm.risk_profile->'risk_scores'->>'impact_score') IS NOT NULL) SELECT account_id,ROUND(srs_raw,3) AS srs FROM srs_calc ORDER BY srs_raw DESC,account_id LIMIT 20;"], "external_knowledge": [4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "fake_account_7", "selected_database": "fake_account", "query": "Show every user that needs immediate attention: they must have the top severity label and an ongoing detection. Rank them by their overall exposure metric (rounded to 3 decimals) from high to low, and include the severity label, only show those larger than 0.7.", "normal_query": "List all High-Risk Accounts: compute SRS per its definition, keep only those with SRS > 0.7, whose overall severity is Critical, and that have at least one ongoing detection. Return the account ID, the score rounded to three decimals, and the severity, sorted by the unrounded score descending.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH srs_calc AS (SELECT rm.acct_risk AS account_id,(rm.risk_profile->'risk_scores'->>'threat_level') AS threat_level,(0.4*(rm.risk_profile->'risk_scores'->>'risk_value')::numeric+0.3*(1-(rm.risk_profile->'risk_scores'->>'trust_score')::numeric)+0.3*(rm.risk_profile->'risk_scores'->>'impact_score')::numeric) AS srs_raw FROM risk_and_moderation rm WHERE(rm.risk_profile->'risk_scores'->>'risk_value') IS NOT NULL AND(rm.risk_profile->'risk_scores'->>'trust_score') IS NOT NULL AND(rm.risk_profile->'risk_scores'->>'impact_score') IS NOT NULL) SELECT sc.account_id,ROUND(sc.srs_raw,3) AS srs,sc.threat_level FROM srs_calc sc WHERE sc.srs_raw>0.7 AND sc.threat_level='Critical' AND EXISTS(SELECT 1 FROM monitoring m WHERE m.acct_mon=sc.account_id AND m.\"InvestState\"='Active') ORDER BY sc.srs_raw DESC,sc.account_id;"], "external_knowledge": [10, 4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "fake_account_8", "selected_database": "fake_account", "query": "For each account, combines multiple bot-detection metrics into a single score, rounded to three decimal places, and display the highest value.", "normal_query": "Calculate the Bot Behavior Index (BBI) for each account, rounded to three decimal places, and show the top one.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    b.acct_beh AS account_id,\n    ROUND(\n 0.4*COALESCE((behavioral_anomaly_scores->'automation_spam'->>'bot_likelihood')::numeric,0) +\n        0.3*COALESCE((behavioral_anomaly_scores->'automation_spam'->>'automated_behavior')::numeric,0) +\n        0.3*(1 - COALESCE(im.\"ConvNat\",0))::numeric\n    ,3) AS bbi\nFROM behavioral_scores b\nJOIN interaction_metrics im ON im.acct_dm = b.acct_beh\nORDER BY bbi DESC\nLIMIT 1;"], "external_knowledge": [3], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "fake_account_9", "selected_database": "fake_account", "query": "Identify all accounts exhibiting systematic VPN usage and report the total count of distinct countries from which they have authenticated.", "normal_query": "Identify all VPN Abuser accounts and show how many different login countries they have used.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH tei_vals AS (\n    SELECT \n        acct_gate AS account_id,\n        0.4* (REGEXP_REPLACE(session_telemetry->>'vpn_usage_pct', '%', '')::numeric / 100)\n        +0.3*((session_telemetry->'ip_reputation'->>'proxy_hits')::numeric/10)\n        +0.3*((session_telemetry->'ip_reputation'->>'country_count')::numeric/20) AS tei,\n        (session_telemetry->'ip_reputation'->>'country_count')::numeric AS country_count\n    FROM security_sessions\n)\nSELECT account_id, country_count\nFROM tei_vals\nWHERE tei > 0.8\n  AND country_count >= 3\nORDER BY country_count DESC;"], "external_knowledge": [16, 7], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "fake_account_10", "selected_database": "fake_account", "query": "Estimate the average authenticity level for content on each platform, rounded to three decimal places and sorted in a descending order.", "normal_query": "Compute the average Content Authenticity Score (CAS) for each platform, rounded to three decimal places and sorted from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH cas_tbl AS (SELECT a.plt_key,(0.3*COALESCE((rm.risk_profile->'risk_scores'->>'authenticity_score')::numeric,0)+0.3*COALESCE((ca.content_metrics->'content_quality'->>'content_uniqueness')::numeric,0)+0.4*COALESCE(im.\"ConvNat\",0)::numeric) AS cas_val FROM accounts a LEFT JOIN risk_and_moderation rm ON rm.acct_risk=a.acct_ref LEFT JOIN content_activity ca ON ca.acct_slot=a.acct_ref LEFT JOIN interaction_metrics im ON im.acct_dm=a.acct_ref) SELECT p.\"PLT_KIND\" AS platform_type,ROUND(AVG(cas_val),3) AS avg_cas FROM cas_tbl ct JOIN platforms p ON p.\"PLT_CODE\"=ct.plt_key GROUP BY platform_type ORDER BY avg_cas DESC;"], "external_knowledge": [1], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "fake_account_M_1", "selected_database": "fake_account", "query": "Update the StateFlag to 'Suspended' for every high-risk account, according to the High-Risk Account rule, excluding those already in suspended state.", "normal_query": "Suspend every High-Risk Account by setting its StateFlag to \"Suspended\", according to the High-Risk Account rule. Make sure accounts already suspended are not updated twice.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Set StateFlag = 'Suspended' for all High-Risk Accounts that are not yet suspended\nUPDATE accounts a\nSET    \"StateFlag\" = 'Suspended'\nFROM   risk_and_moderation rm\nWHERE  rm.acct_risk = a.acct_ref\n  AND  (    0.4*(rm.risk_profile->'risk_scores'->>'risk_value')::numeric\n        +  0.3*(1-COALESCE((rm.risk_profile->'risk_scores'->>'trust_score')::numeric,0))\n        +  0.3*(rm.risk_profile->'risk_scores'->>'impact_score')::numeric ) > 0.8\n  AND  rm.risk_profile->'risk_scores'->>'threat_level' = 'Critical'\n  AND  COALESCE(a.\"StateFlag\", '') <> 'Suspended';"], "external_knowledge": [10, 4], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute('''\n        SELECT COUNT(*)\n        FROM accounts a\n        JOIN risk_and_moderation rm ON rm.acct_risk = a.acct_ref\n        WHERE (0.4*(rm.risk_profile->'risk_scores'->>'risk_value')::numeric +\n               0.3*(1-COALESCE((rm.risk_profile->'risk_scores'->>'trust_score')::numeric,0)) +\n               0.3*(rm.risk_profile->'risk_scores'->>'impact_score')::numeric) > 0.8\n          AND rm.risk_profile->'risk_scores'->>'threat_level' = 'Critical'\n          AND a.\"StateFlag\" <> 'Suspended'\n    ''')\n    result = cursor.fetchone()[0]\n    assert result == 0, 'Some high-risk accounts were not suspended.'\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "fake_account_11", "selected_database": "fake_account", "query": "Show the top 10 accounts that have most content manipulation patterns. Evaluate their scores, rounded to 3 decimal places, and sort in descending order.", "normal_query": "Retrieve the ten accounts with the highest Content Manipulation Score (CMS), sorted in a descending order and rounded to three decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    a.acct_ref AS account_id,\n    ROUND(\n        0.4*(1 - (ca.content_metrics->'content_quality'->>'content_uniqueness')::numeric) +\n        0.3*(ca.content_metrics->'link_media'->>'media_reuse_rate')::numeric +\n        0.3*(1 - im.\"TxtUniq\")::numeric\n    ,3) AS cms\nFROM accounts a\nJOIN content_activity ca ON ca.acct_slot = a.acct_ref\nJOIN interaction_metrics im ON im.acct_dm = a.acct_ref\nORDER BY cms DESC\nLIMIT 10;"], "external_knowledge": [8], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "fake_account_12", "selected_database": "fake_account", "query": "Find all accounts pumping out loads of near-duplicate posts, list their posts-per-day, sorted from most to least.", "normal_query": "List all accounts classified as Content Farms along with their daily post frequency, ordered by frequency from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH cms_tbl AS (\n    SELECT \n        a.acct_ref AS account_id,\n        (ca.content_metrics->'posting'->>'posts_per_day')::numeric AS posts_per_day,\n        0.4*(1 - (ca.content_metrics->'content_quality'->>'content_uniqueness')::numeric) +\n        0.3*(ca.content_metrics->'link_media'->>'media_reuse_rate')::numeric +\n        0.3*(1 - im.\"TxtUniq\") AS cms\n    FROM accounts a\n    JOIN content_activity ca ON ca.acct_slot = a.acct_ref\n    JOIN interaction_metrics im ON im.acct_dm = a.acct_ref\n)\nSELECT account_id, posts_per_day\nFROM cms_tbl\nWHERE cms > 0.7\n  AND posts_per_day > 50\nORDER BY posts_per_day DESC;"], "external_knowledge": [13, 8], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "fake_account_M_2", "selected_database": "fake_account", "query": "Create active, low-priority watch items for accounts that heavily rely on traffic-masking tools and have logged in from at least three different countries, skipping anyone who already has an active watch item. Tag the new items as coming from an automated scan and timestamp them with the current time.", "normal_query": "Insert a low-priority monitoring entry for every account that qualifies under the VPN-abuse rule (TEI > 0.8 and login countries ≥ 3), skipping any account that already has an active monitoring entry. Use a random unique ID, the current timestamp, mark the source as Algorithm, and set the entry state to Active.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["INSERT INTO monitoring(\"RecKey\",snap_ts,acct_mon,\"DetectSrc\",\"MonPrio\",\"InvestState\") SELECT gen_random_uuid()::text,NOW(),s.acct_gate,'Algorithm','Low','Active' FROM security_sessions s LEFT JOIN monitoring m ON m.acct_mon=s.acct_gate AND m.\"InvestState\"='Active' WHERE(0.4*(REGEXP_REPLACE(COALESCE(s.session_telemetry->>'vpn_usage_pct','0%'),'%','')::numeric/100)+0.3*(COALESCE((s.session_telemetry->'ip_reputation'->>'proxy_hits')::numeric,0)/10)+0.3*(COALESCE((s.session_telemetry->'ip_reputation'->>'country_count')::numeric,0)/20))>0.8 AND COALESCE((s.session_telemetry->'ip_reputation'->>'country_count')::int,0)>=3 AND m.acct_mon IS NULL;"], "external_knowledge": [16, 7], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute('SELECT COUNT(*) FROM monitoring;')\n    result = cursor.fetchone()[0]\n    assert isinstance(result, int), 'Monitoring count should be an integer'\n    assert result > 0, 'At least one monitoring entry should be inserted'\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "fake_account_13", "selected_database": "fake_account", "query": "For each account, build one overall risk score by combining an automation-likeness signal, a coordination signal, and the group size; round to three decimals, list those above 0.8, and sort high to low.", "normal_query": "Compute Coordinated Bot Risk (CBR) for each account using the defined BBI and CAS formulas; round to three decimals, return those with CBR greater than 0.8, and sort in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH bbi AS (SELECT b.acct_beh AS account_id,(0.4*COALESCE((behavioral_anomaly_scores->'automation_spam'->>'bot_likelihood')::numeric,0)+0.3*COALESCE((behavioral_anomaly_scores->'automation_spam'->>'automated_behavior')::numeric,0)+0.3*(1-COALESCE(im.\"ConvNat\",0))) AS bbi FROM behavioral_scores b JOIN interaction_metrics im ON im.acct_dm=b.acct_beh),cas AS (SELECT ac.acct_bridge AS account_id,(0.5*COALESCE(ca.\"CoordScore\",0)+0.3*COALESCE(ca.\"NetInfl\",0)+0.2*COALESCE(ca.\"ClusterQty\",0)) AS cas,COALESCE(ca.\"ClusterQty\",0) AS cluster_qty FROM account_clusters ac JOIN cluster_analysis ca ON ca.\"CLSTR_PIN\"=ac.clu_ref),cbr AS (SELECT bbi.account_id,(bbi.bbi*cas.cas*cas.cluster_qty) AS cbr FROM bbi JOIN cas ON cas.account_id=bbi.account_id) SELECT account_id,ROUND(cbr::numeric,3) AS cbr FROM cbr WHERE cbr>0.8 ORDER BY cbr DESC;"], "external_knowledge": [33, 3, 6], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "fake_account_14", "selected_database": "fake_account", "query": "What's the overall average trust score of everyone's connections, rounded to three decimals?", "normal_query": "Determine the overall average Network Trust Score (NTS) across all accounts, rounded to three decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH bbi AS (SELECT b.acct_beh AS account_id,(0.4*COALESCE((behavioral_anomaly_scores->'automation_spam'->>'bot_likelihood')::numeric,0)+0.3*COALESCE((behavioral_anomaly_scores->'automation_spam'->>'automated_behavior')::numeric,0)+0.3*(1-COALESCE(im.\"ConvNat\",0))) AS bbi FROM behavioral_scores b JOIN interaction_metrics im ON im.acct_dm=b.acct_beh),cas AS (SELECT ac.acct_bridge AS account_id,(0.5*COALESCE(ca.\"CoordScore\",0)+0.3*COALESCE(ca.\"NetInfl\",0)+0.2*COALESCE(ca.\"ClusterQty\",0)) AS cas,COALESCE(ca.\"ClusterQty\",0) AS cluster_qty FROM account_clusters ac JOIN cluster_analysis ca ON ca.\"CLSTR_PIN\"=ac.clu_ref),cbr_raw AS (SELECT bbi.account_id,(bbi.bbi*cas.cas*cas.cluster_qty) AS cbr_val FROM bbi JOIN cas ON cas.account_id=bbi.account_id),cbr AS (SELECT account_id,MAX(cbr_val) AS cbr FROM cbr_raw GROUP BY account_id),pci AS (SELECT a.acct_ref AS account_id,(0.3*COALESCE((rm.risk_profile->'risk_scores'->>'credibility_score')::numeric,0)+0.3*COALESCE((rm.risk_profile->'risk_scores'->>'reputation_score')::numeric,0)+0.4*COALESCE(a.\"ProfileScore\",0)) AS pci FROM accounts a LEFT JOIN risk_and_moderation rm ON rm.acct_risk=a.acct_ref),ngv AS (SELECT nm.acct_node AS account_id,SQRT(POWER(COALESCE(nm.\"FollGrow\",0),2)+POWER(COALESCE(nm.\"FingGrow\",0),2)) AS ngv FROM network_metrics nm),nts AS (SELECT a.acct_ref AS account_id,COALESCE(pci.pci,0) AS pci,COALESCE(ngv.ngv,0) AS ngv,COALESCE(cbr.cbr,0) AS cbr,(COALESCE(pci.pci,0)*(1-COALESCE(ngv.ngv,0))*(1-COALESCE(cbr.cbr,0))) AS nts_val FROM accounts a LEFT JOIN pci ON pci.account_id=a.acct_ref LEFT JOIN ngv ON ngv.account_id=a.acct_ref LEFT JOIN cbr ON cbr.account_id=a.acct_ref) SELECT ROUND(AVG(nts_val)::numeric,3) AS avg_nts FROM nts;"], "external_knowledge": [36, 5, 2], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": false}}
{"instance_id": "fake_account_15", "selected_database": "fake_account", "query": "Generate a report of every cluster whose role is “SocialGroup”. For each cluster, show its identifier, the number of unique member accounts, the cluster's maximum coordination score. Quantify each account's position and influence in interaction network if the data are available, otherwise NULL, and indetify whether it is sophisticated influence campaigns", "normal_query": "Generate a report of every cluster whose role is “SocialGroup”. For each cluster, show its identifier, the number of unique member accounts, the average Network Influence Centrality (NIC) of those members if NIC data are available, otherwise NULL, the cluster's maximum coordination score, and a “Yes/No” flag that is “Yes” when the cluster satisfies the Coordinated Influence Operation definition.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH members AS (SELECT ca.\"CLSTR_PIN\" AS cluster_id,ac.acct_bridge AS account_id,ca.\"CoordScore\",ca.\"NetInfl\",ca.\"ClusterQty\" FROM account_clusters ac JOIN cluster_analysis ca ON ca.\"CLSTR_PIN\"=ac.clu_ref WHERE ca.\"CluRole\"='SocialGroup'),nic_per_account AS (SELECT m.cluster_id,m.account_id,(0.4*COALESCE(nm.\"ConnQual\",0)+0.3*COALESCE(m.\"NetInfl\",0)+0.3*COALESCE(nm.\"IntDiv\",0)) AS nic FROM members m LEFT JOIN network_metrics nm ON nm.acct_node=m.account_id),cms_per_account AS (SELECT m.cluster_id,m.account_id,(0.4*(1-COALESCE((ca.content_metrics->'content_quality'->>'content_uniqueness')::numeric,0))+0.3*COALESCE((ca.content_metrics->'link_media'->>'media_reuse_rate')::numeric,0)+0.3*(1-COALESCE(im.\"TxtUniq\",0))) AS cms FROM members m LEFT JOIN content_activity ca ON ca.acct_slot=m.account_id LEFT JOIN interaction_metrics im ON im.acct_dm=m.account_id),mps_per_account AS (SELECT m.cluster_id,m.account_id,(0.3*COALESCE((rm.risk_profile->'violation_history'->>'abuse_count')::numeric,0)/1000+0.4*COALESCE((rm.risk_profile->'risk_scores'->>'impact_score')::numeric,0)+0.3*COALESCE((rm.risk_profile->'risk_scores'->>'risk_value')::numeric,0)) AS mps FROM members m LEFT JOIN risk_and_moderation rm ON rm.acct_risk=m.account_id),cis_per_account AS (SELECT m.cluster_id,m.account_id,(COALESCE(cms.cms,0)*COALESCE(mps.mps,0)*COALESCE(nic.nic,0)) AS cis,COALESCE(nic.nic,0) AS nic FROM members m LEFT JOIN cms_per_account cms ON cms.cluster_id=m.cluster_id AND cms.account_id=m.account_id LEFT JOIN mps_per_account mps ON mps.cluster_id=m.cluster_id AND mps.account_id=m.account_id LEFT JOIN nic_per_account nic ON nic.cluster_id=m.cluster_id AND nic.account_id=m.account_id),reshare_per_account AS (SELECT m.cluster_id,m.account_id,(COALESCE(nm.\"ShareRt\",0)*COALESCE((ca.content_metrics->'posting'->>'total_posts')::numeric,0)) AS reshare_cnt FROM members m LEFT JOIN network_metrics nm ON nm.acct_node=m.account_id LEFT JOIN content_activity ca ON ca.acct_slot=m.account_id),cae_per_cluster AS (SELECT m.cluster_id,AVG(COALESCE(cis.cis,0)*COALESCE(cis.nic,0)*LN(1+COALESCE(rs.reshare_cnt,0))) AS cae FROM members m LEFT JOIN cis_per_account cis ON cis.cluster_id=m.cluster_id AND cis.account_id=m.account_id LEFT JOIN reshare_per_account rs ON rs.cluster_id=m.cluster_id AND rs.account_id=m.account_id GROUP BY m.cluster_id),cas_cluster AS (SELECT cluster_id,(0.5*MAX(\"CoordScore\")+0.3*MAX(\"NetInfl\")+0.2*MAX(\"ClusterQty\")) AS cas FROM members GROUP BY cluster_id),ring AS (SELECT m.cluster_id,COUNT(DISTINCT m.account_id) AS member_count,CASE WHEN COUNT(DISTINCT m.account_id)>5 AND COALESCE(cas.cas,0)>0.8 AND SUM(CASE WHEN COALESCE(cms.cms,-1)>0.7 THEN 1 ELSE 0 END)=COUNT(DISTINCT m.account_id) THEN TRUE ELSE FALSE END AS has_cmr FROM members m LEFT JOIN cas_cluster cas ON cas.cluster_id=m.cluster_id LEFT JOIN cms_per_account cms ON cms.cluster_id=m.cluster_id AND cms.account_id=m.account_id GROUP BY m.cluster_id,cas.cas),avg_nic_per_cluster AS (SELECT m.cluster_id,AVG(nic.nic) AS avg_nic FROM members m LEFT JOIN nic_per_account nic ON nic.cluster_id=m.cluster_id AND nic.account_id=m.account_id GROUP BY m.cluster_id),nsi_per_cluster AS (SELECT DISTINCT cluster_id,NULL::numeric AS nsi FROM members),max_coord AS (SELECT cluster_id,MAX(\"CoordScore\") AS max_coord FROM members GROUP BY cluster_id) SELECT m.cluster_id,r.member_count,ROUND(avg_nic.avg_nic::numeric,3) AS avg_nic,mc.max_coord,CASE WHEN COALESCE(nsi.nsi,0)>0.8 AND COALESCE(cae.cae,0)>0.7 AND r.has_cmr THEN 'Yes' ELSE 'No' END AS coordinated_influence_operation FROM(SELECT DISTINCT cluster_id FROM members)m LEFT JOIN avg_nic_per_cluster avg_nic ON avg_nic.cluster_id=m.cluster_id LEFT JOIN max_coord mc ON mc.cluster_id=m.cluster_id LEFT JOIN nsi_per_cluster nsi ON nsi.cluster_id=m.cluster_id LEFT JOIN cae_per_cluster cae ON cae.cluster_id=m.cluster_id LEFT JOIN ring r ON r.cluster_id=m.cluster_id ORDER BY r.member_count DESC;"], "external_knowledge": [60, 56, 57, 51, 19], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "fake_account_16", "selected_database": "fake_account", "query": "Estimate each account's authentication-related risk, rounded to 3 decimal places. List accounts with a score above 0.7, sorted from highest to lowest.", "normal_query": "Identify accounts with an Authentication Risk Score (ARS) greater than 0.7, round the score to 3 decimal places and order from highest to lowest.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH tei_tbl AS (SELECT ss.acct_gate,0.4*(COALESCE(REGEXP_REPLACE(ss.session_telemetry->>'vpn_usage_pct','%',''),'0')::numeric/100)+0.3*(COALESCE((ss.session_telemetry->'ip_reputation'->>'proxy_hits')::numeric,0)/10)+0.3*(COALESCE((ss.session_telemetry->'ip_reputation'->>'country_count')::numeric,0)/20) AS tei FROM security_sessions ss),pci_tbl AS (SELECT a.acct_ref,0.3*COALESCE((rm.risk_profile->'risk_scores'->>'credibility_score')::numeric,0)+0.3*COALESCE((rm.risk_profile->'risk_scores'->>'reputation_score')::numeric,0)+0.4*COALESCE(a.\"ProfileScore\",0) AS pci FROM accounts a LEFT JOIN risk_and_moderation rm ON rm.acct_risk=a.acct_ref),srs_tbl AS (SELECT rm.acct_risk,0.4*COALESCE((rm.risk_profile->'risk_scores'->>'risk_value')::numeric,0)+0.3*(1-COALESCE((rm.risk_profile->'risk_scores'->>'trust_score')::numeric,0))+0.3*COALESCE((rm.risk_profile->'risk_scores'->>'impact_score')::numeric,0) AS srs FROM risk_and_moderation rm),ars_tbl AS (SELECT a.acct_ref AS account_id,COALESCE(t.tei,0) AS tei,COALESCE(p.pci,0) AS pci,COALESCE(s.srs,0) AS srs,(0.5*COALESCE(t.tei,0)+0.3*(1-COALESCE(p.pci,0))+0.2*COALESCE(s.srs,0)) AS ars_val FROM accounts a LEFT JOIN tei_tbl t ON t.acct_gate=a.acct_ref LEFT JOIN pci_tbl p ON p.acct_ref=a.acct_ref LEFT JOIN srs_tbl s ON s.acct_risk=a.acct_ref) SELECT account_id,ROUND(ars_val::numeric,3) AS ars FROM ars_tbl WHERE ars_val>0.7 ORDER BY ars DESC;"], "external_knowledge": [38, 7, 5, 4], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "fake_account_17", "selected_database": "fake_account", "query": "For each account, show the most recent system estimate of automation likelihood using the latest detection event.", "normal_query": "Return each account's Latest Bot Likelihood Score (LBS).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH last_det AS (SELECT m.acct_mon AS account_id,MAX(m.snap_ts) AS last_ts FROM monitoring m GROUP BY m.acct_mon) SELECT ld.account_id,COALESCE((bs.behavioral_anomaly_scores->'automation_spam'->>'bot_likelihood')::numeric,0) AS latest_bot_likelihood_score FROM last_det ld JOIN behavioral_scores bs ON bs.acct_beh=ld.account_id;"], "external_knowledge": [71, 29], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "fake_account_18", "selected_database": "fake_account", "query": "For each account, measure how much its hourly activity over a day diverges from its usual behavior, round to 3 decimals and sort descending. Show those above 0.7.", "normal_query": "Identify accounts whose Temporal Pattern Deviation Score (TPDS) exceeds 0.7, rounded to 3 decimal places and sorted in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH raw AS (SELECT s.acct_gate AS account_id,s.session_telemetry->'activity_pattern'->'hourly_observed' AS ho,s.session_telemetry->'activity_pattern'->'hourly_expected' AS he FROM security_sessions s),src AS (SELECT * FROM raw WHERE jsonb_typeof(ho)='array' AND jsonb_typeof(he)='array' AND jsonb_array_length(ho)=24 AND jsonb_array_length(he)=24),obs AS (SELECT account_id,o.idx,(o.val)::numeric AS obs_val FROM src CROSS JOIN LATERAL jsonb_array_elements_text(ho) WITH ORDINALITY AS o(val,idx)),exp AS (SELECT account_id,e.idx,(e.val)::numeric AS exp_val FROM src CROSS JOIN LATERAL jsonb_array_elements_text(he) WITH ORDINALITY AS e(val,idx)),dev AS (SELECT o.account_id,CASE WHEN e.exp_val=0 AND o.obs_val=0 THEN 0 WHEN e.exp_val=0 AND o.obs_val<>0 THEN NULL ELSE POWER((o.obs_val-e.exp_val)/e.exp_val,2) END AS dev2 FROM obs o JOIN exp e ON e.account_id=o.account_id AND e.idx=o.idx),tpds AS (SELECT account_id,SQRT(SUM(dev2)) AS tpds FROM dev WHERE dev2 IS NOT NULL GROUP BY account_id),filtered AS (SELECT account_id,ROUND(tpds::numeric,3) AS tpds FROM tpds WHERE tpds>0.7) SELECT account_id,tpds FROM filtered UNION ALL SELECT NULL::text AS account_id,NULL::numeric AS tpds WHERE NOT EXISTS(SELECT 1 FROM filtered) ORDER BY 2 DESC NULLS LAST;"], "external_knowledge": [50], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "fake_account_19", "selected_database": "fake_account", "query": "List all accounts that exert strong influence while posting at a high daily rate, acting as key amplifiers in coordinated networks. List their influence score and daily post frequency, sorted by influence score in a descending order.", "normal_query": "List all High-Impact Amplifier accounts together with their influence score and daily post frequency, sorted by influence score in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH acc_cluster AS (SELECT ac.acct_bridge AS account_id,MAX(ca.\"NetInfl\") AS max_netinfl FROM account_clusters ac JOIN cluster_analysis ca ON ca.\"CLSTR_PIN\"=ac.clu_ref GROUP BY ac.acct_bridge),nic_tbl AS (SELECT nm.acct_node AS account_id,(0.4*COALESCE(nm.\"ConnQual\",0)+0.3*COALESCE(ac.max_netinfl,0)+0.3*COALESCE(nm.\"IntDiv\",0)) AS nic FROM network_metrics nm LEFT JOIN acc_cluster ac ON ac.account_id=nm.acct_node),posts AS (SELECT acct_slot AS account_id,(content_metrics->'posting'->>'posts_per_day')::numeric AS posts_per_day FROM content_activity),filtered AS (SELECT n.account_id,n.nic AS influence_score,p.posts_per_day FROM nic_tbl n JOIN posts p ON p.account_id=n.account_id WHERE n.nic>0.8 AND p.posts_per_day>30) SELECT account_id,influence_score,posts_per_day FROM filtered UNION ALL SELECT NULL::text,NULL::numeric,NULL::numeric WHERE NOT EXISTS(SELECT 1 FROM filtered) ORDER BY influence_score DESC NULLS LAST;"], "external_knowledge": [73], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "fake_account_20", "selected_database": "fake_account", "query": "Measure account-reputation stability, round to 3 decimals, and show whoever has the highest score.", "normal_query": "Show the account with the highest Reputation Volatility Index (RVI), rounded to 3 decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT rm.acct_risk AS account_id, ROUND(((rm.risk_profile->'risk_scores'->>'reputation_score')::numeric), 3) AS rep FROM public.risk_and_moderation rm WHERE (rm.risk_profile->'risk_scores'->>'reputation_score') IS NOT NULL ORDER BY rep DESC LIMIT 1;"], "external_knowledge": [53], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "fake_account_21", "selected_database": "fake_account", "query": "Retrieve accounts with elevated engagement levels based on the number of sessions or total posting frequency. Show their account ID and the daily post number.", "normal_query": "Retrieve accounts classified as High-Activity Accounts, showing their account ID and the daily post number.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH sc AS (SELECT s.acct_gate AS account_id,SUM((s.session_telemetry->'session_stats'->>'session_count')::numeric) AS total_sessions FROM security_sessions AS s GROUP BY s.acct_gate),tpf AS (SELECT ca.acct_slot AS account_id,SUM((ca.content_metrics->'posting'->>'posts_per_day')::numeric) AS posts_per_day FROM content_activity AS ca GROUP BY ca.acct_slot) SELECT COALESCE(sc.account_id,tpf.account_id) AS account_id,COALESCE(tpf.posts_per_day,0) AS posts_per_day FROM sc FULL OUTER JOIN tpf ON sc.account_id=tpf.account_id WHERE COALESCE(sc.total_sessions,0)>1000 OR COALESCE(tpf.posts_per_day,0)>50;"], "external_knowledge": [77], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "fake_account_22", "selected_database": "fake_account", "query": "Group results by platform type and show the average of the 0-1 score indicating how real the interactions feel, keep three decimals, and list them from high to low.", "normal_query": "Compute the average engagement authenticity score for each platform type, rounded to 3 decimal places and sort in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT p.\"PLT_KIND\" AS platform_type,ROUND(AVG(nm.\"EngAuth\")::numeric,3) AS avg_engagement_authenticity FROM accounts AS a JOIN network_metrics AS nm ON nm.acct_node=a.acct_ref JOIN platforms AS p ON p.\"PLT_CODE\"=a.plt_key GROUP BY p.\"PLT_KIND\" ORDER BY avg_engagement_authenticity DESC;"], "external_knowledge": [22], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "fake_account_23", "selected_database": "fake_account", "query": "How many accounts are currently inactive and also classified as automated?", "normal_query": "Count the number of accounts that are both in the inactive status and belong to the automated category.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT COUNT(*) AS dormant_bot_count FROM accounts WHERE \"StateFlag\"='Dormant' AND acct_form='Bot';"], "external_knowledge": [15], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "fake_account_M_3", "selected_database": "fake_account", "query": "If an account meets our trust checks, shows no recent detections for 180 days, and has been quiet for at least 90 days based on the latest activity proxy, mark its monitoring priority as \"Review_Inactive_Trusted\".", "normal_query": "For accounts that pass the trust threshold, have had no detections in the last 180 days, and whose most recent activity proxy is older than 90 days, set their monitoring priority to \"Review_Inactive_Trusted\".", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH pci AS (SELECT a.acct_ref,(0.3*COALESCE((rm.risk_profile->'risk_scores'->>'credibility_score')::numeric,0)+0.3*COALESCE((rm.risk_profile->'risk_scores'->>'reputation_score')::numeric,0)+0.4*COALESCE(a.\"ProfileScore\",0)) AS pci_value FROM accounts AS a LEFT JOIN risk_and_moderation AS rm ON rm.acct_risk=a.acct_ref),last_activity AS (SELECT m.acct_mon AS acct_ref,MAX(m.snap_ts) AS last_detect_ts FROM monitoring AS m GROUP BY m.acct_mon),has_detect_180 AS (SELECT DISTINCT m.acct_mon AS acct_ref FROM monitoring AS m WHERE m.snap_ts>=NOW()-INTERVAL '180 days'),targets AS (SELECT p.acct_ref FROM pci AS p LEFT JOIN last_activity AS la ON la.acct_ref=p.acct_ref LEFT JOIN has_detect_180 AS hd ON hd.acct_ref=p.acct_ref WHERE p.pci_value>0.8 AND hd.acct_ref IS NULL AND COALESCE(la.last_detect_ts,TIMESTAMP 'epoch')<NOW()-INTERVAL '90 days') UPDATE monitoring AS m SET \"MonPrio\"='Review_Inactive_Trusted' FROM targets AS t WHERE m.acct_mon=t.acct_ref AND COALESCE(m.\"MonPrio\",'')<>'Review_Inactive_Trusted';"], "external_knowledge": [12, 86, 85], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT COUNT(*) FROM monitoring WHERE \\\"MonPrio\\\" = 'Review_Inactive_Trusted';\")\n    result = cursor.fetchone()[0]\n    assert isinstance(result, int), 'Result should be an integer'\n    assert result >= 0, 'Count cannot be negative'\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "fake_account_24", "selected_database": "fake_account", "query": "By platform kind, average a score built as: how manipulated the content looks, times how urgently it should be reviewed, times how central it is in the network.", "normal_query": "For each platform kind, compute the average Content Impact Score, where the score equals manipulation intensity multiplied by moderation priority and by network influence centrality.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH cms_mps AS (SELECT a.acct_ref,a.plt_key,(0.4*(1-(ca.content_metrics->'content_quality'->>'content_uniqueness')::numeric)+0.3*COALESCE((ca.content_metrics->'link_media'->>'media_reuse_rate')::numeric,0)+0.3*(1-im.\"TxtUniq\")) AS cms,(0.3*COALESCE((rm.risk_profile->'violation_history'->>'abuse_count')::numeric,0)/1000+0.4*COALESCE((rm.risk_profile->'risk_scores'->>'impact_score')::numeric,0)+0.3*COALESCE((rm.risk_profile->'risk_scores'->>'risk_value')::numeric,0)) AS mps FROM accounts a JOIN content_activity ca ON ca.acct_slot=a.acct_ref JOIN interaction_metrics im ON im.acct_dm=a.acct_ref LEFT JOIN risk_and_moderation rm ON rm.acct_risk=a.acct_ref),nic AS (SELECT a.acct_ref,0.4*nm.\"ConnQual\"+0.3*cl.\"NetInfl\"+0.3*nm.\"IntDiv\" AS nic_val FROM accounts a JOIN network_metrics nm ON nm.acct_node=a.acct_ref JOIN account_clusters ac ON ac.acct_bridge=a.acct_ref JOIN cluster_analysis cl ON cl.\"CLSTR_PIN\"=ac.clu_ref),cis_per_acct AS (SELECT c.plt_key,(c.cms*c.mps*n.nic_val) AS cis FROM cms_mps c JOIN nic n ON n.acct_ref=c.acct_ref) SELECT p.\"PLT_KIND\" AS platform_kind,AVG(x.cis) AS avg_cis FROM cis_per_acct x JOIN platforms p ON p.\"PLT_CODE\"=x.plt_key GROUP BY p.\"PLT_KIND\";"], "external_knowledge": [37], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 3, "distinct": false, "order": true}}
{"instance_id": "fake_account_M_4", "selected_database": "fake_account", "query": "Make a materialized view that shows all accounts with a credibility value above 0.9.", "normal_query": "Create a materialized view listing all accounts whose built-in credibility value is greater than 0.9.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE MATERIALIZED VIEW IF NOT EXISTS mv_accounts_cred_over_0_9 AS SELECT a.acct_ref FROM accounts AS a JOIN risk_and_moderation AS rm ON rm.acct_risk=a.acct_ref WHERE COALESCE((rm.risk_profile->'risk_scores'->>'credibility_score')::numeric,0)>0.9;"], "external_knowledge": [], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "fake_account_M_5", "selected_database": "fake_account", "query": "Analyze the table that keeps monitoring snapshots so the database updates its size and stats.", "normal_query": "Run ANALYZE on the table that stores monitoring snapshots to refresh its statistics.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["INSERT INTO monitoring (\"RecKey\") VALUES ('dummy_key'); ANALYZE monitoring;"], "external_knowledge": [], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "fake_account_M_6", "selected_database": "fake_account", "query": "Write a helper called pct_to_numeric(text) that turns strings like '85%' into decimals like 0.85 and returns a numeric result.", "normal_query": "Create a utility function named pct_to_numeric(text) that converts a percentage string (e.g., '85%') into a numeric value (e.g., 0.85), returning a numeric type.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION pct_to_numeric(p_text text) RETURNS numeric LANGUAGE SQL IMMUTABLE STRICT AS $$SELECT NULLIF(BTRIM(REPLACE($1, '%', '')), '')::numeric / 100;$$;"], "external_knowledge": [], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn): cur = conn.cursor(); cur.execute(\"SELECT pct_to_numeric('72%');\"); val = cur.fetchone()[0]; assert abs(float(val) - 0.72) < 1e-6, 'Function pct_to_numeric returned incorrect value'; return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_1", "selected_database": "cold_chain_pharma_compliance", "query": "Can you check our cold chain data and tell me how long temperature problems typically last on our riskiest shipping routes? I'm looking for the average time in minutes that temperatures went outside acceptable ranges, but only for the shipments marked as high risk. Just round to two decimal places for me.", "normal_query": "I want to find the average Temperature Excursion Duration  for shipments on High Risk routes only. Please show me the route type label and the average excursion duration in minutes, rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH excursion_data AS (  SELECT   s.reckey,  LOWER(TRIM(s.shipment_overview->'route'->>'risk_note')) AS risk_note,  (e.env_metrics->'temperature'->>'excursion_duration_min')::numeric AS excursion_min  FROM shipments s  JOIN environmentalmonitoring e ON s.reckey = e.reckeylink  WHERE e.env_metrics->'temperature'->>'excursion_duration_min' IS NOT NULL    AND LOWER(TRIM(s.shipment_overview->'route'->>'risk_note')) = 'high') SELECT  'High Risk Routes' AS route_type,  ROUND(AVG(excursion_min), 2) AS avg_excursion_duration_min FROM excursion_data;"], "external_knowledge": [0, 3], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_2", "selected_database": "cold_chain_pharma_compliance", "query": "Would check what percentage of our shipments actually stayed in the right temperature range the whole time? I need a simple number showing how many shipments had zero temperature problems compared to our total shipments. Just round it to two decimal places so it's easy to report.", "normal_query": "What is our overall Cold Chain Compliance Rate for all shipments? Please calculate the percentage of compliant shipments out of the total shipments monitored, and round the result to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT  ROUND(         (             COUNT(*) FILTER (WHERE (TRIM(env_metrics->'temperature'->>'excursion_count'))::int = 0) * 100.0         ) / COUNT(*),         2     ) AS cccr_percentage FROM environmentalmonitoring;"], "external_knowledge": [5], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_3", "selected_database": "cold_chain_pharma_compliance", "query": "Will you help me figure out how our shipments are performing in terms of timing? I want to see how many shipments are arriving early, on-time, or running late. Can you count up our shipments by these delivery timing categories? Just give me each category and its count, with the biggest numbers at the top.", "normal_query": "I plan to analyze our cold chain delivery performance using Delivery Performance Classification. Show me the performance category and the number of shipments in each category, sorted by shipment count in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Difficulty: Normal\n-- Result Type: Table\n-- Purpose: Classify shipments by their delivery performance relative to the planned ETA and count the number of shipments in each category.\n-- Knowledge used: Delivery Performance Classification (KB ID: 13)\n-- Advanced functions: Common Table Expressions (CTE), JSONB operators, CASE statement for categorization, data type casting.\n-- Step 1: In a CTE named `performance_data`, calculate the delay in hours for each shipment by subtracting the planned ETA from the actual duration. Extract these values from the `shipment_overview` JSONB column.\n-- Step 2: In the main query, use a CASE statement to categorize each shipment's performance ('Early', 'On-Time', 'Delayed', 'Severely Delayed') based on the `delay_hours` calculated in the CTE, following the definitions in KB 13.\n-- Step 3: Group the results by the assigned performance category and count the number of shipments in each, providing a summary of overall delivery timeliness.\nWITH performance_data AS (\n    SELECT\n        (shipment_overview->'timing_performance'->>'actual_duration_hrs')::numeric -\n        (shipment_overview->'timing_performance'->>'planned_eta_hrs')::numeric AS delay_hours\n    FROM shipments\n    WHERE shipment_overview->'timing_performance'->>'actual_duration_hrs' IS NOT NULL\n      AND shipment_overview->'timing_performance'->>'planned_eta_hrs' IS NOT NULL\n)\nSELECT\n    CASE\n        WHEN delay_hours < -2 THEN 'Early'\n        WHEN delay_hours >= -2 AND delay_hours <= 2 THEN 'On-Time'\n        WHEN delay_hours > 2 AND delay_hours <= 24 THEN 'Delayed'\n        ELSE 'Severely Delayed'\n    END AS performance_category,\n    COUNT(*) AS number_of_shipments\nFROM performance_data\nGROUP BY performance_category\nORDER BY number_of_shipments DESC;\n"], "external_knowledge": [13], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cold_chain_pharma_compliance_4", "selected_database": "cold_chain_pharma_compliance", "query": "I want to figure out if our shipping performance is better when our GPS tracking is working actively versus when it's in the intermittent location tracking state. Can you compare the average on-time performance between these two categories? Just give me the two tracking categories and their average performance scores rounded to two decimal places.", "normal_query": "I am working on comparing the On-Time Delivery Performance between shipments with different Location Tracking States. Specifically, analyze the average OTDP for shipments that have either 'Active' or 'Intermittent' tracking states. Show me each tracking state and its corresponding average OTDP value rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH otdp_data AS (\n  SELECT\n    s.shipment_overview->'timing_performance'->>'planned_eta_hrs' AS planned_hrs,\n    s.shipment_overview->'timing_performance'->>'actual_duration_hrs' AS actual_hrs,\n    LOWER(TRIM(e.env_metrics->'tracking'->>'location_tracking_state')) AS tracking_state\n  FROM shipments s\n  JOIN environmentalmonitoring e ON s.reckey = e.reckeylink\n  WHERE LOWER(TRIM(e.env_metrics->'tracking'->>'location_tracking_state')) IN ('active', 'intermittent')\n    AND s.shipment_overview->'timing_performance'->>'actual_duration_hrs' IS NOT NULL\n    AND (s.shipment_overview->'timing_performance'->>'actual_duration_hrs')::numeric > 0\n)\nSELECT\n  tracking_state,\n  ROUND(AVG((actual_hrs::numeric / planned_hrs::numeric) * 100), 2) AS average_otdp\nFROM otdp_data\nGROUP BY tracking_state;"], "external_knowledge": [12, 18], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_5", "selected_database": "cold_chain_pharma_compliance", "query": "I am trying to figure out if our quality agreements are working properly. Can you check how many shipments failed compliance checks for each type of agreement we have? Just show me each agreement status and how many shipments were flagged as non-compliant under that status.", "normal_query": "I hope to analyze how different Quality Agreement Status types relate to non-compliance issues. Could you count the number of shipments that were classified as 'Non-compliant' for each quality agreement status category? Please show each agreement status with its corresponding count of non-compliant shipments.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT LOWER(TRIM(qc_checklist->'gdp_quality'->>'quality_agreement_status')) AS agreement_status, COUNT(*) FILTER ( WHERE LOWER(TRIM(qc_checklist->'customs_and_regulatory'->>'regulatory_compliance_status')) = 'non-compliant' ) AS non_compliant_shipment_count FROM qualitycompliance WHERE qc_checklist->'gdp_quality'->>'quality_agreement_status' IS NOT NULL GROUP BY LOWER(TRIM(qc_checklist->'gdp_quality'->>'quality_agreement_status'));"], "external_knowledge": [14, 21], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_6", "selected_database": "cold_chain_pharma_compliance", "query": "Our quality team needs to flag any high-risk shipments for immediate review. Could you pull a list of all shipments falling into red quality risk zone? Just show me the shipment ID, what percentage of time it stayed in the acceptable range, and how many total minutes it was out of range. Round the portion value into 2 decimal points.", "normal_query": "I am going to identify shipments in the Red Zone of our Quality Risk Zones for further investigation. For each shipment in the Red Zone, show me the shipment ID, calculated TIRP percentage (rounded to 2 decimal places), and the total excursion duration in minutes.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH shipment_metrics AS (\n    SELECT\n        s.reckey,\n        (s.shipment_overview->'timing_performance'->>'actual_duration_hrs')::numeric * 60 AS total_duration_min,\n        (e.env_metrics->'temperature'->>'excursion_duration_min')::numeric AS ted_min\n    FROM shipments s\n    JOIN environmentalmonitoring e ON s.reckey = e.reckeylink\n    WHERE (s.shipment_overview->'timing_performance'->>'actual_duration_hrs')::numeric > 0\n      AND (e.env_metrics->'temperature'->>'excursion_duration_min')::numeric IS NOT NULL\n)\nSELECT\n    reckey AS shipment_id,\n    ROUND((1 - (ted_min / total_duration_min)) * 100, 2) AS tirp_percentage,\n    ted_min AS total_excursion_duration_min\nFROM shipment_metrics\nWHERE ((1 - (ted_min / total_duration_min)) * 100) < 95;"], "external_knowledge": [0, 25], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_7", "selected_database": "cold_chain_pharma_compliance", "query": "Can you figure out what the average temperature impact was for all our shipments where the temperature went outside the acceptable range? We need that special calculation that accounts for how temperature fluctuations affect products over time. Just give me one number that summarizes this across all problematic shipments.", "normal_query": "I would like to calculate the Mean Kinetic Temperature for all shipments that have experienced temperature excursions. Please provide me with the average MKT value across these shipments.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    ROUND(AVG(\n        (-83144 / 8.3144) / LN(\n            (\n                EXP(-83144 / (8.3144 * ((em.env_metrics->'temperature'->>'max_c')::numeric + 273.15))) + \n                EXP(-83144 / (8.3144 * ((em.env_metrics->'temperature'->>'min_c')::numeric + 273.15))) + \n                EXP(-83144 / (8.3144 * ((em.env_metrics->'temperature'->>'avg_c')::numeric + 273.15)))\n            ) / 3\n        )\n    ), 2) AS average_mkt\nFROM \n    environmentalmonitoring em\nWHERE \n    (em.env_metrics->'temperature'->>'excursion_count')::integer > 0;"], "external_knowledge": [26], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_8", "selected_database": "cold_chain_pharma_compliance", "query": "Please help me find shipment routes where our risk labels don't match what's actually happening. I want to see where we've mislabeled routes compared to what the temperature excursion data shows. Make sure you check all our routes, even if some might be missing monitoring data. Just show me the top 3 most problematic routes according to average excursion count.", "normal_query": "We need to identify where our shipping route risk classifications don't match reality. Using the Route Risk Classification and High-Risk Shipping Origin-Destination Pairs knowledge, compare our documented risk notes against calculated risk levels, even those without environmental monitoring data. Show me only routes where the documented risk level differs from the calculated risk level, ordered by average excursion count from highest to lowest. Limit results to the top 3 discrepancies.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH route_analysis AS (\n    SELECT \n        s.shipment_overview->'route'->>'route_string' AS route,\n        LOWER(TRIM(s.shipment_overview->'route'->>'risk_note')) AS documented_risk,\n        (em.env_metrics->'temperature'->>'excursion_count')::integer AS excursion_count,\n        CASE \n            WHEN (em.env_metrics->'temperature'->>'excursion_count')::integer <= 1 THEN 'low'\n            WHEN (em.env_metrics->'temperature'->>'excursion_count')::integer BETWEEN 2 AND 4 THEN 'medium'\n            WHEN (em.env_metrics->'temperature'->>'excursion_count')::integer >= 5 THEN 'high'\n        END AS calculated_risk\n    FROM shipments s\n    LEFT JOIN environmentalmonitoring em ON s.reckey = em.reckeylink\n    WHERE s.shipment_overview->'route'->>'risk_note' IS NOT NULL\n      AND (em.env_metrics->'temperature'->>'excursion_count') IS NOT NULL\n)\nSELECT \n    route,\n    documented_risk,\n    calculated_risk,\n    AVG(excursion_count) AS avg_excursions\nFROM route_analysis\nWHERE documented_risk IN ('low', 'medium', 'high')\nGROUP BY route, documented_risk, calculated_risk\nHAVING documented_risk != calculated_risk\nORDER BY avg_excursions DESC\nLIMIT 3;"], "external_knowledge": [3, 17], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 0, "distinct": false, "order": true}}
{"instance_id": "cold_chain_pharma_compliance_9", "selected_database": "cold_chain_pharma_compliance", "query": "Just give me a rough idea of how our shipments did in terms of temperature control. You can group them into risk categories like green/yellow/red using time-in-range and total out-of-range time. It’s fine to use a simplified method—doesn’t have to be perfect—as long as we get a general sense.", "normal_query": "Please provide an approximate analysis of cold chain shipment quality by grouping them into Quality Risk Zones. Use Time In Range Percentage (TIRP) and total temperature excursion duration as approximate indicators for classification. A proxy-based zoning approach is acceptable where exact excursion details are unavailable. Return the number and percentage of shipments in each zone (Green, Yellow, Red), sorted from lowest to highest risk.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH risk_zones AS ( SELECT s.reckey, ((s.shipment_overview->'timing_performance'->>'actual_duration_hrs')::numeric * 60 - (em.env_metrics->'temperature'->>'excursion_duration_min')::numeric) / NULLIF((s.shipment_overview->'timing_performance'->>'actual_duration_hrs')::numeric * 60, 0) * 100 AS tirp, (em.env_metrics->'temperature'->>'excursion_duration_min')::numeric AS excursion_duration, CASE WHEN ((s.shipment_overview->'timing_performance'->>'actual_duration_hrs')::numeric * 60 - (em.env_metrics->'temperature'->>'excursion_duration_min')::numeric) / NULLIF((s.shipment_overview->'timing_performance'->>'actual_duration_hrs')::numeric * 60, 0) * 100 > 98 AND (em.env_metrics->'temperature'->>'excursion_duration_min')::numeric <= 30 THEN 'Green Zone' WHEN (((s.shipment_overview->'timing_performance'->>'actual_duration_hrs')::numeric * 60 - (em.env_metrics->'temperature'->>'excursion_duration_min')::numeric) / NULLIF((s.shipment_overview->'timing_performance'->>'actual_duration_hrs')::numeric * 60, 0) * 100 BETWEEN 95 AND 98) OR ((em.env_metrics->'temperature'->>'excursion_duration_min')::numeric BETWEEN 30 AND 60) THEN 'Yellow Zone' WHEN ((s.shipment_overview->'timing_performance'->>'actual_duration_hrs')::numeric * 60 - (em.env_metrics->'temperature'->>'excursion_duration_min')::numeric) / NULLIF((s.shipment_overview->'timing_performance'->>'actual_duration_hrs')::numeric * 60, 0) * 100 < 95 OR ((em.env_metrics->'temperature'->>'excursion_duration_min')::numeric > 60) THEN 'Red Zone' ELSE 'Unknown' END AS quality_risk_zone FROM shipments s LEFT JOIN environmentalmonitoring em ON s.reckey = em.reckeylink WHERE (em.env_metrics->'temperature'->>'excursion_duration_min') IS NOT NULL AND (s.shipment_overview->'timing_performance'->>'actual_duration_hrs') IS NOT NULL ) SELECT quality_risk_zone, COUNT(*) AS shipment_count, ROUND(COUNT(*)::numeric * 100.0 / (SELECT COUNT(*) FROM risk_zones), 2) AS percentage FROM risk_zones GROUP BY quality_risk_zone ORDER BY CASE quality_risk_zone WHEN 'Green Zone' THEN 1 WHEN 'Yellow Zone' THEN 2 WHEN 'Red Zone' THEN 3 ELSE 4 END;"], "external_knowledge": [0, 25, 27], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "cold_chain_pharma_compliance_10", "selected_database": "cold_chain_pharma_compliance", "query": "Can you help me figure out how strong our supply chain is overall? Let’s turn the inputs into scores like this: route risk gets 8 if it’s low, 5 for medium, 2 for high; carrier certification gets 10 for both types, 8 if it’s just 'gdp' or 'ceiv pharma', 4 otherwise; vehicles score 9 if validated, 7 if qualified, and 5 otherwise; and compliance gets 9 for full, 6 for partial, 3 otherwise. Use the 0.4/0.3/0.2/0.1 weighting and round the final number to two decimal places.", "normal_query": "I want to calculate the overall Supply Chain Resilience Score using a weighted average of several proxy indicators. For this, please map: 'low' route risk to 8, 'medium' to 5, and 'high' to 2; for carrier certification, use 10 for 'both', 8 for 'gdp' or 'ceiv pharma', and 4 otherwise; for vehicle qualification, use 9 for 'validated', 7 for 'qualified', and 5 otherwise; and for GDP compliance, use 9 for 'full', 6 for 'partial', and 3 otherwise. Then apply weights: 0.4 for route risk, 0.3 for carrier certification, 0.2 for vehicle, and 0.1 for compliance. Round to two decimals.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH proxy_scores AS (\n  SELECT\n    AVG(CASE\n      WHEN LOWER(TRIM(s.shipment_overview->'route'->>'risk_note')) = 'low' THEN 8\n      WHEN LOWER(TRIM(s.shipment_overview->'route'->>'risk_note')) = 'medium' THEN 5\n      WHEN LOWER(TRIM(s.shipment_overview->'route'->>'risk_note')) = 'high' THEN 2\n      ELSE 4\n    END) AS ART,\n    AVG(CASE\n      WHEN LOWER(TRIM(c.carriercert)) = 'both' THEN 10\n      WHEN LOWER(TRIM(c.carriercert)) IN ('gdp', 'ceiv pharma') THEN 8\n      ELSE 4\n    END) AS RRD,\n    AVG(CASE\n      WHEN LOWER(TRIM(v.veh_qual)) = 'validated' THEN 9\n      WHEN LOWER(TRIM(v.veh_qual)) = 'qualified' THEN 7\n      ELSE 5\n    END) AS SBP,\n    AVG(CASE\n      WHEN LOWER(TRIM(qc.qc_checklist->'gdp_quality'->>'gdp_compliance')) = 'full' THEN 9\n      WHEN LOWER(TRIM(qc.qc_checklist->'gdp_quality'->>'gdp_compliance')) = 'partial' THEN 6\n      ELSE 3\n    END) AS SMC\n  FROM shipments s\n  LEFT JOIN qualitycompliance qc ON s.reckey = qc.reckeyqc\n  LEFT JOIN vehicles v ON v.carrierbond IS NOT NULL\n  LEFT JOIN carriers c ON v.carrierbond = c.carriertag\n)\nSELECT ROUND(0.4 * ART + 0.3 * RRD + 0.2 * SBP + 0.1 * SMC, 2) AS supply_chain_resilience_score\nFROM proxy_scores;"], "external_knowledge": [30], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_11", "selected_database": "cold_chain_pharma_compliance", "query": "I wonder that, for each type of GDP certification, how many different shipping companies have that certification, and out of those, how many actually have at least one fully validated vehicle. Please show the certification type, the total number of unique certified companies, and how many of those companies have validated vehicles. Put the ones with the most companies using validated vehicles at the top.", "normal_query": "For each GDP Certification Status, I want to know how many distinct carriers hold that certification, and among them, how many distinct carriers also have at least one vehicle with Validated Cold Chain Vehicle Qualification Status. Please display the GDP certification level, the total number of distinct GDP-certified carriers, and the number of those distinct carriers with validated vehicles. Sort the results by the number of carriers with validated vehicles in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT \n    LOWER(TRIM(c.carriercert)) AS gdp_certification_level,\n    COUNT(DISTINCT c.carriertag) AS total_gdp_carriers,\n    COUNT(DISTINCT CASE WHEN LOWER(TRIM(v.veh_qual)) = 'validated' THEN c.carriertag END) AS carriers_with_validated_vehicles\nFROM carriers c\nLEFT JOIN vehicles v ON c.carriertag = v.carrierbond\nGROUP BY LOWER(TRIM(c.carriercert))\nORDER BY carriers_with_validated_vehicles DESC;"], "external_knowledge": [4, 24], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 0, "distinct": true, "order": true}}
{"instance_id": "cold_chain_pharma_compliance_14", "selected_database": "cold_chain_pharma_compliance", "query": "I need to know, on average, how much of their allowed stability budget temperature-sensitive shipments are using up. Only count shipments where there actually was a temperature excursion. Give me the average percentage used, rounded to two decimal places.", "normal_query": "Calculate the average Stability Budget Consumption Rate for all shipments of temperature-sensitive products. Return the average SBCR as a percentage, rounded to two decimal places and only count shipments where there actually was a temperature excursion.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH sbcr_data AS (SELECT em.reckeylink, pb.TempSense, (em.env_metrics->'temperature'->>'excursion_duration_min')::numeric AS ted_minutes FROM environmentalmonitoring em JOIN shipsensorlink ssl ON em.devlink = ssl.devnode AND em.reckeylink = ssl.shpnode JOIN productbatches pb ON ssl.devnode = pb.prodlink WHERE (em.env_metrics->'temperature'->>'excursion_count')::integer > 0 AND (em.env_metrics->'temperature'->>'excursion_duration_min') IS NOT NULL AND pb.TempSense IN ('Low', 'Medium', 'High')) SELECT ROUND(AVG(ted_minutes / CASE WHEN TempSense = 'High' THEN 120.0 WHEN TempSense = 'Medium' THEN 480.0 WHEN TempSense = 'Low' THEN 1440.0 ELSE NULL END * 100), 2) AS average_sbcr_percentage FROM sbcr_data WHERE ted_minutes > 0;"], "external_knowledge": [0, 6, 31, 32], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_15", "selected_database": "cold_chain_pharma_compliance", "query": "Out of all the biologics product batches, what percent need to be kept at ultra-low temperatures? Round it to two decimal places.", "normal_query": "Please calculate the percentage of biologics products that require ultra-low temperature storage. The result should be rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT ROUND(COUNT(*) FILTER (WHERE LOWER(TRIM(pb.store_cond)) = '-70°c') * 100.0 / COUNT(*), 2) AS biologics_ultra_low_temp_percentage FROM products p JOIN productbatches pb ON p.prodcode = pb.prodlink WHERE LOWER(TRIM(p.prodcat)) = 'biologics';"], "external_knowledge": [9], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_16", "selected_database": "cold_chain_pharma_compliance", "query": "I want to know, on average, how much carbon was produced by shipments that were way behind schedule—specifically, those whose delivery performance category counted as severely delayed. Return this value into two decimal places.", "normal_query": "Calculate the average carbon footprint for all shipments that are classified as 'Severely Delayed' based on the Delivery Performance Classification standard. Please return a value rounded to two decimal places.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Difficulty: Normal\n-- Result Type: Scalar\n-- Purpose: Calculate the average carbon footprint (in kg) for shipments that were classified as 'Severely Delayed'.\n-- Knowledge used: Delivery Performance Classification (KB ID: 13)\n-- Advanced functions: CTE, JOIN, JSONB operators, CASE statement for categorization.\n-- Step 1: In the `delivery_performance` CTE, join `shipments` and `reviewsandimprovements`. Calculate the delay in hours.\n-- Step 2: Use a CASE statement to categorize the delivery performance based on the delay, identifying 'Severely Delayed' shipments as per KB 13.\n-- Step 3: In the main query, filter for the 'Severely Delayed' category and calculate the average `carbonkg`.\n\nWITH delivery_performance AS (\n    SELECT\n        r.carbonkg,\n        CASE\n            WHEN ((s.shipment_overview->'timing_performance'->>'actual_duration_hrs')::NUMERIC -\n                  (s.shipment_overview->'timing_performance'->>'planned_eta_hrs')::NUMERIC) > 24\n            THEN 'Severely Delayed'\n            ELSE 'Not Severely Delayed'\n        END AS performance_category\n    FROM\n        shipments s\n    JOIN\n        reviewsandimprovements r ON s.reckey = r.reckeyrev\n    WHERE\n        s.shipment_overview->'timing_performance'->>'actual_duration_hrs' IS NOT NULL\n        AND r.carbonkg IS NOT NULL\n)\nSELECT\n    ROUND(AVG(carbonkg)::NUMERIC, 2) AS avg_carbon_kg_for_severe_delays\nFROM\n    delivery_performance\nWHERE\n    performance_category = 'Severely Delayed';\n"], "external_knowledge": [13], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_17", "selected_database": "cold_chain_pharma_compliance", "query": "Could you figure out the overall reliability score for all our temperature data loggers? Treat any logger with a missing recording interval as having a reading failure, and count a calibration failure if the calibration date is before June 26, 2024.", "normal_query": "Estimate the overall Data Logger Reliability Score for all monitoring devices in the fleet. Assume a reading failure if the recording interval is missing, and a calibration failure if the calibration date is before 2024-06-26.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Difficulty: Advanced\n-- Result Type: Scalar\n-- Purpose: Estimate the Data Logger Reliability Score (DLRS) for the entire fleet of monitoring devices.\n-- Knowledge used: Data Logger Reliability Score (DLRS) (KB ID: 48)\n-- Advanced functions: CTE, aggregation, mathematical calculation.\n-- Step 1: In the `device_failures` CTE, simulate failure rates. Assume a reading failure if `recintmin` is null, and a calibration failure if the calibration date is older than a year from the current date (`2025-06-26`). Other failures are assumed to be 0 for this example.\n-- Step 2: Calculate the rates per 100 deployments by dividing the count of failures by the total number of devices and multiplying by 100.\n-- Step 3: In the main query, apply the DLRS formula from KB 48: `100 - (10*Fr + 5*Fd + 3*Fc + 2*Fb)`.\n\nWITH device_failures AS (\n    SELECT\n        COUNT(*) AS total_devices,\n        COUNT(*) FILTER (WHERE recintmin IS NULL) AS reading_failures,\n        COUNT(*) FILTER (WHERE calibts < '2024-06-26') AS calibration_failures\n    FROM\n        monitoringdevices\n)\nSELECT\n    100 - (\n        10 * (reading_failures * 100.0 / total_devices) +\n        3 * (calibration_failures * 100.0 / total_devices)\n    ) AS estimated_dlrs\nFROM\n    device_failures;\n"], "external_knowledge": [48], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_18", "selected_database": "cold_chain_pharma_compliance", "query": "Which three people have had the most shipments rejected when making product release decisions? I just want a list of their names and how many times each had a rejection, sorted so the person with the most rejections is at the top.", "normal_query": "Using the Product Release Decision Framework, identify the top 3 responsible persons with the highest number of 'Rejected' product releases based on the Product Release Decision Framework. Please list each responsible person and their count of rejected shipments, ordered from highest to lowest count.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n    LOWER(TRIM(qc_checklist->'gdp_quality'->>'responsible_person')) AS responsible_person,\n    COUNT(*) AS rejected_shipment_count\nFROM\n    qualitycompliance\nWHERE\n    qc_checklist->'release'->>'product_release_status' = 'Rejected'\n    AND qc_checklist->'gdp_quality'->>'responsible_person' IS NOT NULL\nGROUP BY\n    LOWER(TRIM(qc_checklist->'gdp_quality'->>'responsible_person'))\nORDER BY\n    rejected_shipment_count DESC\nLIMIT 3;\n"], "external_knowledge": [34], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cold_chain_pharma_compliance_19", "selected_database": "cold_chain_pharma_compliance", "query": "Can you show me, for each type of regulatory compliance status, what the average number of temperature and humidity excursions is? And sort the results so the highest average temperature excursions come first.", "normal_query": "For each regulatory compliance status, calculate the average number of temperature excursions and average number of humidity excursions. Display a table with compliance status, average temperature excursions, and average humidity excursions, sorted by temperature excursions in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT\n    LOWER(TRIM(q.qc_checklist->'customs_and_regulatory'->>'regulatory_compliance_status')) AS compliance_status,\n    ROUND(AVG((e.env_metrics->'temperature'->>'excursion_count')::NUMERIC), 2) AS avg_temp_excursions,\n    ROUND(AVG((e.env_metrics->'humidity'->>'excursion_count')::NUMERIC), 2) AS avg_humidity_excursions\nFROM\n    qualitycompliance q\nJOIN\n    environmentalmonitoring e ON q.reckeyqc = e.reckeylink\nWHERE\n    q.qc_checklist->'customs_and_regulatory'->>'regulatory_compliance_status' IS NOT NULL\nGROUP BY\n    LOWER(TRIM(q.qc_checklist->'customs_and_regulatory'->>'regulatory_compliance_status'))\nORDER BY\n    avg_temp_excursions DESC;"], "external_knowledge": [0, 21], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "cold_chain_pharma_compliance_20", "selected_database": "cold_chain_pharma_compliance", "query": "Can you show me the three riskiest shipping routes based on temperature excursions and shipping delays? Just give me the route, how many shipments went that way, and the risk score. Only count routes with more than one shipment.", "normal_query": "I want to identify the top 3 riskiest shipping lanes by calculating the Lane Risk Potential for each route. Only include temperature excursions and shipping delays in the risk score. Please provide a report listing the route string, total number of shipments, and the calculated lane risk potential, sorted by lane risk potential in descending order. Only include lanes with more than one shipment.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH lane_data AS ( SELECT LOWER(TRIM(s.shipment_overview->'route'->>'route_string')) AS route_string, COALESCE((e.env_metrics->'temperature'->>'excursion_count')::INT, 0) AS temp_excursions, CASE WHEN (s.shipment_overview->'timing_performance'->>'actual_duration_hrs')::NUMERIC > (s.shipment_overview->'timing_performance'->>'planned_eta_hrs')::NUMERIC THEN 1 ELSE 0 END AS is_shipping_delayed FROM shipments s JOIN environmentalmonitoring e ON s.reckey = e.reckeylink WHERE s.shipment_overview->'route'->>'route_string' IS NOT NULL ), lane_risk AS ( SELECT route_string, SUM(temp_excursions) AS total_excursions, SUM(is_shipping_delayed) AS total_shipping_delays, COUNT(*) AS total_shipments FROM lane_data GROUP BY route_string ) SELECT route_string, total_shipments, ROUND((total_excursions + total_shipping_delays)::NUMERIC / total_shipments, 2) AS lane_risk_potential FROM lane_risk WHERE total_shipments > 1 ORDER BY lane_risk_potential DESC LIMIT 3;"], "external_knowledge": [0, 36, 13], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": true}}
{"instance_id": "cold_chain_pharma_compliance_M_1", "selected_database": "cold_chain_pharma_compliance", "query": "Can you make a function calculate_ted that, when I give you a shipment's ID, tells me how many minutes it spent outside the right temperature range?", "normal_query": "For a given shipment, try to create a function calculate_ted that calculates the Temperature Excursion Duration. The input is a shipment's ID and output the TED value as an integer.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION calculate_ted(shipment_reckey TEXT)\nRETURNS INTEGER AS $$\nDECLARE\n    total_excursion_duration INTEGER := 0;\n    env_data JSONB;\nBEGIN\n    SELECT env_metrics INTO env_data\n    FROM environmentalmonitoring \n    WHERE reckeylink = shipment_reckey;\n    IF env_data IS NOT NULL AND env_data->'temperature'->>'excursion_duration_min' IS NOT NULL THEN\n        total_excursion_duration := (env_data->'temperature'->>'excursion_duration_min')::INTEGER;\n    END IF;\n    RETURN COALESCE(total_excursion_duration, 0);\nEND;\n$$ LANGUAGE plpgsql;", "SELECT \n    s.reckey AS shipment_id,\n    calculate_ted(s.reckey) AS ted\nFROM shipments s\nLEFT JOIN environmentalmonitoring em\n    ON s.reckey = em.reckeylink\nORDER BY ted DESC\nLIMIT 5;"], "external_knowledge": [0], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute(\"\"\"\n        SELECT EXISTS (\n            SELECT 1 FROM pg_proc WHERE proname = 'calculate_ted'\n        )\n    \"\"\")\n    assert cursor.fetchone()[0], \"calculate_ted function not created\"\n    cursor.execute(\"SELECT calculate_ted('CC892358')\")\n    result = cursor.fetchone()[0]\n    assert isinstance(result, int), \"Function should return integer\"\n    assert result >= 0, \"TED should be non-negative\"\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": 0, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_M_2", "selected_database": "cold_chain_pharma_compliance", "query": "For each shipment, I want you to include two things in the summary. First, tell me if it managed to stay in the right temperature the whole way. Second, show what percent of all shipments got that right. Even if some shipments don’t have temperature info, still include both pieces of info in their summary.", "normal_query": "Please update every shipment so that its summary includes two clear insights. First, show whether that shipment stayed within the correct temperature range the entire time. Second, include the percentage of all shipments that successfully stayed within the proper temperature range from start to finish. This information should be added for every shipment, even for those where no temperature data is available.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["WITH shipment_counts AS (\n    SELECT \n        COUNT(*) AS total_shipments,\n        SUM(\n            CASE \n                WHEN em.env_metrics IS NOT NULL AND (em.env_metrics->'temperature'->>'excursion_count')::INTEGER = 0 \n                THEN 1\n                ELSE 0 \n            END\n        ) AS compliant_shipments\n    FROM shipments s\n    LEFT JOIN environmentalmonitoring em ON s.reckey = em.reckeylink\n),\nshipment_status AS (\n    SELECT\n        s.reckey,\n        CASE \n            WHEN em.env_metrics IS NULL THEN 'UNKNOWN'\n            WHEN (em.env_metrics->'temperature'->>'excursion_count')::INTEGER = 0 THEN 'COMPLIANT'\n            ELSE 'NON_COMPLIANT'\n        END AS compliance_status,\n        sc.compliant_shipments,\n        sc.total_shipments\n    FROM shipments s\n    LEFT JOIN environmentalmonitoring em ON s.reckey = em.reckeylink\n    CROSS JOIN shipment_counts sc\n)\nUPDATE shipments s\nSET shipment_overview = shipment_overview || jsonb_build_object(\n    'compliance_status', ss.compliance_status,\n    'compliance_rate', ROUND((ss.compliant_shipments::decimal / NULLIF(ss.total_shipments, 0)) * 100, 2)\n)\nFROM shipment_status ss\nWHERE s.reckey = ss.reckey\nRETURNING 1;"], "external_knowledge": [5], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT COUNT(*) FROM shipments WHERE shipment_overview ? 'compliance_rate'\")\n    updated_count = cursor.fetchone()[0]\n    assert updated_count > 0, \"No shipments were updated with compliance rate\"\n    cursor.execute(\"SELECT (shipment_overview->>'compliance_rate')::DECIMAL FROM shipments WHERE shipment_overview ? 'compliance_rate' LIMIT 1\")\n    rate = cursor.fetchone()[0]\n    assert 0 <= rate <= 100, \"Compliance rate should be between 0 and 100\"\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_M_3", "selected_database": "cold_chain_pharma_compliance", "query": "Can you make a view called v_product_temperature_analysis that breaks down our products by how sensitive they are to temperature changes and by their storage type? For each kind of product and storage group, show how many batches there are, what the average temperature range is, a list of the different sensitivity levels, and the lowest and highest storage temps. Please sort the results by product type and storage method.", "normal_query": "Create a view named v_product_temperature_analysis that analyzes pharmaceutical products by both Temperature Sensitivity Tiers and Product Storage Classifications. For each product category and storage classification, display the batch count, average temperature range, all unique sensitivity descriptions, and the minimum and maximum storage temperatures. Order the results by product category and storage classification.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE VIEW v_product_temperature_analysis AS WITH product_sensitivity AS ( SELECT p.prodcode, p.prodlabel, p.prodcat, pb.batchtag, pb.store_cond, pb.tempmin, pb.tempmax, pb.tempsense, CASE pb.tempsense WHEN 'Low' THEN '24 hours tolerance' WHEN 'Medium' THEN '8 hours tolerance' WHEN 'High' THEN '2 hours tolerance' ELSE 'Unknown sensitivity' END AS sensitivity_description, CASE pb.store_cond WHEN '2-8°C' THEN 'Refrigerated' WHEN '-20°C' THEN 'Frozen' WHEN '-70°C' THEN 'Ultra-low Temperature' WHEN '15-25°C' THEN 'Controlled Room Temperature' ELSE 'Non-standard Storage' END AS storage_classification, COALESCE(pb.tempmax - pb.tempmin, 0) AS temp_range_width FROM products p INNER JOIN productbatches pb ON p.prodcode = pb.prodlink WHERE pb.store_cond IS NOT NULL AND pb.tempsense IS NOT NULL ) SELECT prodcat, storage_classification, COUNT(*) AS batch_count, AVG(temp_range_width) AS avg_temp_range, STRING_AGG(DISTINCT sensitivity_description, ', ') AS sensitivity_levels, MIN(tempmin) AS min_storage_temp, MAX(tempmax) AS max_storage_temp FROM product_sensitivity GROUP BY prodcat, storage_classification ORDER BY prodcat, storage_classification;"], "external_knowledge": [6, 9], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT EXISTS (SELECT 1 FROM pg_views WHERE viewname = 'v_product_temperature_analysis')\")\n    assert cursor.fetchone()[0], \"View v_product_temperature_analysis not created\"\n    cursor.execute(\"SELECT COUNT(*) FROM v_product_temperature_analysis\")\n    row_count = cursor.fetchone()[0]\n    assert row_count > 0, \"View should return data\"\n    cursor.execute(\"SELECT DISTINCT storage_classification FROM v_product_temperature_analysis\")\n    classifications = [row[0] for row in cursor.fetchall()]\n    valid_classes = ['Refrigerated', 'Frozen', 'Ultra-low Temperature', 'Controlled Room Temperature', 'Non-standard Storage']\n    assert all(c in valid_classes for c in classifications), \"Invalid storage classifications\"\n    return 1"], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cold_chain_pharma_compliance_M_4", "selected_database": "cold_chain_pharma_compliance", "query": "Can you show, for every delivery, how well it matched the expected timing — like how close it was to being on time? And also include something simple that tells whether it was early, late, or arrived just right. Keep everything else as it is.", "normal_query": "Please enhance each delivery record by adding two insights. The first is the On-Time Delivery Performance, which shows how closely the actual delivery matched the planned schedule, expressed as a percentage. The second is the Delivery Performance Classification, which gives a simple label describing the delivery’s overall timeliness. These additions should be included along with the original delivery details.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Intent: Add calculated columns for On-Time Delivery Performance analysis\n-- Knowledge Used: KB ID 12 - On-Time Delivery Performance (OTDP), KB ID 13 - Delivery Performance Classification\n-- Advanced PostgreSQL: ALTER TABLE with expressions, UPDATE with calculations, GENERATED columns\n\n-- Step 1: Add column for On-Time Delivery Performance percentage\n-- OTDP = (actual_duration / planned_duration) × 100%\nALTER TABLE shipments \nADD COLUMN delivery_performance_pct NUMERIC(7,2);\n\n-- Step 2: Add column for delivery classification based on variance\n-- Classifications: Early, On-Time, Delayed, Severely Delayed\nALTER TABLE shipments \nADD COLUMN delivery_classification TEXT;\n\n-- Step 3: Update existing records with OTDP calculations\n-- Using JSON path extraction and null handling\nUPDATE shipments \nSET \n    delivery_performance_pct = CASE \n        WHEN (shipment_overview->'timing_performance'->>'planned_eta_hrs')::NUMERIC > 0 \n        THEN ROUND(\n            (shipment_overview->'timing_performance'->>'actual_duration_hrs')::NUMERIC / \n            (shipment_overview->'timing_performance'->>'planned_eta_hrs')::NUMERIC * 100, 2\n        )\n        ELSE NULL\n    END,\n    -- Step 4: Apply delivery performance classification from KB\n    -- Early: >2hrs before, On-Time: ±2hrs, Delayed: 2-24hrs late, Severely Delayed: >24hrs late\n    delivery_classification = CASE \n        WHEN (shipment_overview->'timing_performance'->>'actual_duration_hrs')::NUMERIC IS NULL THEN 'Unknown'\n        WHEN (shipment_overview->'timing_performance'->>'actual_duration_hrs')::NUMERIC < \n             (shipment_overview->'timing_performance'->>'planned_eta_hrs')::NUMERIC - 2 THEN 'Early'\n        WHEN ABS((shipment_overview->'timing_performance'->>'actual_duration_hrs')::NUMERIC - \n                 (shipment_overview->'timing_performance'->>'planned_eta_hrs')::NUMERIC) <= 2 THEN 'On-Time'\n        WHEN (shipment_overview->'timing_performance'->>'actual_duration_hrs')::NUMERIC BETWEEN \n             (shipment_overview->'timing_performance'->>'planned_eta_hrs')::NUMERIC + 2 AND\n             (shipment_overview->'timing_performance'->>'planned_eta_hrs')::NUMERIC + 24 THEN 'Delayed'\n        WHEN (shipment_overview->'timing_performance'->>'actual_duration_hrs')::NUMERIC > \n             (shipment_overview->'timing_performance'->>'planned_eta_hrs')::NUMERIC + 24 THEN 'Severely Delayed'\n        ELSE 'Unknown'\n    END\nWHERE shipment_overview->'timing_performance' IS NOT NULL;\n"], "external_knowledge": [12, 13], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    \n    # Check if columns were added\n    cursor.execute(\"\"\"\n        SELECT column_name FROM information_schema.columns \n        WHERE table_name = 'shipments' \n        AND column_name IN ('delivery_performance_pct', 'delivery_classification')\n    \"\"\")\n    columns = [row[0] for row in cursor.fetchall()]\n    assert 'delivery_performance_pct' in columns, \"delivery_performance_pct column not added\"\n    assert 'delivery_classification' in columns, \"delivery_classification column not added\"\n    \n    # Verify classification values are valid\n    cursor.execute(\"\"\"\n        SELECT DISTINCT delivery_classification \n        FROM shipments \n        WHERE delivery_classification IS NOT NULL\n    \"\"\")\n    classifications = [row[0] for row in cursor.fetchall()]\n    valid_classes = ['Early', 'On-Time', 'Delayed', 'Severely Delayed', 'Unknown']\n    assert all(c in valid_classes for c in classifications), \"Invalid delivery classifications\"\n    \n    return 1\n"], "category": "Management", "high_level": false, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_M_5", "selected_database": "cold_chain_pharma_compliance", "query": "Whenever we add or change a shipment’s temperature data, I want the system to automatically figure out how much of the time the temperature stayed where it should, and also give it a simple color rating based on that. These two things should be added back into the same record.", "normal_query": "Whenever a new or updated environmental monitoring entry is recorded, the system should automatically assess the Quality Risk Zones for that shipment. It should use the temperature data to calculate Time In Range Percentage based on a standard 72-hour journey, then assign the appropriate risk level. Both the quality risk zone and time in range percentage should be added back into the same record.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["CREATE OR REPLACE FUNCTION assess_quality_risk_zone()\nRETURNS TRIGGER AS $$\nDECLARE\n    excursion_duration INTEGER;\n    excursion_count INTEGER;\n    time_in_range_pct NUMERIC;\n    risk_zone TEXT;\nBEGIN\n    excursion_duration := COALESCE((NEW.env_metrics->'temperature'->>'excursion_duration_min')::INTEGER, 0);\n    excursion_count := COALESCE((NEW.env_metrics->'temperature'->>'excursion_count')::INTEGER, 0);\n\n    time_in_range_pct := CASE \n        WHEN excursion_duration = 0 THEN 100.0\n        ELSE ROUND(((4320 - excursion_duration)::NUMERIC / 4320.0) * 100, 2)\n    END;\n\n    IF time_in_range_pct > 98 AND excursion_duration <= 30 THEN\n        risk_zone := 'GREEN';\n    ELSIF (time_in_range_pct >= 95 AND time_in_range_pct <= 98) OR (excursion_duration BETWEEN 31 AND 60) THEN\n        risk_zone := 'YELLOW';\n    ELSIF time_in_range_pct < 95 OR excursion_duration > 60 THEN\n        risk_zone := 'RED';\n    ELSE\n        risk_zone := 'UNKNOWN';\n    END IF;\n\n    NEW.env_metrics := NEW.env_metrics || jsonb_build_object(\n        'quality_risk_zone', risk_zone,\n        'time_in_range_pct', time_in_range_pct\n    );\n\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;", "CREATE TRIGGER tr_quality_risk_assessment\n    BEFORE INSERT OR UPDATE ON environmentalmonitoring\n    FOR EACH ROW\n    EXECUTE FUNCTION assess_quality_risk_zone();"], "external_knowledge": [25, 27], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT EXISTS (SELECT 1 FROM pg_proc WHERE proname = 'assess_quality_risk_zone')\")\n    assert cursor.fetchone()[0], \"assess_quality_risk_zone function not created\"\n    cursor.execute(\"SELECT EXISTS (SELECT 1 FROM pg_trigger WHERE tgname = 'tr_quality_risk_assessment')\")\n    assert cursor.fetchone()[0], \"tr_quality_risk_assessment trigger not created\"\n    cursor.execute(\"SELECT reckeylink FROM environmentalmonitoring LIMIT 1\")\n    existing_record = cursor.fetchone()\n    if existing_record:\n        cursor.execute(\"\"\"\n            UPDATE environmentalmonitoring \n            SET env_metrics = env_metrics || '{\"temperature\": {\"excursion_duration_min\": 45, \"excursion_count\": 2}}'\n            WHERE reckeylink = %s\n        \"\"\", (existing_record[0],))\n        cursor.execute(\"SELECT env_metrics->'quality_risk_zone' FROM environmentalmonitoring WHERE reckeylink = %s\", (existing_record[0],))\n        result = cursor.fetchone()\n        if result and result[0]:\n            risk_zone = result[0].strip('\"')\n            assert risk_zone in ['GREEN', 'YELLOW', 'RED', 'UNKNOWN'], \"Invalid risk zone calculation\"\n    return 1"], "category": "Management", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_M_6", "selected_database": "cold_chain_pharma_compliance", "query": "I need to find all the product batches that count as top-level cold chain products. For each one, list its batch tag, product code, name, and value, and make sure it’s clearly marked as tier 1 in the records. Also, let me know how many you found, and if anything goes wrong, just keep going and let me know about the problem.", "normal_query": "I want to batch identify all Tier 1 Cold Chain Products in our database. For each product batch, check if it meets the Tier 1 Cold Chain Products criteria. For every qualifying batch, record its batch tag, product code, product label, and value in USD, and flag it as Tier 1. Also, update the product batch records to append a '[TIER1]' flag to the value field for all identified Tier 1 Cold Chain Products. Please ensure the process logs the number of Tier 1 products found and handles any errors gracefully.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["DO $$\nDECLARE\n    rec RECORD;\n    tier1_count INTEGER := 0;\n    batch_cursor CURSOR FOR \n        SELECT pb.batchtag, p.prodcode, p.prodlabel, pb.valusd, pb.tempsense\n        FROM productbatches pb\n        INNER JOIN products p ON pb.prodlink = p.prodcode\n        WHERE LOWER(TRIM(pb.tempsense)) = 'high' \n        AND REPLACE(REPLACE(pb.valusd, '$', ''), ',', '')::NUMERIC > 100000;\nBEGIN\n    CREATE TEMP TABLE IF NOT EXISTS tier1_products (\n        batchtag TEXT,\n        prodcode TEXT,\n        prodlabel TEXT,\n        value_usd NUMERIC,\n        flagged_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    );\n\n    FOR rec IN batch_cursor LOOP\n        BEGIN\n            INSERT INTO tier1_products (batchtag, prodcode, prodlabel, value_usd)\n            VALUES (\n                rec.batchtag, \n                rec.prodcode, \n                rec.prodlabel,\n                REPLACE(REPLACE(rec.valusd, '$', ''), ',', '')::NUMERIC\n            );\n\n            tier1_count := tier1_count + 1;\n\n            RAISE NOTICE 'Flagged Tier 1 product: % (Batch: %, Value: %)', \n                rec.prodlabel, rec.batchtag, rec.valusd;\n\n        EXCEPTION\n            WHEN OTHERS THEN\n                RAISE NOTICE 'Error processing batch %: %', rec.batchtag, SQLERRM;\n        END;\n    END LOOP;\n\n    RAISE NOTICE 'Batch processing complete. Total Tier 1 products identified: %', tier1_count;\n\n    IF tier1_count > 0 THEN\n        UPDATE productbatches \n        SET valusd = valusd || ' [TIER1]'\n        WHERE batchtag IN (SELECT batchtag FROM tier1_products)\n        AND valusd NOT LIKE '%[TIER1]%';\n    END IF;\nEND $$;\n"], "external_knowledge": [7], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    import psycopg2\n    cursor = conn.cursor()\n    \n    # Check if any products were flagged as Tier 1\n    cursor.execute(\"\"\"\n        SELECT COUNT(*) FROM productbatches \n        WHERE LOWER(TRIM(tempsense)) = 'high' AND valusd LIKE '%[TIER1]%'\n    \"\"\")\n    tier1_count = cursor.fetchone()[0]\n    \n    # If there are high-value, high-sensitivity products, they should be flagged\n    cursor.execute(\"\"\"\n        SELECT COUNT(*) FROM productbatches pb\n        INNER JOIN products p ON pb.prodlink = p.prodcode\n        WHERE LOWER(TRIM(pb.tempsense)) = 'high' \n        AND REPLACE(REPLACE(pb.valusd, '$', ''), ',', '')::NUMERIC > 100000\n        AND pb.valusd NOT LIKE '%[TIER1]%'\n    \"\"\")\n    unflagged_tier1 = cursor.fetchone()[0]\n    \n    # Verify processing worked correctly\n    assert unflagged_tier1 == 0 or tier1_count > 0, \"Tier 1 products should be properly flagged\"\n    \n    return 1\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_M_7", "selected_database": "cold_chain_pharma_compliance", "query": "I need to refresh our monitoring device reliability records using the data logger value for reliability assessment. For every device, figure out its reliability value. Show all the details and scores in a temp table first. Then, save a backup of the current device list, wipe it clean, and fill it back up with the updated info from the results. Make sure to include all the device details from the original record, the made-up failure rates, the reliability score, and when you did the analysis.", "normal_query": "I plan to recalculate and rebuild the reliability tracking for all monitoring devices in our system using the Data Logger Reliability Score. For each device, calculate DLRS and store the results in a staging table. Then, back up the current monitoringdevices table and repopulate it with the original device columns from the staging results. Please include the device reference, calibration timestamp, device accuracy, recording interval, temperature points, all simulated failure rates, the calculated DLRS, and the analysis timestamp in the staging output.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Intent: Truncate and rebuild monitoring device reliability tracking using Data Logger Reliability Score (DLRS)\n-- Knowledge Used: KB ID 48 - Data Logger Reliability Score (DLRS)\n-- Advanced PostgreSQL: TRUNCATE CASCADE, window functions, date arithmetic, conditional aggregates, CTEs\n\n-- Step 1: Create a staging table for DLRS calculation.\n-- DLRS = 100 - (10 * F_r + 5 * F_d + 3 * F_c + 2 * F_b)\n-- F_r: reading failure rate per 100 deployments\n-- F_d: download failure rate per 100 deployments\n-- F_c: calibration drift rate per 100 deployments\n-- F_b: battery failure rate per 100 deployments\n-- For demonstration, we simulate these rates using random() and device usage as we lack real failure logs.\n\nCREATE TABLE device_dlrs_staging AS\nWITH device_usage AS (\n    -- Step 2: Simulate device usage and failure rates for each device.\n    SELECT\n        md.mondevref,\n        md.calibts,\n        md.devacc,\n        md.recintmin,\n        md.temppts,\n        -- Simulate deployments as temppts/100 (as a proxy for number of deployments)\n        GREATEST(md.temppts / 100, 1) AS deployments,\n        -- Simulate failure rates (in real DB, these would come from logs)\n        ROUND(LEAST(md.temppts / 1000.0, 1) * 100, 2) AS fr, -- reading failure rate\n        ROUND(LEAST(md.recintmin / 100.0, 1) * 100, 2) AS fd, -- download failure rate\n        ROUND(CASE WHEN md.calibts < CURRENT_DATE - INTERVAL '12 months' THEN 0.5 ELSE 0.1 END * 100, 2) AS fc, -- calibration drift rate\n        ROUND(CASE WHEN md.devacc::NUMERIC > 0.3 THEN 0.3 ELSE 0.1 END * 100, 2) AS fb -- battery failure rate\n    FROM monitoringdevices md\n),\ndlrs_calc AS (\n    -- Step 3: Calculate DLRS for each device using the KB formula.\n    SELECT\n        mondevref,\n        calibts,\n        devacc,\n        recintmin,\n        temppts,\n        fr, fd, fc, fb,\n        100 - (10 * fr + 5 * fd + 3 * fc + 2 * fb) AS dlrs,\n        CURRENT_TIMESTAMP AS analysis_timestamp\n    FROM device_usage\n)\nSELECT\n    mondevref,\n    calibts,\n    devacc,\n    recintmin,\n    temppts,\n    fr AS reading_failure_rate,\n    fd AS download_failure_rate,\n    fc AS calibration_drift_rate,\n    fb AS battery_failure_rate,\n    dlrs,\n    analysis_timestamp\nFROM dlrs_calc;\n\n-- Step 4: Backup current monitoringdevices table.\nCREATE TABLE IF NOT EXISTS monitoringdevices_backup AS \nSELECT *, CURRENT_TIMESTAMP as backup_timestamp \nFROM monitoringdevices;\n\n-- Step 5: Truncate the main table for a clean rebuild.\nTRUNCATE TABLE monitoringdevices RESTART IDENTITY CASCADE;\n\n-- Step 6: Rebuild monitoringdevices table (original columns only, as per schema).\nINSERT INTO monitoringdevices (mondevref, calibts, devacc, recintmin, temppts)\nSELECT \n    mondevref,\n    calibts,\n    devacc,\n    recintmin,\n    temppts\nFROM device_dlrs_staging;\n"], "external_knowledge": [48], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    # Check if staging table was created and contains DLRS\n    cursor.execute(\"\"\"\n        SELECT EXISTS (\n            SELECT 1 FROM pg_tables WHERE tablename LIKE '%device_dlrs_staging%'\n        )\n    \"\"\")\n    assert cursor.fetchone()[0], \"device_dlrs_staging table not created\"\n    cursor.execute(\"SELECT COUNT(*) FROM device_dlrs_staging\")\n    assert cursor.fetchone()[0] > 0, \"device_dlrs_staging should have rows\"\n    cursor.execute(\"SELECT dlrs FROM device_dlrs_staging LIMIT 1\")\n    dlrs = cursor.fetchone()[0]\n    assert dlrs is not None, \"DLRS should be calculated\"\n    # Check if main table was rebuilt\n    cursor.execute(\"SELECT COUNT(*) FROM monitoringdevices\")\n    assert cursor.fetchone()[0] > 0, \"monitoringdevices should have data after rebuild\"\n    return 1\n"], "category": "Management", "high_level": true, "conditions": {"decimal": 2, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_M_8", "selected_database": "cold_chain_pharma_compliance", "query": "I'm looking to see, for every shipment, what's the biggest number of shock events it went through, basically the highest shock count for each shipment, even if some shipments didn't have any shock data. Just show me the shipment code and that top shock number for each one.", "normal_query": "For each shipment in the cold chain database, I want to determine the maximum shock event count observed, using the concept of Shock Event Significance Levels. Please provide the shipment identifier and its corresponding maximum shock event count, ensuring that all shipments are included in the results even if no shock data is present, just use 0 to represent null data. The output should display the shipment code alongside the highest shock event count recorded for that shipment.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["SELECT s.reckey AS shipment_id, COALESCE(MAX((em.env_metrics->'light_and_shock'->>'shock_event_count')::int), 0) AS max_shock_event_count FROM shipments s LEFT JOIN environmentalmonitoring em ON s.reckey = em.reckeylink GROUP BY s.reckey ORDER BY s.reckey;"], "external_knowledge": [23], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT COUNT(*) FROM shipments\")\n    total_shipments = cursor.fetchone()[0]\n    cursor.execute(\"SELECT COUNT(*) FROM (SELECT s.reckey, COALESCE(MAX((em.env_metrics->'light_and_shock'->>'shock_event_count')::int), 0) AS max_cnt FROM shipments s LEFT JOIN environmentalmonitoring em ON s.reckey = em.reckeylink GROUP BY s.reckey) AS subquery\")\n    result_rows = cursor.fetchone()[0]\n    assert result_rows == total_shipments, \"Not all shipments included\"\n    return 1"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": true}}
{"instance_id": "cold_chain_pharma_compliance_M_9", "selected_database": "cold_chain_pharma_compliance", "query": "If I give you a shipment, would you tell me what its regulatory compliance status is, and also just let me know with a true or false if it’s considered compliant?", "normal_query": "For a specified shipment, I require a summary of its regulatory compliance status according to the concept of Regulatory Compliance Status Definitions. The output should include both the compliance status and an indicator of whether the shipment is compliant, with the indicator expressed as a boolean value.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["\n-- Purpose: Create a function to return the regulatory compliance summary for a given shipment.\n-- Knowledge used: Regulatory Compliance Status Definitions (KB ID: 21)\n-- Advanced PostgreSQL: RETURNS TABLE, JSONB extraction, CASE\n\nCREATE OR REPLACE FUNCTION get_regulatory_compliance_summary(shipment_id TEXT)\nRETURNS TABLE (\n    compliance_status TEXT,\n    is_compliant BOOLEAN\n) AS $$\nBEGIN\n    RETURN QUERY\n    SELECT\n        qc.qc_checklist->'customs_and_regulatory'->>'regulatory_compliance_status',\n        CASE\n            WHEN qc.qc_checklist->'customs_and_regulatory'->>'regulatory_compliance_status' = 'Compliant' THEN TRUE\n            ELSE FALSE\n        END\n    FROM qualitycompliance qc\n    WHERE qc.reckeyqc = shipment_id;\nEND;\n$$ LANGUAGE plpgsql;\n"], "external_knowledge": [21], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n    # Test function returns correct status for a known shipment\n    cursor.execute(\"\"\"\n        SELECT compliance_status, is_compliant FROM get_regulatory_compliance_summary('CC880430')\n    \"\"\")\n    row = cursor.fetchone()\n    assert row is not None, \"Function did not return a result\"\n    assert row[0] in ['Compliant', 'Non-compliant', 'Under Review'], \"Invalid compliance status\"\n    assert isinstance(row[1], bool), \"is_compliant should be boolean\"\n    return 1\n"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
{"instance_id": "cold_chain_pharma_compliance_M_10", "selected_database": "cold_chain_pharma_compliance", "query": "Whenever a shipment’s product release status is set to rejected, go ahead and mark its insurance claim as rejected too, and don’t bother updating claims that are already marked as rejected.", "normal_query": "For all shipments, please update the insurance claim status to 'Rejected' in the insuranceclaims table for every case where the product release status is 'Rejected', strictly following the Product Release Decision Framework. Ensure that only claims not already marked as 'Rejected' are updated.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["UPDATE insuranceclaims ic\nSET claimstat = 'Rejected'\nFROM qualitycompliance qc\nWHERE ic.reckeyclaim = qc.reckeyqc\n  AND LOWER(TRIM(qc.qc_checklist->'release'->>'product_release_status')) = 'rejected'\n  AND ic.claimstat IS DISTINCT FROM 'Rejected';"], "external_knowledge": [34], "test_cases": ["def test_case(pred_sqls, sol_sqls, db_name, conn):\n    cursor = conn.cursor()\n\n    # Count how many rows are eligible to be updated before the update\n    cursor.execute(\"\"\"\n        SELECT COUNT(*) FROM insuranceclaims ic\n        JOIN qualitycompliance qc ON ic.reckeyclaim = qc.reckeyqc\n        WHERE LOWER(TRIM(qc.qc_checklist->'release'->>'product_release_status')) = 'rejected'\n          AND ic.claimstat IS DISTINCT FROM 'Rejected'\n    \"\"\")\n    expected_updates = cursor.fetchone()[0]\n\n    for sql in sol_sqls:\n        cursor.execute(sql)\n    conn.commit()\n\n    # Count how many now match 'Rejected' + release status\n    cursor.execute(\"\"\"\n        SELECT COUNT(*) FROM insuranceclaims ic\n        JOIN qualitycompliance qc ON ic.reckeyclaim = qc.reckeyqc\n        WHERE LOWER(TRIM(qc.qc_checklist->'release'->>'product_release_status')) = 'rejected'\n          AND ic.claimstat = 'Rejected'\n    \"\"\")\n    post_update_count = cursor.fetchone()[0]\n\n    assert post_update_count >= expected_updates, \"Not all valid claims were updated to 'Rejected'\"\n    return 1"], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false, "order": false}}
